[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": ""
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": ""
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": ""
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": ""
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": ""
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": ""
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": ""
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": ""
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": ""
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": ""
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": ""
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": ""
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": ""
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": ""
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": ""
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": ""
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": ""
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": ""
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": ""
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Breaking the Curse of Quality Saturation with User-Centric Ranking",
        "url": "http://arxiv.org/abs/2305.15333v1",
        "pub_date": "2023-05-24",
        "summary": "A key puzzle in search, ads, and recommendation is that the ranking model can\nonly utilize a small portion of the vastly available user interaction data. As\na result, increasing data volume, model size, or computation FLOPs will quickly\nsuffer from diminishing returns. We examined this problem and found that one of\nthe root causes may lie in the so-called ``item-centric'' formulation, which\nhas an unbounded vocabulary and thus uncontrolled model complexity. To mitigate\nquality saturation, we introduce an alternative formulation named\n``user-centric ranking'', which is based on a transposed view of the dyadic\nuser-item interaction data. We show that this formulation has a promising\nscaling property, enabling us to train better-converged models on substantially\nlarger data sets.",
        "translated": ""
    },
    {
        "title": "Neural Summarization of Electronic Health Records",
        "url": "http://arxiv.org/abs/2305.15222v1",
        "pub_date": "2023-05-24",
        "summary": "Hospital discharge documentation is among the most essential, yet\ntime-consuming documents written by medical practitioners. The objective of\nthis study was to automatically generate hospital discharge summaries using\nneural network summarization models. We studied various data preparation and\nneural network training techniques that generate discharge summaries. Using\nnursing notes and discharge summaries from the MIMIC-III dataset, we studied\nthe viability of the automatic generation of various sections of a discharge\nsummary using four state-of-the-art neural network summarization models (BART,\nT5, Longformer and FLAN-T5). Our experiments indicated that training\nenvironments including nursing notes as the source, and discrete sections of\nthe discharge summary as the target output (e.g. \"History of Present Illness\")\nimprove language model efficiency and text quality. According to our findings,\nthe fine-tuned BART model improved its ROUGE F1 score by 43.6% against its\nstandard off-the-shelf version. We also found that fine-tuning the baseline\nBART model with other setups caused different degrees of improvement (up to 80%\nrelative improvement). We also observed that a fine-tuned T5 generally achieves\nhigher ROUGE F1 scores than other fine-tuned models and a fine-tuned FLAN-T5\nachieves the highest ROUGE score overall, i.e., 45.6. For majority of the\nfine-tuned language models, summarizing discharge summary report sections\nseparately outperformed the summarization the entire report quantitatively. On\nthe other hand, fine-tuning language models that were previously instruction\nfine-tuned showed better performance in summarizing entire reports. This study\nconcludes that a focused dataset designed for the automatic generation of\ndischarge summaries by a language model can produce coherent Discharge Summary\nsections.",
        "translated": ""
    },
    {
        "title": "Collaborative Recommendation Model Based on Multi-modal Multi-view\n  Attention Network: Movie and literature cases",
        "url": "http://arxiv.org/abs/2305.15159v1",
        "pub_date": "2023-05-24",
        "summary": "The existing collaborative recommendation models that use multi-modal\ninformation emphasize the representation of users' preferences but easily\nignore the representation of users' dislikes. Nevertheless, modelling users'\ndislikes facilitates comprehensively characterizing user profiles. Thus, the\nrepresentation of users' dislikes should be integrated into the user modelling\nwhen we construct a collaborative recommendation model. In this paper, we\npropose a novel Collaborative Recommendation Model based on Multi-modal\nmulti-view Attention Network (CRMMAN), in which the users are represented from\nboth preference and dislike views. Specifically, the users' historical\ninteractions are divided into positive and negative interactions, used to model\nthe user's preference and dislike views, respectively. Furthermore, the\nsemantic and structural information extracted from the scene is employed to\nenrich the item representation. We validate CRMMAN by designing contrast\nexperiments based on two benchmark MovieLens-1M and Book-Crossing datasets.\nMovielens-1m has about a million ratings, and Book-Crossing has about 300,000\nratings. Compared with the state-of-the-art knowledge-graph-based and\nmulti-modal recommendation methods, the AUC, NDCG@5 and NDCG@10 are improved by\n2.08%, 2.20% and 2.26% on average of two datasets. We also conduct controlled\nexperiments to explore the effects of multi-modal information and multi-view\nmechanism. The experimental results show that both of them enhance the model's\nperformance.",
        "translated": ""
    },
    {
        "title": "Bert4CMR: Cross-Market Recommendation with Bidirectional Encoder\n  Representations from Transformer",
        "url": "http://arxiv.org/abs/2305.15145v1",
        "pub_date": "2023-05-24",
        "summary": "Real-world multinational e-commerce companies, such as Amazon and eBay, serve\nin multiple countries and regions. Obviously, these markets have similar goods\nbut different users. Some markets are data-scarce, while others are data-rich.\nIn recent years, cross-market recommendation (CMR) has been proposed to enhance\ndata-scarce markets by leveraging auxiliary information from data-rich markets.\nPrevious works fine-tune the pre-trained model on the local market after\nfreezing part of the parameters or introducing inter-market similarity into the\nlocal market to improve the performance of CMR. However, they generally do not\nconsider eliminating the mutual interference between markets. Therefore, the\nexisting methods are neither unable to learn unbiased general knowledge nor\nefficient transfer reusable information across markets. In this paper, we\npropose a novel attention-based model called Bert4CMR to simultaneously improve\nall markets' recommendation performance. Specifically, we employ the attention\nmechanism to capture user interests by modelling user behavioural sequences. We\npre-train the proposed model on global data to learn the general knowledge of\nitems. Then we fine-tune specific target markets to perform local\nrecommendations. We propose market embedding to model the bias of each market\nand reduce the mutual inference between the parallel markets. Extensive\nexperiments conducted on seven markets show that our model is state-of-the-art.\nOur model outperforms the suboptimal model by 4.82%, 4.73%, 7.66% and 6.49% on\naverage of seven datasets in terms of four metrics, respectively. We conduct\nablation experiments to analyse the effectiveness of the proposed components.\nExperimental results indicate that our model is able to learn general knowledge\nthrough global data and shield the mutual interference between markets.",
        "translated": ""
    },
    {
        "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning\n  Strategies",
        "url": "http://arxiv.org/abs/2305.15115v1",
        "pub_date": "2023-05-24",
        "summary": "Recently, a new paradigm called Differentiable Search Index (DSI) has been\nproposed for document retrieval, wherein a sequence-to-sequence model is\nlearned to directly map queries to relevant document identifiers. The key idea\nbehind DSI is to fully parameterize traditional ``index-retrieve'' pipelines\nwithin a single neural model, by encoding all documents in the corpus into the\nmodel parameters. In essence, DSI needs to resolve two major questions: (1) how\nto assign an identifier to each document, and (2) how to learn the associations\nbetween a document and its identifier. In this work, we propose a\nSemantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the\narea of Cognitive Psychology. Our approach advances original DSI in two ways:\n(1) For the document identifier, we take inspiration from Elaboration\nStrategies in human learning. Specifically, we assign each document an\nElaborative Description based on the query generation technique, which is more\nmeaningful than a string of integers in the original DSI; and (2) For the\nassociations between a document and its identifier, we take inspiration from\nRehearsal Strategies in human learning. Specifically, we select fine-grained\nsemantic features from a document as Rehearsal Contents to improve document\nmemorization. Both the offline and online experiments show improved retrieval\nperformance over prevailing baselines.",
        "translated": ""
    },
    {
        "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
        "url": "http://arxiv.org/abs/2305.15053v1",
        "pub_date": "2023-05-24",
        "summary": "When re-finding items, users who forget or are uncertain about identifying\ndetails often rely on creative strategies for expressing their information\nneeds -- complex queries that describe content elements (e.g., book characters\nor events), information beyond the document text (e.g., descriptions of book\ncovers), or personal context (e.g., when they read a book). This retrieval\nsetting, called tip of the tongue (TOT), is especially challenging for models\nheavily reliant on lexical and semantic overlap between query and document\ntext. In this work, we introduce a simple yet effective framework for handling\nsuch complex queries by decomposing the query into individual clues, routing\nthose as sub-queries to specialized retrievers, and ensembling the results.\nThis approach allows us to take advantage of off-the-shelf retrievers (e.g.,\nCLIP for retrieving images of book covers) or incorporate retriever-specific\nlogic (e.g., date constraints). We show that our framework incorportating query\ndecompositions into retrievers can improve gold book recall up to 7% relative\nagain for Recall@5 on a new collection of 14,441 real-world query-book pairs\nfrom an online community for resolving TOT inquiries.",
        "translated": ""
    },
    {
        "title": "Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation",
        "url": "http://arxiv.org/abs/2305.15048v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of\neffect-size-based meta-analysis for multi-task evaluation in NLP and IR. We\nobserved that our communities often face the challenge of aggregating results\nover incomparable metrics and scenarios, which makes conclusions and take-away\nmessages less reliable. With Ranger, we aim to address this issue by providing\na task-agnostic toolkit that combines the effect of a treatment on multiple\ntasks into one statistical evaluation, allowing for comparison of metrics and\ncomputation of an overall summary effect. Our toolkit produces\npublication-ready forest plots that enable clear communication of evaluation\nresults over multiple tasks. Our goal with the ready-to-use Ranger toolkit is\nto promote robust, effect-size-based evaluation and improve evaluation\nstandards in the community. We provide two case studies for common IR and NLP\nsettings to highlight Ranger's benefits.",
        "translated": ""
    },
    {
        "title": "Exploring Adapter-based Transfer Learning for Recommender Systems:\n  Empirical Studies and Practical Insights",
        "url": "http://arxiv.org/abs/2305.15036v1",
        "pub_date": "2023-05-24",
        "summary": "Adapters, a plug-in neural network module with some tunable parameters, have\nemerged as a parameter-efficient transfer learning technique for adapting\npre-trained models to downstream tasks, especially for natural language\nprocessing (NLP) and computer vision (CV) fields. Meanwhile, learning\nrecommendation models directly from raw item modality features -- e.g., texts\nof NLP and images of CV -- can enable effective and transferable recommender\nsystems (called TransRec). In view of this, a natural question arises: can\nadapter-based learning techniques achieve parameter-efficient TransRec with\ngood performance?\n  To this end, we perform empirical studies to address several key\nsub-questions. First, we ask whether the adapter-based TransRec performs\ncomparably to TransRec based on standard full-parameter fine-tuning? does it\nhold for recommendation with different item modalities, e.g., textual RS and\nvisual RS. If yes, we benchmark these existing adapters, which have been shown\nto be effective in NLP and CV tasks, in the item recommendation settings.\nThird, we carefully study several key factors for the adapter-based TransRec in\nterms of where and how to insert these adapters? Finally, we look at the\neffects of adapter-based TransRec by either scaling up its source training data\nor scaling down its target training data. Our paper provides key insights and\npractical guidance on unified &amp; transferable recommendation -- a less studied\nrecommendation scenario. We promise to release all code &amp; datasets for future\nresearch.",
        "translated": ""
    },
    {
        "title": "How Graph Convolutions Amplify Popularity Bias for Recommendation?",
        "url": "http://arxiv.org/abs/2305.14886v1",
        "pub_date": "2023-05-24",
        "summary": "Graph convolutional networks (GCNs) have become prevalent in recommender\nsystem (RS) due to their superiority in modeling collaborative patterns.\nAlthough improving the overall accuracy, GCNs unfortunately amplify popularity\nbias -- tail items are less likely to be recommended. This effect prevents the\nGCN-based RS from making precise and fair recommendations, decreasing the\neffectiveness of recommender systems in the long run.\n  In this paper, we investigate how graph convolutions amplify the popularity\nbias in RS. Through theoretical analyses, we identify two fundamental factors:\n(1) with graph convolution (\\textit{i.e.,} neighborhood aggregation), popular\nitems exert larger influence than tail items on neighbor users, making the\nusers move towards popular items in the representation space; (2) after\nmultiple times of graph convolution, popular items would affect more high-order\nneighbors and become more influential. The two points make popular items get\ncloser to almost users and thus being recommended more frequently. To rectify\nthis, we propose to estimate the amplified effect of popular nodes on each\nnode's representation, and intervene the effect after each graph convolution.\nSpecifically, we adopt clustering to discover highly-influential nodes and\nestimate the amplification effect of each node, then remove the effect from the\nnode embeddings at each graph convolution layer. Our method is simple and\ngeneric -- it can be used in the inference stage to correct existing models\nrather than training a new model from scratch, and can be applied to various\nGCN models. We demonstrate our method on two representative GCN backbones\nLightGCN and UltraGCN, verifying its ability in improving the recommendations\nof tail items without sacrificing the performance of popular items. Codes are\nopen-sourced \\footnote{https://github.com/MEICRS/DAP}.",
        "translated": ""
    },
    {
        "title": "Machine Reading Comprehension using Case-based Reasoning",
        "url": "http://arxiv.org/abs/2305.14815v1",
        "pub_date": "2023-05-24",
        "summary": "We present an accurate and interpretable method for answer extraction in\nmachine reading comprehension that is reminiscent of case-based reasoning (CBR)\nfrom classical AI. Our method (CBR-MRC) builds on the hypothesis that\ncontextualized answers to similar questions share semantic similarities with\neach other. Given a target question, CBR-MRC retrieves a set of similar\nquestions from a memory of observed cases and predicts an answer by selecting\nthe span in the target context that is most similar to the contextualized\nrepresentations of answers in the retrieved cases. The semi-parametric nature\nof our approach allows CBR-MRC to attribute a prediction to the specific set of\ncases used during inference, making it a desirable choice for building reliable\nand debuggable QA systems. We show that CBR-MRC achieves high test accuracy\ncomparable with large reader models, outperforming baselines by 11.5 and 8.4 EM\non NaturalQuestions and NewsQA, respectively. Further, we also demonstrate the\nability of CBR-MRC in identifying not just the correct answer tokens but also\nthe span with the most relevant supporting evidence. Lastly, we observe that\ncontexts for certain question types show higher lexical diversity than others\nand find CBR-MRC to be robust to these variations while performance using\nfully-parametric methods drops.",
        "translated": ""
    },
    {
        "title": "Candidate Set Re-ranking for Composed Image Retrieval with Dual\n  Multi-modal Encoder",
        "url": "http://arxiv.org/abs/2305.16304v1",
        "pub_date": "2023-05-25",
        "summary": "Composed image retrieval aims to find an image that best matches a given\nmulti-modal user query consisting of a reference image and text pair. Existing\nmethods commonly pre-compute image embeddings over the entire corpus and\ncompare these to a reference image embedding modified by the query text at test\ntime. Such a pipeline is very efficient at test time since fast vector\ndistances can be used to evaluate candidates, but modifying the reference image\nembedding guided only by a short textual description can be difficult,\nespecially independent of potential candidates. An alternative approach is to\nallow interactions between the query and every possible candidate, i.e.,\nreference-text-candidate triplets, and pick the best from the entire set.\nThough this approach is more discriminative, for large-scale datasets the\ncomputational cost is prohibitive since pre-computation of candidate embeddings\nis no longer possible. We propose to combine the merits of both schemes using a\ntwo-stage model. Our first stage adopts the conventional vector distancing\nmetric and performs a fast pruning among candidates. Meanwhile, our second\nstage employs a dual-encoder architecture, which effectively attends to the\ninput triplet of reference-text-candidate and re-ranks the candidates. Both\nstages utilize a vision-and-language pre-trained network, which has proven\nbeneficial for various downstream tasks. Our method consistently outperforms\nstate-of-the-art approaches on standard benchmarks for the task.",
        "translated": ""
    },
    {
        "title": "A Survey on Asking Clarification Questions Datasets in Conversational\n  Systems",
        "url": "http://arxiv.org/abs/2305.15933v1",
        "pub_date": "2023-05-25",
        "summary": "The ability to understand a user's underlying needs is critical for\nconversational systems, especially with limited input from users in a\nconversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to\nreveal users' true intent from their queries or utterances arise as an\nessential task. However, it is noticeable that a key limitation of the existing\nACQs studies is their incomparability, from inconsistent use of data, distinct\nexperimental setups and evaluation strategies. Therefore, in this paper, to\nassist the development of ACQs techniques, we comprehensively analyse the\ncurrent ACQs research status, which offers a detailed comparison of publicly\navailable datasets, and discusses the applied evaluation metrics, joined with\nbenchmarks for multiple ACQs-related tasks. In particular, given a thorough\nanalysis of the ACQs task, we discuss a number of corresponding research\ndirections for the investigation of ACQs as well as the development of\nconversational systems.",
        "translated": ""
    },
    {
        "title": "Enhancing the Ranking Context of Dense Retrieval Methods through\n  Reciprocal Nearest Neighbors",
        "url": "http://arxiv.org/abs/2305.15720v1",
        "pub_date": "2023-05-25",
        "summary": "Sparse annotation poses persistent challenges to training dense retrieval\nmodels, such as the problem of false negatives, i.e. unlabeled relevant\ndocuments that are spuriously used as negatives in contrastive learning,\ndistorting the training signal. To alleviate this problem, we introduce\nevidence-based label smoothing, a computationally efficient method that\nprevents penalizing the model for assigning high relevance to false negatives.\nTo compute the target relevance distribution over candidate documents within\nthe ranking context of a given query, candidates most similar to the ground\ntruth are assigned a non-zero relevance probability based on the degree of\ntheir similarity to the ground-truth document(s). As a relevance estimate we\nleverage an improved similarity metric based on reciprocal nearest neighbors,\nwhich can also be used independently to rerank candidates in post-processing.\nThrough extensive experiments on two large-scale ad hoc text retrieval datasets\nwe demonstrate that both methods can improve the ranking effectiveness of dense\nretrieval models.",
        "translated": ""
    },
    {
        "title": "BookGPT: A General Framework for Book Recommendation Empowered by Large\n  Language Model",
        "url": "http://arxiv.org/abs/2305.15673v1",
        "pub_date": "2023-05-25",
        "summary": "With the continuous development and change exhibited by large language model\n(LLM) technology, represented by generative pretrained transformers (GPTs),\nmany classic scenarios in various fields have re-emerged with new\nopportunities. This paper takes ChatGPT as the modeling object, incorporates\nLLM technology into the typical book resource understanding and recommendation\nscenario for the first time, and puts it into practice. By building a\nChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT,\nthis paper attempts to apply ChatGPT to recommendation modeling for three\ntypical tasks, book rating recommendation, user rating recommendation, and book\nsummary recommendation, and explores the feasibility of LLM technology in book\nrecommendation scenarios. At the same time, based on different evaluation\nschemes for book recommendation tasks and the existing classic recommendation\nmodels, this paper discusses the advantages and disadvantages of the BookGPT in\nbook recommendation scenarios and analyzes the opportunities and improvement\ndirections for subsequent LLMs in these scenarios.",
        "translated": ""
    },
    {
        "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
        "url": "http://arxiv.org/abs/2305.15645v1",
        "pub_date": "2023-05-25",
        "summary": "In conversational search, the user's real search intent for the current turn\nis dependent on the previous conversation history. It is challenging to\ndetermine a good search query from the whole conversation context. To avoid the\nexpensive re-training of the query encoder, most existing methods try to learn\na rewriting model to de-contextualize the current query by mimicking the manual\nquery rewriting. However, manually rewritten queries are not always the best\nsearch queries. Training a rewriting model on them would limit the model's\nability to produce good search queries. Another useful hint is the potential\nanswer to the question. In this paper, we propose ConvGQR, a new framework to\nreformulate conversational queries based on generative pre-trained language\nmodels (PLMs), one for query rewriting and another for generating potential\nanswers. By combining both, ConvGQR can produce better search queries. In\naddition, to relate query reformulation to retrieval performance, we propose a\nknowledge infusion mechanism to optimize both query reformulation and\nretrieval. Extensive experiments on four conversational search datasets\ndemonstrate the effectiveness of ConvGQR.",
        "translated": ""
    },
    {
        "title": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language\n  Models",
        "url": "http://arxiv.org/abs/2305.15597v1",
        "pub_date": "2023-05-24",
        "summary": "The mission of open knowledge graph (KG) completion is to draw new findings\nfrom known facts. Existing works that augment KG completion require either (1)\nfactual triples to enlarge the graph reasoning space or (2) manually designed\nprompts to extract knowledge from a pre-trained language model (PLM),\nexhibiting limited performance and requiring expensive efforts from experts. To\nthis end, we propose TAGREAL that automatically generates quality query prompts\nand retrieves support information from large text corpora to probe knowledge\nfrom PLM for KG completion. The results show that TAGREAL achieves\nstate-of-the-art performance on two benchmark datasets. We find that TAGREAL\nhas superb performance even with limited training data, outperforming existing\nembedding-based, graph-based, and PLM-based methods.",
        "translated": ""
    },
    {
        "title": "Representation Online Matters: Practical End-to-End Diversification in\n  Search and Recommender Systems",
        "url": "http://arxiv.org/abs/2305.15534v1",
        "pub_date": "2023-05-24",
        "summary": "As the use of online platforms continues to grow across all demographics,\nusers often express a desire to feel represented in the content. To improve\nrepresentation in search results and recommendations, we introduce end-to-end\ndiversification, ensuring that diverse content flows throughout the various\nstages of these systems, from retrieval to ranking. We develop, experiment, and\ndeploy scalable diversification mechanisms in multiple production surfaces on\nthe Pinterest platform, including Search, Related Products, and New User\nHomefeed, to improve the representation of different skin tones in beauty and\nfashion content. Diversification in production systems includes three\ncomponents: identifying requests that will trigger diversification, ensuring\ndiverse content is retrieved from the large content corpus during the retrieval\nstage, and finally, balancing the diversity-utility trade-off in a\nself-adjusting manner in the ranking stage. Our approaches, which evolved from\nusing Strong-OR logical operator to bucketized retrieval at the retrieval stage\nand from greedy re-rankers to multi-objective optimization using determinantal\npoint processes for the ranking stage, balances diversity and utility while\nenabling fast iterations and scalable expansion to diversification over\nmultiple dimensions. Our experiments indicate that these approaches\nsignificantly improve diversity metrics, with a neutral to a positive impact on\nutility metrics and improved user satisfaction, both qualitatively and\nquantitatively, in production.",
        "translated": ""
    },
    {
        "title": "Large Language Models for User Interest Journeys",
        "url": "http://arxiv.org/abs/2305.15498v1",
        "pub_date": "2023-05-24",
        "summary": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
        "translated": ""
    },
    {
        "title": "Adversarial Attacks on Online Learning to Rank with Click Feedback",
        "url": "http://arxiv.org/abs/2305.17071v1",
        "pub_date": "2023-05-26",
        "summary": "Online learning to rank (OLTR) is a sequential decision-making problem where\na learning agent selects an ordered list of items and receives feedback through\nuser clicks. Although potential attacks against OLTR algorithms may cause\nserious losses in real-world applications, little is known about adversarial\nattacks on OLTR. This paper studies attack strategies against multiple variants\nof OLTR. Our first result provides an attack strategy against the UCB algorithm\non classical stochastic bandits with binary feedback, which solves the key\nissues caused by bounded and discrete feedback that previous works can not\nhandle. Building on this result, we design attack algorithms against UCB-based\nOLTR algorithms in position-based and cascade models. Finally, we propose a\ngeneral attack strategy against any algorithm under the general click model.\nEach attack algorithm manipulates the learning agent into choosing the target\nattack item $T-o(T)$ times, incurring a cumulative cost of $o(T)$. Experiments\non synthetic and real data further validate the effectiveness of our proposed\nattack algorithms.",
        "translated": ""
    },
    {
        "title": "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System",
        "url": "http://arxiv.org/abs/2305.17034v1",
        "pub_date": "2023-05-26",
        "summary": "Significant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type.",
        "translated": ""
    },
    {
        "title": "Is googling risky? A study on risk perception and experiences of adverse\n  consequences in web search",
        "url": "http://arxiv.org/abs/2305.16990v1",
        "pub_date": "2023-05-26",
        "summary": "Search engines, such as Google, have a considerable impact on society.\nTherefore, undesirable consequences, such as retrieving incorrect search\nresults, pose a risk to users. Although previous research has reported the\nadverse outcomes of web search, little is known about how search engine users\nevaluate those outcomes. In this study, we show which aspects of web search are\nperceived as risky using a sample (N = 3,884) representative of the German\nInternet population. We found that many participants are often concerned with\nadverse consequences immediately appearing on the search engine result page.\nMoreover, participants' experiences with adverse consequences are directly\nrelated to their risk perception. Our results demonstrate that people perceive\nrisks related to web search. In addition to our study, there is a need for more\nindependent research on the possible detrimental outcomes of web search to\nmonitor and mitigate risks. Apart from risks for individuals, search engines\nwith a massive number of users have an extraordinary impact on society;\ntherefore, the acceptable risks of web search should be discussed.",
        "translated": ""
    },
    {
        "title": "Efficient Decoding of Compositional Structure in Holistic\n  Representations",
        "url": "http://arxiv.org/abs/2305.16873v1",
        "pub_date": "2023-05-26",
        "summary": "We investigate the task of retrieving information from compositional\ndistributed representations formed by Hyperdimensional Computing/Vector\nSymbolic Architectures and present novel techniques which achieve new\ninformation rate bounds. First, we provide an overview of the decoding\ntechniques that can be used to approach the retrieval task. The techniques are\ncategorized into four groups. We then evaluate the considered techniques in\nseveral settings that involve, e.g., inclusion of external noise and storage\nelements with reduced precision. In particular, we find that the decoding\ntechniques from the sparse coding and compressed sensing literature (rarely\nused for Hyperdimensional Computing/Vector Symbolic Architectures) are also\nwell-suited for decoding information from the compositional distributed\nrepresentations. Combining these decoding techniques with interference\ncancellation ideas from communications improves previously reported bounds\n(Hersche et al., 2021) of the information rate of the distributed\nrepresentations from 1.20 to 1.40 bits per dimension for smaller codebooks and\nfrom 0.60 to 1.26 bits per dimension for larger codebooks.",
        "translated": ""
    },
    {
        "title": "Automating the Analysis of Institutional Design in International\n  Agreements",
        "url": "http://arxiv.org/abs/2305.16750v1",
        "pub_date": "2023-05-26",
        "summary": "This paper explores the automatic knowledge extraction of formal\ninstitutional design - norms, rules, and actors - from international\nagreements. The focus was to analyze the relationship between the visibility\nand centrality of actors in the formal institutional design in regulating\ncritical aspects of cultural heritage relations. The developed tool utilizes\ntechniques such as collecting legal documents, annotating them with\nInstitutional Grammar, and using graph analysis to explore the formal\ninstitutional design. The system was tested against the 2003 UNESCO Convention\nfor the Safeguarding of the Intangible Cultural Heritage.",
        "translated": ""
    },
    {
        "title": "The Search for Stability: Learning Dynamics of Strategic Publishers with\n  Initial Documents",
        "url": "http://arxiv.org/abs/2305.16695v1",
        "pub_date": "2023-05-26",
        "summary": "We study a game-theoretic model of information retrieval, in which strategic\npublishers aim to maximize their chances of being ranked first by the search\nengine, while maintaining the integrity of their original documents. We show\nthat the commonly used PRP ranking scheme results in an unstable environment\nwhere games often fail to reach pure Nash equilibrium. We propose the Relative\nRanking Principle (RRP) as an alternative ranking principle, and introduce two\nranking functions that are instances of the RRP. We provide both theoretical\nand empirical evidence that these methods lead to a stable search ecosystem, by\nproviding positive results on the learning dynamics convergence. We also define\nthe publishers' and users' welfare, and demonstrate a possible publisher-user\ntrade-off, which highlights the complexity of determining which ranking\nfunction should be selected by the search engine designer.",
        "translated": ""
    },
    {
        "title": "Multiview Identifiers Enhanced Generative Retrieval",
        "url": "http://arxiv.org/abs/2305.16675v1",
        "pub_date": "2023-05-26",
        "summary": "Instead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
        "translated": ""
    },
    {
        "title": "FARA: Future-aware Ranking Algorithm for Fairness Optimization",
        "url": "http://arxiv.org/abs/2305.16637v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking systems are the key components of modern Information Retrieval (IR)\napplications, such as search engines and recommender systems. Besides the\nranking relevance to users, the exposure fairness to item providers has also\nbeen considered an important factor in ranking optimization. Many fair ranking\nalgorithms have been proposed to jointly optimize both ranking relevance and\nfairness. However, we find that most existing fair ranking methods adopt greedy\nalgorithms that only optimize rankings for the next immediate session or\nrequest. As shown in this paper, such a myopic paradigm could limit the upper\nbound of ranking optimization and lead to suboptimal performance in the long\nterm. To this end, we propose FARA, a novel Future-Aware Ranking Algorithm for\nranking relevance and fairness optimization. Instead of greedily optimizing\nrankings for the next immediate session, FARA plans ahead by jointly optimizing\nmultiple ranklists together and saving them for future sessions. Particularly,\nFARA first uses the Taylor expansion to investigate how future ranklists will\ninfluence the overall fairness of the system. Then, based on the analysis of\nthe Taylor expansion, FARA adopts a two-phase optimization algorithm where we\nfirst solve an optimal future exposure planning problem and then construct the\noptimal ranklists according to the optimal future exposure planning.\nTheoretically, we show that FARA is optimal for ranking relevance and fairness\njoint optimization. Empirically, our extensive experiments on three\nsemi-synthesized datasets show that FARA is efficient, effective, and can\ndeliver significantly better ranking performance compared to state-of-the-art\nfair ranking methods.",
        "translated": ""
    },
    {
        "title": "DataFinder: Scientific Dataset Recommendation from Natural Language\n  Descriptions",
        "url": "http://arxiv.org/abs/2305.16636v1",
        "pub_date": "2023-05-26",
        "summary": "Modern machine learning relies on datasets to develop and validate research\nideas. Given the growth of publicly available data, finding the right dataset\nto use is increasingly difficult. Any research question imposes explicit and\nimplicit constraints on how well a given dataset will enable researchers to\nanswer this question, such as dataset size, modality, and domain. We introduce\na new task of recommending relevant datasets given a short natural language\ndescription of a research idea, to help people find relevant datasets for their\nneeds. Dataset recommendation poses unique challenges as an information\nretrieval problem; datasets are hard to directly index for search and there are\nno corpora readily available for this task. To operationalize this task, we\nbuild the DataFinder Dataset which consists of a larger\nautomatically-constructed training set (17.5K queries) and a smaller\nexpert-annotated evaluation set (392 queries). Using this data, we compare\nvarious information retrieval algorithms on our test set and present the\nfirst-ever published system for text-based dataset recommendation using machine\nlearning techniques. This system, trained on the DataFinder Dataset, finds more\nrelevant search results than existing third-party dataset search engines. To\nencourage progress on dataset recommendation, we release our dataset and models\nto the public.",
        "translated": ""
    },
    {
        "title": "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach",
        "url": "http://arxiv.org/abs/2305.16606v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
        "translated": ""
    },
    {
        "title": "Large Language Models are not Fair Evaluators",
        "url": "http://arxiv.org/abs/2305.17926v1",
        "pub_date": "2023-05-29",
        "summary": "We uncover a systematic bias in the evaluation paradigm of adopting large\nlanguage models~(LLMs), e.g., GPT-4, as a referee to score the quality of\nresponses generated by candidate models. We find that the quality ranking of\ncandidate responses can be easily hacked by simply altering their order of\nappearance in the context. This manipulation allows us to skew the evaluation\nresult, making one model appear considerably superior to the other, e.g.,\nvicuna could beat ChatGPT on 66 over 80 tested queries. To address this issue,\nwe propose two simple yet effective calibration strategies: 1) Multiple\nEvidence Calibration, which requires the evaluator model to generate multiple\ndetailed pieces of evidence before assigning ratings; 2) Balanced Position\nCalibration, which aggregates results across various orders to determine the\nfinal score. Extensive experiments demonstrate that our approach successfully\nmitigates evaluation bias, resulting in closer alignment with human judgments.\nTo facilitate future research on more robust large language model comparison,\nwe integrate the techniques in the paper into an easy-to-use toolkit\n\\emph{FairEval}, along with the human\nannotations.\\footnote{\\url{https://github.com/i-Eval/FairEval}}",
        "translated": ""
    },
    {
        "title": "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation",
        "url": "http://arxiv.org/abs/2305.17866v1",
        "pub_date": "2023-05-29",
        "summary": "Traditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "HyperFormer: Learning Expressive Sparse Feature Representations via\n  Hypergraph Transformer",
        "url": "http://arxiv.org/abs/2305.17386v1",
        "pub_date": "2023-05-27",
        "summary": "Learning expressive representations for high-dimensional yet sparse features\nhas been a longstanding problem in information retrieval. Though recent deep\nlearning methods can partially solve the problem, they often fail to handle the\nnumerous sparse features, particularly those tail feature values with\ninfrequent occurrences in the training data. Worse still, existing methods\ncannot explicitly leverage the correlations among different instances to help\nfurther improve the representation learning on sparse features since such\nrelational prior knowledge is not provided. To address these challenges, in\nthis paper, we tackle the problem of representation learning on feature-sparse\ndata from a graph learning perspective. Specifically, we propose to model the\nsparse features of different instances using hypergraphs where each node\nrepresents a data instance and each hyperedge denotes a distinct feature value.\nBy passing messages on the constructed hypergraphs based on our Hypergraph\nTransformer (HyperFormer), the learned feature representations capture not only\nthe correlations among different instances but also the correlations among\nfeatures. Our experiments demonstrate that the proposed approach can\neffectively improve feature representation learning on sparse features.",
        "translated": ""
    },
    {
        "title": "Counterfactual Evaluation of Peer-Review Assignment Policies",
        "url": "http://arxiv.org/abs/2305.17339v1",
        "pub_date": "2023-05-27",
        "summary": "Peer review assignment algorithms aim to match research papers to suitable\nexpert reviewers, working to maximize the quality of the resulting reviews. A\nkey challenge in designing effective assignment policies is evaluating how\nchanges to the assignment algorithm map to changes in review quality. In this\nwork, we leverage recently proposed policies that introduce randomness in\npeer-review assignment--in order to mitigate fraud--as a valuable opportunity\nto evaluate counterfactual assignment policies. Specifically, we exploit how\nsuch randomized assignments provide a positive probability of observing the\nreviews of many assignment policies of interest. To address challenges in\napplying standard off-policy evaluation methods, such as violations of\npositivity, we introduce novel methods for partial identification based on\nmonotonicity and Lipschitz smoothness assumptions for the mapping between\nreviewer-paper covariates and outcomes. We apply our methods to peer-review\ndata from two computer science venues: the TPDP'21 workshop (95 papers and 35\nreviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We\nconsider estimates of (i) the effect on review quality when changing weights in\nthe assignment algorithm, e.g., weighting reviewers' bids vs. textual\nsimilarity (between the review's past papers and the submission), and (ii) the\n\"cost of randomization\", capturing the difference in expected quality between\nthe perturbed and unperturbed optimal match. We find that placing higher weight\non text similarity results in higher review quality and that introducing\nrandomization in the reviewer-paper assignment only marginally reduces the\nreview quality. Our methods for partial identification may be of independent\ninterest, while our off-policy approach can likely find use evaluating a broad\nclass of algorithmic matching systems.",
        "translated": ""
    },
    {
        "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and\n  Document Deduplication",
        "url": "http://arxiv.org/abs/2305.17310v1",
        "pub_date": "2023-05-27",
        "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To\nremove duplicate results in a Web search, for example, a common approach looks\nat the Jaccard index between all pairs of pages. In social network analysis, a\nmuch-celebrated metric is the Adamic-Adar index, widely used to compare node\nneighborhood sets in the important problem of predicting links. However, with\nthe increasing amount of data to be processed, calculating the exact similarity\nbetween all pairs can be intractable. The challenge of working at this scale\nhas motivated research into efficient estimators for set similarity metrics.\nThe two most popular estimators, MinHash and SimHash, are indeed used in\napplications such as document deduplication and recommender systems where large\nvolumes of data need to be processed. Given the importance of these tasks, the\ndemand for advancing estimators is evident. We propose DotHash, an unbiased\nestimator for the intersection size of two sets. DotHash can be used to\nestimate the Jaccard index and, to the best of our knowledge, is the first\nmethod that can also estimate the Adamic-Adar index and a family of related\nmetrics. We formally define this family of metrics, provide theoretical bounds\non the probability of estimate errors, and analyze its empirical performance.\nOur experimental results indicate that DotHash is more accurate than the other\nestimators in link prediction and detecting duplicate documents with the same\ncomplexity and similar comparison time.",
        "translated": ""
    },
    {
        "title": "Table Detection for Visually Rich Document Images",
        "url": "http://arxiv.org/abs/2305.19181v1",
        "pub_date": "2023-05-30",
        "summary": "Table Detection (TD) is a fundamental task towards visually rich document\nunderstanding. Current studies usually formulate the TD problem as an object\ndetection problem, then leverage Intersection over Union (IoU) based metrics to\nevaluate the model performance and IoU-based loss functions to optimize the\nmodel. TD applications usually require the prediction results to cover all the\ntable contents and avoid information loss. However, IoU and IoU-based loss\nfunctions cannot directly reflect the degree of information loss for the\nprediction results. Therefore, we propose to decouple IoU into a ground truth\ncoverage term and a prediction coverage term, in which the former can be used\nto measure the information loss of the prediction results.\n  Besides, tables in the documents are usually large, sparsely distributed, and\nhave no overlaps because they are designed to summarize essential information\nto make it easy to read and interpret for human readers. Therefore, in this\nstudy, we use SparseR-CNN as the base model, and further improve the model by\nusing Gaussian Noise Augmented Image Size region proposals and many-to-one\nlabel assignments.\n  To demonstrate the effectiveness of proposed method and compare with\nstate-of-the-art methods fairly, we conduct experiments and use IoU-based\nevaluation metrics to evaluate the model performance. The experimental results\nshow that the proposed method can consistently outperform state-of-the-art\nmethods under different IoU-based metric on a variety of datasets. We conduct\nfurther experiments to show the superiority of the proposed decoupled IoU for\nthe TD applications by replacing the IoU-based loss functions and evaluation\nmetrics with proposed decoupled IoU counterparts. The experimental results show\nthat our proposed decoupled IoU loss can encourage the model to alleviate\ninformation loss.",
        "translated": ""
    },
    {
        "title": "Event-Centric Query Expansion in Web Search",
        "url": "http://arxiv.org/abs/2305.19019v1",
        "pub_date": "2023-05-30",
        "summary": "In search engines, query expansion (QE) is a crucial technique to improve\nsearch experience. Previous studies often rely on long-term search log mining,\nwhich leads to slow updates and is sub-optimal for time-sensitive news\nsearches. In this work, we present Event-Centric Query Expansion (EQE), a novel\nQE system that addresses these issues by mining the best expansion from a\nsignificant amount of potential events rapidly and accurately. This system\nconsists of four stages, i.e., event collection, event reformulation, semantic\nretrieval and online ranking. Specifically, we first collect and filter news\nheadlines from websites. Then we propose a generation model that incorporates\ncontrastive learning and prompt-tuning techniques to reformulate these\nheadlines to concise candidates. Additionally, we fine-tune a dual-tower\nsemantic model to function as an encoder for event retrieval and explore a\ntwo-stage contrastive training approach to enhance the accuracy of event\nretrieval. Finally, we rank the retrieved events and select the optimal one as\nQE, which is then used to improve the retrieval of event-related documents.\nThrough offline analysis and online A/B testing, we observe that the EQE system\nsignificantly improves many metrics compared to the baseline. The system has\nbeen deployed in Tencent QQ Browser Search and served hundreds of millions of\nusers. The dataset and baseline codes are available at\nhttps://open-event-hub.github.io/eqe .",
        "translated": ""
    },
    {
        "title": "A Recipe for Efficient SBIR Models: Combining Relative Triplet Loss with\n  Batch Normalization and Knowledge Distillation",
        "url": "http://arxiv.org/abs/2305.18988v1",
        "pub_date": "2023-05-30",
        "summary": "Sketch-Based Image Retrieval (SBIR) is a crucial task in multimedia\nretrieval, where the goal is to retrieve a set of images that match a given\nsketch query. Researchers have already proposed several well-performing\nsolutions for this task, but most focus on enhancing embedding through\ndifferent approaches such as triplet loss, quadruplet loss, adding data\naugmentation, and using edge extraction. In this work, we tackle the problem\nfrom various angles. We start by examining the training data quality and show\nsome of its limitations. Then, we introduce a Relative Triplet Loss (RTL), an\nadapted triplet loss to overcome those limitations through loss weighting based\non anchors similarity. Through a series of experiments, we demonstrate that\nreplacing a triplet loss with RTL outperforms previous state-of-the-art without\nthe need for any data augmentation. In addition, we demonstrate why batch\nnormalization is more suited for SBIR embeddings than l2-normalization and show\nthat it improves significantly the performance of our models. We further\ninvestigate the capacity of models required for the photo and sketch domains\nand demonstrate that the photo encoder requires a higher capacity than the\nsketch encoder, which validates the hypothesis formulated in [34]. Then, we\npropose a straightforward approach to train small models, such as ShuffleNetv2\n[22] efficiently with a marginal loss of accuracy through knowledge\ndistillation. The same approach used with larger models enabled us to\noutperform previous state-of-the-art results and achieve a recall of 62.38% at\nk = 1 on The Sketchy Database [30].",
        "translated": ""
    },
    {
        "title": "The Information Retrieval Experiment Platform",
        "url": "http://arxiv.org/abs/2305.18932v1",
        "pub_date": "2023-05-30",
        "summary": "We integrate ir_datasets, ir_measures, and PyTerrier with TIRA in the\nInformation Retrieval Experiment Platform (TIREx) to promote more standardized,\nreproducible, scalable, and even blinded retrieval experiments. Standardization\nis achieved when a retrieval approach implements PyTerrier's interfaces and the\ninput and output of an experiment are compatible with ir_datasets and\nir_measures. However, none of this is a must for reproducibility and\nscalability, as TIRA can run any dockerized software locally or remotely in a\ncloud-native execution environment. Version control and caching ensure\nefficient (re)execution. TIRA allows for blind evaluation when an experiment\nruns on a remote server or cloud not under the control of the experimenter. The\ntest data and ground truth are then hidden from public access, and the\nretrieval software has to process them in a sandbox that prevents data leaks.\n  We currently host an instance of TIREx with 15 corpora (1.9 billion\ndocuments) on which 32 shared retrieval tasks are based. Using Docker images of\n50 standard retrieval approaches, we automatically evaluated all approaches on\nall tasks (50 $\\cdot$ 32 = 1,600~runs) in less than a week on a midsize cluster\n(1,620 CPU cores and 24 GPUs). This instance of TIREx is open for submissions\nand will be integrated with the IR Anthology, as well as released open source.",
        "translated": ""
    },
    {
        "title": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light\n  Graph Convolution for Effective Multi-Criteria Recommendation",
        "url": "http://arxiv.org/abs/2305.18885v1",
        "pub_date": "2023-05-30",
        "summary": "The multi-criteria (MC) recommender system, which leverages MC rating\ninformation in a wide range of e-commerce areas, is ubiquitous nowadays.\nSurprisingly, although graph neural networks (GNNs) have been widely applied to\ndevelop various recommender systems due to GNN's high expressive capability in\nlearning graph representations, it has been still unexplored how to design MC\nrecommender systems with GNNs. In light of this, we make the first attempt\ntowards designing a GNN-aided MC recommender system. Specifically, rather than\nstraightforwardly adopting existing GNN-based recommendation methods, we devise\na novel criteria preference-aware light graph convolution CPA-LGC method, which\nis capable of precisely capturing the criteria preference of users as well as\nthe collaborative signal in complex high-order connectivities. To this end, we\nfirst construct an MC expansion graph that transforms user--item MC ratings\ninto an expanded bipartite graph to potentially learn from the collaborative\nsignal in MC ratings. Next, to strengthen the capability of criteria preference\nawareness, CPA-LGC incorporates newly characterized embeddings, including\nuser-specific criteria-preference embeddings and item-specific criterion\nembeddings, into our graph convolution model. Through comprehensive evaluations\nusing four real-world datasets, we demonstrate (a) the superiority over\nbenchmark MC recommendation methods and benchmark recommendation methods using\nGNNs with tremendous gains, (b) the effectiveness of core components in\nCPA-LGC, and (c) the computational efficiency.",
        "translated": ""
    },
    {
        "title": "Robust Reinforcement Learning Objectives for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.18820v1",
        "pub_date": "2023-05-30",
        "summary": "Attention-based sequential recommendation methods have demonstrated promising\nresults by accurately capturing users' dynamic interests from historical\ninteractions. In addition to generating superior user representations, recent\nstudies have begun integrating reinforcement learning (RL) into these models.\nFraming sequential recommendation as an RL problem with reward signals, unlocks\ndeveloping recommender systems (RS) that consider a vital aspect-incorporating\ndirect user feedback in the form of rewards to deliver a more personalized\nexperience. Nonetheless, employing RL algorithms presents challenges, including\noff-policy training, expansive combinatorial action spaces, and the scarcity of\ndatasets with sufficient reward signals. Contemporary approaches have attempted\nto combine RL and sequential modeling, incorporating contrastive-based\nobjectives and negative sampling strategies for training the RL component. In\nthis study, we further emphasize the efficacy of contrastive-based objectives\npaired with augmentation to address datasets with extended horizons.\nAdditionally, we recognize the potential instability issues that may arise\nduring the application of negative sampling. These challenges primarily stem\nfrom the data imbalance prevalent in real-world datasets, which is a common\nissue in offline RL contexts. While our established baselines attempt to\nmitigate this through various techniques, instability remains an issue.\nTherefore, we introduce an enhanced methodology aimed at providing a more\neffective solution to these challenges.",
        "translated": ""
    },
    {
        "title": "Who Would be Interested in Services? An Entity Graph Learning System for\n  User Targeting",
        "url": "http://arxiv.org/abs/2305.18780v1",
        "pub_date": "2023-05-30",
        "summary": "With the growing popularity of various mobile devices, user targeting has\nreceived a growing amount of attention, which aims at effectively and\nefficiently locating target users that are interested in specific services.\nMost pioneering works for user targeting tasks commonly perform\nsimilarity-based expansion with a few active users as seeds, suffering from the\nfollowing major issues: the unavailability of seed users for newcoming services\nand the unfriendliness of black-box procedures towards marketers. In this\npaper, we design an Entity Graph Learning (EGL) system to provide explainable\nuser targeting ability meanwhile applicable to addressing the cold-start issue.\nEGL System follows the hybrid online-offline architecture to satisfy the\nrequirements of scalability and timeliness. Specifically, in the offline stage,\nthe system focuses on the heavyweight entity graph construction and user entity\npreference learning, in which we propose a Three-stage Relation Mining\nProcedure (TRMP), breaking loose from the expensive seed users. At the online\nstage, the system offers the ability of user targeting in real-time based on\nthe entity graph from the offline stage. Since the user targeting process is\nbased on graph reasoning, the whole process is transparent and\noperation-friendly to marketers. Finally, extensive offline experiments and\nonline A/B testing demonstrate the superior performance of the proposed EGL\nSystem.",
        "translated": ""
    },
    {
        "title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts",
        "url": "http://arxiv.org/abs/2305.18727v1",
        "pub_date": "2023-05-30",
        "summary": "With a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
        "translated": ""
    },
    {
        "title": "Known by the Company it Keeps: Proximity-Based Indexing for Physical\n  Content in Archival Repositories",
        "url": "http://arxiv.org/abs/2305.18683v1",
        "pub_date": "2023-05-30",
        "summary": "Despite the plethora of born-digital content, vast troves of important\ncontent remain accessible only on physical media such as paper or microfilm.\nThe traditional approach to indexing undigitized content is using manually\ncreated metadata that describes content at some level of aggregation (e.g.,\nfolder, box, or collection). Searchers led in this way to some subset of the\ncontent often must then manually examine substantial quantities of physical\nmedia to find what they are looking for. This paper proposes a complementary\napproach, in which selective digitization of a small portion of the content is\nused as a basis for proximity-based indexing as a way of bringing the user\ncloser to the specific content for which they are looking. Experiments with 35\nboxes of partially digitized US State Department records indicate that\nbox-level indexes built in this way can provide a useful basis for search.",
        "translated": ""
    },
    {
        "title": "Improving Generalization for Multimodal Fake News Detection",
        "url": "http://arxiv.org/abs/2305.18599v1",
        "pub_date": "2023-05-29",
        "summary": "The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for fake news\ndetection. However, state-of-the-art approaches are usually trained on datasets\nof smaller size or with a limited set of specific topics. As a consequence,\nthese models lack generalization capabilities and are not applicable to\nreal-world data. In this paper, we propose three models that adopt and\nfine-tune state-of-the-art multimodal transformers for multimodal fake news\ndetection. We conduct an in-depth analysis by manipulating the input data aimed\nto explore models performance in realistic use cases on social media. Our study\nacross multiple models demonstrates that these systems suffer significant\nperformance drops against manipulated data. To reduce the bias and improve\nmodel generalization, we suggest training data augmentation to conduct more\nmeaningful experiments for fake news detection on social media. The proposed\ndata augmentation techniques enable models to generalize better and yield\nimproved state-of-the-art results.",
        "translated": ""
    },
    {
        "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on\n  Structured Data",
        "url": "http://arxiv.org/abs/2305.19912v1",
        "pub_date": "2023-05-31",
        "summary": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which\nencodes user queries and structured data in one universal embedding space for\nretrieving structured data. SANTA proposes two pretraining methods to make\nlanguage models structure-aware and learn effective representations for\nstructured data: 1) Structured Data Alignment, which utilizes the natural\nalignment relations between structured data and unstructured data for\nstructure-aware pretraining. It contrastively trains language models to\nrepresent multi-modal text data and teaches models to distinguish matched\nstructured data for unstructured texts. 2) Masked Entity Prediction, which\ndesigns an entity-oriented mask strategy and asks language models to fill in\nthe masked entities. Our experiments show that SANTA achieves state-of-the-art\non code search and product search and conducts convincing results in the\nzero-shot setting. SANTA learns tailored representations for multi-modal text\ndata by aligning structured and unstructured data pairs and capturing\nstructural semantics by masking and predicting entities in the structured data.\nAll codes are available at https://github.com/OpenMatch/OpenMatch.",
        "translated": ""
    },
    {
        "title": "Web scraping: a promising tool for geographic data acquisition",
        "url": "http://arxiv.org/abs/2305.19893v1",
        "pub_date": "2023-05-31",
        "summary": "With much of our lives taking place online, researchers are increasingly\nturning to information from the World Wide Web to gain insights into geographic\npatterns and processes. Web scraping as an online data acquisition technique\nallows us to gather intelligence especially on social and economic actions for\nwhich the Web serves as a platform. Specific opportunities relate to\nnear-real-time access to object-level geolocated data, which can be captured in\na cost-effective way. The studied geographic phenomena include, but are not\nlimited to, the rental market and associated processes such as gentrification,\nentrepreneurial ecosystems, or spatial planning processes. Since the\ninformation retrieved from the Web is not made available for that purpose, Web\nscraping faces several unique challenges, several of which relate to location.\nEthical and legal issues mainly relate to intellectual property rights,\ninformed consent and (geo-) privacy, and website integrity and contract. These\nissues also effect the practice of open science. In addition, there are\ntechnical and statistical challenges that relate to dependability and\nincompleteness, data inconsistencies and bias, as well as the limited\nhistorical coverage. Geospatial analyses furthermore usually require the\nautomated extraction and subsequent resolution of toponyms or addresses\n(geoparsing, geocoding). A study on apartment rent in Leipzig, Germany is used\nto illustrate the use of Web scraping and its challenges. We conclude that\ngeographic researchers should embrace Web scraping as a powerful and affordable\ndigital fieldwork tool while paying special attention to its legal, ethical,\nand methodological challenges.",
        "translated": ""
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.19860v1",
        "pub_date": "2023-05-31",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in the field of\nNatural Language Processing (NLP) and have recently gained significant\nattention in the domain of Recommendation Systems (RS). These models, trained\non massive amounts of data using self-supervised learning, have demonstrated\nremarkable success in learning universal representations and have the potential\nto enhance various aspects of recommendation systems by some effective transfer\ntechniques such as fine-tuning and prompt tuning, and so on. The crucial aspect\nof harnessing the power of language models in enhancing recommendation quality\nis the utilization of their high-quality representations of textual features\nand their extensive coverage of external knowledge to establish correlations\nbetween items and users. To provide a comprehensive understanding of the\nexisting LLM-based recommendation systems, this survey presents a taxonomy that\ncategorizes these models into two major paradigms, respectively Discriminative\nLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation\n(GLLM4Rec), with the latter being systematically sorted out for the first time.\nFurthermore, we systematically review and analyze existing LLM-based\nrecommendation systems within each paradigm, providing insights into their\nmethodologies, techniques, and performance. Additionally, we identify key\nchallenges and several valuable findings to provide researchers and\npractitioners with inspiration.",
        "translated": ""
    },
    {
        "title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish\n  Language",
        "url": "http://arxiv.org/abs/2305.19840v1",
        "pub_date": "2023-05-31",
        "summary": "The BEIR dataset is a large, heterogeneous benchmark for Information\nRetrieval (IR) in zero-shot settings, garnering considerable attention within\nthe research community. However, BEIR and analogous datasets are predominantly\nrestricted to the English language. Our objective is to establish extensive\nlarge-scale resources for IR in the Polish language, thereby advancing the\nresearch in this NLP area. In this work, inspired by mMARCO and Mr.~TyDi\ndatasets, we translated all accessible open IR datasets into Polish, and we\nintroduced the BEIR-PL benchmark -- a new benchmark which comprises 13\ndatasets, facilitating further development, training and evaluation of modern\nPolish language models for IR tasks. We executed an evaluation and comparison\nof numerous IR models on the newly introduced BEIR-PL benchmark. Furthermore,\nwe publish pre-trained open IR models for Polish language,d marking a\npioneering development in this field. Additionally, the evaluation revealed\nthat BM25 achieved significantly lower scores for Polish than for English,\nwhich can be attributed to high inflection and intricate morphological\nstructure of the Polish language. Finally, we trained various re-ranking models\nto enhance the BM25 retrieval, and we compared their performance to identify\ntheir unique characteristic features. To ensure accurate model comparisons, it\nis necessary to scrutinise individual results rather than to average across the\nentire benchmark. Thus, we thoroughly analysed the outcomes of IR models in\nrelation to each individual data subset encompassed by the BEIR benchmark. The\nbenchmark data is available at URL {\\bf https://huggingface.co/clarin-knext}.",
        "translated": ""
    },
    {
        "title": "Medication Recommendation via Domain Knowledge Informed Deep Learning",
        "url": "http://arxiv.org/abs/2305.19604v1",
        "pub_date": "2023-05-31",
        "summary": "Medication recommendation is a fundamental yet crucial branch of healthcare,\nwhich provides opportunities to support clinical physicians with more accurate\nmedication prescriptions for patients with complex health conditions. Learning\nfrom electronic health records (EHR) to recommend medications is the most\ncommon way in previous studies. However, most of them neglect incorporating\ndomain knowledge according to the clinical manifestations in the EHR of the\npatient. To address these issues, we propose a novel \\textbf{D}omain\n\\textbf{K}nowledge \\textbf{I}nformed \\textbf{Net}work (DKINet) to integrate\ndomain knowledge with observable clinical manifestations of the patient, which\nis the first dynamic domain knowledge informed framework toward medication\nrecommendation. In particular, we first design a knowledge-driven encoder to\ncapture the domain information and then develop a data-driven encoder to\nintegrate domain knowledge into the observable EHR. To endow the model with the\ncapability of temporal decision, we design an explicit medication encoder for\nlearning the longitudinal dependence of the patient. Extensive experiments on\nthree publicly available datasets verify the superiority of our method. The\ncode will be public upon acceptance.",
        "translated": ""
    },
    {
        "title": "Towards Semi-supervised Universal Graph Classification",
        "url": "http://arxiv.org/abs/2305.19598v1",
        "pub_date": "2023-05-31",
        "summary": "Graph neural networks have pushed state-of-the-arts in graph classifications\nrecently. Typically, these methods are studied within the context of supervised\nend-to-end training, which necessities copious task-specific labels. However,\nin real-world circumstances, labeled data could be limited, and there could be\na massive corpus of unlabeled data, even from unknown classes as a\ncomplementary. Towards this end, we study the problem of semi-supervised\nuniversal graph classification, which not only identifies graph samples which\ndo not belong to known classes, but also classifies the remaining samples into\ntheir respective classes. This problem is challenging due to a severe lack of\nlabels and potential class shifts. In this paper, we propose a novel graph\nneural network framework named UGNN, which makes the best of unlabeled data\nfrom the subgraph perspective. To tackle class shifts, we estimate the\ncertainty of unlabeled graphs using multiple subgraphs, which facilities the\ndiscovery of unlabeled data from unknown categories. Moreover, we construct\nsemantic prototypes in the embedding space for both known and unknown\ncategories and utilize posterior prototype assignments inferred from the\nSinkhorn-Knopp algorithm to learn from abundant unlabeled graphs across\ndifferent subgraph views. Extensive experiments on six datasets verify the\neffectiveness of UGNN in different settings.",
        "translated": ""
    },
    {
        "title": "Multi-Epoch Learning for Deep Click-Through Rate Prediction Models",
        "url": "http://arxiv.org/abs/2305.19531v1",
        "pub_date": "2023-05-31",
        "summary": "The one-epoch overfitting phenomenon has been widely observed in industrial\nClick-Through Rate (CTR) applications, where the model performance experiences\na significant degradation at the beginning of the second epoch. Recent advances\ntry to understand the underlying factors behind this phenomenon through\nextensive experiments. However, it is still unknown whether a multi-epoch\ntraining paradigm could achieve better results, as the best performance is\nusually achieved by one-epoch training. In this paper, we hypothesize that the\nemergence of this phenomenon may be attributed to the susceptibility of the\nembedding layer to overfitting, which can stem from the high-dimensional\nsparsity of data. To maintain feature sparsity while simultaneously avoiding\noverfitting of embeddings, we propose a novel Multi-Epoch learning with Data\nAugmentation (MEDA), which can be directly applied to most deep CTR models.\nMEDA achieves data augmentation by reinitializing the embedding layer in each\nepoch, thereby avoiding embedding overfitting and simultaneously improving\nconvergence. To our best knowledge, MEDA is the first multi-epoch training\nparadigm designed for deep CTR prediction models. We conduct extensive\nexperiments on several public datasets, and the effectiveness of our proposed\nMEDA is fully verified. Notably, the results show that MEDA can significantly\noutperform the conventional one-epoch training. Besides, MEDA has exhibited\nsignificant benefits in a real-world scene on Kuaishou.",
        "translated": ""
    },
    {
        "title": "AdANNS: A Framework for Adaptive Semantic Search",
        "url": "http://arxiv.org/abs/2305.19435v1",
        "pub_date": "2023-05-30",
        "summary": "Web-scale search systems learn an encoder to embed a given query which is\nthen hooked into an approximate nearest neighbor search (ANNS) pipeline to\nretrieve similar data points. To accurately capture tail queries and data\npoints, learned representations typically are rigid, high-dimensional vectors\nthat are generally used as-is in the entire ANNS pipeline and can lead to\ncomputationally expensive retrieval. In this paper, we argue that instead of\nrigid representations, different stages of ANNS can leverage adaptive\nrepresentations of varying capacities to achieve significantly better\naccuracy-compute trade-offs, i.e., stages of ANNS that can get away with more\napproximate computation should use a lower-capacity representation of the same\ndata point. To this end, we introduce AdANNS, a novel ANNS design framework\nthat explicitly leverages the flexibility of Matryoshka Representations. We\ndemonstrate state-of-the-art accuracy-compute trade-offs using novel\nAdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF)\nand quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is\nup to 1.5% more accurate than the rigid representations-based IVF at the same\ncompute budget; and matches accuracy while being up to 90x faster in wall-clock\ntime. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the\n64-byte OPQ baseline constructed using rigid representations -- same accuracy\nat half the cost! We further show that the gains from AdANNS translate to\nmodern-day composite ANNS indices that combine search structures and\nquantization. Finally, we demonstrate that AdANNS can enable inference-time\nadaptivity for compute-aware search on ANNS indices built non-adaptively on\nmatryoshka representations. Code is open-sourced at\nhttps://github.com/RAIVNLab/AdANNS.",
        "translated": ""
    },
    {
        "title": "DuoSearch: A Novel Search Engine for Bulgarian Historical Documents",
        "url": "http://arxiv.org/abs/2305.19392v1",
        "pub_date": "2023-05-30",
        "summary": "Search in collections of digitised historical documents is hindered by a\ntwo-prong problem, orthographic variety and optical character recognition (OCR)\nmistakes. We present a new search engine for historical documents, DuoSearch,\nwhich uses ElasticSearch and machine learning methods based on deep neural\nnetworks to offer a solution to this problem. It was tested on a collection of\nhistorical newspapers in Bulgarian from the mid-19th to the mid-20th century.\nThe system provides an interactive and intuitive interface for the end-users\nallowing them to enter search terms in modern Bulgarian and search across\nhistorical spellings. This is the first solution facilitating the use of\ndigitised historical documents in Bulgarian.",
        "translated": ""
    },
    {
        "title": "AMR4NLI: Interpretable and robust NLI measures from semantic graphs",
        "url": "http://arxiv.org/abs/2306.00936v1",
        "pub_date": "2023-06-01",
        "summary": "The task of natural language inference (NLI) asks whether a given premise\n(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human\nratings of entailment, but the meaning relationships driving these ratings are\nnot formalized. Can the underlying sentence pair relationships be made more\nexplicit in an interpretable yet robust fashion? We compare semantic structures\nto represent premise and hypothesis, including sets of contextualized\nembeddings and semantic graphs (Abstract Meaning Representations), and measure\nwhether the hypothesis is a semantic substructure of the premise, utilizing\ninterpretable metrics. Our evaluation on three English benchmarks finds value\nin both contextualized embeddings and semantic graphs; moreover, they provide\ncomplementary signals, and can be leveraged together in a hybrid model.",
        "translated": ""
    },
    {
        "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach\n  for Low-Resource Complex NER",
        "url": "http://arxiv.org/abs/2306.00928v1",
        "pub_date": "2023-06-01",
        "summary": "Complex Named Entity Recognition (NER) is the task of detecting\nlinguistically complex named entities in low-context text. In this paper, we\npresent ACLM Attention-map aware keyword selection for Conditional Language\nModel fine-tuning), a novel data augmentation approach based on conditional\ngeneration to address the data scarcity problem in low-resource complex NER.\nACLM alleviates the context-entity mismatch issue, a problem existing NER data\naugmentation techniques suffer from and often generates incoherent\naugmentations by placing complex named entities in the wrong context. ACLM\nbuilds on BART and is optimized on a novel text reconstruction or denoising\ntask - we use selective masking (aided by attention maps) to retain the named\nentities and certain keywords in the input sentence that provide contextually\nrelevant additional knowledge or hints about the named entities. Compared with\nother data augmentation strategies, ACLM can generate more diverse and coherent\naugmentations preserving the true word sense of complex entities in the\nsentence. We demonstrate the effectiveness of ACLM both qualitatively and\nquantitatively on monolingual, cross-lingual, and multilingual complex NER\nacross various low-resource settings. ACLM outperforms all our neural baselines\nby a significant margin (1%-36%). In addition, we demonstrate the application\nof ACLM to other domains that suffer from data scarcity (e.g., biomedical). In\npractice, ACLM generates more effective and factual augmentations for these\ndomains than prior methods. Code: https://github.com/Sreyan88/ACLM",
        "translated": ""
    },
    {
        "title": "SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in\n  Graph Neural Networks",
        "url": "http://arxiv.org/abs/2306.00899v1",
        "pub_date": "2023-06-01",
        "summary": "Graph Neural Networks (GNNs) have demonstrated promising outcomes across\nvarious tasks, including node classification and link prediction. Despite their\nremarkable success in various high-impact applications, we have identified\nthree common pitfalls in message passing for link prediction. Particularly, in\nprevalent GNN frameworks (e.g., DGL and PyTorch-Geometric), the target edges\n(i.e., the edges being predicted) consistently exist as message passing edges\nin the graph during training. Consequently, this results in overfitting and\ndistribution shift, both of which adversely impact the generalizability to test\nthe target edges. Additionally, during test time, the failure to exclude the\ntest target edges leads to implicit test leakage caused by neighborhood\naggregation. In this paper, we analyze these three pitfalls and investigate the\nimpact of including or excluding target edges on the performance of nodes with\nvarying degrees during training and test phases. Our theoretical and empirical\nanalysis demonstrates that low-degree nodes are more susceptible to these\npitfalls. These pitfalls can have detrimental consequences when GNNs are\nimplemented in production systems. To systematically address these pitfalls, we\npropose SpotTarget, an effective and efficient GNN training framework. During\ntraining, SpotTarget leverages our insight regarding low-degree nodes and\nexcludes train target edges connected to at least one low-degree node. During\ntest time, it emulates real-world scenarios of GNN usage in production and\nexcludes all test target edges. Our experiments conducted on diverse real-world\ndatasets, demonstrate that SpotTarget significantly enhances GNNs, achieving up\nto a 15x increase in accuracy in sparse graphs. Furthermore, SpotTarget\nconsistently and dramatically improves the performance for low-degree nodes in\ndense graphs.",
        "translated": ""
    },
    {
        "title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection",
        "url": "http://arxiv.org/abs/2306.00765v1",
        "pub_date": "2023-06-01",
        "summary": "Stance Detection is concerned with identifying the attitudes expressed by an\nauthor towards a target of interest. This task spans a variety of domains\nranging from social media opinion identification to detecting the stance for a\nlegal claim. However, the framing of the task varies within these domains, in\nterms of the data collection protocol, the label dictionary and the number of\navailable annotations. Furthermore, these stance annotations are significantly\nimbalanced on a per-topic and inter-topic basis. These make multi-domain stance\ndetection a challenging task, requiring standardization and domain adaptation.\nTo overcome this challenge, we propose $\\textbf{T}$opic $\\textbf{E}$fficient\n$\\textbf{St}$anc$\\textbf{E}$ $\\textbf{D}$etection (TESTED), consisting of a\ntopic-guided diversity sampling technique and a contrastive objective that is\nused for fine-tuning a stance classifier. We evaluate the method on an existing\nbenchmark of $16$ datasets with in-domain, i.e. all topics seen and\nout-of-domain, i.e. unseen topics, experiments. The results show that our\nmethod outperforms the state-of-the-art with an average of $3.5$ F1 points\nincrease in-domain, and is more generalizable with an averaged increase of\n$10.2$ F1 on out-of-domain evaluation while using $\\leq10\\%$ of the training\ndata. We show that our sampling technique mitigates both inter- and per-topic\nclass imbalances. Finally, our analysis demonstrates that the contrastive\nlearning objective allows the model a more pronounced segmentation of samples\nwith varying labels.",
        "translated": ""
    },
    {
        "title": "End-to-End Document Classification and Key Information Extraction using\n  Assignment Optimization",
        "url": "http://arxiv.org/abs/2306.00750v1",
        "pub_date": "2023-06-01",
        "summary": "We propose end-to-end document classification and key information extraction\n(KIE) for automating document processing in forms. Through accurate document\nclassification we harness known information from templates to enhance KIE from\nforms. We use text and layout encoding with a cosine similarity measure to\nclassify visually-similar documents. We then demonstrate a novel application of\nmixed integer programming by using assignment optimization to extract key\ninformation from documents. Our approach is validated on an in-house dataset of\nnoisy scanned forms. The best performing document classification approach\nachieved 0.97 f1 score. A mean f1 score of 0.94 for the KIE task suggests there\nis significant potential in applying optimization techniques. Abation results\nshow that the method relies on document preprocessing techniques to mitigate\nType II errors and achieve optimal performance.",
        "translated": ""
    },
    {
        "title": "Class Anchor Margin Loss for Content-Based Image Retrieval",
        "url": "http://arxiv.org/abs/2306.00630v1",
        "pub_date": "2023-06-01",
        "summary": "The performance of neural networks in content-based image retrieval (CBIR) is\nhighly influenced by the chosen loss (objective) function. The majority of\nobjective functions for neural models can be divided into metric learning and\nstatistical learning. Metric learning approaches require a pair mining strategy\nthat often lacks efficiency, while statistical learning approaches are not\ngenerating highly compact features due to their indirect feature optimization.\nTo this end, we propose a novel repeller-attractor loss that falls in the\nmetric learning paradigm, yet directly optimizes for the L2 metric without the\nneed of generating pairs. Our loss is formed of three components. One leading\nobjective ensures that the learned features are attracted to each designated\nlearnable class anchor. The second loss component regulates the anchors and\nforces them to be separable by a margin, while the third objective ensures that\nthe anchors do not collapse to zero. Furthermore, we develop a more efficient\ntwo-stage retrieval system by harnessing the learned class anchors during the\nfirst stage of the retrieval process, eliminating the need of comparing the\nquery with every image in the database. We establish a set of four datasets\n(CIFAR-100, Food-101, SVHN, and Tiny ImageNet) and evaluate the proposed\nobjective in the context of few-shot and full-set training on the CBIR task, by\nusing both convolutional and transformer architectures. Compared to existing\nobjective functions, our empirical evidence shows that the proposed objective\nis generating superior and more consistent results.",
        "translated": ""
    },
    {
        "title": "End-to-end Knowledge Retrieval with Multi-modal Queries",
        "url": "http://arxiv.org/abs/2306.00424v1",
        "pub_date": "2023-06-01",
        "summary": "We investigate knowledge retrieval with multi-modal queries, i.e. queries\ncontaining information split across image and text inputs, a challenging task\nthat differs from previous work on cross-modal retrieval. We curate a new\ndataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a\nsystem to retrieve knowledge from a large corpus by integrating contents from\nboth text and image queries. We introduce a retriever model ``ReViz'' that can\ndirectly process input text and images to retrieve relevant knowledge in an\nend-to-end fashion without being dependent on intermediate modules such as\nobject detectors or caption generators. We introduce a new pretraining task\nthat is effective for learning knowledge retrieval with multimodal queries and\nalso improves performance on downstream tasks. We demonstrate superior\nperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot\nsettings as well as further improvements when finetuned on these datasets.",
        "translated": ""
    },
    {
        "title": "A Survey on Fairness-aware Recommender Systems",
        "url": "http://arxiv.org/abs/2306.00403v1",
        "pub_date": "2023-06-01",
        "summary": "As information filtering services, recommender systems have extremely\nenriched our daily life by providing personalized suggestions and facilitating\npeople in decision-making, which makes them vital and indispensable to human\nsociety in the information era. However, as people become more dependent on\nthem, recent studies show that recommender systems potentially own\nunintentional impacts on society and individuals because of their unfairness\n(e.g., gender discrimination in job recommendations). To develop trustworthy\nservices, it is crucial to devise fairness-aware recommender systems that can\nmitigate these bias issues. In this survey, we summarise existing methodologies\nand practices of fairness in recommender systems. Firstly, we present concepts\nof fairness in different recommendation scenarios, comprehensively categorize\ncurrent advances, and introduce typical methods to promote fairness in\ndifferent stages of recommender systems. Next, after introducing datasets and\nevaluation metrics applied to assess the fairness of recommender systems, we\nwill delve into the significant influence that fairness-aware recommender\nsystems exert on real-world industrial applications. Subsequently, we highlight\nthe connection between fairness and other principles of trustworthy recommender\nsystems, aiming to consider trustworthiness principles holistically while\nadvocating for fairness. Finally, we summarize this review, spotlighting\npromising opportunities in comprehending concepts, frameworks, the balance\nbetween accuracy and fairness, and the ties with trustworthiness, with the\nultimate goal of fostering the development of fairness-aware recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "Explicit Feature Interaction-aware Uplift Network for Online Marketing",
        "url": "http://arxiv.org/abs/2306.00315v1",
        "pub_date": "2023-06-01",
        "summary": "As a key component in online marketing, uplift modeling aims to accurately\ncapture the degree to which different treatments motivate different users, such\nas coupons or discounts, also known as the estimation of individual treatment\neffect (ITE). In an actual business scenario, the options for treatment may be\nnumerous and complex, and there may be correlations between different\ntreatments. In addition, each marketing instance may also have rich user and\ncontextual features. However, existing methods still fall short in both fully\nexploiting treatment information and mining features that are sensitive to a\nparticular treatment. In this paper, we propose an explicit feature\ninteraction-aware uplift network (EFIN) to address these two problems. Our EFIN\nincludes four customized modules: 1) a feature encoding module encodes not only\nthe user and contextual features, but also the treatment features; 2) a\nself-interaction module aims to accurately model the user's natural response\nwith all but the treatment features; 3) a treatment-aware interaction module\naccurately models the degree to which a particular treatment motivates a user\nthrough interactions between the treatment features and other features, i.e.,\nITE; and 4) an intervention constraint module is used to balance the ITE\ndistribution of users between the control and treatment groups so that the\nmodel would still achieve a accurate uplift ranking on data collected from a\nnon-random intervention marketing scenario. We conduct extensive experiments on\ntwo public datasets and one product dataset to verify the effectiveness of our\nEFIN. In addition, our EFIN has been deployed in a credit card bill payment\nscenario of a large online financial platform with a significant improvement.",
        "translated": ""
    },
    {
        "title": "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest",
        "url": "http://arxiv.org/abs/2306.00248v1",
        "pub_date": "2023-05-31",
        "summary": "Sequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
        "translated": ""
    },
    {
        "title": "Fresh Content Needs More Attention: Multi-funnel Fresh Content\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.01720v1",
        "pub_date": "2023-06-02",
        "summary": "Recommendation system serves as a conduit connecting users to an incredibly\nlarge, diverse and ever growing collection of contents. In practice, missing\ninformation on fresh (and tail) contents needs to be filled in order for them\nto be exposed and discovered by their audience. We here share our success\nstories in building a dedicated fresh content recommendation stack on a large\ncommercial platform. To nominate fresh contents, we built a multi-funnel\nnomination system that combines (i) a two-tower model with strong\ngeneralization power for coverage, and (ii) a sequence model with near\nreal-time update on user feedback for relevance. The multi-funnel setup\neffectively balances between coverage and relevance. An in-depth study uncovers\nthe relationship between user activity level and their proximity toward fresh\ncontents, which further motivates a contextual multi-funnel setup. Nominated\nfresh candidates are then scored and ranked by systems considering prediction\nuncertainty to further bootstrap content with less exposure. We evaluate the\nbenefits of the dedicated fresh content recommendation stack, and the\nmulti-funnel nomination system in particular, through user corpus co-diverted\nlive experiments. We conduct multiple rounds of live experiments on a\ncommercial platform serving billion of users demonstrating efficacy of our\nproposed methods.",
        "translated": ""
    },
    {
        "title": "Pretrained Language Model based Web Search Ranking: From Relevance to\n  Satisfaction",
        "url": "http://arxiv.org/abs/2306.01599v1",
        "pub_date": "2023-06-02",
        "summary": "Search engine plays a crucial role in satisfying users' diverse information\nneeds. Recently, Pretrained Language Models (PLMs) based text ranking models\nhave achieved huge success in web search. However, many state-of-the-art text\nranking approaches only focus on core relevance while ignoring other dimensions\nthat contribute to user satisfaction, e.g., document quality, recency,\nauthority, etc. In this work, we focus on ranking user satisfaction rather than\nrelevance in web search, and propose a PLM-based framework, namely SAT-Ranker,\nwhich comprehensively models different dimensions of user satisfaction in a\nunified manner. In particular, we leverage the capacities of PLMs on both\ntextual and numerical inputs, and apply a multi-field input that modularizes\neach dimension of user satisfaction as an input field. Overall, SAT-Ranker is\nan effective, extensible, and data-centric framework that has huge potential\nfor industrial applications. On rigorous offline and online experiments,\nSAT-Ranker obtains remarkable gains on various evaluation sets targeting\ndifferent dimensions of user satisfaction. It is now fully deployed online to\nimprove the usability of our search engine.",
        "translated": ""
    },
    {
        "title": "Influence Maximization with Fairness at Scale (Extended Version)",
        "url": "http://arxiv.org/abs/2306.01587v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we revisit the problem of influence maximization with\nfairness, which aims to select k influential nodes to maximise the spread of\ninformation in a network, while ensuring that selected sensitive user\nattributes are fairly affected, i.e., are proportionally similar between the\noriginal network and the affected users. Recent studies on this problem focused\nonly on extremely small networks, hence the challenge remains on how to achieve\na scalable solution, applicable to networks with millions or billions of nodes.\nWe propose an approach that is based on learning node representations for fair\nspread from diffusion cascades, instead of the social connectivity s.t. we can\ndeal with very large graphs. We propose two data-driven approaches: (a)\nfairness-based participant sampling (FPS), and (b) fairness as context (FAC).\nSpread related user features, such as the probability of diffusing information\nto others, are derived from the historical information cascades, using a deep\nneural network. The extracted features are then used in selecting influencers\nthat maximize the influence spread, while being also fair with respect to the\nchosen sensitive attributes. In FPS, fairness and cascade length information\nare considered independently in the decision-making process, while FAC\nconsiders these information facets jointly and considers correlations between\nthem. The proposed algorithms are generic and represent the first policy-driven\nsolutions that can be applied to arbitrary sets of sensitive attributes at\nscale. We evaluate the performance of our solutions on a real-world public\ndataset (Sina Weibo) and on a hybrid real-synthethic dataset (Digg), which\nexhibit all the facets that we exploit, namely diffusion network, diffusion\ntraces, and user profiles. These experiments show that our methods outperform\nthe state-the-art solutions in terms of spread, fairness, and scalability.",
        "translated": ""
    },
    {
        "title": "Système de recommandations basé sur les contraintes pour les\n  simulations de gestion de crise",
        "url": "http://arxiv.org/abs/2306.01504v1",
        "pub_date": "2023-06-02",
        "summary": "In the context of the evacuation of populations, some citizens/volunteers may\nwant and be able to participate in the evacuation of populations in difficulty\nby coming to lend a hand to emergency/evacuation vehicles with their own\nvehicles. One way of framing these impulses of solidarity would be to be able\nto list in real-time the citizens/volunteers available with their vehicles\n(land, sea, air, etc.), to be able to geolocate them according to the risk\nareas to be evacuated, and adding them to the evacuation/rescue vehicles.\nBecause it is difficult to propose an effective real-time operational system on\nthe field in a real crisis situation, in this work, we propose to add a module\nfor recommending driver/vehicle pairs (with their specificities) to a system of\ncrisis management simulation. To do that, we chose to model and develop an\nontology-supported constraint-based recommender system for crisis management\nsimulations.",
        "translated": ""
    },
    {
        "title": "Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking\n  Intent in Recommender Systems",
        "url": "http://arxiv.org/abs/2306.01476v1",
        "pub_date": "2023-06-02",
        "summary": "Recommending novel content, which expands user horizons by introducing them\nto new interests, has been shown to improve users' long-term experience on\nrecommendation platforms \\cite{chen2021values}. Users however are not\nconstantly looking to explore novel content. It is therefore crucial to\nunderstand their novelty-seeking intent and adjust the recommendation policy\naccordingly. Most existing literature models a user's propensity to choose\nnovel content or to prefer a more diverse set of recommendations at individual\ninteractions. Hierarchical structure, on the other hand, exists in a user's\nnovelty-seeking intent, which is manifested as a static and intrinsic user\npreference for seeking novelty along with a dynamic session-based propensity.\nTo this end, we propose a novel hierarchical reinforcement learning-based\nmethod to model the hierarchical user novelty-seeking intent, and to adapt the\nrecommendation policy accordingly based on the extracted user novelty-seeking\npropensity. We further incorporate diversity and novelty-related measurement in\nthe reward function of the hierarchical RL (HRL) agent to encourage user\nexploration \\cite{chen2021values}. We demonstrate the benefits of explicitly\nmodeling hierarchical user novelty-seeking intent in recommendations through\nextensive experiments on simulated and real-world datasets. In particular, we\ndemonstrate that the effectiveness of our proposed hierarchical RL-based method\nlies in its ability to capture such hierarchically-structured intent. As a\nresult, the proposed HRL model achieves superior performance on several public\ndatasets, compared with state-of-art baselines.",
        "translated": ""
    },
    {
        "title": "Prompt Tuning Large Language Models on Personalized Aspect Extraction\n  for Recommendations",
        "url": "http://arxiv.org/abs/2306.01475v1",
        "pub_date": "2023-06-02",
        "summary": "Existing aspect extraction methods mostly rely on explicit or ground truth\naspect information, or using data mining or machine learning approaches to\nextract aspects from implicit user feedback such as user reviews. It however\nremains under-explored how the extracted aspects can help generate more\nmeaningful recommendations to the users. Meanwhile, existing research on\naspect-based recommendations often relies on separate aspect extraction models\nor assumes the aspects are given, without accounting for the fact the optimal\nset of aspects could be dependent on the recommendation task at hand.\n  In this work, we propose to combine aspect extraction together with\naspect-based recommendations in an end-to-end manner, achieving the two goals\ntogether in a single framework. For the aspect extraction component, we\nleverage the recent advances in large language models and design a new prompt\nlearning mechanism to generate aspects for the end recommendation task. For the\naspect-based recommendation component, the extracted aspects are concatenated\nwith the usual user and item features used by the recommendation model. The\nrecommendation task mediates the learning of the user embeddings and item\nembeddings, which are used as soft prompts to generate aspects. Therefore, the\nextracted aspects are personalized and contextualized by the recommendation\ntask. We showcase the effectiveness of our proposed method through extensive\nexperiments on three industrial datasets, where our proposed framework\nsignificantly outperforms state-of-the-art baselines in both the personalized\naspect extraction and aspect-based recommendation tasks. In particular, we\ndemonstrate that it is necessary and beneficial to combine the learning of\naspect extraction and aspect-based recommendation together. We also conduct\nextensive ablation studies to understand the contribution of each design\ncomponent in our framework.",
        "translated": ""
    },
    {
        "title": "An OPC UA-based industrial Big Data architecture",
        "url": "http://arxiv.org/abs/2306.01418v1",
        "pub_date": "2023-06-02",
        "summary": "Industry 4.0 factories are complex and data-driven. Data is yielded from many\nsources, including sensors, PLCs, and other devices, but also from IT, like ERP\nor CRM systems. We ask how to collect and process this data in a way, such that\nit includes metadata and can be used for industrial analytics or to derive\nintelligent support systems. This paper describes a new, query model based\napproach, which uses a big data architecture to capture data from various\nsources using OPC UA as a foundation. It buffers and preprocesses the\ninformation for the purpose of harmonizing and providing a holistic state space\nof a factory, as well as mappings to the current state of a production site.\nThat information can be made available to multiple processing sinks, decoupled\nfrom the data sources, which enables them to work with the information without\ninterfering with devices of the production, disturbing the network devices they\nare working in, or influencing the production process negatively. Metadata and\nconnected semantic information is kept throughout the process, allowing to feed\nalgorithms with meaningful data, so that it can be accessed in its entirety to\nperform time series analysis, machine learning or similar evaluations as well\nas replaying the data from the buffer for repeatable simulations.",
        "translated": ""
    },
    {
        "title": "DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG\n  2000 Compressed Documents",
        "url": "http://arxiv.org/abs/2306.01359v1",
        "pub_date": "2023-06-02",
        "summary": "For any digital application with document images such as retrieval, the\nclassification of document images becomes an essential stage. Conventionally\nfor the purpose, the full versions of the documents, that is the uncompressed\ndocument images make the input dataset, which poses a threat due to the big\nvolume required to accommodate the full versions of the documents. Therefore,\nit would be novel, if the same classification task could be accomplished\ndirectly (with some partial decompression) with the compressed representation\nof documents in order to make the whole process computationally more efficient.\nIn this research work, a novel deep learning model, DWT CompCNN is proposed for\nclassification of documents that are compressed using High Throughput JPEG 2000\n(HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional\nlayers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each\nincreasing layer to improve learning from the wavelet coefficients extracted\nfrom the compressed images. Experiments are performed on two benchmark\ndatasets- Tobacco-3482 and RVL-CDIP, which demonstrate that the proposed model\nis time and space efficient, and also achieves a better classification accuracy\nin compressed domain.",
        "translated": ""
    },
    {
        "title": "Reducing Popularity Bias in Recommender Systems through AUC-Optimal\n  Negative Sampling",
        "url": "http://arxiv.org/abs/2306.01348v1",
        "pub_date": "2023-06-02",
        "summary": "Popularity bias is a persistent issue associated with recommendation systems,\nposing challenges to both fairness and efficiency. Existing literature widely\nacknowledges that reducing popularity bias often requires sacrificing\nrecommendation accuracy. In this paper, we challenge this commonly held belief.\nOur analysis under general bias-variance decomposition framework shows that\nreducing bias can actually lead to improved model performance under certain\nconditions. To achieve this win-win situation, we propose to intervene in model\ntraining through negative sampling thereby modifying model predictions.\nSpecifically, we provide an optimal negative sampling rule that maximizes\npartial AUC to preserve the accuracy of any given model, while correcting\nsample information and prior information to reduce popularity bias in a\nflexible and principled way. Our experimental results on real-world datasets\ndemonstrate the superiority of our approach in improving recommendation\nperformance and reducing popularity bias.",
        "translated": ""
    },
    {
        "title": "LyricSIM: A novel Dataset and Benchmark for Similarity Detection in\n  Spanish Song LyricS",
        "url": "http://arxiv.org/abs/2306.01325v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we present a new dataset and benchmark tailored to the task of\nsemantic similarity in song lyrics. Our dataset, originally consisting of 2775\npairs of Spanish songs, was annotated in a collective annotation experiment by\n63 native annotators. After collecting and refining the data to ensure a high\ndegree of consensus and data integrity, we obtained 676 high-quality annotated\npairs that were used to evaluate the performance of various state-of-the-art\nmonolingual and multilingual language models. Consequently, we established\nbaseline results that we hope will be useful to the community in all future\nacademic and industrial applications conducted in this context.",
        "translated": ""
    },
    {
        "title": "Learning Similarity among Users for Personalized Session-Based\n  Recommendation from hierarchical structure of User-Session-Item",
        "url": "http://arxiv.org/abs/2306.03040v1",
        "pub_date": "2023-06-05",
        "summary": "The task of the session-based recommendation is to predict the next\ninteraction of the user based on the anonymized user's behavior pattern. And\npersonalized version of this system is a promising research field due to its\navailability to deal with user information. However, there's a problem that the\nuser's preferences and historical sessions were not considered in the typical\nsession-based recommendation since it concentrates only on user-item\ninteraction. In addition, the existing personalized session-based\nrecommendation model has a limited capability in that it only considers the\npreference of the current user without considering those of similar users. It\nmeans there can be the loss of information included within the hierarchical\ndata structure of the user-session-item. To tackle with this problem, we\npropose USP-SBR(abbr. of User Similarity Powered - Session Based Recommender).\nTo model global historical sessions of users, we propose UserGraph that has two\ntypes of nodes - ItemNode and UserNode. We then connect the nodes with three\ntypes of edges. The first type of edges connects ItemNode as chronological\norder, and the second connects ItemNode to UserNode, and the last connects\nUserNode to ItemNode. With these user embeddings, we propose additional\ncontrastive loss, that makes users with similar intention be close to each\nother in the vector space. we apply graph neural network on these UserGraph and\nupdate nodes. Experimental results on two real-world datasets demonstrate that\nour method outperforms some state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Gen-IR @ SIGIR 2023: The First Workshop on Generative Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2306.02887v1",
        "pub_date": "2023-06-05",
        "summary": "Generative information retrieval (IR) has experienced substantial growth\nacross multiple research communities (e.g., information retrieval, computer\nvision, natural language processing, and machine learning), and has been highly\nvisible in the popular press. Theoretical, empirical, and actual user-facing\nproducts have been released that retrieve documents (via generation) or\ndirectly generate answers given an input request. We would like to investigate\nwhether end-to-end generative models are just another trend or, as some claim,\na paradigm change for IR. This necessitates new metrics, theoretical grounding,\nevaluation methods, task definitions, models, user interfaces, etc. The goal of\nthis workshop (https://coda.io/@sigir/gen-ir) is to focus on previously\nexplored Generative IR techniques like document retrieval and direct Grounded\nAnswer Generation, while also offering a venue for the discussion and\nexploration of how Generative IR can be applied to new domains like\nrecommendation systems, summarization, etc. The format of the workshop is\ninteractive, including roundtable and keynote sessions and tends to avoid the\none-sided dialogue of a mini-conference.",
        "translated": ""
    },
    {
        "title": "Benchmarking Middle-Trained Language Models for Neural Search",
        "url": "http://arxiv.org/abs/2306.02867v1",
        "pub_date": "2023-06-05",
        "summary": "Middle training methods aim to bridge the gap between the Masked Language\nModel (MLM) pre-training and the final finetuning for retrieval. Recent models\nsuch as CoCondenser, RetroMAE, and LexMAE argue that the MLM task is not\nsufficient enough to pre-train a transformer network for retrieval and hence\npropose various tasks to do so. Intrigued by those novel methods, we noticed\nthat all these models used different finetuning protocols, making it hard to\nassess the benefits of middle training. We propose in this paper a benchmark of\nCoCondenser, RetroMAE, and LexMAE, under the same finetuning conditions. We\ncompare both dense and sparse approaches under various finetuning protocols and\nmiddle training on different collections (MS MARCO, Wikipedia or Tripclick). We\nuse additional middle training baselines, such as a standard MLM finetuning on\nthe retrieval collection, optionally augmented by a CLS predicting the passage\nterm frequency. For the sparse approach, our study reveals that there is almost\nno statistical difference between those methods: the more effective the\nfinetuning procedure is, the less difference there is between those models. For\nthe dense approach, RetroMAE using MS MARCO as middle-training collection shows\nexcellent results in almost all the settings. Finally, we show that middle\ntraining on the retrieval collection, thus adapting the language model to it,\nis a critical factor. Overall, a better experimental setup should be adopted to\nevaluate middle training methods. Code available at\nhttps://github.com/naver/splade/tree/benchmarch-SIGIR23",
        "translated": ""
    },
    {
        "title": "CTRL: Connect Tabular and Language Model for CTR Prediction",
        "url": "http://arxiv.org/abs/2306.02841v1",
        "pub_date": "2023-06-05",
        "summary": "Traditional click-through rate (CTR) prediction models convert the tabular\ndata into one-hot vectors and leverage the collaborative relations among\nfeatures for inferring user's preference over items. This modeling paradigm\ndiscards the essential semantic information. Though some recent works like P5\nand M6-Rec have explored the potential of using Pre-trained Language Models\n(PLMs) to extract semantic signals for CTR prediction, they are computationally\nexpensive and suffer from low efficiency. Besides, the beneficial collaborative\nrelations are not considered, hindering the recommendation performance. To\nsolve these problems, in this paper, we propose a novel framework\n\\textbf{CTRL}, which is industrial friendly and model-agnostic with high\ntraining and inference efficiency. Specifically, the original tabular data is\nfirst converted into textual data. Both tabular data and converted textual data\nare regarded as two different modalities and are separately fed into the\ncollaborative CTR model and pre-trained language model. A cross-modal knowledge\nalignment procedure is performed to fine-grained align and integrate the\ncollaborative and semantic signals, and the lightweight collaborative model can\nbe deployed online for efficient serving after fine-tuned with supervised\nsignals. Experimental results on three public datasets show that CTRL\noutperforms the SOTA CTR models significantly. Moreover, we further verify its\neffectiveness on a large-scale industrial recommender system.",
        "translated": ""
    },
    {
        "title": "Path-Specific Counterfactual Fairness for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02615v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender systems (RSs) have become an indispensable part of online\nplatforms. With the growing concerns of algorithmic fairness, RSs are not only\nexpected to deliver high-quality personalized content, but are also demanded\nnot to discriminate against users based on their demographic information.\nHowever, existing RSs could capture undesirable correlations between sensitive\nfeatures and observed user behaviors, leading to biased recommendations. Most\nfair RSs tackle this problem by completely blocking the influences of sensitive\nfeatures on recommendations. But since sensitive features may also affect user\ninterests in a fair manner (e.g., race on culture-based preferences),\nindiscriminately eliminating all the influences of sensitive features\ninevitably degenerate the recommendations quality and necessary diversities. To\naddress this challenge, we propose a path-specific fair RS (PSF-RS) for\nrecommendations. Specifically, we summarize all fair and unfair correlations\nbetween sensitive features and observed ratings into two latent proxy\nmediators, where the concept of path-specific bias (PS-Bias) is defined based\non path-specific counterfactual inference. Inspired by Pearl's minimal change\nprinciple, we address the PS-Bias by minimally transforming the biased factual\nworld into a hypothetically fair world, where a fair RS model can be learned\naccordingly by solving a constrained optimization problem. For the technical\npart, we propose a feasible implementation of PSF-RS, i.e., PSF-VAE, with\nweakly-supervised variational inference, which robustly infers the latent\nmediators such that unfairness can be mitigated while necessary recommendation\ndiversities can be maximally preserved simultaneously. Experiments conducted on\nsemi-simulated and real-world datasets demonstrate the effectiveness of PSF-RS.",
        "translated": ""
    },
    {
        "title": "Learning to Relate to Previous Turns in Conversational Search",
        "url": "http://arxiv.org/abs/2306.02553v1",
        "pub_date": "2023-06-05",
        "summary": "Conversational search allows a user to interact with a search system in\nmultiple turns. A query is strongly dependent on the conversation context. An\neffective way to improve retrieval effectiveness is to expand the current query\nwith historical queries. However, not all the previous queries are related to,\nand useful for expanding the current query. In this paper, we propose a new\nmethod to select relevant historical queries that are useful for the current\nquery. To cope with the lack of labeled training data, we use a pseudo-labeling\napproach to annotate useful historical queries based on their impact on the\nretrieval results. The pseudo-labeled data are used to train a selection model.\nWe further propose a multi-task learning framework to jointly train the\nselector and the retriever during fine-tuning, allowing us to mitigate the\npossible inconsistency between the pseudo labels and the changed retriever.\nExtensive experiments on four conversational search datasets demonstrate the\neffectiveness and broad applicability of our method compared with several\nstrong baselines.",
        "translated": ""
    },
    {
        "title": "RecAgent: A Novel Simulation Paradigm for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02552v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender system has deeply revolutionized people's daily life and\nproduction, bringing a large amount of business value. In the recommendation\ndomain, simulation and real data-based studies are two typical research\nparadigms, with each having different advantages. Previously, real data-based\nstudies occupy more important positions, since accurately simulating the user\npreference is quite difficult. Recently, large language models (LLM) have shown\ngreat potential to achieve human-like intelligence, which provides new\nopportunities to overcome the shortcomings of simulation-based studies and thus\nhighlight their advantages, such as much more application scenarios and cheaper\ndata acquisition strategies. To shed lights on this direction, in this paper,\nwe introduce an LLM-based recommender simulator called RecAgent. Our simulator\nis composed of two modules: (1) the user module and (2) the recommender module.\nThe user module can browse the recommendation website, communicate with other\nusers and broadcast messages on the social media. The recommender module is\ndesigned to provide search or recommendation lists to the users, and one can\ndesign different models to implement the recommender. All the users take\nactions based on LLMs, and can freely evolve like in the real world. We present\nseveral case studies to demonstrate that the users in our simulator can indeed\nbehave in a reasonable manner as expected. Our project has been released at\nhttps://github.com/RUC-GSAI/YuLan-Rec.",
        "translated": ""
    },
    {
        "title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions",
        "url": "http://arxiv.org/abs/2306.02549v1",
        "pub_date": "2023-06-05",
        "summary": "This paper investigates the use of artificial intelligence chatbots for\npatient-specific question answering (QA) from clinical notes using several\nlarge language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google\nBard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and\ncoherence of the answers generated by each model using a 5-point Likert scale\non a set of patient-specific questions.",
        "translated": ""
    },
    {
        "title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models\n  with Same Tower Negatives",
        "url": "http://arxiv.org/abs/2306.02516v1",
        "pub_date": "2023-06-05",
        "summary": "Dual encoders have been used for retrieval tasks and representation learning\nwith good results. A standard way to train dual encoders is using a contrastive\nloss with in-batch negatives. In this work, we propose an improved contrastive\nlearning objective by adding queries or documents from the same encoder towers\nto the negatives, for which we name it as \"contrastive loss with SAMe TOwer\nNEgatives\" (SamToNe). By evaluating on question answering retrieval benchmarks\nfrom MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval\nbenchmarks (BEIR), we demonstrate that SamToNe can effectively improve the\nretrieval quality for both symmetric and asymmetric dual encoders. By directly\nprobing the embedding spaces of the two encoding towers via the t-SNE algorithm\n(van der Maaten and Hinton, 2008), we observe that SamToNe ensures the\nalignment between the embedding spaces from the two encoder towers. Based on\nthe analysis of the embedding distance distributions of the top-$1$ retrieved\nresults, we further explain the efficacy of the method from the perspective of\nregularisation.",
        "translated": ""
    },
    {
        "title": "I^3 Retriever: Incorporating Implicit Interaction in Pre-trained\n  Language Models for Passage Retrieval",
        "url": "http://arxiv.org/abs/2306.02371v1",
        "pub_date": "2023-06-04",
        "summary": "Passage retrieval is a fundamental task in many information systems, such as\nweb search and question answering, where both efficiency and effectiveness are\ncritical concerns. In recent years, neural retrievers based on pre-trained\nlanguage models (PLM), such as dual-encoders, have achieved huge success. Yet,\nstudies have found that the performance of dual-encoders are often limited due\nto the neglecting of the interaction information between queries and candidate\npassages. Therefore, various interaction paradigms have been proposed to\nimprove the performance of vanilla dual-encoders. Particularly, recent\nstate-of-the-art methods often introduce late-interaction during the model\ninference process. However, such late-interaction based methods usually bring\nextensive computation and storage cost on large corpus. Despite their\neffectiveness, the concern of efficiency and space footprint is still an\nimportant factor that limits the application of interaction-based neural\nretrieval models. To tackle this issue, we incorporate implicit interaction\ninto dual-encoders, and propose I^3 retriever. In particular, our implicit\ninteraction paradigm leverages generated pseudo-queries to simulate\nquery-passage interaction, which jointly optimizes with query and passage\nencoders in an end-to-end manner. It can be fully pre-computed and cached, and\nits inference process only involves simple dot product operation of the query\nvector and passage vector, which makes it as efficient as the vanilla dual\nencoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep\nLearning Datasets, demonstrating the I^3 retriever's superiority in terms of\nboth effectiveness and efficiency. Moreover, the proposed implicit interaction\nis compatible with special pre-training and knowledge distillation for passage\nretrieval, which brings a new state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "On Manipulating Signals of User-Item Graph: A Jacobi Polynomial-based\n  Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2306.03624v1",
        "pub_date": "2023-06-06",
        "summary": "Collaborative filtering (CF) is an important research direction in\nrecommender systems that aims to make recommendations given the information on\nuser-item interactions. Graph CF has attracted more and more attention in\nrecent years due to its effectiveness in leveraging high-order information in\nthe user-item bipartite graph for better recommendations. Specifically, recent\nstudies show the success of graph neural networks (GNN) for CF is attributed to\nits low-pass filtering effects. However, current researches lack a study of how\ndifferent signal components contributes to recommendations, and how to design\nstrategies to properly use them well. To this end, from the view of spectral\ntransformation, we analyze the important factors that a graph filter should\nconsider to achieve better performance. Based on the discoveries, we design\nJGCF, an efficient and effective method for CF based on Jacobi polynomial bases\nand frequency decomposition strategies. Extensive experiments on four widely\nused public datasets show the effectiveness and efficiency of the proposed\nmethods, which brings at most 27.06% performance gain on Alibaba-iFashion.\nBesides, the experimental results also show that JGCF is better at handling\nsparse datasets, which shows potential in making recommendations for cold-start\nusers.",
        "translated": ""
    },
    {
        "title": "Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR\n  Prediction in Taobao",
        "url": "http://arxiv.org/abs/2306.03527v1",
        "pub_date": "2023-06-06",
        "summary": "Click-Through Rate (CTR) prediction serves as a fundamental component in\nonline advertising. A common practice is to train a CTR model on advertisement\n(ad) impressions with user feedback. Since ad impressions are purposely\nselected by the model itself, their distribution differs from the inference\ndistribution and thus exhibits sample selection bias (SSB) that affects model\nperformance. Existing studies on SSB mainly employ sample re-weighting\ntechniques which suffer from high variance and poor model calibration. Another\nline of work relies on costly uniform data that is inadequate to train\nindustrial models. Thus mitigating SSB in industrial models with a\nuniform-data-free framework is worth exploring. Fortunately, many platforms\ndisplay mixed results of organic items (i.e., recommendations) and sponsored\nitems (i.e., ads) to users, where impressions of ads and recommendations are\nselected by different systems but share the same user decision rationales.\nBased on the above characteristics, we propose to leverage recommendations\nsamples as a free lunch to mitigate SSB for ads CTR model (Rec4Ad). After\nelaborating data augmentation, Rec4Ad learns disentangled representations with\nalignment and decorrelation modules for enhancement. When deployed in Taobao\ndisplay advertising system, Rec4Ad achieves substantial gains in key business\nmetrics, with a lift of up to +6.6\\% CTR and +2.9\\% RPM.",
        "translated": ""
    },
    {
        "title": "COPR: Consistency-Oriented Pre-Ranking for Online Advertising",
        "url": "http://arxiv.org/abs/2306.03516v1",
        "pub_date": "2023-06-06",
        "summary": "Cascading architecture has been widely adopted in large-scale advertising\nsystems to balance efficiency and effectiveness. In this architecture, the\npre-ranking model is expected to be a lightweight approximation of the ranking\nmodel, which handles more candidates with strict latency requirements. Due to\nthe gap in model capacity, the pre-ranking and ranking models usually generate\ninconsistent ranked results, thus hurting the overall system effectiveness. The\nparadigm of score alignment is proposed to regularize their raw scores to be\nconsistent. However, it suffers from inevitable alignment errors and error\namplification by bids when applied in online advertising. To this end, we\nintroduce a consistency-oriented pre-ranking framework for online advertising,\nwhich employs a chunk-based sampling module and a plug-and-play rank alignment\nmodule to explicitly optimize consistency of ECPM-ranked results. A $\\Delta\nNDCG$-based weighting mechanism is adopted to better distinguish the importance\nof inter-chunk samples in optimization. Both online and offline experiments\nhave validated the superiority of our framework. When deployed in Taobao\ndisplay advertising system, it achieves an improvement of up to +12.3\\% CTR and\n+5.6\\% RPM.",
        "translated": ""
    },
    {
        "title": "Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search",
        "url": "http://arxiv.org/abs/2306.03411v1",
        "pub_date": "2023-06-06",
        "summary": "Customers interacting with product search engines are increasingly\nformulating information-seeking queries. Frequently Asked Question (FAQ)\nretrieval aims to retrieve common question-answer pairs for a user query with\nquestion intent. Integrating FAQ retrieval in product search can not only\nempower users to make more informed purchase decisions, but also enhance user\nretention through efficient post-purchase support. Determining when an FAQ\nentry can satisfy a user's information need within product search, without\ndisrupting their shopping experience, represents an important challenge. We\npropose an intent-aware FAQ retrieval system consisting of (1) an intent\nclassifier that predicts when a user's information need can be answered by an\nFAQ; (2) a reformulation model that rewrites a query into a natural question.\nOffline evaluation demonstrates that our approach improves Hit@1 by 13% on\nretrieving ground-truth FAQs, while reducing latency by 95% compared to\nbaseline systems. These improvements are further validated by real user\nfeedback, where 71% of displayed FAQs on top of product search results received\nexplicit positive user feedback. Overall, our findings show promising\ndirections for integrating FAQ retrieval into product search at scale.",
        "translated": ""
    },
    {
        "title": "Computational Technologies for Fashion Recommendation: A Survey",
        "url": "http://arxiv.org/abs/2306.03395v1",
        "pub_date": "2023-06-06",
        "summary": "Fashion recommendation is a key research field in computational fashion\nresearch and has attracted considerable interest in the computer vision,\nmultimedia, and information retrieval communities in recent years. Due to the\ngreat demand for applications, various fashion recommendation tasks, such as\npersonalized fashion product recommendation, complementary (mix-and-match)\nrecommendation, and outfit recommendation, have been posed and explored in the\nliterature. The continuing research attention and advances impel us to look\nback and in-depth into the field for a better understanding. In this paper, we\ncomprehensively review recent research efforts on fashion recommendation from a\ntechnological perspective. We first introduce fashion recommendation at a macro\nlevel and analyse its characteristics and differences with general\nrecommendation tasks. We then clearly categorize different fashion\nrecommendation efforts into several sub-tasks and focus on each sub-task in\nterms of its problem formulation, research focus, state-of-the-art methods, and\nlimitations. We also summarize the datasets proposed in the literature for use\nin fashion recommendation studies to give readers a brief illustration.\nFinally, we discuss several promising directions for future research in this\nfield. Overall, this survey systematically reviews the development of fashion\nrecommendation research. It also discusses the current limitations and gaps\nbetween academic research and the real needs of the fashion industry. In the\nprocess, we offer a deep insight into how the fashion industry could benefit\nfrom fashion recommendation technologies. the computational technologies of\nfashion recommendation.",
        "translated": ""
    },
    {
        "title": "Tree based Progressive Regression Model for Watch-Time Prediction in\n  Short-video Recommendation",
        "url": "http://arxiv.org/abs/2306.03392v1",
        "pub_date": "2023-06-06",
        "summary": "An accurate prediction of watch time has been of vital importance to enhance\nuser engagement in video recommender systems. To achieve this, there are four\nproperties that a watch time prediction framework should satisfy: first,\ndespite its continuous value, watch time is also an ordinal variable and the\nrelative ordering between its values reflects the differences in user\npreferences. Therefore the ordinal relations should be reflected in watch time\npredictions. Second, the conditional dependence between the video-watching\nbehaviors should be captured in the model. For instance, one has to watch half\nof the video before he/she finishes watching the whole video. Third, modeling\nwatch time with a point estimation ignores the fact that models might give\nresults with high uncertainty and this could cause bad cases in recommender\nsystems. Therefore the framework should be aware of prediction uncertainty.\nForth, the real-life recommender systems suffer from severe bias amplifications\nthus an estimation without bias amplification is expected. Therefore we propose\nTPM for watch time prediction. Specifically, the ordinal ranks of watch time\nare introduced into TPM and the problem is decomposed into a series of\nconditional dependent classification tasks which are organized into a tree\nstructure. The expectation of watch time can be generated by traversing the\ntree and the variance of watch time predictions is explicitly introduced into\nthe objective function as a measurement for uncertainty. Moreover, we\nillustrate that backdoor adjustment can be seamlessly incorporated into TPM,\nwhich alleviates bias amplifications. Extensive offline evaluations have been\nconducted in public datasets and TPM have been deployed in a real-world video\napp Kuaishou with over 300 million DAUs. The results indicate that TPM\noutperforms state-of-the-art approaches and indeed improves video consumption\nsignificantly.",
        "translated": ""
    },
    {
        "title": "Towards Alleviating the Object Bias in Prompt Tuning-based Factual\n  Knowledge Extraction",
        "url": "http://arxiv.org/abs/2306.03378v1",
        "pub_date": "2023-06-06",
        "summary": "Many works employed prompt tuning methods to automatically optimize prompt\nqueries and extract the factual knowledge stored in Pretrained Language Models.\nIn this paper, we observe that the optimized prompts, including discrete\nprompts and continuous prompts, exhibit undesirable object bias. To handle this\nproblem, we propose a novel prompt tuning method called MeCoD. consisting of\nthree modules: Prompt Encoder, Object Equalization and Biased Object\nObstruction. Experimental results show that MeCoD can significantly reduce the\nobject bias and at the same time improve accuracy of factual knowledge\nextraction.",
        "translated": ""
    },
    {
        "title": "Construction d'un système de recommandation basé sur des contraintes\n  via des graphes de connaissances",
        "url": "http://arxiv.org/abs/2306.03247v1",
        "pub_date": "2023-06-05",
        "summary": "Knowledge graphs in RDF model entities and their relations using ontologies,\nand have gained popularity for information modeling. In recommender systems,\nknowledge graphs help represent more links and relationships between users and\nitems. Constraint-based recommender systems leverage deep recommendation\nknowledge to identify relevant suggestions. When combined with knowledge\ngraphs, they offer benefits in constraint sets. This paper explores a\nconstraint-based recommender system using RDF knowledge graphs for the vehicle\npurchase/sale domain. Our experiments demonstrate that the proposed approach\nefficiently identifies recommendations based on user preferences.",
        "translated": ""
    },
    {
        "title": "Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time\n  Light Patterns",
        "url": "http://arxiv.org/abs/2306.03195v1",
        "pub_date": "2023-06-05",
        "summary": "We introduce NightPulse, an interactive tool for Night-time light (NTL) data\nvisualization and analytics, which enables researchers and stakeholders to\nexplore and analyze NTL data with a user-friendly platform. Powered by\nefficient system architecture, NightPulse supports image segmentation,\nclustering, and change pattern detection to identify urban development and\nsprawl patterns. It captures temporal trends of NTL and semantics of cities,\nanswering questions about demographic factors, city boundaries, and unusual\ndifferences.",
        "translated": ""
    },
    {
        "title": "Personalized Federated Domain Adaptation for Item-to-Item Recommendation",
        "url": "http://arxiv.org/abs/2306.03191v1",
        "pub_date": "2023-06-05",
        "summary": "Item-to-Item (I2I) recommendation is an important function in most\nrecommendation systems, which generates replacement or complement suggestions\nfor a particular item based on its semantic similarities to other cataloged\nitems. Given that subsets of items in a recommendation system might be\nco-interacted with by the same set of customers, graph-based models, such as\ngraph neural networks (GNNs), provide a natural framework to combine, ingest\nand extract valuable insights from such high-order relational interactions\nbetween cataloged items, as well as their metadata features, as has been shown\nin many recent studies. However, learning GNNs effectively for I2I requires\ningesting a large amount of relational data, which might not always be\navailable, especially in new, emerging market segments. To mitigate this data\nbottleneck, we postulate that recommendation patterns learned from existing\nmature market segments (with private data) could be adapted to build effective\nwarm-start models for emerging ones. To achieve this, we propose and\ninvestigate a personalized federated modeling framework based on GNNs to\nsummarize, assemble and adapt recommendation patterns across market segments\nwith heterogeneous customer behaviors into effective local models. Our key\ncontribution is a personalized graph adaptation model that bridges the gap\nbetween recent literature on federated GNNs and (non-graph) personalized\nfederated learning, which either does not optimize for the adaptability of the\nfederated model or is restricted to local models with homogeneous\nparameterization, excluding GNNs with heterogeneous local graphs.",
        "translated": ""
    },
    {
        "title": "MarineVRS: Marine Video Retrieval System with Explainability via\n  Semantic Understanding",
        "url": "http://arxiv.org/abs/2306.04593v1",
        "pub_date": "2023-06-07",
        "summary": "Building a video retrieval system that is robust and reliable, especially for\nthe marine environment, is a challenging task due to several factors such as\ndealing with massive amounts of dense and repetitive data, occlusion,\nblurriness, low lighting conditions, and abstract queries. To address these\nchallenges, we present MarineVRS, a novel and flexible video retrieval system\ndesigned explicitly for the marine domain. MarineVRS integrates\nstate-of-the-art methods for visual and linguistic object representation to\nenable efficient and accurate search and analysis of vast volumes of underwater\nvideo data. In addition, unlike the conventional video retrieval system, which\nonly permits users to index a collection of images or videos and search using a\nfree-form natural language sentence, our retrieval system includes an\nadditional Explainability module that outputs the segmentation masks of the\nobjects that the input query referred to. This feature allows users to identify\nand isolate specific objects in the video footage, leading to more detailed\nanalysis and understanding of their behavior and movements. Finally, with its\nadaptability, explainability, accuracy, and scalability, MarineVRS is a\npowerful tool for marine researchers and scientists to efficiently and\naccurately process vast amounts of data and gain deeper insights into the\nbehavior and movements of marine species.",
        "translated": ""
    },
    {
        "title": "Constraint-based recommender system for crisis management simulations",
        "url": "http://arxiv.org/abs/2306.04553v1",
        "pub_date": "2023-06-07",
        "summary": "In the context of the evacuation of populations, some citizens/volunteers may\nwant and be able to participate in the evacuation of populations in difficulty\nby coming to lend a hand to emergency/evacuation vehicles with their own\nvehicles. One way of framing these impulses of solidarity would be to be able\nto list in real-time the citizens/volunteers available with their vehicles\n(land, sea, air, etc.), to be able to geolocate them according to the risk\nareas to be evacuated, and adding them to the evacuation/rescue vehicles.\nBecause it is difficult to propose an effective real-time operational system on\nthe field in a real crisis situation, in this work, we propose to add a module\nfor recommending driver/vehicle pairs (with their specificities) to a system of\ncrisis management simulation. To do that, we chose to model and develop an\nontology-supported constraint-based recommender system for crisis management\nsimulations.",
        "translated": ""
    },
    {
        "title": "Embracing Uncertainty: Adaptive Vague Preference Policy Learning for\n  Multi-round Conversational Recommendation",
        "url": "http://arxiv.org/abs/2306.04487v1",
        "pub_date": "2023-06-07",
        "summary": "Conversational recommendation systems (CRS) effectively address information\nasymmetry by dynamically eliciting user preferences through multi-turn\ninteractions. Existing CRS widely assumes that users have clear preferences.\nUnder this assumption, the agent will completely trust the user feedback and\ntreat the accepted or rejected signals as strong indicators to filter items and\nreduce the candidate space, which may lead to the problem of over-filtering.\nHowever, in reality, users' preferences are often vague and volatile, with\nuncertainty about their desires and changing decisions during interactions.\n  To address this issue, we introduce a novel scenario called Vague Preference\nMulti-round Conversational Recommendation (VPMCR), which considers users' vague\nand volatile preferences in CRS.VPMCR employs a soft estimation mechanism to\nassign a non-zero confidence score for all candidate items to be displayed,\nnaturally avoiding the over-filtering problem. In the VPMCR setting, we\nintroduce an solution called Adaptive Vague Preference Policy Learning (AVPPL),\nwhich consists of two main components: Uncertainty-aware Soft Estimation (USE)\nand Uncertainty-aware Policy Learning (UPL). USE estimates the uncertainty of\nusers' vague feedback and captures their dynamic preferences using a\nchoice-based preferences extraction module and a time-aware decaying strategy.\nUPL leverages the preference distribution estimated by USE to guide the\nconversation and adapt to changes in users' preferences to make recommendations\nor ask for attributes.\n  Our extensive experiments demonstrate the effectiveness of our method in the\nVPMCR scenario, highlighting its potential for practical applications and\nimproving the overall performance and applicability of CRS in real-world\nsettings, particularly for users with vague or dynamic preferences.",
        "translated": ""
    },
    {
        "title": "RD-Suite: A Benchmark for Ranking Distillation",
        "url": "http://arxiv.org/abs/2306.04455v1",
        "pub_date": "2023-06-07",
        "summary": "The distillation of ranking models has become an important topic in both\nacademia and industry. In recent years, several advanced methods have been\nproposed to tackle this problem, often leveraging ranking information from\nteacher rankers that is absent in traditional classification settings. To date,\nthere is no well-established consensus on how to evaluate this class of models.\nMoreover, inconsistent benchmarking on a wide range of tasks and datasets make\nit difficult to assess or invigorate advances in this field. This paper first\nexamines representative prior arts on ranking distillation, and raises three\nquestions to be answered around methodology and reproducibility. To that end,\nwe propose a systematic and unified benchmark, Ranking Distillation Suite\n(RD-Suite), which is a suite of tasks with 4 large real-world datasets,\nencompassing two major modalities (textual and numeric) and two applications\n(standard distillation and distillation transfer). RD-Suite consists of\nbenchmark results that challenge some of the common wisdom in the field, and\nthe release of datasets with teacher scores and evaluation scripts for future\nresearch. RD-Suite paves the way towards better understanding of ranking\ndistillation, facilities more research in this direction, and presents new\nchallenges.",
        "translated": ""
    },
    {
        "title": "Modeling Dual Period-Varying Preferences for Takeaway Recommendation",
        "url": "http://arxiv.org/abs/2306.04370v1",
        "pub_date": "2023-06-07",
        "summary": "Takeaway recommender systems, which aim to accurately provide stores that\noffer foods meeting users' interests, have served billions of users in our\ndaily life. Different from traditional recommendation, takeaway recommendation\nfaces two main challenges: (1) Dual Interaction-Aware Preference Modeling.\nTraditional recommendation commonly focuses on users' single preferences for\nitems while takeaway recommendation needs to comprehensively consider users'\ndual preferences for stores and foods. (2) Period-Varying Preference Modeling.\nConventional recommendation generally models continuous changes in users'\npreferences from a session-level or day-level perspective. However, in\npractical takeaway systems, users' preferences vary significantly during the\nmorning, noon, night, and late night periods of the day. To address these\nchallenges, we propose a Dual Period-Varying Preference modeling (DPVP) for\ntakeaway recommendation. Specifically, we design a dual interaction-aware\nmodule, aiming to capture users' dual preferences based on their interactions\nwith stores and foods. Moreover, to model various preferences in different time\nperiods of the day, we propose a time-based decomposition module as well as a\ntime-aware gating mechanism. Extensive offline and online experiments\ndemonstrate that our model outperforms state-of-the-art methods on real-world\ndatasets and it is capable of modeling the dual period-varying preferences.\nMoreover, our model has been deployed online on Meituan Takeaway platform,\nleading to an average improvement in GMV (Gross Merchandise Value) of 0.70%.",
        "translated": ""
    },
    {
        "title": "An Overview of Challenges in Egocentric Text-Video Retrieval",
        "url": "http://arxiv.org/abs/2306.04345v1",
        "pub_date": "2023-06-07",
        "summary": "Text-video retrieval contains various challenges, including biases coming\nfrom diverse sources. We highlight some of them supported by illustrations to\nopen a discussion. Besides, we address one of the biases, frame length bias,\nwith a simple method which brings a very incremental but promising increase. We\nconclude with future directions.",
        "translated": ""
    },
    {
        "title": "Phrase Retrieval for Open-Domain Conversational Question Answering with\n  Conversational Dependency Modeling via Contrastive Learning",
        "url": "http://arxiv.org/abs/2306.04293v1",
        "pub_date": "2023-06-07",
        "summary": "Open-Domain Conversational Question Answering (ODConvQA) aims at answering\nquestions through a multi-turn conversation based on a retriever-reader\npipeline, which retrieves passages and then predicts answers with them.\nHowever, such a pipeline approach not only makes the reader vulnerable to the\nerrors propagated from the retriever, but also demands additional effort to\ndevelop both the retriever and the reader, which further makes it slower since\nthey are not runnable in parallel. In this work, we propose a method to\ndirectly predict answers with a phrase retrieval scheme for a sequence of\nwords, reducing the conventional two distinct subtasks into a single one. Also,\nfor the first time, we study its capability for ODConvQA tasks. However, simply\nadopting it is largely problematic, due to the dependencies between previous\nand current turns in a conversation. To address this problem, we further\nintroduce a novel contrastive learning strategy, making sure to reflect\nprevious turns when retrieving the phrase for the current context, by\nmaximizing representational similarities of consecutive turns in a conversation\nwhile minimizing irrelevant conversational contexts. We validate our model on\ntwo ODConvQA datasets, whose experimental results show that it substantially\noutperforms the relevant baselines with the retriever-reader. Code is available\nat: https://github.com/starsuzi/PRO-ConvQA.",
        "translated": ""
    },
    {
        "title": "Set-to-Sequence Ranking-based Concept-aware Learning Path Recommendation",
        "url": "http://arxiv.org/abs/2306.04234v1",
        "pub_date": "2023-06-07",
        "summary": "With the development of the online education system, personalized education\nrecommendation has played an essential role. In this paper, we focus on\ndeveloping path recommendation systems that aim to generating and recommending\nan entire learning path to the given user in each session. Noticing that\nexisting approaches fail to consider the correlations of concepts in the path,\nwe propose a novel framework named Set-to-Sequence Ranking-based Concept-aware\nLearning Path Recommendation (SRC), which formulates the recommendation task\nunder a set-to-sequence paradigm. Specifically, we first design a concept-aware\nencoder module which can capture the correlations among the input learning\nconcepts. The outputs are then fed into a decoder module that sequentially\ngenerates a path through an attention mechanism that handles correlations\nbetween the learning and target concepts. Our recommendation policy is\noptimized by policy gradient. In addition, we also introduce an auxiliary\nmodule based on knowledge tracing to enhance the model's stability by\nevaluating students' learning effects on learning concepts. We conduct\nextensive experiments on two real-world public datasets and one industrial\ndataset, and the experimental results demonstrate the superiority and\neffectiveness of SRC. Code will be available at\nhttps://gitee.com/mindspore/models/tree/master/research/recommend/SRC.",
        "translated": ""
    },
    {
        "title": "SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet",
        "url": "http://arxiv.org/abs/2306.04148v1",
        "pub_date": "2023-06-07",
        "summary": "It is very important to access a rich music dataset that is useful in a wide\nvariety of applications. Currently, available datasets are mostly focused on\nstoring vocal or instrumental recording data and ignoring the requirement of\nits visual representation and retrieval. This paper attempts to build an\nXML-based public dataset, called SANGEET, that stores comprehensive information\nof Hindustani Sangeet (North Indian Classical Music) compositions written by\nfamous musicologist Pt. Vishnu Narayan Bhatkhande. SANGEET preserves all the\nrequired information of any given composition including metadata, structural,\nnotational, rhythmic, and melodic information in a standardized way for easy\nand efficient storage and extraction of musical information. The dataset is\nintended to provide the ground truth information for music information research\ntasks, thereby supporting several data-driven analysis from a machine learning\nperspective. We present the usefulness of the dataset by demonstrating its\napplication on music information retrieval using XQuery, visualization through\nOmenad rendering system. Finally, we propose approaches to transform the\ndataset for performing statistical and machine learning tasks for a better\nunderstanding of Hindustani Sangeet. The dataset can be found at\nhttps://github.com/cmisra/Sangeet.",
        "translated": ""
    },
    {
        "title": "Answering Compositional Queries with Set-Theoretic Embeddings",
        "url": "http://arxiv.org/abs/2306.04133v1",
        "pub_date": "2023-06-07",
        "summary": "The need to compactly and robustly represent item-attribute relations arises\nin many important tasks, such as faceted browsing and recommendation systems. A\npopular machine learning approach for this task denotes that an item has an\nattribute by a high dot-product between vectors for the item and attribute -- a\nrepresentation that is not only dense, but also tends to correct noisy and\nincomplete data. While this method works well for queries retrieving items by a\nsingle attribute (such as \\emph{movies that are comedies}), we find that vector\nembeddings do not so accurately support compositional queries (such as movies\nthat are comedies and British but not romances). To address these set-theoretic\ncompositions, this paper proposes to replace vectors with box embeddings, a\nregion-based representation that can be thought of as learnable Venn diagrams.\nWe introduce a new benchmark dataset for compositional queries, and present\nexperiments and analysis providing insights into the behavior of both. We find\nthat, while vector and box embeddings are equally suited to single attribute\nqueries, for compositional queries box embeddings provide substantial\nadvantages over vectors, particularly at the moderate and larger retrieval set\nsizes that are most useful for users' search and browsing.",
        "translated": ""
    },
    {
        "title": "Safe Collaborative Filtering",
        "url": "http://arxiv.org/abs/2306.05292v1",
        "pub_date": "2023-06-08",
        "summary": "Excellent tail performance is crucial for modern machine learning tasks, such\nas algorithmic fairness, class imbalance, and risk-sensitive decision making,\nas it ensures the effective handling of challenging samples within a dataset.\nTail performance is also a vital determinant of success for personalised\nrecommender systems to reduce the risk of losing users with low satisfaction.\nThis study introduces a \"safe\" collaborative filtering method that prioritises\nrecommendation quality for less-satisfied users rather than focusing on the\naverage performance. Our approach minimises the conditional value at risk\n(CVaR), which represents the average risk over the tails of users' loss. To\novercome computational challenges for web-scale recommender systems, we develop\na robust yet practical algorithm that extends the most scalable method,\nimplicit alternating least squares (iALS). Empirical evaluation on real-world\ndatasets demonstrates the excellent tail performance of our approach while\nmaintaining competitive computational efficiency.",
        "translated": ""
    },
    {
        "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
        "url": "http://arxiv.org/abs/2306.05212v1",
        "pub_date": "2023-06-08",
        "summary": "Although Large Language Models (LLMs) have demonstrated extraordinary\ncapabilities in many domains, they still have a tendency to hallucinate and\ngenerate fictitious responses to user requests. This problem can be alleviated\nby augmenting LLMs with information retrieval (IR) systems (also known as\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\nfactual texts in response to user input according to the relevant content\nretrieved by IR systems from external corpora as references. In addition, by\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\nquestions that cannot be answered by solely relying on the world knowledge\nstored in parameters. To support research in this area and facilitate the\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\nto help researchers and users build their customized in-domain LLM-based\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\nprovides more plug-and-play modules to support better interaction between IR\nsystems and LLMs, including {request rewriting, document retrieval, passage\nextraction, answer generation, and fact checking} modules. Our toolkit is\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",
        "translated": ""
    },
    {
        "title": "Controllable Multi-Objective Re-ranking with Policy Hypernetworks",
        "url": "http://arxiv.org/abs/2306.05118v1",
        "pub_date": "2023-06-08",
        "summary": "Multi-stage ranking pipelines have become widely used strategies in modern\nrecommender systems, where the final stage aims to return a ranked list of\nitems that balances a number of requirements such as user preference,\ndiversity, novelty etc. Linear scalarization is arguably the most widely used\ntechnique to merge multiple requirements into one optimization objective, by\nsumming up the requirements with certain preference weights. Existing\nfinal-stage ranking methods often adopt a static model where the preference\nweights are determined during offline training and kept unchanged during online\nserving. Whenever a modification of the preference weights is needed, the model\nhas to be re-trained, which is time and resources inefficient. Meanwhile, the\nmost appropriate weights may vary greatly for different groups of targeting\nusers or at different time periods (e.g., during holiday promotions). In this\npaper, we propose a framework called controllable multi-objective re-ranking\n(CMR) which incorporates a hypernetwork to generate parameters for a re-ranking\nmodel according to different preference weights. In this way, CMR is enabled to\nadapt the preference weights according to the environment changes in an online\nmanner, without retraining the models. Moreover, we classify practical\nbusiness-oriented tasks into four main categories and seamlessly incorporate\nthem in a new proposed re-ranking model based on an Actor-Evaluator framework,\nwhich serves as a reliable real-world testbed for CMR. Offline experiments\nbased on the dataset collected from Taobao App showed that CMR improved several\npopular re-ranking models by using them as underlying models. Online A/B tests\nalso demonstrated the effectiveness and trustworthiness of CMR.",
        "translated": ""
    },
    {
        "title": "Attention Weighted Mixture of Experts with Contrastive Learning for\n  Personalized Ranking in E-commerce",
        "url": "http://arxiv.org/abs/2306.05011v1",
        "pub_date": "2023-06-08",
        "summary": "Ranking model plays an essential role in e-commerce search and\nrecommendation. An effective ranking model should give a personalized ranking\nlist for each user according to the user preference. Existing algorithms\nusually extract a user representation vector from the user behavior sequence,\nthen feed the vector into a feed-forward network (FFN) together with other\nfeatures for feature interactions, and finally produce a personalized ranking\nscore. Despite tremendous progress in the past, there is still room for\nimprovement. Firstly, the personalized patterns of feature interactions for\ndifferent users are not explicitly modeled. Secondly, most of existing\nalgorithms have poor personalized ranking results for long-tail users with few\nhistorical behaviors due to the data sparsity. To overcome the two challenges,\nwe propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive\nlearning for personalized ranking. Firstly, AW-MoE leverages the MoE framework\nto capture personalized feature interactions for different users. To model the\nuser preference, the user behavior sequence is simultaneously fed into expert\nnetworks and the gate network. Within the gate network, one gate unit and one\nactivation unit are designed to adaptively learn the fine-grained activation\nvector for experts using an attention mechanism. Secondly, a random masking\nstrategy is applied to the user behavior sequence to simulate long-tail users,\nand an auxiliary contrastive loss is imposed to the output of the gate network\nto improve the model generalization for these users. This is validated by a\nhigher performance gain on the long-tail user test set. Experiment results on a\nJD real production dataset and a public dataset demonstrate the effectiveness\nof AW-MoE, which significantly outperforms state-of-art methods. Notably,\nAW-MoE has been successfully deployed in the JD e-commerce search engine, ...",
        "translated": ""
    },
    {
        "title": "Unified Embedding Based Personalized Retrieval in Etsy Search",
        "url": "http://arxiv.org/abs/2306.04833v1",
        "pub_date": "2023-06-07",
        "summary": "Embedding-based neural retrieval is a prevalent approach to address the\nsemantic gap problem which often arises in product search on tail queries. In\ncontrast, popular queries typically lack context and have a broad intent where\nadditional context from users historical interaction can be helpful. In this\npaper, we share our novel approach to address both: the semantic gap problem\nfollowed by an end to end trained model for personalized semantic retrieval. We\npropose learning a unified embedding model incorporating graph, transformer and\nterm-based embeddings end to end and share our design choices for optimal\ntradeoff between performance and efficiency. We share our learnings in feature\nengineering, hard negative sampling strategy, and application of transformer\nmodel, including a novel pre-training strategy and other tricks for improving\nsearch relevance and deploying such a model at industry scale. Our personalized\nretrieval model significantly improves the overall search experience, as\nmeasured by a 5.58% increase in search purchase rate and a 2.63% increase in\nsite-wide conversion rate, aggregated across multiple A/B tests - on live\ntraffic.",
        "translated": ""
    },
    {
        "title": "SKG: A Versatile Information Retrieval and Analysis Framework for\n  Academic Papers with Semantic Knowledge Graphs",
        "url": "http://arxiv.org/abs/2306.04758v1",
        "pub_date": "2023-06-07",
        "summary": "The number of published research papers has experienced exponential growth in\nrecent years, which makes it crucial to develop new methods for efficient and\nversatile information extraction and knowledge discovery. To address this need,\nwe propose a Semantic Knowledge Graph (SKG) that integrates semantic concepts\nfrom abstracts and other meta-information to represent the corpus. The SKG can\nsupport various semantic queries in academic literature thanks to the high\ndiversity and rich information content stored within. To extract knowledge from\nunstructured text, we develop a Knowledge Extraction Module that includes a\nsemi-supervised pipeline for entity extraction and entity normalization. We\nalso create an ontology to integrate the concepts with other meta information,\nenabling us to build the SKG. Furthermore, we design and develop a dataflow\nsystem that demonstrates how to conduct various semantic queries flexibly and\ninteractively over the SKG. To demonstrate the effectiveness of our approach,\nwe conduct the research based on the visualization literature and provide\nreal-world use cases to show the usefulness of the SKG.\n  The dataset and codes for this work are available at\nhttps://osf.io/aqv8p/?view_only=2c26b36e3e3941ce999df47e4616207f.",
        "translated": ""
    },
    {
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey",
        "url": "http://arxiv.org/abs/2306.05817v1",
        "pub_date": "2023-06-09",
        "summary": "Recommender systems (RS) play important roles to match users' information\nneeds for Internet applications. In natural language processing (NLP) domains,\nlarge language model (LLM) has shown astonishing emergent abilities (e.g.,\ninstruction following, reasoning), thus giving rise to the promising research\ndirection of adapting LLM to RS for performance enhancements and user\nexperience improvements. In this paper, we conduct a comprehensive survey on\nthis research direction from an application-oriented view. We first summarize\nexisting research works from two orthogonal perspectives: where and how to\nadapt LLM to RS. For the \"WHERE\" question, we discuss the roles that LLM could\nplay in different stages of the recommendation pipeline, i.e., feature\nengineering, feature encoder, scoring/ranking function, and pipeline\ncontroller. For the \"HOW\" question, we investigate the training and inference\nstrategies, resulting in two fine-grained taxonomy criteria, i.e., whether to\ntune LLMs or not, and whether to involve conventional recommendation model\n(CRM) for inference. Detailed analysis and general development trajectories are\nprovided for both questions, respectively. Then, we highlight key challenges in\nadapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and\nethics. Finally, we summarize the survey and discuss the future prospects. We\nalso actively maintain a GitHub repository for papers and other related\nresources in this rising direction:\n$\\href{https://github.com/CHIANGEL/Awesome-LLM-for-RecSys}{[GitHub\\;Link]}$.",
        "translated": ""
    },
    {
        "title": "Interactive Explanation with Varying Level of Details in an Explainable\n  Scientific Literature Recommender System",
        "url": "http://arxiv.org/abs/2306.05809v1",
        "pub_date": "2023-06-09",
        "summary": "Explainable recommender systems (RS) have traditionally followed a\none-size-fits-all approach, delivering the same explanation level of detail to\neach user, without considering their individual needs and goals. Further,\nexplanations in RS have so far been presented mostly in a static and\nnon-interactive manner. To fill these research gaps, we aim in this paper to\nadopt a user-centered, interactive explanation model that provides explanations\nwith different levels of detail and empowers users to interact with, control,\nand personalize the explanations based on their needs and preferences. We\nfollowed a user-centered approach to design interactive explanations with three\nlevels of detail (basic, intermediate, and advanced) and implemented them in\nthe transparent Recommendation and Interest Modeling Application (RIMA). We\nconducted a qualitative user study (N=14) to investigate the impact of\nproviding interactive explanations with varying level of details on the users'\nperception of the explainable RS. Our study showed qualitative evidence that\nfostering interaction and giving users control in deciding which explanation\nthey would like to see can meet the demands of users with different needs,\npreferences, and goals, and consequently can have positive effects on different\ncrucial aspects in explainable recommendation, including transparency, trust,\nsatisfaction, and user experience.",
        "translated": ""
    },
    {
        "title": "RankFormer: Listwise Learning-to-Rank Using Listwide Labels",
        "url": "http://arxiv.org/abs/2306.05808v1",
        "pub_date": "2023-06-09",
        "summary": "Web applications where users are presented with a limited selection of items\nhave long employed ranking models to put the most relevant results first. Any\nfeedback received from users is typically assumed to reflect a relative\njudgement on the utility of items, e.g. a user clicking on an item only implies\nit is better than items not clicked in the same ranked list. Hence, the\nobjectives optimized in Learning-to-Rank (LTR) tend to be pairwise or listwise.\n  Yet, by only viewing feedback as relative, we neglect the user's absolute\nfeedback on the list's overall quality, e.g. when no items in the selection are\nclicked. We thus reconsider the standard LTR paradigm and argue the benefits of\nlearning from this listwide signal. To this end, we propose the RankFormer as\nan architecture that, with a Transformer at its core, can jointly optimize a\nnovel listwide assessment objective and a traditional listwise LTR objective.\n  We simulate implicit feedback on public datasets and observe that the\nRankFormer succeeds in benefitting from listwide signals. Additionally, we\nconduct experiments in e-commerce on Amazon Search data and find the RankFormer\nto be superior to all baselines offline. An online experiment shows that\nknowledge distillation can be used to find immediate practical use for the\nRankFormer.",
        "translated": ""
    },
    {
        "title": "Customizing General-Purpose Foundation Models for Medical Report\n  Generation",
        "url": "http://arxiv.org/abs/2306.05642v1",
        "pub_date": "2023-06-09",
        "summary": "Medical caption prediction which can be regarded as a task of medical report\ngeneration (MRG), requires the automatic generation of coherent and accurate\ncaptions for the given medical images. However, the scarcity of labelled\nmedical image-report pairs presents great challenges in the development of deep\nand large-scale neural networks capable of harnessing the potential artificial\ngeneral intelligence power like large language models (LLMs). In this work, we\npropose customizing off-the-shelf general-purpose large-scale pre-trained\nmodels, i.e., foundation models (FMs), in computer vision and natural language\nprocessing with a specific focus on medical report generation. Specifically,\nfollowing BLIP-2, a state-of-the-art vision-language pre-training approach, we\nintroduce our encoder-decoder-based MRG model. This model utilizes a\nlightweight query Transformer to connect two FMs: the giant vision Transformer\nEVA-ViT-g and a bilingual LLM trained to align with human intentions (referred\nto as ChatGLM-6B). Furthermore, we conduct ablative experiments on the\ntrainable components of the model to identify the crucial factors for effective\ntransfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn\nmedical image representations, followed by parameter-efficient training of\nChatGLM-6B to capture the writing styles of medical reports, is essential for\nachieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and\nthe 2nd, respectively, out of 13 participating teams, based on the BERTScore\nand ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction\nTask competition.",
        "translated": ""
    },
    {
        "title": "Bayesian Knowledge-driven Critiquing with Indirect Evidence",
        "url": "http://arxiv.org/abs/2306.05636v1",
        "pub_date": "2023-06-09",
        "summary": "Conversational recommender systems (CRS) enhance the expressivity and\npersonalization of recommendations through multiple turns of user-system\ninteraction. Critiquing is a well-known paradigm for CRS that allows users to\niteratively refine recommendations by providing feedback about attributes of\nrecommended items. While existing critiquing methodologies utilize direct\nattributes of items to address user requests such as 'I prefer Western movies',\nthe opportunity of incorporating richer contextual and side information about\nitems stored in Knowledge Graphs (KG) into the critiquing paradigm has been\noverlooked. Employing this substantial knowledge together with a\nwell-established reasoning methodology paves the way for critique-based\nrecommenders to allow for complex knowledge-based feedback (e.g., 'I like\nmovies featuring war side effects on veterans') which may arise in natural\nuser-system conversations. In this work, we aim to increase the flexibility of\ncritique-based recommendation by integrating KGs and propose a novel Bayesian\ninference framework that enables reasoning with relational knowledge-based\nfeedback. We study and formulate the framework considering a Gaussian\nlikelihood and evaluate it on two well-known recommendation datasets with KGs.\nOur evaluations demonstrate the effectiveness of our framework in leveraging\nindirect KG-based feedback (i.e., preferred relational properties of items\nrather than preferred items themselves), often improving personalized\nrecommendations over a one-shot recommender by more than 15%. This work enables\na new paradigm for using rich knowledge content and reasoning over indirect\nevidence as a mechanism for critiquing interactions with CRS.",
        "translated": ""
    },
    {
        "title": "CLC: Cluster Assignment via Contrastive Representation Learning",
        "url": "http://arxiv.org/abs/2306.05439v1",
        "pub_date": "2023-06-08",
        "summary": "Clustering remains an important and challenging task of grouping samples into\nclusters without manual annotations. Recent works have achieved excellent\nresults on small datasets by performing clustering on feature representations\nlearned from self-supervised learning. However, for datasets with a large\nnumber of clusters, such as ImageNet, current methods still can not achieve\nhigh clustering performance. In this paper, we propose Contrastive\nLearning-based Clustering (CLC), which uses contrastive learning to directly\nlearn cluster assignment. We decompose the representation into two parts: one\nencodes the categorical information under an equipartition constraint, and the\nother captures the instance-wise factors. We propose a contrastive loss using\nboth parts of the representation. We theoretically analyze the proposed\ncontrastive loss and reveal that CLC sets different weights for the negative\nsamples while learning cluster assignments. Further gradient analysis shows\nthat the larger weights tend to focus more on the hard negative samples.\nTherefore, the proposed loss has high expressiveness that enables us to\nefficiently learn cluster assignments. Experimental evaluation shows that CLC\nachieves overall state-of-the-art or highly competitive clustering performance\non multiple benchmark datasets. In particular, we achieve 53.4% accuracy on the\nfull ImageNet dataset and outperform existing methods by large margins (+\n10.2%).",
        "translated": ""
    },
    {
        "title": "Weakly-Supervised Scientific Document Classification via\n  Retrieval-Augmented Multi-Stage Training",
        "url": "http://arxiv.org/abs/2306.07193v1",
        "pub_date": "2023-06-12",
        "summary": "Scientific document classification is a critical task for a wide range of\napplications, but the cost of obtaining massive amounts of human-labeled data\ncan be prohibitive. To address this challenge, we propose a weakly-supervised\napproach for scientific document classification using label names only. In\nscientific domains, label names often include domain-specific concepts that may\nnot appear in the document corpus, making it difficult to match labels and\ndocuments precisely. To tackle this issue, we propose WANDER, which leverages\ndense retrieval to perform matching in the embedding space to capture the\nsemantics of label names. We further design the label name expansion module to\nenrich the label name representations. Lastly, a self-training step is used to\nrefine the predictions. The experiments on three datasets show that WANDER\noutperforms the best baseline by 11.9% on average. Our code will be published\nat https://github.com/ritaranx/wander.",
        "translated": ""
    },
    {
        "title": "Fair Learning to Rank with Distribution-free Risk Control",
        "url": "http://arxiv.org/abs/2306.07188v1",
        "pub_date": "2023-06-12",
        "summary": "Learning to Rank (LTR) methods are vital in online economies, affecting users\nand item providers. Fairness in LTR models is crucial to allocate exposure\nproportionally to item relevance. The deterministic ranking model can lead to\nunfair exposure distribution when items with the same relevance receive\nslightly different scores. Stochastic LTR models, incorporating the\nPlackett-Luce (PL) model, address fairness issues but have limitations in\ncomputational cost and performance guarantees. To overcome these limitations,\nwe propose FairLTR-RC, a novel post-hoc model-agnostic method. FairLTR-RC\nleverages a pretrained scoring function to create a stochastic LTR model,\neliminating the need for expensive training. Furthermore, FairLTR-RC provides\nfinite-sample guarantees on a user-specified utility using distribution-free\nrisk control framework. By additionally incorporating the Thresholded PL (TPL)\nmodel, we are able to achieve an effective trade-off between utility and\nfairness. Experimental results on several benchmark datasets demonstrate that\nFairLTR-RC significantly improves fairness in widely-used deterministic LTR\nmodels while guaranteeing a specified level of utility.",
        "translated": ""
    },
    {
        "title": "Video-to-Music Recommendation using Temporal Alignment of Segments",
        "url": "http://arxiv.org/abs/2306.07187v1",
        "pub_date": "2023-06-12",
        "summary": "We study cross-modal recommendation of music tracks to be used as soundtracks\nfor videos. This problem is known as the music supervision task. We build on a\nself-supervised system that learns a content association between music and\nvideo. In addition to the adequacy of content, adequacy of structure is crucial\nin music supervision to obtain relevant recommendations. We propose a novel\napproach to significantly improve the system's performance using\nstructure-aware recommendation. The core idea is to consider not only the full\naudio-video clips, but rather shorter segments for training and inference. We\nfind that using semantic segments and ranking the tracks according to sequence\nalignment costs significantly improves the results. We investigate the impact\nof different ranking metrics and segmentation methods.",
        "translated": ""
    },
    {
        "title": "Adversarial Constrained Bidding via Minimax Regret Optimization with\n  Causality-Aware Reinforcement Learning",
        "url": "http://arxiv.org/abs/2306.07106v1",
        "pub_date": "2023-06-12",
        "summary": "The proliferation of the Internet has led to the emergence of online\nadvertising, driven by the mechanics of online auctions. In these repeated\nauctions, software agents participate on behalf of aggregated advertisers to\noptimize for their long-term utility. To fulfill the diverse demands, bidding\nstrategies are employed to optimize advertising objectives subject to different\nspending constraints. Existing approaches on constrained bidding typically rely\non i.i.d. train and test conditions, which contradicts the adversarial nature\nof online ad markets where different parties possess potentially conflicting\nobjectives. In this regard, we explore the problem of constrained bidding in\nadversarial bidding environments, which assumes no knowledge about the\nadversarial factors. Instead of relying on the i.i.d. assumption, our insight\nis to align the train distribution of environments with the potential test\ndistribution meanwhile minimizing policy regret. Based on this insight, we\npropose a practical Minimax Regret Optimization (MiRO) approach that\ninterleaves between a teacher finding adversarial environments for tutoring and\na learner meta-learning its policy over the given distribution of environments.\nIn addition, we pioneer to incorporate expert demonstrations for learning\nbidding strategies. Through a causality-aware policy design, we improve upon\nMiRO by distilling knowledge from the experts. Extensive experiments on both\nindustrial data and synthetic data show that our method, MiRO with\nCausality-aware reinforcement Learning (MiROCL), outperforms prior methods by\nover 30%.",
        "translated": ""
    },
    {
        "title": "Imbalanced Multi-label Classification for Business-related Text with\n  Moderately Large Label Spaces",
        "url": "http://arxiv.org/abs/2306.07046v1",
        "pub_date": "2023-06-12",
        "summary": "In this study, we compared the performance of four different methods for\nmulti label text classification using a specific imbalanced business dataset.\nThe four methods we evaluated were fine tuned BERT, Binary Relevance,\nClassifier Chains, and Label Powerset. The results show that fine tuned BERT\noutperforms the other three methods by a significant margin, achieving high\nvalues of accuracy, F1 Score, Precision, and Recall. Binary Relevance also\nperforms well on this dataset, while Classifier Chains and Label Powerset\ndemonstrate relatively poor performance. These findings highlight the\neffectiveness of fine tuned BERT for multi label text classification tasks, and\nsuggest that it may be a useful tool for businesses seeking to analyze complex\nand multifaceted texts.",
        "translated": ""
    },
    {
        "title": "Skellam Rank: Fair Learning to Rank Algorithm Based on Poisson Process\n  and Skellam Distribution for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.06607v1",
        "pub_date": "2023-06-11",
        "summary": "Recommender system is a widely adopted technology in a diversified class of\nproduct lines. Modern day recommender system approaches include matrix\nfactorization, learning to rank and deep learning paradigms, etc. Unlike many\nother approaches, learning to rank builds recommendation results based on\nmaximization of the probability of ranking orders. There are intrinsic issues\nrelated to recommender systems such as selection bias, exposure bias and\npopularity bias. In this paper, we propose a fair recommender system algorithm\nthat uses Poisson process and Skellam distribution. We demonstrate in our\nexperiments that our algorithm is competitive in accuracy metrics and far more\nsuperior than other modern algorithms in fairness metrics.",
        "translated": ""
    },
    {
        "title": "Mean-Variance Efficient Collaborative Filtering for Stock Recommendation",
        "url": "http://arxiv.org/abs/2306.06590v1",
        "pub_date": "2023-06-11",
        "summary": "The rise of FinTech has transformed financial services onto online platforms,\nyet stock investment recommender systems have received limited attention\ncompared to other industries. Personalized stock recommendations can\nsignificantly impact customer engagement and satisfaction within the industry.\nHowever, traditional investment recommendations focus on high-return stocks or\nhighly diversified portfolios based on the modern portfolio theory, often\nneglecting user preferences. On the other hand, collaborative filtering (CF)\nmethods also may not be directly applicable to stock recommendations, because\nit is inappropriate to just recommend stocks that users like. The key is to\noptimally blend users preference with the portfolio theory. However, research\non stock recommendations within the recommender system domain remains\ncomparatively limited, and no existing model considers both the preference of\nusers and the risk-return characteristics of stocks. In this regard, we propose\na mean-variance efficient collaborative filtering (MVECF) model for stock\nrecommendations that consider both aspects. Our model is specifically designed\nto improve the pareto optimality (mean-variance efficiency) in a trade-off\nbetween the risk (variance of return) and return (mean return) by systemically\nhandling uncertainties in stock prices. Such improvements are incorporated into\nthe MVECF model using regularization, and the model is restructured to fit into\nthe ordinary matrix factorization scheme to boost computational efficiency.\nExperiments on real-world fund holdings data show that our model can increase\nthe mean-variance efficiency of suggested portfolios while sacrificing just a\nsmall amount of mean average precision and recall. Finally, we further show\nMVECF is easily applicable to the state-of-the-art graph-based ranking models.",
        "translated": ""
    },
    {
        "title": "GuP: Fast Subgraph Matching by Guard-based Pruning",
        "url": "http://arxiv.org/abs/2306.06557v1",
        "pub_date": "2023-06-11",
        "summary": "Subgraph matching, which finds subgraphs isomorphic to a query, is the key to\ninformation retrieval from data represented as a graph. To avoid redundant\nexploration in the data, existing methods restrict the search space by\nextracting candidate vertices and candidate edges that may constitute\nisomorphic subgraphs. However, it still requires expensive computation because\ncandidate vertices induce many subgraphs that are not isomorphic to the query.\nIn this paper, we propose GuP, a subgraph matching algorithm with pruning based\non guards. Guards are a pattern of intermediate search states that never find\nisomorphic subgraphs. GuP attaches a guard on each candidate vertex and edge\nand filters out them adaptively to the search state. The experimental results\nshowed that GuP can efficiently solve various queries, including those that the\nstate-of-the-art methods could not solve in practical time.",
        "translated": ""
    },
    {
        "title": "Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain\n  Recommendation in an AI Assistant Application",
        "url": "http://arxiv.org/abs/2306.06302v1",
        "pub_date": "2023-06-09",
        "summary": "Recommender systems have found significant commercial success but still\nstruggle with integrating new users. Since users often interact with content in\ndifferent domains, it is possible to leverage a user's interactions in previous\ndomains to improve that user's recommendations in a new one (multi-domain\nrecommendation). A separate research thread on knowledge graph enhancement uses\nexternal knowledge graphs to improve single domain recommendations (knowledge\ngraph enhancement). Both research threads incorporate related information to\nimprove predictions in a new domain. We propose in this work to unify these\napproaches: Using information from interactions in other domains as well as\nexternal knowledge graphs to make predictions in a new domain that would be\nimpossible with either information source alone. We apply these ideas to a\ndataset derived from millions of users' requests for content across three\ndomains (videos, music, and books) in a live virtual assistant application. We\ndemonstrate the advantage of combining knowledge graph enhancement with\nprevious multi-domain recommendation techniques to provide better overall\nrecommendations as well as for better recommendations on new users of a domain.",
        "translated": ""
    },
    {
        "title": "Open Data on GitHub: Unlocking the Potential of AI",
        "url": "http://arxiv.org/abs/2306.06191v1",
        "pub_date": "2023-06-09",
        "summary": "GitHub is the world's largest platform for collaborative software\ndevelopment, with over 100 million users. GitHub is also used extensively for\nopen data collaboration, hosting more than 800 million open data files,\ntotaling 142 terabytes of data. This study highlights the potential of open\ndata on GitHub and demonstrates how it can accelerate AI research. We analyze\nthe existing landscape of open data on GitHub and the patterns of how users\nshare datasets. Our findings show that GitHub is one of the largest hosts of\nopen data in the world and has experienced an accelerated growth of open data\nassets over the past four years. By examining the open data landscape on\nGitHub, we aim to empower users and organizations to leverage existing open\ndatasets and improve their discoverability -- ultimately contributing to the\nongoing AI revolution to help address complex societal issues. We release the\nthree datasets that we have collected to support this analysis as open datasets\nat https://github.com/github/open-data-on-github.",
        "translated": ""
    },
    {
        "title": "Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal\n  Rank with Lexicographic Precision",
        "url": "http://arxiv.org/abs/2306.07908v1",
        "pub_date": "2023-06-13",
        "summary": "Across a variety of ranking tasks, researchers use reciprocal rank to measure\nthe effectiveness for users interested in exactly one relevant item. Despite\nits widespread use, evidence suggests that reciprocal rank is brittle when\ndiscriminating between systems. This brittleness, in turn, is compounded in\nmodern evaluation settings where current, high-precision systems may be\ndifficult to distinguish. We address the lack of sensitivity of reciprocal rank\nby introducing and connecting it to the concept of best-case retrieval, an\nevaluation method focusing on assessing the quality of a ranking for the most\nsatisfied possible user across possible recall requirements. This perspective\nallows us to generalize reciprocal rank and define a new preference-based\nevaluation we call lexicographic precision or lexiprecision. By mathematical\nconstruction, we ensure that lexiprecision preserves differences detected by\nreciprocal rank, while empirically improving sensitivity and robustness across\na broad set of retrieval and recommendation tasks.",
        "translated": ""
    },
    {
        "title": "ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support\n  Lateral Reading",
        "url": "http://arxiv.org/abs/2306.07875v1",
        "pub_date": "2023-06-13",
        "summary": "With the rapid growth and spread of online misinformation, people need tools\nto help them evaluate the credibility and accuracy of online information.\nLateral reading, a strategy that involves cross-referencing information with\nmultiple sources, may be an effective approach to achieving this goal. In this\npaper, we present ReadProbe, a tool to support lateral reading, powered by\ngenerative large language models from OpenAI and the Bing search engine. Our\ntool is able to generate useful questions for lateral reading, scour the web\nfor relevant documents, and generate well-attributed answers to help people\nbetter evaluate online information. We made a web-based application to\ndemonstrate how ReadProbe can help reduce the risk of being misled by false\ninformation. The code is available at\nhttps://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won\nthe first prize in a national AI misinformation hackathon.",
        "translated": ""
    },
    {
        "title": "KuaiSAR: A Unified Search And Recommendation Dataset",
        "url": "http://arxiv.org/abs/2306.07705v1",
        "pub_date": "2023-06-13",
        "summary": "The confluence of Search and Recommendation services is a vital aspect of\nonline content platforms like Kuaishou and TikTok. The integration of S&amp;R\nmodeling is a highly intuitive approach adopted by industry practitioners.\nHowever, there is a noticeable lack of research conducted in this area within\nthe academia, primarily due to the absence of publicly available datasets.\nConsequently, a substantial gap has emerged between academia and industry\nregarding research endeavors in this field. To bridge this gap, we introduce\nthe first large-scale, real-world dataset KuaiSAR of integrated Search And\nRecommendation behaviors collected from Kuaishou, a leading short-video app in\nChina with over 300 million daily active users. Previous research in this field\nhas predominantly employed publicly available datasets that are semi-synthetic\nand simulated, with artificially fabricated search behaviors. Distinct from\nprevious datasets, KuaiSAR records genuine user behaviors, the occurrence of\neach interaction within either search or recommendation service, and the users'\ntransitions between the two services. This work aids in joint modeling of S&amp;R,\nand the utilization of search data for recommenders (and recommendation data\nfor search engines). Additionally, due to the diverse feedback labels of\nuser-video interactions, KuaiSAR also supports a wide range of other tasks,\nincluding intent recommendation, multi-task learning, and long sequential\nmulti-behavior modeling etc. We believe this dataset will facilitate innovative\nresearch and enrich our understanding of S&amp;R services integration in real-world\napplications.",
        "translated": ""
    },
    {
        "title": "Practice with Graph-based ANN Algorithms on Sparse Data: Chi-square\n  Two-tower model, HNSW, Sign Cauchy Projections",
        "url": "http://arxiv.org/abs/2306.07607v1",
        "pub_date": "2023-06-13",
        "summary": "Sparse data are common. The traditional ``handcrafted'' features are often\nsparse. Embedding vectors from trained models can also be very sparse, for\nexample, embeddings trained via the ``ReLu'' activation function. In this\npaper, we report our exploration of efficient search in sparse data with\ngraph-based ANN algorithms (e.g., HNSW, or SONG which is the GPU version of\nHNSW), which are popular in industrial practice, e.g., search and ads\n(advertising).\n  We experiment with the proprietary ads targeting application, as well as\nbenchmark public datasets. For ads targeting, we train embeddings with the\nstandard ``cosine two-tower'' model and we also develop the ``chi-square\ntwo-tower'' model. Both models produce (highly) sparse embeddings when they are\nintegrated with the ``ReLu'' activation function. In EBR (embedding-based\nretrieval) applications, after we the embeddings are trained, the next crucial\ntask is the approximate near neighbor (ANN) search for serving. While there are\nmany ANN algorithms we can choose from, in this study, we focus on the\ngraph-based ANN algorithm (e.g., HNSW-type).\n  Sparse embeddings should help improve the efficiency of EBR. One benefit is\nthe reduced memory cost for the embeddings. The other obvious benefit is the\nreduced computational time for evaluating similarities, because, for\ngraph-based ANN algorithms such as HNSW, computing similarities is often the\ndominating cost. In addition to the effort on leveraging data sparsity for\nstorage and computation, we also integrate ``sign cauchy random projections''\n(SignCRP) to hash vectors to bits, to further reduce the memory cost and speed\nup the ANN search. In NIPS'13, SignCRP was proposed to hash the chi-square\nsimilarity, which is a well-adopted nonlinear kernel in NLP and computer\nvision. Therefore, the chi-square two-tower model, SignCRP, and HNSW are now\ntightly integrated.",
        "translated": ""
    },
    {
        "title": "Unified Off-Policy Learning to Rank: a Reinforcement Learning\n  Perspective",
        "url": "http://arxiv.org/abs/2306.07528v1",
        "pub_date": "2023-06-13",
        "summary": "Off-policy Learning to Rank (LTR) aims to optimize a ranker from data\ncollected by a deployed logging policy. However, existing off-policy learning\nto rank methods often make strong assumptions about how users generate the\nclick data, i.e., the click model, and hence need to tailor their methods\nspecifically under different click models. In this paper, we unified the\nranking process under general stochastic click models as a Markov Decision\nProcess (MDP), and the optimal ranking could be learned with offline\nreinforcement learning (RL) directly. Building upon this, we leverage offline\nRL techniques for off-policy LTR and propose the Click Model-Agnostic Unified\nOff-policy Learning to Rank (CUOLR) method, which could be easily applied to a\nwide range of click models. Through a dedicated formulation of the MDP, we show\nthat offline RL algorithms can adapt to various click models without complex\ndebiasing techniques and prior knowledge of the model. Results on various\nlarge-scale datasets demonstrate that CUOLR consistently outperforms the\nstate-of-the-art off-policy learning to rank algorithms while maintaining\nconsistency and robustness under different click models.",
        "translated": ""
    },
    {
        "title": "Topic-Centric Explanations for News Recommendation",
        "url": "http://arxiv.org/abs/2306.07506v1",
        "pub_date": "2023-06-13",
        "summary": "News recommender systems (NRS) have been widely applied for online news\nwebsites to help users find relevant articles based on their interests. Recent\nmethods have demonstrated considerable success in terms of recommendation\nperformance. However, the lack of explanation for these recommendations can\nlead to mistrust among users and lack of acceptance of recommendations. To\naddress this issue, we propose a new explainable news model to construct a\ntopic-aware explainable recommendation approach that can both accurately\nidentify relevant articles and explain why they have been recommended, using\ninformation from associated topics. Additionally, our model incorporates two\ncoherence metrics applied to assess topic quality, providing measure of the\ninterpretability of these explanations. The results of our experiments on the\nMIND dataset indicate that the proposed explainable NRS outperforms several\nother baseline systems, while it is also capable of producing interpretable\ntopics compared to those generated by a classical LDA topic model. Furthermore,\nwe present a case study through a real-world example showcasing the usefulness\nof our NRS for generating explanations.",
        "translated": ""
    },
    {
        "title": "Incentivizing High-Quality Content in Online Recommender Systems",
        "url": "http://arxiv.org/abs/2306.07479v1",
        "pub_date": "2023-06-13",
        "summary": "For content recommender systems such as TikTok and YouTube, the platform's\ndecision algorithm shapes the incentives of content producers, including how\nmuch effort the content producers invest in the quality of their content. Many\nplatforms employ online learning, which creates intertemporal incentives, since\ncontent produced today affects recommendations of future content. In this\npaper, we study the incentives arising from online learning, analyzing the\nquality of content produced at a Nash equilibrium. We show that classical\nonline learning algorithms, such as Hedge and EXP3, unfortunately incentivize\nproducers to create low-quality content. In particular, the quality of content\nis upper bounded in terms of the learning rate and approaches zero for typical\nlearning rate schedules. Motivated by this negative result, we design a\ndifferent learning algorithm -- based on punishing producers who create\nlow-quality content -- that correctly incentivizes producers to create\nhigh-quality content. At a conceptual level, our work illustrates the\nunintended impact that a platform's learning algorithm can have on content\nquality and opens the door towards designing platform learning algorithms that\nincentivize the creation of high-quality content.",
        "translated": ""
    },
    {
        "title": "Resources for Brewing BEIR: Reproducible Reference Models and an\n  Official Leaderboard",
        "url": "http://arxiv.org/abs/2306.07471v1",
        "pub_date": "2023-06-13",
        "summary": "BEIR is a benchmark dataset for zero-shot evaluation of information retrieval\nmodels across 18 different domain/task combinations. In recent years, we have\nwitnessed the growing popularity of a representation learning approach to\nbuilding retrieval models, typically using pretrained transformers in a\nsupervised setting. This naturally begs the question: How effective are these\nmodels when presented with queries and documents that differ from the training\ndata? Examples include searching in different domains (e.g., medical or legal\ntext) and with different types of queries (e.g., keywords vs. well-formed\nquestions). While BEIR was designed to answer these questions, our work\naddresses two shortcomings that prevent the benchmark from achieving its full\npotential: First, the sophistication of modern neural methods and the\ncomplexity of current software infrastructure create barriers to entry for\nnewcomers. To this end, we provide reproducible reference implementations that\ncover the two main classes of approaches: learned dense and sparse models.\nSecond, there does not exist a single authoritative nexus for reporting the\neffectiveness of different models on BEIR, which has led to difficulty in\ncomparing different methods. To remedy this, we present an official\nself-service BEIR leaderboard that provides fair and consistent comparisons of\nretrieval models. By addressing both shortcomings, our work facilitates future\nexplorations in a range of interesting research questions that BEIR enables.",
        "translated": ""
    },
    {
        "title": "Web of Things and Trends in Agriculture: A Systematic Literature Review",
        "url": "http://arxiv.org/abs/2306.09079v1",
        "pub_date": "2023-06-15",
        "summary": "In the past few years, the Web of Things (WOT) became a beneficial\ngame-changing technology within the Agriculture domain as it introduces\ninnovative and promising solutions to the Internet of Things (IoT) agricultural\napplications problems by providing its services. WOT provides the support for\nintegration, interoperability for heterogeneous devices, infrastructures,\nplatforms, and the emergence of various other technologies. The main aim of\nthis study is about understanding and providing a growing and existing research\ncontent, issues, and directions for the future regarding WOT-based agriculture.\nTherefore, a systematic literature review (SLR) of research articles is\npresented by categorizing the selected studies published between 2010 and 2020\ninto the following categories: research type, approaches, and their application\ndomains. Apart from reviewing the state-of-the-art articles on WOT solutions\nfor the agriculture field, a taxonomy of WOT-base agriculture application\ndomains has also been presented in this study. A model has also presented to\nshow the picture of WOT based Smart Agriculture. Lastly, the findings of this\nSLR and the research gaps in terms of open issues have been presented to\nprovide suggestions on possible future directions for the researchers for\nfuture research.",
        "translated": ""
    },
    {
        "title": "Fast and Examination-agnostic Reciprocal Recommendation in Matching\n  Markets",
        "url": "http://arxiv.org/abs/2306.09060v1",
        "pub_date": "2023-06-15",
        "summary": "In matching markets such as job posting and online dating platforms, the\nrecommender system plays a critical role in the success of the platform. Unlike\nstandard recommender systems that suggest items to users, reciprocal\nrecommender systems (RRSs) that suggest other users must take into account the\nmutual interests of users. In addition, ensuring that recommendation\nopportunities do not disproportionately favor popular users is essential for\nthe total number of matches and for fairness among users. Existing\nrecommendation methods in matching markets, however, face computational\nchallenges on large-scale platforms and depend on specific examination\nfunctions in the position-based model (PBM). In this paper, we introduce the\nreciprocal recommendation method based on the matching with transferable\nutility (TU matching) model in the context of ranking recommendations in\nmatching markets and propose a fast and examination-model-free algorithm.\nFurthermore, we evaluate our approach on experiments with synthetic data and\nreal-world data from an online dating platform in Japan. Our method performs\nbetter than or as well as existing methods in terms of the total number of\nmatches and works well even in a large-scale dataset for which one existing\nmethod does not work.",
        "translated": ""
    },
    {
        "title": "Mapping Researcher Activity based on Publication Data by means of\n  Transformers",
        "url": "http://arxiv.org/abs/2306.09049v1",
        "pub_date": "2023-06-15",
        "summary": "Modern performance on several natural language processing (NLP) tasks has\nbeen enhanced thanks to the Transformer-based pre-trained language model BERT.\nWe employ this concept to investigate a local publication database. Research\npapers are encoded and clustered to form a landscape view of the scientific\ntopics, in which research is active. Authors working on similar topics can be\nidentified by calculating the similarity between their papers. Based on this,\nwe define a similarity metric between authors. Additionally we introduce the\nconcept of self-similarity to indicate the topical variety of authors.",
        "translated": ""
    },
    {
        "title": "RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation",
        "url": "http://arxiv.org/abs/2306.08947v1",
        "pub_date": "2023-06-15",
        "summary": "In this paper we propose RecFusion, which comprise a set of diffusion models\nfor recommendation. Unlike image data which contain spatial correlations, a\nuser-item interaction matrix, commonly utilized in recommendation, lacks\nspatial relationships between users and items. We formulate diffusion on a 1D\nvector and propose binomial diffusion, which explicitly models binary user-item\ninteractions with a Bernoulli process. We show that RecFusion approaches the\nperformance of complex VAE baselines on the core recommendation setting (top-n\nrecommendation for binary non-sequential feedback) and the most common datasets\n(MovieLens and Netflix). Our proposed diffusion models that are specialized for\n1D and/or binary setups have implications beyond recommendation systems, such\nas in the medical domain with MRI and CT scans.",
        "translated": ""
    },
    {
        "title": "Document Entity Retrieval with Massive and Noisy Pre-training",
        "url": "http://arxiv.org/abs/2306.08937v1",
        "pub_date": "2023-06-15",
        "summary": "Visually-Rich Document Entity Retrieval (VDER) is a type of machine learning\ntask that aims at recovering text spans in the documents for each of the\nentities in question. VDER has gained significant attention in recent years\nthanks to its broad applications in enterprise AI. Unfortunately, as document\nimages often contain personally identifiable information (PII), publicly\navailable data have been scarce, not only because of privacy constraints but\nalso the costs of acquiring annotations. To make things worse, each dataset\nwould often define its own sets of entities, and the non-overlapping entity\nspaces between datasets make it difficult to transfer knowledge between\ndocuments. In this paper, we propose a method to collect massive-scale, noisy,\nand weakly labeled data from the web to benefit the training of VDER models.\nSuch a method will generate a huge amount of document image data to compensate\nfor the lack of training data in many VDER settings. Moreover, the collected\ndataset named DocuNet would not need to be dependent on specific document types\nor entity sets, making it universally applicable to all VDER tasks. Empowered\nby DocuNet, we present a lightweight multimodal architecture named UniFormer,\nwhich can learn a unified representation from text, layout, and image crops\nwithout needing extra visual pertaining. We experiment with our methods on\npopular VDER models in various settings and show the improvements when this\nmassive dataset is incorporated with UniFormer on both classic entity retrieval\nand few-shot learning settings.",
        "translated": ""
    },
    {
        "title": "Community Detection Attack against Collaborative Learning-based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2306.08929v1",
        "pub_date": "2023-06-15",
        "summary": "Collaborative-learning based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while keeping their history of consumed items on their\ndevices. While these solutions seemed appealing for preserving the privacy of\nthe participants at a first glance, recent studies have shown that\ncollaborative learning can be vulnerable to a variety of privacy attacks. In\nthis paper we propose a novel privacy attack called Community Detection Attack\n(CDA), which allows an adversary to discover the members of a community based\non a set of items of her choice (e.g., discovering users interested in LGBT\ncontent). Through experiments on three real recommendation datasets and by\nusing two state-of-the-art recommendation models, we assess the sensitivity of\nan FL-based recommender system as well as two flavors of Gossip Learning-based\nrecommender systems to CDA. Results show that on all models and all datasets,\nthe FL setting is more vulnerable to CDA than Gossip settings. We further\nevaluated two off-the-shelf mitigation strategies, namely differential privacy\n(DP) and a share less policy, which consists in sharing a subset of model\nparameters. Results show a better privacy-utility trade-off for the share less\npolicy compared to DP especially in the Gossip setting.",
        "translated": ""
    },
    {
        "title": "Prompt Performance Prediction for Generative IR",
        "url": "http://arxiv.org/abs/2306.08915v1",
        "pub_date": "2023-06-15",
        "summary": "The ability to predict the performance of a query in Information Retrieval\n(IR) systems has been a longstanding challenge. In this paper, we introduce a\nnovel task called \"Prompt Performance Prediction\" that aims to predict the\nperformance of a query, referred to as a prompt, before obtaining the actual\nsearch results. The context of our task leverages a generative model as an IR\nengine to evaluate the prompts' performance on image retrieval tasks. We\ndemonstrate the plausibility of our task by measuring the correlation\ncoefficient between predicted and actual performance scores across three\ndatasets containing pairs of prompts and generated images. Our results show\npromising performance prediction capabilities, suggesting potential\napplications for optimizing generative IR systems.",
        "translated": ""
    },
    {
        "title": "Description-Enhanced Label Embedding Contrastive Learning for Text\n  Classification",
        "url": "http://arxiv.org/abs/2306.08817v1",
        "pub_date": "2023-06-15",
        "summary": "Text Classification is one of the fundamental tasks in natural language\nprocessing, which requires an agent to determine the most appropriate category\nfor input sentences. Recently, deep neural networks have achieved impressive\nperformance in this area, especially Pre-trained Language Models (PLMs).\nUsually, these methods concentrate on input sentences and corresponding\nsemantic embedding generation. However, for another essential component:\nlabels, most existing works either treat them as meaningless one-hot vectors or\nuse vanilla embedding methods to learn label representations along with model\ntraining, underestimating the semantic information and guidance that these\nlabels reveal. To alleviate this problem and better exploit label information,\nin this paper, we employ Self-Supervised Learning (SSL) in model learning\nprocess and design a novel self-supervised Relation of Relation (R2)\nclassification task for label utilization from a one-hot manner perspective.\nThen, we propose a novel Relation of Relation Learning Network (R2-Net) for\ntext classification, in which text classification and R2 classification are\ntreated as optimization targets. Meanwhile, triplet loss is employed to enhance\nthe analysis of differences and connections among labels. Moreover, considering\nthat one-hot usage is still short of exploiting label information, we\nincorporate external knowledge from WordNet to obtain multi-aspect descriptions\nfor label semantic learning and extend R2-Net to a novel Description-Enhanced\nLabel Embedding network (DELE) from a label embedding perspective. ...",
        "translated": ""
    },
    {
        "title": "ReLoop2: Building Self-Adaptive Recommendation Models via Responsive\n  Error Compensation Loop",
        "url": "http://arxiv.org/abs/2306.08808v1",
        "pub_date": "2023-06-15",
        "summary": "Industrial recommender systems face the challenge of operating in\nnon-stationary environments, where data distribution shifts arise from evolving\nuser behaviors over time. To tackle this challenge, a common approach is to\nperiodically re-train or incrementally update deployed deep models with newly\nobserved data, resulting in a continual training process. However, the\nconventional learning paradigm of neural networks relies on iterative\ngradient-based updates with a small learning rate, making it slow for large\nrecommendation models to adapt. In this paper, we introduce ReLoop2, a\nself-correcting learning loop that facilitates fast model adaptation in online\nrecommender systems through responsive error compensation. Inspired by the\nslow-fast complementary learning system observed in human brains, we propose an\nerror memory module that directly stores error samples from incoming data\nstreams. These stored samples are subsequently leveraged to compensate for\nmodel prediction errors during testing, particularly under distribution shifts.\nThe error memory module is designed with fast access capabilities and undergoes\ncontinual refreshing with newly observed data samples during the model serving\nphase to support fast model adaptation. We evaluate the effectiveness of\nReLoop2 on three open benchmark datasets as well as a real-world production\ndataset. The results demonstrate the potential of ReLoop2 in enhancing the\nresponsiveness and adaptiveness of recommender systems operating in\nnon-stationary environments.",
        "translated": ""
    },
    {
        "title": "Learning to Rank when Grades Matter",
        "url": "http://arxiv.org/abs/2306.08650v1",
        "pub_date": "2023-06-14",
        "summary": "Graded labels are ubiquitous in real-world learning-to-rank applications,\nespecially in human rated relevance data. Traditional learning-to-rank\ntechniques aim to optimize the ranked order of documents. They typically,\nhowever, ignore predicting actual grades. This prevents them from being adopted\nin applications where grades matter, such as filtering out ``poor'' documents.\nAchieving both good ranking performance and good grade prediction performance\nis still an under-explored problem. Existing research either focuses only on\nranking performance by not calibrating model outputs, or treats grades as\nnumerical values, assuming labels are on a linear scale and failing to leverage\nthe ordinal grade information. In this paper, we conduct a rigorous study of\nlearning to rank with grades, where both ranking performance and grade\nprediction performance are important. We provide a formal discussion on how to\nperform ranking with non-scalar predictions for grades, and propose a\nmultiobjective formulation to jointly optimize both ranking and grade\npredictions. In experiments, we verify on several public datasets that our\nmethods are able to push the Pareto frontier of the tradeoff between ranking\nand grade prediction performance, showing the benefit of leveraging ordinal\ngrade information.",
        "translated": ""
    },
    {
        "title": "GRM: Generative Relevance Modeling Using Relevance-Aware Sample\n  Estimation for Document Retrieval",
        "url": "http://arxiv.org/abs/2306.09938v1",
        "pub_date": "2023-06-16",
        "summary": "Recent studies show that Generative Relevance Feedback (GRF), using text\ngenerated by Large Language Models (LLMs), can enhance the effectiveness of\nquery expansion. However, LLMs can generate irrelevant information that harms\nretrieval effectiveness. To address this, we propose Generative Relevance\nModeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more\naccurate weighting of expansion terms. Specifically, we identify similar real\ndocuments for each generated document and use a neural re-ranker to estimate\ntheir relevance. Experiments on three standard document ranking benchmarks show\nthat GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.",
        "translated": ""
    },
    {
        "title": "Smart Sentiment Analysis-based Search Engine Classification Intelligence",
        "url": "http://arxiv.org/abs/2306.09777v1",
        "pub_date": "2023-06-16",
        "summary": "Search engines are widely used for finding information on the internet.\nHowever, there are limitations in the current search approach, such as\nproviding popular but not necessarily relevant results. This research addresses\nthe issue of polysemy in search results by implementing a search function that\ndetermines the sentimentality of the retrieved information. The study utilizes\na web crawler to collect data from the British Broadcasting Corporation (BBC)\nnews site, and the sentimentality of the news articles is determined using the\nSentistrength program. The results demonstrate that the proposed search\nfunction improves recall value while accurately retrieving nonpolysemous news.\nFurthermore, Sentistrength outperforms deep learning and clustering methods in\nclassifying search results. The methodology presented in this article can be\napplied to analyze the sentimentality and reputation of entities on the\ninternet.",
        "translated": ""
    },
    {
        "title": "Online Distillation for Pseudo-Relevance Feedback",
        "url": "http://arxiv.org/abs/2306.09657v1",
        "pub_date": "2023-06-16",
        "summary": "Model distillation has emerged as a prominent technique to improve neural\nsearch models. To date, distillation taken an offline approach, wherein a new\nneural model is trained to predict relevance scores between arbitrary queries\nand documents. In this paper, we explore a departure from this offline\ndistillation strategy by investigating whether a model for a specific query can\nbe effectively distilled from neural re-ranking results (i.e., distilling in an\nonline setting). Indeed, we find that a lexical model distilled online can\nreasonably replicate the re-ranking of a neural model. More importantly, these\nmodels can be used as queries that execute efficiently on indexes. This second\nretrieval stage can enrich the pool of documents for re-ranking by identifying\ndocuments that were missed in the first retrieval stage. Empirically, we show\nthat this approach performs favourably when compared with established pseudo\nrelevance feedback techniques, dense retrieval methods, and sparse-dense\nensemble \"hybrid\" approaches.",
        "translated": ""
    },
    {
        "title": "I Want This, Not That: Personalized Summarization of Scientific\n  Scholarly Texts",
        "url": "http://arxiv.org/abs/2306.09604v1",
        "pub_date": "2023-06-16",
        "summary": "In this paper, we present a proposal for an unsupervised algorithm, P-Summ,\nthat generates an extractive summary of scientific scholarly text to meet the\npersonal knowledge needs of the user. The method delves into the latent\nsemantic space of the document exposed by Weighted Non-negative Matrix\nFactorization, and scores sentences in consonance with the knowledge needs of\nthe user. The novelty of the algorithm lies in its ability to include desired\nknowledge and eliminate unwanted knowledge in the personal summary.\n  We also propose a multi-granular evaluation framework, which assesses the\nquality of generated personal summaries at three levels of granularity -\nsentence, terms and semantic. The framework uses system generated generic\nsummary instead of human generated summary as gold standard for evaluating the\nquality of personal summary generated by the algorithm. The effectiveness of\nthe algorithm at the semantic level is evaluated by taking into account the\nreference summary and the knowledge signals. We evaluate the performance of\nP-Summ algorithm over four data-sets consisting of scientific articles. Our\nempirical investigations reveal that the proposed method has the capability to\nmeet negative (or positive) knowledge preferences of the user.",
        "translated": ""
    },
    {
        "title": "Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive\n  Volume Lines",
        "url": "http://arxiv.org/abs/2306.11612v1",
        "pub_date": "2023-06-20",
        "summary": "To visually compare ensembles of volumes, dynamic volume lines (DVLs)\nrepresent each ensemble member as a 1D polyline. To compute these, the volume\ncells are sorted on a space-filling curve and scaled by the ensemble's local\nvariation. The resulting 1D plot can augment or serve as an alternative to a 3D\nvolume visualization free of visual clutter and occlusion. Interactively\ncomputing DVLs is challenging when the data is large, and the volume grid is\nnot structured/regular, as is often the case with computational fluid dynamics\nsimulations. We extend DVLs to support large-scale, multi-field adaptive mesh\nrefinement (AMR) data that can be explored interactively. Our GPU-based system\nupdates the DVL representation whenever the data or the alpha transfer function\nchanges. We demonstrate and evaluate our interactive prototype using large AMR\nvolumes from astrophysics simulations.",
        "translated": ""
    },
    {
        "title": "Mining Interest Trends and Adaptively Assigning SampleWeight for\n  Session-based Recommendation",
        "url": "http://arxiv.org/abs/2306.11610v1",
        "pub_date": "2023-06-20",
        "summary": "Session-based Recommendation (SR) aims to predict users' next click based on\ntheir behavior within a short period, which is crucial for online platforms.\nHowever, most existing SR methods somewhat ignore the fact that user preference\nis not necessarily strongly related to the order of interactions. Moreover,\nthey ignore the differences in importance between different samples, which\nlimits the model-fitting performance. To tackle these issues, we put forward\nthe method, Mining Interest Trends and Adaptively Assigning Sample Weight,\nabbreviated as MTAW. Specifically, we model users' instant interest based on\ntheir present behavior and all their previous behaviors. Meanwhile, we\ndiscriminatively integrate instant interests to capture the changing trend of\nuser interest to make more personalized recommendations. Furthermore, we devise\na novel loss function that dynamically weights the samples according to their\nprediction difficulty in the current epoch. Extensive experimental results on\ntwo benchmark datasets demonstrate the effectiveness and superiority of our\nmethod.",
        "translated": ""
    },
    {
        "title": "Polytope: An Algorithm for Efficient Feature Extraction on Hypercubes",
        "url": "http://arxiv.org/abs/2306.11553v1",
        "pub_date": "2023-06-20",
        "summary": "Data extraction algorithms on data hypercubes, or datacubes, are\ntraditionally only capable of cutting boxes of data along the datacube axes.\nFor many use cases however, this is not a sufficient approach and returns more\ndata than users might actually need. This not only forces users to apply\npost-processing after extraction, but more importantly this consumes more I/O\nresources than is necessary. When considering very large datacubes from which\nusers only want to extract small non-rectangular subsets, the box approach does\nnot scale well. Indeed, with this traditional approach, I/O systems quickly\nreach capacity, trying to read and return unwanted data to users. In this\npaper, we propose a novel technique, based on computational geometry concepts,\nwhich instead carefully pre-selects the precise bytes of data which the user\nneeds in order to then only read those from the datacube. As we discuss later\non, this novel extraction method will considerably help scale access to large\npetabyte size data hypercubes in a variety of scientific fields.",
        "translated": ""
    },
    {
        "title": "Generative Retrieval as Dense Retrieval",
        "url": "http://arxiv.org/abs/2306.11397v1",
        "pub_date": "2023-06-20",
        "summary": "Generative retrieval is a promising new neural retrieval paradigm that aims\nto optimize the retrieval pipeline by performing both indexing and retrieval\nwith a single transformer model. However, this new paradigm faces challenges\nwith updating the index and scaling to large collections. In this paper, we\nanalyze two prominent variants of generative retrieval and show that they can\nbe conceptually viewed as bi-encoders for dense retrieval. Specifically, we\nanalytically demonstrate that the generative retrieval process can be\ndecomposed into dot products between query and document vectors, similar to\ndense retrieval. This analysis leads us to propose a new variant of generative\nretrieval, called Tied-Atomic, which addresses the updating and scaling issues\nby incorporating techniques from dense retrieval. In experiments on two\ndatasets, NQ320k and the full MSMARCO, we confirm that this approach does not\nreduce retrieval effectiveness while enabling the model to scale to large\ncollections.",
        "translated": ""
    },
    {
        "title": "CAPRI: Context-Aware Interpretable Point-of-Interest Recommendation\n  Framework",
        "url": "http://arxiv.org/abs/2306.11395v1",
        "pub_date": "2023-06-20",
        "summary": "Point-of-Interest (POI ) recommendation systems have gained popularity for\ntheir unique ability to suggest geographical destinations with the\nincorporation of contextual information such as time, location, and user-item\ninteraction. Existing recommendation frameworks lack the contextual fusion\nrequired for POI systems. This paper presents CAPRI, a novel POI recommendation\nframework that effectively integrates context-aware models, such as GeoSoCa,\nLORE, and USG, and introduces a novel strategy for the efficient merging of\ncontextual information. CAPRI integrates an evaluation module that expands the\nevaluation scope beyond accuracy to include novelty, personalization,\ndiversity, and fairness. With an aim to establish a new industry standard for\nreproducible results in the realm of POI recommendation systems, we have made\nCAPRI openly accessible on GitHub, facilitating easy access and contribution to\nthe continued development and refinement of this innovative framework.",
        "translated": ""
    },
    {
        "title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF\n  Synthesis",
        "url": "http://arxiv.org/abs/2306.11296v1",
        "pub_date": "2023-06-20",
        "summary": "We use prompt engineering to guide ChatGPT in the automation of text mining\nof metal-organic frameworks (MOFs) synthesis conditions from diverse formats\nand styles of the scientific literature. This effectively mitigates ChatGPT's\ntendency to hallucinate information -- an issue that previously made the use of\nLarge Language Models (LLMs) in scientific fields challenging. Our approach\ninvolves the development of a workflow implementing three different processes\nfor text mining, programmed by ChatGPT itself. All of them enable parsing,\nsearching, filtering, classification, summarization, and data unification with\ndifferent tradeoffs between labor, speed, and accuracy. We deploy this system\nto extract 26,257 distinct synthesis parameters pertaining to approximately 800\nMOFs sourced from peer-reviewed research articles. This process incorporates\nour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,\nresulting in impressive precision, recall, and F1 scores of 90-99%.\nFurthermore, with the dataset built by text mining, we constructed a\nmachine-learning model with over 86% accuracy in predicting MOF experimental\ncrystallization outcomes and preliminarily identifying important factors in MOF\ncrystallization. We also developed a reliable data-grounded MOF chatbot to\nanswer questions on chemical reactions and synthesis procedures. Given that the\nprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesis\ninformation in a unified format, while using only narrative language requiring\nno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be\nvery useful across various other chemistry sub-disciplines.",
        "translated": ""
    },
    {
        "title": "Representation Sparsification with Hybrid Thresholding for Fast\n  SPLADE-based Document Retrieval",
        "url": "http://arxiv.org/abs/2306.11293v1",
        "pub_date": "2023-06-20",
        "summary": "Learned sparse document representations using a transformer-based neural\nmodel has been found to be attractive in both relevance effectiveness and time\nefficiency. This paper describes a representation sparsification scheme based\non hard and soft thresholding with an inverted index approximation for faster\nSPLADE-based document retrieval. It provides analytical and experimental\nresults on the impact of this learnable hybrid thresholding scheme.",
        "translated": ""
    },
    {
        "title": "Less Can Be More: Exploring Population Rating Dispositions with\n  Partitioned Models in Recommender Systems",
        "url": "http://arxiv.org/abs/2306.11279v1",
        "pub_date": "2023-06-20",
        "summary": "In this study, we partition users by rating disposition - looking first at\ntheir percentage of negative ratings, and then at the general use of the rating\nscale. We hypothesize that users with different rating dispositions may use the\nrecommender system differently and therefore the agreement with their past\nratings may be less predictive of the future agreement.\n  We use data from a large movie rating website to explore whether users should\nbe grouped by disposition, focusing on identifying their various rating\ndistributions that may hurt recommender effectiveness. We find that such\npartitioning not only improves computational efficiency but also improves top-k\nperformance and predictive accuracy. Though such effects are largest for the\nuser-based KNN CF, smaller for item-based KNN CF, and smallest for latent\nfactor algorithms such as SVD.",
        "translated": ""
    },
    {
        "title": "Hybrid Multi-Criteria Preference Ranking by Subsorting",
        "url": "http://arxiv.org/abs/2306.11233v1",
        "pub_date": "2023-06-20",
        "summary": "Multi-criteria recommender systems can improve the quality of recommendations\nby considering user preferences on multiple criteria. One promising approach\nproposed recently is multi-criteria ranking, which uses Pareto ranking to\nassign a ranking score based on the dominance relationship between predicted\nratings across criteria. However, applying Pareto ranking to all criteria may\nresult in non-differentiable ranking scores. To alleviate this issue, we\nproposed a hybrid multi-criteria ranking method by using subsorting. More\nspecifically, we utilize one ranking method as the major sorting approach,\nwhile we apply another preference ordering method as subsorting. Our\nexperimental results on the OpenTable and Yahoo!Movies data present the\nadvantages of this hybrid ranking approach. In addition, the experiments also\nreveal more insights about the sustainability of the multi-criteria ranking for\ntop-N item recommendations.",
        "translated": ""
    },
    {
        "title": "Co-design Hardware and Algorithm for Vector Search",
        "url": "http://arxiv.org/abs/2306.11182v1",
        "pub_date": "2023-06-19",
        "summary": "Vector search has emerged as the foundation for large-scale information\nretrieval and machine learning systems, with search engines like Google and\nBing processing tens of thousands of queries per second on petabyte-scale\ndocument datasets by evaluating vector similarities between encoded query texts\nand web documents. As performance demands for vector search systems surge,\naccelerated hardware offers a promising solution in the post-Moore's Law era.\nWe introduce \\textit{FANNS}, an end-to-end and scalable vector search framework\non FPGAs. Given a user-provided recall requirement on a dataset and a hardware\nresource budget, \\textit{FANNS} automatically co-designs hardware and\nalgorithm, subsequently generating the corresponding accelerator. The framework\nalso supports scale-out by incorporating a hardware TCP/IP stack in the\naccelerator. \\textit{FANNS} attains up to 23.0$\\times$ and 37.2$\\times$ speedup\ncompared to FPGA and CPU baselines, respectively, and demonstrates superior\nscalability to GPUs, achieving 5.5$\\times$ and 7.6$\\times$ speedup in median\nand 95\\textsuperscript{th} percentile (P95) latency within an eight-accelerator\nconfiguration. The remarkable performance of \\textit{FANNS} lays a robust\ngroundwork for future FPGA integration in data centers and AI supercomputers.",
        "translated": ""
    },
    {
        "title": "Knowledge-based Multimodal Music Similarity",
        "url": "http://arxiv.org/abs/2306.12249v1",
        "pub_date": "2023-06-21",
        "summary": "Music similarity is an essential aspect of music retrieval, recommendation\nsystems, and music analysis. Moreover, similarity is of vital interest for\nmusic experts, as it allows studying analogies and influences among composers\nand historical periods. Current approaches to musical similarity rely mainly on\nsymbolic content, which can be expensive to produce and is not always readily\navailable. Conversely, approaches using audio signals typically fail to provide\nany insight about the reasons behind the observed similarity. This research\naddresses the limitations of current approaches by focusing on the study of\nmusical similarity using both symbolic and audio content. The aim of this\nresearch is to develop a fully explainable and interpretable system that can\nprovide end-users with more control and understanding of music similarity and\nclassification systems.",
        "translated": ""
    },
    {
        "title": "CompMix: A Benchmark for Heterogeneous Question Answering",
        "url": "http://arxiv.org/abs/2306.12235v1",
        "pub_date": "2023-06-21",
        "summary": "Fact-centric question answering (QA) often requires access to multiple,\nheterogeneous, information sources. By jointly considering several sources like\na knowledge base (KB), a text collection, and tables from the web, QA systems\ncan enhance their answer coverage and confidence. However, existing QA\nbenchmarks are mostly constructed with a single source of knowledge in mind.\nThis limits capabilities of these benchmarks to fairly evaluate QA systems that\ncan tap into more than one information repository. To bridge this gap, we\nrelease CompMix, a crowdsourced QA benchmark which naturally demands the\nintegration of a mixture of input sources. CompMix has a total of 9,410\nquestions, and features several complex intents like joins and temporal\nconditions. Evaluation of a range of QA systems on CompMix highlights the need\nfor further research on leveraging information from heterogeneous sources.",
        "translated": ""
    },
    {
        "title": "STAN: Stage-Adaptive Network for Multi-Task Recommendation by Learning\n  User Lifecycle-Based Representation",
        "url": "http://arxiv.org/abs/2306.12232v1",
        "pub_date": "2023-06-21",
        "summary": "Recommendation systems play a vital role in many online platforms, with their\nprimary objective being to satisfy and retain users. As directly optimizing\nuser retention is challenging, multiple evaluation metrics are often employed.\nExisting methods generally formulate the optimization of these evaluation\nmetrics as a multitask learning problem, but often overlook the fact that user\npreferences for different tasks are personalized and change over time.\nIdentifying and tracking the evolution of user preferences can lead to better\nuser retention. To address this issue, we introduce the concept of \"user\nlifecycle\", consisting of multiple stages characterized by users' varying\npreferences for different tasks. We propose a novel Stage-Adaptive Network\n(STAN) framework for modeling user lifecycle stages. STAN first identifies\nlatent user lifecycle stages based on learned user preferences, and then\nemploys the stage representation to enhance multi-task learning performance.\nOur experimental results using both public and industrial datasets demonstrate\nthat the proposed model significantly improves multi-task prediction\nperformance compared to state-of-the-art methods, highlighting the importance\nof considering user lifecycle stages in recommendation systems. Furthermore,\nonline A/B testing reveals that our model outperforms the existing model,\nachieving a significant improvement of 3.05% in staytime per user and 0.88% in\nCVR. These results indicate that our approach effectively improves the overall\nefficiency of the multi-task recommendation system.",
        "translated": ""
    },
    {
        "title": "Post-hoc Selection of Pareto-Optimal Solutions in Search and\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.12165v1",
        "pub_date": "2023-06-21",
        "summary": "Information Retrieval (IR) and Recommender Systems (RS) tasks are moving from\ncomputing a ranking of final results based on a single metric to\nmulti-objective problems. Solving these problems leads to a set of\nPareto-optimal solutions, known as Pareto frontier, in which no objective can\nbe further improved without hurting the others. In principle, all the points on\nthe Pareto frontier are potential candidates to represent the best model\nselected with respect to the combination of two, or more, metrics. To our\nknowledge, there are no well-recognized strategies to decide which point should\nbe selected on the frontier. In this paper, we propose a novel, post-hoc,\ntheoretically-justified technique, named \"Population Distance from Utopia\"\n(PDU), to identify and select the one-best Pareto-optimal solution from the\nfrontier. In detail, PDU analyzes the distribution of the points by\ninvestigating how far each point is from its utopia point (the ideal\nperformance for the objectives). The possibility of considering fine-grained\nutopia points allows PDU to select solutions tailored to individual user\npreferences, a novel feature we call \"calibration\". We compare PDU against\nexisting state-of-the-art strategies through extensive experiments on tasks\nfrom both IR and RS. Experimental results show that PDU and combined with\ncalibration notably impact the solution selection. Furthermore, the results\nshow that the proposed framework selects a solution in a principled way,\nirrespective of its position on the frontier, thus overcoming the limits of\nother strategies.",
        "translated": ""
    },
    {
        "title": "Visualizing Relation Between (De)Motivating Topics and Public Stance\n  toward COVID-19 Vaccine",
        "url": "http://arxiv.org/abs/2306.12118v1",
        "pub_date": "2023-06-21",
        "summary": "While social media plays a vital role in communication nowadays,\nmisinformation and trolls can easily take over the conversation and steer\npublic opinion on these platforms. We saw the effect of misinformation during\nthe {COVID-19} pandemic when public health officials faced significant\npush-back while trying to motivate the public to vaccinate. To tackle the\ncurrent and any future threats in emergencies and motivate the public towards a\ncommon goal, it is essential to understand how public motivation shifts and\nwhich topics resonate among the general population. In this study, we proposed\nan interactive visualization tool to inspect and analyze the topics that\nresonated among Twitter-sphere during the {COVID-19} pandemic and understand\nthe key factors that shifted public stance for vaccination. This tool can\neasily be generalized for any scenario for visual analysis and to increase the\ntransparency of social media data for researchers and the general population\nalike.",
        "translated": ""
    },
    {
        "title": "Comparative analysis of various web crawler algorithms",
        "url": "http://arxiv.org/abs/2306.12027v1",
        "pub_date": "2023-06-21",
        "summary": "This presentation focuses on the importance of web crawling and page ranking\nalgorithms in dealing with the massive amount of data present on the World Wide\nWeb. As the web continues to grow exponentially, efficient search and retrieval\nmethods become crucial. Web crawling is a process that converts unstructured\ndata into structured data, enabling effective information retrieval.\nAdditionally, page ranking algorithms play a significant role in assessing the\nquality and popularity of web pages. The presentation explores the background\nof these algorithms and evaluates five different crawling algorithms: Shark\nSearch, Priority-Based Queue, Naive Bayes, Breadth-First, and Depth-First. The\ngoal is to identify the most effective algorithm for crawling web pages. By\nunderstanding these algorithms, we can enhance our ability to navigate the web\nand extract valuable information efficiently.",
        "translated": ""
    },
    {
        "title": "Addressing the Rank Degeneration in Sequential Recommendation via\n  Singular Spectrum Smoothing",
        "url": "http://arxiv.org/abs/2306.11986v1",
        "pub_date": "2023-06-21",
        "summary": "Sequential recommendation (SR) investigates the dynamic user preferences\nmodeling and generates the next-item prediction. The next item preference is\ntypically generated by the affinity between the sequence and item\nrepresentations. However, both sequence and item representations suffer from\nthe rank degeneration issue due to the data sparsity problem. The rank\ndegeneration issue significantly impairs the representations for SR. This\nmotivates us to measure how severe is the rank degeneration issue and alleviate\nthe sequence and item representation rank degeneration issues simultaneously\nfor SR.\n  In this work, we theoretically connect the sequence representation\ndegeneration issue with the item rank degeneration, particularly for short\nsequences and cold items. We also identify the connection between the fast\nsingular value decay phenomenon and the rank collapse issue in transformer\nsequence output and item embeddings. We propose the area under the singular\nvalue curve metric to evaluate the severity of the singular value decay\nphenomenon and use it as an indicator of rank degeneration. We further\nintroduce a novel singular spectrum smoothing regularization to alleviate the\nrank degeneration on both sequence and item sides, which is the Singular\nsPectrum sMoothing for sequential Recommendation (SPMRec). We also establish a\ncorrelation between the ranks of sequence and item embeddings and the rank of\nthe user-item preference prediction matrix, which can affect recommendation\ndiversity. We conduct experiments on four benchmark datasets to demonstrate the\nsuperiority of SPMRec over the state-of-the-art recommendation methods,\nespecially in short sequences. The experiments also demonstrate a strong\nconnection between our proposed singular spectrum smoothing and recommendation\ndiversity.",
        "translated": ""
    },
    {
        "title": "Sampling Individually-Fair Rankings that are Always Group Fair",
        "url": "http://arxiv.org/abs/2306.11964v1",
        "pub_date": "2023-06-21",
        "summary": "Rankings on online platforms help their end-users find the relevant\ninformation -- people, news, media, and products -- quickly. Fair ranking\ntasks, which ask to rank a set of items to maximize utility subject to\nsatisfying group-fairness constraints, have gained significant interest in the\nAlgorithmic Fairness, Information Retrieval, and Machine Learning literature.\nRecent works, however, identify uncertainty in the utilities of items as a\nprimary cause of unfairness and propose introducing randomness in the output.\nThis randomness is carefully chosen to guarantee an adequate representation of\neach item (while accounting for the uncertainty). However, due to this\nrandomness, the output rankings may violate group fairness constraints. We give\nan efficient algorithm that samples rankings from an individually-fair\ndistribution while ensuring that every output ranking is group fair. The\nexpected utility of the output ranking is at least $\\alpha$ times the utility\nof the optimal fair solution. Here, $\\alpha$ depends on the utilities,\nposition-discounts, and constraints -- it approaches 1 as the range of\nutilities or the position-discounts shrinks, or when utilities satisfy\ndistributional assumptions. Empirically, we observe that our algorithm achieves\nindividual and group fairness and that Pareto dominates the state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "Multimodality Fusion for Smart Healthcare: a Journey from Data,\n  Information, Knowledge to Wisdom",
        "url": "http://arxiv.org/abs/2306.11963v1",
        "pub_date": "2023-06-21",
        "summary": "Multimodal medical data fusion has emerged as a transformative approach in\nsmart healthcare, enabling a comprehensive understanding of patient health and\npersonalized treatment plans. In this paper, a journey from data, information,\nand knowledge to wisdom (DIKW) is explored through multimodal fusion for smart\nhealthcare. A comprehensive review of multimodal medical data fusion focuses on\nthe integration of various data modalities are presented. It explores different\napproaches such as Feature selection, Rule-based systems, Machine learning,\nDeep learning, and Natural Language Processing for fusing and analyzing\nmultimodal data. The paper also highlights the challenges associated with\nmultimodal fusion in healthcare. By synthesizing the reviewed frameworks and\ninsights, a generic framework for multimodal medical data fusion is proposed\nwhile aligning with the DIKW mechanism. Moreover, it discusses future\ndirections aligned with the four pillars of healthcare: Predictive, Preventive,\nPersonalized, and Participatory approaches based on the DIKW and the generic\nframework. The components from this comprehensive survey form the foundation\nfor the successful implementation of multimodal fusion in smart healthcare. The\nfindings of this survey can guide researchers and practitioners in leveraging\nthe power of multimodal fusion with the approaches to revolutionize healthcare\nand improve patient outcomes.",
        "translated": ""
    },
    {
        "title": "Retrieval-Based Transformer for Table Augmentation",
        "url": "http://arxiv.org/abs/2306.11843v1",
        "pub_date": "2023-06-20",
        "summary": "Data preparation, also called data wrangling, is considered one of the most\nexpensive and time-consuming steps when performing analytics or building\nmachine learning models. Preparing data typically involves collecting and\nmerging data from complex heterogeneous, and often large-scale data sources,\nsuch as data lakes. In this paper, we introduce a novel approach toward\nautomatic data wrangling in an attempt to alleviate the effort of end-users,\ne.g. data analysts, in structuring dynamic views from data lakes in the form of\ntabular data. We aim to address table augmentation tasks, including row/column\npopulation and data imputation. Given a corpus of tables, we propose a\nretrieval augmented self-trained transformer model. Our self-learning strategy\nconsists in randomly ablating tables from the corpus and training the\nretrieval-based model to reconstruct the original values or headers given the\npartial tables as input. We adopt this strategy to first train the dense neural\nretrieval model encoding table-parts to vectors, and then the end-to-end model\ntrained to perform table augmentation tasks. We test on EntiTables, the\nstandard benchmark for table augmentation, as well as introduce a new benchmark\nto advance further research: WebTables. Our model consistently and\nsubstantially outperforms both supervised statistical methods and the current\nstate-of-the-art transformer-based models.",
        "translated": ""
    },
    {
        "title": "Data augmentation for recommender system: A semi-supervised approach\n  using maximum margin matrix factorization",
        "url": "http://arxiv.org/abs/2306.13050v1",
        "pub_date": "2023-06-22",
        "summary": "Collaborative filtering (CF) has become a popular method for developing\nrecommender systems (RS) where ratings of a user for new items is predicted\nbased on her past preferences and available preference information of other\nusers. Despite the popularity of CF-based methods, their performance is often\ngreatly limited by the sparsity of observed entries. In this study, we explore\nthe data augmentation and refinement aspects of Maximum Margin Matrix\nFactorization (MMMF), a widely accepted CF technique for the rating\npredictions, which have not been investigated before. We exploit the inherent\ncharacteristics of CF algorithms to assess the confidence level of individual\nratings and propose a semi-supervised approach for rating augmentation based on\nself-training. We hypothesize that any CF algorithm's predictions with low\nconfidence are due to some deficiency in the training data and hence, the\nperformance of the algorithm can be improved by adopting a systematic data\naugmentation strategy. We iteratively use some of the ratings predicted with\nhigh confidence to augment the training data and remove low-confidence entries\nthrough a refinement process. By repeating this process, the system learns to\nimprove prediction accuracy. Our method is experimentally evaluated on several\nstate-of-the-art CF algorithms and leads to informative rating augmentation,\nimproving the performance of the baseline approaches.",
        "translated": ""
    },
    {
        "title": "Efficient Partitioning Method of Large-Scale Public Safety\n  Spatio-Temporal Data based on Information Loss Constraints",
        "url": "http://arxiv.org/abs/2306.12857v1",
        "pub_date": "2023-06-22",
        "summary": "The storage, management, and application of massive spatio-temporal data are\nwidely applied in various practical scenarios, including public safety.\nHowever, due to the unique spatio-temporal distribution characteristics of\nre-al-world data, most existing methods have limitations in terms of the\nspatio-temporal proximity of data and load balancing in distributed storage.\nThere-fore, this paper proposes an efficient partitioning method of large-scale\npublic safety spatio-temporal data based on information loss constraints\n(IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal\npoint da-ta by combining the spatio-temporal partitioning module (STPM) with\nthe graph partitioning module (GPM). This approach can significantly reduce the\nscale of data while maintaining the model's accuracy, in order to improve the\npartitioning efficiency. It can also ensure the load balancing of distributed\nstorage while maintaining spatio-temporal proximity of the data partitioning\nresults. This method provides a new solution for distributed storage of\nmas-sive spatio-temporal data. The experimental results on multiple real-world\nda-tasets demonstrate the effectiveness and superiority of IFL-LSTP.",
        "translated": ""
    },
    {
        "title": "HypeRS: Building a Hypergraph-driven ensemble Recommender System",
        "url": "http://arxiv.org/abs/2306.12800v1",
        "pub_date": "2023-06-22",
        "summary": "Recommender systems are designed to predict user preferences over collections\nof items. These systems process users' previous interactions to decide which\nitems should be ranked higher to satisfy their desires. An ensemble recommender\nsystem can achieve great recommendation performance by effectively combining\nthe decisions generated by individual models. In this paper, we propose a novel\nensemble recommender system that combines predictions made by different models\ninto a unified hypergraph ranking framework. This is the first time that\nhypergraph ranking has been employed to model an ensemble of recommender\nsystems. Hypergraphs are generalizations of graphs where multiple vertices can\nbe connected via hyperedges, efficiently modeling high-order relations. We\ndifferentiate real and predicted connections between users and items by\nassigning different hyperedge weights to individual recommender systems. We\nperform experiments using four datasets from the fields of movie, music and\nnews media recommendation. The obtained results show that the ensemble\nhypergraph ranking method generates more accurate recommendations compared to\nthe individual models and a weighted hybrid approach. The assignment of\ndifferent hyperedge weights to the ensemble hypergraph further improves the\nperformance compared to a setting with identical hyperedge weights.",
        "translated": ""
    },
    {
        "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution\n  Perspective",
        "url": "http://arxiv.org/abs/2306.12756v1",
        "pub_date": "2023-06-22",
        "summary": "Recently, we have witnessed generative retrieval increasingly gaining\nattention in the information retrieval (IR) field, which retrieves documents by\ndirectly generating their identifiers. So far, much effort has been devoted to\ndeveloping effective generative retrieval models. There has been less attention\npaid to the robustness perspective. When a new retrieval paradigm enters into\nthe real-world application, it is also critical to measure the\nout-of-distribution (OOD) generalization, i.e., how would generative retrieval\nmodels generalize to new distributions. To answer this question, firstly, we\ndefine OOD robustness from three perspectives in retrieval problems: 1) The\nquery variations; 2) The unforeseen query types; and 3) The unforeseen tasks.\nBased on this taxonomy, we conduct empirical studies to analyze the OOD\nrobustness of several representative generative retrieval models against dense\nretrieval models. The empirical results indicate that the OOD robustness of\ngenerative retrieval models requires enhancement. We hope studying the OOD\nrobustness of generative retrieval models would be advantageous to the IR\ncommunity.",
        "translated": ""
    },
    {
        "title": "Vec2Vec: A Compact Neural Network Approach for Transforming Text\n  Embeddings with High Fidelity",
        "url": "http://arxiv.org/abs/2306.12689v1",
        "pub_date": "2023-06-22",
        "summary": "Vector embeddings have become ubiquitous tools for many language-related\ntasks. A leading embedding model is OpenAI's text-ada-002 which can embed\napproximately 6,000 words into a 1,536-dimensional vector. While powerful,\ntext-ada-002 is not open source and is only available via API. We trained a\nsimple neural network to convert open-source 768-dimensional MPNet embeddings\ninto text-ada-002 embeddings. We compiled a subset of 50,000 online food\nreviews. We calculated MPNet and text-ada-002 embeddings for each review and\ntrained a simple neural network to for 75 epochs. The neural network was\ndesigned to predict the corresponding text-ada-002 embedding for a given MPNET\nembedding. Our model achieved an average cosine similarity of 0.932 on 10,000\nunseen reviews in our held-out test dataset. We manually assessed the quality\nof our predicted embeddings for vector search over text-ada-002-embedded\nreviews. While not as good as real text-ada-002 embeddings, predicted\nembeddings were able to retrieve highly relevant reviews. Our final model,\nVec2Vec, is lightweight (&lt;80 MB) and fast. Future steps include training a\nneural network with a more sophisticated architecture and a larger dataset of\npaired embeddings to achieve greater performance. The ability to convert\nbetween and align embedding spaces may be helpful for interoperability,\nlimiting dependence on proprietary models, protecting data privacy, reducing\ncosts, and offline operations.",
        "translated": ""
    },
    {
        "title": "Recent Developments in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2306.12680v1",
        "pub_date": "2023-06-22",
        "summary": "In this technical survey, we comprehensively summarize the latest\nadvancements in the field of recommender systems. The objective of this study\nis to provide an overview of the current state-of-the-art in the field and\nhighlight the latest trends in the development of recommender systems. The\nstudy starts with a comprehensive summary of the main taxonomy of recommender\nsystems, including personalized and group recommender systems, and then delves\ninto the category of knowledge-based recommender systems. In addition, the\nsurvey analyzes the robustness, data bias, and fairness issues in recommender\nsystems, summarizing the evaluation metrics used to assess the performance of\nthese systems. Finally, the study provides insights into the latest trends in\nthe development of recommender systems and highlights the new directions for\nfuture research in the field.",
        "translated": ""
    },
    {
        "title": "Resources and Evaluations for Multi-Distribution Dense Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2306.12601v1",
        "pub_date": "2023-06-21",
        "summary": "We introduce and define the novel problem of multi-distribution information\nretrieval (IR) where given a query, systems need to retrieve passages from\nwithin multiple collections, each drawn from a different distribution. Some of\nthese collections and distributions might not be available at training time. To\nevaluate methods for multi-distribution retrieval, we design three benchmarks\nfor this task from existing single-distribution datasets, namely, a dataset\nbased on question answering and two based on entity matching. We propose simple\nmethods for this task which allocate the fixed retrieval budget (top-k\npassages) strategically across domains to prevent the known domains from\nconsuming most of the budget. We show that our methods lead to an average of\n3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and\nthat improvements are consistent when fine-tuning different base retrieval\nmodels. Our benchmarks are made publicly available.",
        "translated": ""
    },
    {
        "title": "Fuzzification-based Feature Selection for Enhanced Website Content\n  Encryption",
        "url": "http://arxiv.org/abs/2306.13548v1",
        "pub_date": "2023-06-23",
        "summary": "We propose a novel approach that utilizes fuzzification theory to perform\nfeature selection on website content for encryption purposes. Our objective is\nto identify and select the most relevant features from the website by\nharnessing the principles of fuzzy logic. Fuzzification allows us to transform\nthe crisp website content into fuzzy representations, enabling a more nuanced\nanalysis of their characteristics. By considering the degree of membership of\neach feature in different fuzzy categories, we can evaluate their importance\nand relevance for encryption. This approach enables us to prioritize and focus\non the features that exhibit higher membership degrees, indicating their\nsignificance in the encryption process. By employing fuzzification-based\nfeature selection, we aim to enhance the effectiveness and efficiency of\nwebsite content encryption, ultimately improving the overall internet security.",
        "translated": ""
    },
    {
        "title": "OptMSM: Optimizing Multi-Scenario Modeling for Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2306.13382v1",
        "pub_date": "2023-06-23",
        "summary": "A large-scale industrial recommendation platform typically consists of\nmultiple associated scenarios, requiring a unified click-through rate (CTR)\nprediction model to serve them simultaneously. Existing approaches for\nmulti-scenario CTR prediction generally consist of two main modules: i) a\nscenario-aware learning module that learns a set of multi-functional\nrepresentations with scenario-shared and scenario-specific information from\ninput features, and ii) a scenario-specific prediction module that serves each\nscenario based on these representations. However, most of these approaches\nprimarily focus on improving the former module and neglect the latter module.\nThis can result in challenges such as increased model parameter size, training\ndifficulty, and performance bottlenecks for each scenario. To address these\nissues, we propose a novel framework called OptMSM (\\textbf{Opt}imizing\n\\textbf{M}ulti-\\textbf{S}cenario \\textbf{M}odeling). First, we introduce a\nsimplified yet effective scenario-enhanced learning module to alleviate the\naforementioned challenges. Specifically, we partition the input features into\nscenario-specific and scenario-shared features, which are mapped to specific\ninformation embedding encodings and a set of shared information embeddings,\nrespectively. By imposing an orthogonality constraint on the shared information\nembeddings to facilitate the disentanglement of shared information\ncorresponding to each scenario, we combine them with the specific information\nembeddings to obtain multi-functional representations. Second, we introduce a\nscenario-specific hypernetwork in the scenario-specific prediction module to\ncapture interactions within each scenario more effectively, thereby alleviating\nthe performance bottlenecks. Finally, we conduct extensive offline experiments\nand an online A/B test to demonstrate the effectiveness of OptMSM.",
        "translated": ""
    },
    {
        "title": "Human Activity Behavioural Pattern Recognition in Smarthome with\n  Long-hour Data Collection",
        "url": "http://arxiv.org/abs/2306.13374v1",
        "pub_date": "2023-06-23",
        "summary": "The research on human activity recognition has provided novel solutions to\nmany applications like healthcare, sports, and user profiling. Considering the\ncomplex nature of human activities, it is still challenging even after\neffective and efficient sensors are available. The existing works on human\nactivity recognition using smartphone sensors focus on recognizing basic human\nactivities like sitting, sleeping, standing, stair up and down and running.\nHowever, more than these basic activities is needed to analyze human\nbehavioural pattern. The proposed framework recognizes basic human activities\nusing deep learning models. Also, ambient sensors like PIR, pressure sensors,\nand smartphone-based sensors like accelerometers and gyroscopes are combined to\nmake it hybrid-sensor-based human activity recognition. The hybrid approach\nhelped derive more activities than the basic ones, which also helped derive\nhuman activity patterns or user profiling. User profiling provides sufficient\ninformation to identify daily living activity patterns and predict whether any\nanomaly exists. The framework provides the base for applications such as\nelderly monitoring when they are alone at home. The GRU model's accuracy of\n95\\% is observed to recognize the basic activities. Finally, Human activity\npatterns over time are recognized based on the duration and frequency of the\nactivities. It is observed that human activity pattern, like, morning walking\nduration, varies depending on the day of the week.",
        "translated": ""
    },
    {
        "title": "A Decade of Scholarly Research on Open Knowledge Graphs",
        "url": "http://arxiv.org/abs/2306.13186v1",
        "pub_date": "2023-06-22",
        "summary": "The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.",
        "translated": ""
    },
    {
        "title": "An overview on the evaluated video retrieval tasks at TRECVID 2022",
        "url": "http://arxiv.org/abs/2306.13118v1",
        "pub_date": "2023-06-22",
        "summary": "The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis\nand retrieval evaluation with the goal of promoting progress in research and\ndevelopment of content-based exploitation and retrieval of information from\ndigital video via open, tasks-based evaluation supported by metrology. Over the\nlast twenty-one years this effort has yielded a better understanding of how\nsystems can effectively accomplish such processing and how one can reliably\nbenchmark their performance. TRECVID has been funded by NIST (National\nInstitute of Standards and Technology) and other US government agencies. In\naddition, many organizations and individuals worldwide contribute significant\ntime and effort. TRECVID 2022 planned for the following six tasks: Ad-hoc video\nsearch, Video to text captioning, Disaster scene description and indexing,\nActivity in extended videos, deep video understanding, and movie summarization.\nIn total, 35 teams from various research organizations worldwide signed up to\njoin the evaluation campaign this year. This paper introduces the tasks,\ndatasets used, evaluation frameworks and metrics, as well as a high-level\nresults overview.",
        "translated": ""
    },
    {
        "title": "Scalable Neural Contextual Bandit for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.14834v1",
        "pub_date": "2023-06-26",
        "summary": "High-quality recommender systems ought to deliver both innovative and\nrelevant content through effective and exploratory interactions with users.\nYet, supervised learning-based neural networks, which form the backbone of many\nexisting recommender systems, only leverage recognized user interests, falling\nshort when it comes to efficiently uncovering unknown user preferences. While\nthere has been some progress with neural contextual bandit algorithms towards\nenabling online exploration through neural networks, their onerous\ncomputational demands hinder widespread adoption in real-world recommender\nsystems. In this work, we propose a scalable sample-efficient neural contextual\nbandit algorithm for recommender systems. To do this, we design an epistemic\nneural network architecture, Epistemic Neural Recommendation (ENR), that\nenables Thompson sampling at a large scale. In two distinct large-scale\nexperiments with real-world tasks, ENR significantly boosts click-through rates\nand user ratings by at least 9% and 6% respectively compared to\nstate-of-the-art neural contextual bandit algorithms. Furthermore, it achieves\nequivalent performance with at least 29% fewer user interactions compared to\nthe best-performing baseline algorithm. Remarkably, while accomplishing these\nimprovements, ENR demands orders of magnitude fewer computational resources\nthan neural contextual bandit baseline algorithms.",
        "translated": ""
    },
    {
        "title": "Reciprocal Sequential Recommendation",
        "url": "http://arxiv.org/abs/2306.14712v1",
        "pub_date": "2023-06-26",
        "summary": "Reciprocal recommender system (RRS), considering a two-way matching between\ntwo parties, has been widely applied in online platforms like online dating and\nrecruitment. Existing RRS models mainly capture static user preferences, which\nhave neglected the evolving user tastes and the dynamic matching relation\nbetween the two parties. Although dynamic user modeling has been well-studied\nin sequential recommender systems, existing solutions are developed in a\nuser-oriented manner. Therefore, it is non-trivial to adapt sequential\nrecommendation algorithms to reciprocal recommendation. In this paper, we\nformulate RRS as a distinctive sequence matching task, and further propose a\nnew approach ReSeq for RRS, which is short for Reciprocal Sequential\nrecommendation. To capture dual-perspective matching, we propose to learn\nfine-grained sequence similarities by co-attention mechanism across different\ntime steps. Further, to improve the inference efficiency, we introduce the\nself-distillation technique to distill knowledge from the fine-grained matching\nmodule into the more efficient student module. In the deployment stage, only\nthe efficient student module is used, greatly speeding up the similarity\ncomputation. Extensive experiments on five real-world datasets from two\nscenarios demonstrate the effectiveness and efficiency of the proposed method.\nOur code is available at https://github.com/RUCAIBox/ReSeq/.",
        "translated": ""
    },
    {
        "title": "PTVD: A Large-Scale Plot-Oriented Multimodal Dataset Based on Television\n  Dramas",
        "url": "http://arxiv.org/abs/2306.14644v1",
        "pub_date": "2023-06-26",
        "summary": "Art forms such as movies and television (TV) dramas are reflections of the\nreal world, which have attracted much attention from the multimodal learning\ncommunity recently. However, existing corpora in this domain share three\nlimitations: (1) annotated in a scene-oriented fashion, they ignore the\ncoherence within plots; (2) their text lacks empathy and seldom mentions\nsituational context; (3) their video clips fail to cover long-form relationship\ndue to short duration. To address these fundamental issues, using 1,106 TV\ndrama episodes and 24,875 informative plot-focused sentences written by\nprofessionals, with the help of 449 human annotators, we constructed PTVD, the\nfirst plot-oriented multimodal dataset in the TV domain. It is also the first\nnon-English dataset of its kind. Additionally, PTVD contains more than 26\nmillion bullet screen comments (BSCs), powering large-scale pre-training. Next,\naiming to open-source a strong baseline for follow-up works, we developed the\nmultimodal algorithm that attacks different cinema/TV modelling problems with a\nunified architecture. Extensive experiments on three cognitive-inspired tasks\nyielded a number of novel observations (some of them being quite\ncounter-intuition), further validating the value of PTVD in promoting\nmultimodal research. The dataset and codes are released at\n\\url{https://ptvd.github.io/}.",
        "translated": ""
    },
    {
        "title": "Multi-task Item-attribute Graph Pre-training for Strict Cold-start Item\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.14462v1",
        "pub_date": "2023-06-26",
        "summary": "Recommendation systems suffer in the strict cold-start (SCS) scenario, where\nthe user-item interactions are entirely unavailable. The ID-based approaches\ncompletely fail to work. Cold-start recommenders, on the other hand, leverage\nitem contents to map the new items to the existing ones. However, the existing\nSCS recommenders explore item contents in coarse-grained manners that introduce\nnoise or information loss. Moreover, informative data sources other than item\ncontents, such as users' purchase sequences and review texts, are ignored. We\nexplore the role of the fine-grained item attributes in bridging the gaps\nbetween the existing and the SCS items and pre-train a knowledgeable\nitem-attribute graph for SCS item recommendation. Our proposed framework,\nColdGPT, models item-attribute correlations into an item-attribute graph by\nextracting fine-grained attributes from item contents. ColdGPT then transfers\nknowledge into the item-attribute graph from various available data sources,\ni.e., item contents, historical purchase sequences, and review texts of the\nexisting items, via multi-task learning. To facilitate the positive transfer,\nColdGPT designs submodules according to the natural forms of the data sources\nand coordinates the multiple pre-training tasks via unified\nalignment-and-uniformity losses. Our pre-trained item-attribute graph acts as\nan implicit, extendable item embedding matrix, which enables the SCS item\nembeddings to be easily acquired by inserting these items and propagating their\nattributes' embeddings. We carefully process three public datasets, i.e., Yelp,\nAmazon-home, and Amazon-sports, to guarantee the SCS setting for evaluation.\nExtensive experiments show that ColdGPT consistently outperforms the existing\nSCS recommenders by large margins and even surpasses models that are\npre-trained on 75-224 times more, cross-domain data on two out of four\ndatasets.",
        "translated": ""
    },
    {
        "title": "Contrastive Multi-view Framework for Customer Lifetime Value Prediction",
        "url": "http://arxiv.org/abs/2306.14400v1",
        "pub_date": "2023-06-26",
        "summary": "Accurate customer lifetime value (LTV) prediction can help service providers\noptimize their marketing policies in customer-centric applications. However,\nthe heavy sparsity of consumption events and the interference of data variance\nand noise obstruct LTV estimation. Many existing LTV prediction methods\ndirectly train a single-view LTV predictor on consumption samples, which may\nyield inaccurate and even biased knowledge extraction. In this paper, we\npropose a contrastive multi-view framework for LTV prediction, which is a\nplug-and-play solution compatible with various backbone models. It synthesizes\nmultiple heterogeneous LTV regressors with complementary knowledge to improve\nmodel robustness and captures sample relatedness via contrastive learning to\nmitigate the dependency on data abundance. Concretely, we use a decomposed\nscheme that converts the LTV prediction problem into a combination of\nestimating consumption probability and payment amount. To alleviate the impact\nof noisy data on model learning, we propose a multi-view framework that jointly\noptimizes multiple types of regressors with diverse characteristics and\nadvantages to encode and fuse comprehensive knowledge. To fully exploit the\npotential of limited training samples, we propose a hybrid contrastive learning\nmethod to help capture the relatedness between samples in both classification\nand regression tasks. We conduct extensive experiments on a real-world game LTV\nprediction dataset and the results validate the effectiveness of our method. We\nhave deployed our solution online in Huawei's mobile game center and achieved\n32.26% of total payment amount gains.",
        "translated": ""
    },
    {
        "title": "G-STO: Sequential Main Shopping Intention Detection via\n  Graph-Regularized Stochastic Transformer",
        "url": "http://arxiv.org/abs/2306.14314v1",
        "pub_date": "2023-06-25",
        "summary": "Sequential recommendation requires understanding the dynamic patterns of\nusers' behaviors, contexts, and preferences from their historical interactions.\nMost existing works focus on modeling user-item interactions only from the item\nlevel, ignoring that they are driven by latent shopping intentions (e.g.,\nballpoint pens, miniatures, etc). The detection of the underlying shopping\nintentions of users based on their historical interactions is a crucial aspect\nfor e-commerce platforms, such as Amazon, to enhance the convenience and\nefficiency of their customers' shopping experiences. Despite its significance,\nthe area of main shopping intention detection remains under-investigated in the\nacademic literature. To fill this gap, we propose a graph-regularized\nstochastic Transformer method, G-STO. By considering intentions as sets of\nproducts and user preferences as compositions of intentions, we model both of\nthem as stochastic Gaussian embeddings in the latent representation space.\nInstead of training the stochastic representations from scratch, we develop a\nglobal intention relational graph as prior knowledge for regularization,\nallowing relevant shopping intentions to be distributionally close. Finally, we\nfeed the newly regularized stochastic embeddings into Transformer-based models\nto encode sequential information from the intention transitions. We evaluate\nour main shopping intention identification model on three different real-world\ndatasets, where G-STO achieves significantly superior performances to the\nbaselines by 18.08% in Hit@1, 7.01% in Hit@10, and 6.11% in NDCG@10 on average.",
        "translated": ""
    },
    {
        "title": "RecBaselines2023: a new dataset for choosing baselines for recommender\n  models",
        "url": "http://arxiv.org/abs/2306.14292v1",
        "pub_date": "2023-06-25",
        "summary": "The number of proposed recommender algorithms continues to grow. The authors\npropose new approaches and compare them with existing models, called baselines.\nDue to the large number of recommender models, it is difficult to estimate\nwhich algorithms to choose in the article. To solve this problem, we have\ncollected and published a dataset containing information about the recommender\nmodels used in 903 papers, both as baselines and as proposed approaches. This\ndataset can be seen as a typical dataset with interactions between papers and\npreviously proposed models. In addition, we provide a descriptive analysis of\nthe dataset and highlight possible challenges to be investigated with the data.\nFurthermore, we have conducted extensive experiments using a well-established\nmethodology to build a good recommender algorithm under the dataset. Our\nexperiments show that the selection of the best baselines for proposing new\nrecommender approaches can be considered and successfully solved by existing\nstate-of-the-art collaborative filtering models. Finally, we discuss\nlimitations and future work.",
        "translated": ""
    },
    {
        "title": "Mining Stable Preferences: Adaptive Modality Decorrelation for\n  Multimedia Recommendation",
        "url": "http://arxiv.org/abs/2306.14179v1",
        "pub_date": "2023-06-25",
        "summary": "Multimedia content is of predominance in the modern Web era. In real\nscenarios, multiple modalities reveal different aspects of item attributes and\nusually possess different importance to user purchase decisions. However, it is\ndifficult for models to figure out users' true preference towards different\nmodalities since there exists strong statistical correlation between\nmodalities. Even worse, the strong statistical correlation might mislead models\nto learn the spurious preference towards inconsequential modalities. As a\nresult, when data (modal features) distribution shifts, the learned spurious\npreference might not guarantee to be as effective on the inference set as on\nthe training set. We propose a novel MOdality DEcorrelating STable learning\nframework, MODEST for brevity, to learn users' stable preference. Inspired by\nsample re-weighting techniques, the proposed method aims to estimate a weight\nfor each item, such that the features from different modalities in the weighted\ndistribution are decorrelated. We adopt Hilbert Schmidt Independence Criterion\n(HSIC) as independence testing measure which is a kernel-based method capable\nof evaluating the correlation degree between two multi-dimensional and\nnon-linear variables. Our method could be served as a play-and-plug module for\nexisting multimedia recommendation backbones. Extensive experiments on four\npublic datasets and four state-of-the-art multimedia recommendation backbones\nunequivocally show that our proposed method can improve the performances by a\nlarge margin.",
        "translated": ""
    },
    {
        "title": "Enhancing Dynamic Image Advertising with Vision-Language Pre-training",
        "url": "http://arxiv.org/abs/2306.14112v1",
        "pub_date": "2023-06-25",
        "summary": "In the multimedia era, image is an effective medium in search advertising.\nDynamic Image Advertising (DIA), a system that matches queries with ad images\nand generates multimodal ads, is introduced to improve user experience and ad\nrevenue. The core of DIA is a query-image matching module performing ad image\nretrieval and relevance modeling. Current query-image matching suffers from\nlimited and inconsistent data, and insufficient cross-modal interaction. Also,\nthe separate optimization of retrieval and relevance models affects overall\nperformance. To address this issue, we propose a vision-language framework\nconsisting of two parts. First, we train a base model on large-scale image-text\npairs to learn general multimodal representation. Then, we fine-tune the base\nmodel on advertising business data, unifying relevance modeling and retrieval\nthrough multi-objective learning. Our framework has been implemented in Baidu\nsearch advertising system \"Phoneix Nest\". Online evaluation shows that it\nimproves cost per mille (CPM) and click-through rate (CTR) by 1.04% and 1.865%.",
        "translated": ""
    },
    {
        "title": "Cross-domain Recommender Systems via Multimodal Domain Adaptation",
        "url": "http://arxiv.org/abs/2306.13887v1",
        "pub_date": "2023-06-24",
        "summary": "Collaborative Filtering (CF) has emerged as one of the most prominent\nimplementation strategies for building recommender systems. The key idea is to\nexploit the usage patterns of individuals to generate personalized\nrecommendations. CF techniques, especially for newly launched platforms, often\nface a critical issue known as the data sparsity problem, which greatly limits\ntheir performance. Several approaches have been proposed in the literature to\ntackle the problem of data sparsity, among which cross-domain collaborative\nfiltering (CDCF) has gained significant attention in the recent past. In order\nto compensate for the scarcity of available feedback in a target domain, the\nCDCF approach makes use of information available in other auxiliary domains.\nMost of the traditional CDCF approach aim is to find a common set of entities\n(users or items) across the domains and then use them as a bridge for knowledge\ntransfer. However, most real-world datasets are collected from different\ndomains, so they often lack information about anchor points or reference\ninformation for entity alignment. In this paper, we propose a domain adaptation\ntechnique to align the embeddings of users and items across the two domains.\nOur approach first exploits the available textual and visual information to\nindependently learn a multi-view latent representation for each user and item\nin the auxiliary and target domains. The different representations of a user or\nitem are then fused to generate the corresponding unified representation. A\ndomain classifier is then trained to learn the embedding for the domain\nalignment by fixing the unified features as the anchor points. Experiments on\ntwo publicly benchmark datasets indicate the effectiveness of our proposed\napproach.",
        "translated": ""
    },
    {
        "title": "Unleashing the Power of User Reviews: Exploring Airline Choices at\n  Catania Airport, Italy",
        "url": "http://arxiv.org/abs/2306.15541v1",
        "pub_date": "2023-06-27",
        "summary": "This study aims to investigate the possible relationship between the\nmechanisms of social influence and the choice of airline, through the use of\nnew tools, with the aim of understanding whether they can contribute to a\nbetter understanding of the factors influencing the decisions of consumers in\nthe aviation sector. We have chosen to extract user reviews from well-known\nplatforms: Trustpilot, Google, and Twitter. By combining web scraping\ntechniques, we have been able to collect a comprehensive dataset comprising a\nwide range of user opinions, feedback, and ratings. We then refined the BERT\nmodel to focus on insightful sentiment in the context of airline reviews.\nThrough our analysis, we observed an intriguing trend of average negative\nsentiment scores across various airlines, giving us deeper insight into the\ndynamics between airlines and helping us identify key partnerships, popular\nroutes, and airlines that play a central role in the aeronautical ecosystem of\nCatania airport during the specified period. Our investigation led us to find\nthat, despite an airline having received prestigious awards as a low-cost\nleader in Europe for two consecutive years 2021 and 2022, the \"Catanese\" user\ntends to suffer the dominant position of other companies. Understanding the\nimpact of positive reviews and leveraging sentiment analysis can help airlines\nimprove their reputation, attract more customers, and ultimately gain a\ncompetitive edge in the marketplace.",
        "translated": ""
    },
    {
        "title": "Learning to Rank in Generative Retrieval",
        "url": "http://arxiv.org/abs/2306.15222v1",
        "pub_date": "2023-06-27",
        "summary": "Generative retrieval is a promising new paradigm in text retrieval that\ngenerates identifier strings of relevant passages as the retrieval target. This\nparadigm leverages powerful generation models and represents a new paradigm\ndistinct from traditional learning-to-rank methods. However, despite its rapid\ndevelopment, current generative retrieval methods are still limited. They\ntypically rely on a heuristic function to transform predicted identifiers into\na passage rank list, which creates a gap between the learning objective of\ngenerative retrieval and the desired passage ranking target. Moreover, the\ninherent exposure bias problem of text generation also persists in generative\nretrieval. To address these issues, we propose a novel framework, called LTRGR,\nthat combines generative retrieval with the classical learning-to-rank\nparadigm. Our approach involves training an autoregressive model using a\npassage rank loss, which directly optimizes the autoregressive model toward the\noptimal passage ranking. This framework only requires an additional training\nstep to enhance current generative retrieval systems and does not add any\nburden to the inference stage. We conducted experiments on three public\ndatasets, and our results demonstrate that LTRGR achieves state-of-the-art\nperformance among generative retrieval methods, indicating its effectiveness\nand robustness.",
        "translated": ""
    },
    {
        "title": "Off-Policy Evaluation of Ranking Policies under Diverse User Behavior",
        "url": "http://arxiv.org/abs/2306.15098v1",
        "pub_date": "2023-06-26",
        "summary": "Ranking interfaces are everywhere in online platforms. There is thus an ever\ngrowing interest in their Off-Policy Evaluation (OPE), aiming towards an\naccurate performance evaluation of ranking policies using logged data. A\nde-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides\nan unbiased and consistent value estimate. However, it becomes extremely\ninaccurate in the ranking setup due to its high variance under large action\nspaces. To deal with this problem, previous studies assume either independent\nor cascade user behavior, resulting in some ranking versions of IPS. While\nthese estimators are somewhat effective in reducing the variance, all existing\nestimators apply a single universal assumption to every user, causing excessive\nbias and variance. Therefore, this work explores a far more general formulation\nwhere user behavior is diverse and can vary depending on the user context. We\nshow that the resulting estimator, which we call Adaptive IPS (AIPS), can be\nunbiased under any complex user behavior. Moreover, AIPS achieves the minimum\nvariance among all unbiased estimators based on IPS. We further develop a\nprocedure to identify the appropriate user behavior model to minimize the mean\nsquared error (MSE) of AIPS in a data-driven fashion. Extensive experiments\ndemonstrate that the empirical accuracy improvement can be significant,\nenabling effective OPE of ranking systems even under diverse user behavior.",
        "translated": ""
    },
    {
        "title": "Efficient High-Resolution Template Matching with Vector Quantized\n  Nearest Neighbour Fields",
        "url": "http://arxiv.org/abs/2306.15010v1",
        "pub_date": "2023-06-26",
        "summary": "Template matching is a fundamental problem in computer vision and has\napplications in various fields, such as object detection, image registration,\nand object tracking. The current state-of-the-art methods rely on\nnearest-neighbour (NN) matching in which the query feature space is converted\nto NN space by representing each query pixel with its NN in the template\npixels. The NN-based methods have been shown to perform better in occlusions,\nchanges in appearance, illumination variations, and non-rigid transformations.\nHowever, NN matching scales poorly with high-resolution data and high feature\ndimensions. In this work, we present an NN-based template-matching method which\nefficiently reduces the NN computations and introduces filtering in the NN\nfields to consider deformations. A vector quantization step first represents\nthe template with $k$ features, then filtering compares the template and query\ndistributions over the $k$ features. We show that state-of-the-art performance\nwas achieved in low-resolution data, and our method outperforms previous\nmethods at higher resolution showing the robustness and scalability of the\napproach.",
        "translated": ""
    },
    {
        "title": "SE-PQA: Personalized Community Question Answering",
        "url": "http://arxiv.org/abs/2306.16261v1",
        "pub_date": "2023-06-28",
        "summary": "Personalization in Information Retrieval is a topic studied for a long time.\nNevertheless, there is still a lack of high-quality, real-world datasets to\nconduct large-scale experiments and evaluate models for personalized search.\nThis paper contributes to filling this gap by introducing SE-PQA (StackExchange\n- Personalized Question Answering), a new curated resource to design and\nevaluate personalized models related to the task of community Question\nAnswering (cQA). The contributed dataset includes more than 1 million queries\nand 2 million answers, annotated with a rich set of features modeling the\nsocial interactions among the users of a popular cQA platform. We describe the\ncharacteristics of SE-PQA and detail the features associated with questions and\nanswers. We also provide reproducible baseline methods for the cQA task based\non the resource, including deep learning models and personalization approaches.\nThe results of the preliminary experiments conducted show the appropriateness\nof SE-PQA to train effective cQA models; they also show that personalization\nremarkably improves the effectiveness of all the methods tested. Furthermore,\nwe show the benefits in terms of robustness and generalization of combining\ndata from multiple communities for personalization purposes.",
        "translated": ""
    },
    {
        "title": "Query Understanding in the Age of Large Language Models",
        "url": "http://arxiv.org/abs/2306.16004v1",
        "pub_date": "2023-06-28",
        "summary": "Querying, conversing, and controlling search and information-seeking\ninterfaces using natural language are fast becoming ubiquitous with the rise\nand adoption of large-language models (LLM). In this position paper, we\ndescribe a generic framework for interactive query-rewriting using LLMs. Our\nproposal aims to unfold new opportunities for improved and transparent intent\nunderstanding while building high-performance retrieval systems using LLMs. A\nkey aspect of our framework is the ability of the rewriter to fully specify the\nmachine intent by the search engine in natural language that can be further\nrefined, controlled, and edited before the final retrieval phase. The ability\nto present, interact, and reason over the underlying machine intent in natural\nlanguage has profound implications on transparency, ranking performance, and a\ndeparture from the traditional way in which supervised signals were collected\nfor understanding intents. We detail the concept, backed by initial\nexperiments, along with open questions for this interactive query understanding\nframework.",
        "translated": ""
    },
    {
        "title": "Streamlining Social Media Information Retrieval for Public Health\n  Research with Deep Learning",
        "url": "http://arxiv.org/abs/2306.16001v1",
        "pub_date": "2023-06-28",
        "summary": "The utilization of social media in epidemic surveillance has been well\nestablished. Nonetheless, bias is often introduced when pre-defined lexicons\nare used to retrieve relevant corpus. This study introduces a framework aimed\nat curating extensive dictionaries of medical colloquialisms and Unified\nMedical Language System (UMLS) concepts. The framework comprises three modules:\na BERT-based Named Entity Recognition (NER) model that identifies medical\nentities from social media content, a deep-learning powered normalization\nmodule that standardizes the extracted entities, and a semi-supervised\nclustering module that assigns the most probable UMLS concept to each\nstandardized entity. We applied this framework to COVID-19-related tweets from\nFebruary 1, 2020, to April 30, 2022, generating a symptom dictionary (available\nat https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249\nstandardized entities mapped to 876 UMLS concepts and 38,175 colloquial\nexpressions. This framework demonstrates encouraging potential in addressing\nthe constraints of keyword matching information retrieval in social media-based\npublic health research.",
        "translated": ""
    },
    {
        "title": "Disentangled Variational Auto-encoder Enhanced by Counterfactual Data\n  for Debiasing Recommendation",
        "url": "http://arxiv.org/abs/2306.15961v1",
        "pub_date": "2023-06-28",
        "summary": "Recommender system always suffers from various recommendation biases,\nseriously hindering its development. In this light, a series of debias methods\nhave been proposed in the recommender system, especially for two most common\nbiases, i.e., popularity bias and amplified subjective bias. However, exsisting\ndebias methods usually concentrate on correcting a single bias. Such\nsingle-functionality debiases neglect the bias-coupling issue in which the\nrecommended items are collectively attributed to multiple biases. Besides,\nprevious work cannot tackle the lacking supervised signals brought by sparse\ndata, yet which has become a commonplace in the recommender system. In this\nwork, we introduce a disentangled debias variational auto-encoder\nframework(DB-VAE) to address the single-functionality issue as well as a\ncounterfactual data enhancement method to mitigate the adverse effect due to\nthe data sparsity. In specific, DB-VAE first extracts two types of extreme\nitems only affected by a single bias based on the collier theory, which are\nrespectively employed to learn the latent representation of corresponding\nbiases, thereby realizing the bias decoupling. In this way, the exact unbiased\nuser representation can be learned by these decoupled bias representations.\nFurthermore, the data generation module employs Pearl's framework to produce\nmassive counterfactual data, making up the lacking supervised signals due to\nthe sparse data. Extensive experiments on three real-world datasets demonstrate\nthe effectiveness of our proposed model. Besides, the counterfactual data can\nfurther improve DB-VAE, especially on the dataset with low sparsity.",
        "translated": ""
    },
    {
        "title": "Pb-Hash: Partitioned b-bit Hashing",
        "url": "http://arxiv.org/abs/2306.15944v1",
        "pub_date": "2023-06-28",
        "summary": "Many hashing algorithms including minwise hashing (MinHash), one permutation\nhashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$\nbits. With $k$ hashes for each data vector, the storage would be $B\\times k$\nbits; and when used for large-scale learning, the model size would be\n$2^B\\times k$, which can be expensive. A standard strategy is to use only the\nlowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of\nhashes. In this study, we propose to re-use the hashes by partitioning the $B$\nbits into $m$ chunks, e.g., $b\\times m =B$. Correspondingly, the model size\nbecomes $m\\times 2^b \\times k$, which can be substantially smaller than the\noriginal $2^B\\times k$.\n  Our theoretical analysis reveals that by partitioning the hash values into\n$m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$\nbits would not be as accurate as directly using $B$ bits. This is due to the\ncorrelation from re-using the same hash. On the other hand, our analysis also\nshows that the accuracy would not drop much for (e.g.,) $m=2\\sim 4$. In some\nregions, Pb-Hash still works well even for $m$ much larger than 4. We expect\nPb-Hash would be a good addition to the family of hashing methods/applications\nand benefit industrial practitioners.\n  We verify the effectiveness of Pb-Hash in machine learning tasks, for linear\nSVM models as well as deep learning models. Since the hashed data are\nessentially categorical (ID) features, we follow the standard practice of using\nembedding tables for each hash. With Pb-Hash, we need to design an effective\nstrategy to combine $m$ embeddings. Our study provides an empirical evaluation\non four pooling schemes: concatenation, max pooling, mean pooling, and product\npooling. There is no definite answer which pooling would be always better and\nwe leave that for future study.",
        "translated": ""
    },
    {
        "title": "Confidence-Calibrated Ensemble Dense Phrase Retrieval",
        "url": "http://arxiv.org/abs/2306.15917v1",
        "pub_date": "2023-06-28",
        "summary": "In this paper, we consider the extent to which the transformer-based Dense\nPassage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can\nbe optimized without further pre-training. Our method involves two particular\ninsights: we apply the DPR context encoder at various phrase lengths (e.g.\none-sentence versus five-sentence segments), and we take a\nconfidence-calibrated ensemble prediction over all of these different\nsegmentations. This somewhat exhaustive approach achieves start-of-the-art\nresults on benchmark datasets such as Google NQ and SQuAD. We also apply our\nmethod to domain-specific datasets, and the results suggest how different\ngranularities are optimal for different domains",
        "translated": ""
    },
    {
        "title": "Dimension Independent Mixup for Hard Negative Sample in Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2306.15905v1",
        "pub_date": "2023-06-28",
        "summary": "Collaborative filtering (CF) is a widely employed technique that predicts\nuser preferences based on past interactions. Negative sampling plays a vital\nrole in training CF-based models with implicit feedback. In this paper, we\npropose a novel perspective based on the sampling area to revisit existing\nsampling methods. We point out that current sampling methods mainly focus on\nPoint-wise or Line-wise sampling, lacking flexibility and leaving a significant\nportion of the hard sampling area un-explored. To address this limitation, we\npropose Dimension Independent Mixup for Hard Negative Sampling (DINS), which is\nthe first Area-wise sampling method for training CF-based models. DINS\ncomprises three modules: Hard Boundary Definition, Dimension Independent Mixup,\nand Multi-hop Pooling. Experiments with real-world datasets on both matrix\nfactorization and graph-based models demonstrate that DINS outperforms other\nnegative sampling methods, establishing its effectiveness and superiority. Our\nwork contributes a new perspective, introduces Area-wise sampling, and presents\nDINS as a novel approach that achieves state-of-the-art performance for\nnegative sampling. Our implementations are available in PyTorch.",
        "translated": ""
    },
    {
        "title": "Blockwise Feature Interaction in Recommendation Systems",
        "url": "http://arxiv.org/abs/2306.15881v1",
        "pub_date": "2023-06-28",
        "summary": "Feature interactions can play a crucial role in recommendation systems as\nthey capture complex relationships between user preferences and item\ncharacteristics. Existing methods such as Deep &amp; Cross Network (DCNv2) may\nsuffer from high computational requirements due to their cross-layer\noperations. In this paper, we propose a novel approach called blockwise feature\ninteraction (BFI) to help alleviate this issue. By partitioning the feature\ninteraction process into smaller blocks, we can significantly reduce both the\nmemory footprint and the computational burden. Four variants (denoted by P, Q,\nT, S, respectively) of BFI have been developed and empirically compared. Our\nexperimental results demonstrate that the proposed algorithms achieves close\naccuracy compared to the standard DCNv2, while greatly reducing the\ncomputational overhead and the number of parameters. This paper contributes to\nthe development of efficient recommendation systems by providing a practical\nsolution for improving feature interaction efficiency.",
        "translated": ""
    },
    {
        "title": "Ducho: A Unified Framework for the Extraction of Multimodal Features in\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.17125v1",
        "pub_date": "2023-06-29",
        "summary": "In multimodal-aware recommendation, the extraction of meaningful multimodal\nfeatures is at the basis of high-quality recommendations. Generally, each\nrecommendation framework implements its multimodal extraction procedures with\nspecific strategies and tools. This is limiting for two reasons: (i) different\nextraction strategies do not ease the interdependence among multimodal\nrecommendation frameworks; thus, they cannot be efficiently and fairly\ncompared; (ii) given the large plethora of pre-trained deep learning models\nmade available by different open source tools, model designers do not have\naccess to shared interfaces to extract features. Motivated by the outlined\naspects, we propose Ducho, a unified framework for the extraction of multimodal\nfeatures in recommendation. By integrating three widely-adopted deep learning\nlibraries as backends, namely, TensorFlow, PyTorch, and Transformers, we\nprovide a shared interface to extract and process features where each backend's\nspecific methods are abstracted to the end user. Noteworthy, the extraction\npipeline is easily configurable with a YAML-based file where the user can\nspecify, for each modality, the list of models (and their specific\nbackends/parameters) to perform the extraction. Finally, to make Ducho\naccessible to the community, we build a public Docker image equipped with a\nready-to-use CUDA environment and propose three demos to test its\nfunctionalities for different scenarios and tasks. The GitHub repository and\nthe documentation is accessible at this link:\nhttps://github.com/sisinflab/Ducho.",
        "translated": ""
    },
    {
        "title": "Re-Rank - Expand - Repeat: Adaptive Query Expansion for Document\n  Retrieval Using Words and Entities",
        "url": "http://arxiv.org/abs/2306.17082v1",
        "pub_date": "2023-06-29",
        "summary": "Sparse and dense pseudo-relevance feedback (PRF) approaches perform poorly on\nchallenging queries due to low precision in first-pass retrieval. However,\nrecent advances in neural language models (NLMs) can re-rank relevant documents\nto top ranks, even when few are in the re-ranking pool. This paper first\naddresses the problem of poor pseudo-relevance feedback by simply applying\nre-ranking prior to query expansion and re-executing this query. We find that\nthis change alone can improve the retrieval effectiveness of sparse and dense\nPRF approaches by 5-8%. Going further, we propose a new expansion model, Latent\nEntity Expansion (LEE), a fine-grained word and entity-based relevance\nmodelling incorporating localized features. Finally, we include an \"adaptive\"\ncomponent to the retrieval process, which iteratively refines the re-ranking\npool during scoring using the expansion model, i.e. we \"re-rank - expand -\nrepeat\". Using LEE, we achieve (to our knowledge) the best NDCG, MAP and R@1000\nresults on the TREC Robust 2004 and CODEC adhoc document datasets,\ndemonstrating a significant advancement in expansion effectiveness.",
        "translated": ""
    },
    {
        "title": "Harnessing the Power of Hugging Face Transformers for Predicting Mental\n  Health Disorders in Social Networks",
        "url": "http://arxiv.org/abs/2306.16891v1",
        "pub_date": "2023-06-29",
        "summary": "Early diagnosis of mental disorders and intervention can facilitate the\nprevention of severe injuries and the improvement of treatment results. Using\nsocial media and pre-trained language models, this study explores how\nuser-generated data can be used to predict mental disorder symptoms. Our study\ncompares four different BERT models of Hugging Face with standard machine\nlearning techniques used in automatic depression diagnosis in recent\nliterature. The results show that new models outperform the previous approach\nwith an accuracy rate of up to 97%. Analyzing the results while complementing\npast findings, we find that even tiny amounts of data (like users' bio\ndescriptions) have the potential to predict mental disorders. We conclude that\nsocial media data is an excellent source of mental health screening, and\npre-trained models can effectively automate this critical task.",
        "translated": ""
    },
    {
        "title": "Computing all-vs-all MEMs in grammar-compressed text",
        "url": "http://arxiv.org/abs/2306.16815v1",
        "pub_date": "2023-06-29",
        "summary": "We describe a compression-aware method to compute all-vs-all maximal exact\nmatches (MEM) among strings of a repetitive collection $\\mathcal{T}$. The key\nconcept in our work is the construction of a fully-balanced grammar\n$\\mathcal{G}$ from $\\mathcal{T}$ that meets a property that we call\n\\emph{fix-free}: the expansions of the nonterminals that have the same height\nin the parse tree form a fix-free set (i.e., prefix-free and suffix-free). The\nfix-free property allows us to compute the MEMs of $\\mathcal{T}$ incrementally\nover $\\mathcal{G}$ using a standard suffix-tree-based MEM algorithm, which runs\non a subset of grammar rules at a time and does not decompress nonterminals. By\nmodifying the locally-consistent grammar of Christiansen et al 2020., we show\nhow we can build $\\mathcal{G}$ from $\\mathcal{T}$ in linear time and space. We\nalso demonstrate that our MEM algorithm runs on top of $\\mathcal{G}$ in $O(G\n+occ)$ time and uses $O(\\log G(G+occ))$ bits, where $G$ is the grammar size,\nand $occ$ is the number of MEMs in $\\mathcal{T}$. In the conclusions, we\ndiscuss how our idea can be modified to implement approximate pattern matching\nin compressed space.",
        "translated": ""
    },
    {
        "title": "Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall\n  Classification",
        "url": "http://arxiv.org/abs/2306.16760v1",
        "pub_date": "2023-06-29",
        "summary": "We present working notes on transfer learning with semi-supervised dataset\nannotation for the BirdCLEF 2023 competition, focused on identifying African\nbird species in recorded soundscapes. Our approach utilizes existing\noff-the-shelf models, BirdNET and MixIT, to address representation and labeling\nchallenges in the competition. We explore the embedding space learned by\nBirdNET and propose a process to derive an annotated dataset for supervised\nlearning. Our experiments involve various models and feature engineering\napproaches to maximize performance on the competition leaderboard. The results\ndemonstrate the effectiveness of our approach in classifying bird species and\nhighlight the potential of transfer learning and semi-supervised dataset\nannotation in similar tasks.",
        "translated": ""
    },
    {
        "title": "Multi-Scenario Ranking with Adaptive Feature Learning",
        "url": "http://arxiv.org/abs/2306.16732v1",
        "pub_date": "2023-06-29",
        "summary": "Recently, Multi-Scenario Learning (MSL) is widely used in recommendation and\nretrieval systems in the industry because it facilitates transfer learning from\ndifferent scenarios, mitigating data sparsity and reducing maintenance cost.\nThese efforts produce different MSL paradigms by searching more optimal network\nstructure, such as Auxiliary Network, Expert Network, and Multi-Tower Network.\nIt is intuitive that different scenarios could hold their specific\ncharacteristics, activating the user's intents quite differently. In other\nwords, different kinds of auxiliary features would bear varying importance\nunder different scenarios. With more discriminative feature representations\nrefined in a scenario-aware manner, better ranking performance could be easily\nobtained without expensive search for the optimal network structure.\nUnfortunately, this simple idea is mainly overlooked but much desired in\nreal-world systems.Further analysis also validates the rationality of adaptive\nfeature learning under a multi-scenario scheme. Moreover, our A/B test results\non the Alibaba search advertising platform also demonstrate that Maria is\nsuperior in production environments.",
        "translated": ""
    },
    {
        "title": "Exploring the Representation Power of SPLADE Models",
        "url": "http://arxiv.org/abs/2306.16680v1",
        "pub_date": "2023-06-29",
        "summary": "The SPLADE (SParse Lexical AnD Expansion) model is a highly effective\napproach to learned sparse retrieval, where documents are represented by term\nimpact scores derived from large language models. During training, SPLADE\napplies regularization to ensure postings lists are kept sparse -- with the aim\nof mimicking the properties of natural term distributions -- allowing efficient\nand effective lexical matching and ranking. However, we hypothesize that SPLADE\nmay encode additional signals into common postings lists to further improve\neffectiveness. To explore this idea, we perform a number of empirical analyses\nwhere we re-train SPLADE with different, controlled vocabularies and measure\nhow effective it is at ranking passages. Our findings suggest that SPLADE can\neffectively encode useful ranking signals in documents even when the vocabulary\nis constrained to terms that are not traditionally useful for ranking, such as\nstopwords or even random words.",
        "translated": ""
    },
    {
        "title": "Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of\n  Information Retrieval Models",
        "url": "http://arxiv.org/abs/2306.16668v1",
        "pub_date": "2023-06-29",
        "summary": "As in other fields of artificial intelligence, the information retrieval\ncommunity has grown interested in investigating the power consumption\nassociated with neural models, particularly models of search. This interest has\nbecome particularly relevant as the energy consumption of information retrieval\nmodels has risen with new neural models based on large language models, leading\nto an associated increase of CO2 emissions, albeit relatively low compared to\nfields such as natural language processing.",
        "translated": ""
    },
    {
        "title": "Event Detection from Social Media Stream: Methods, Datasets and\n  Opportunities",
        "url": "http://arxiv.org/abs/2306.16495v1",
        "pub_date": "2023-06-28",
        "summary": "Social media streams contain large and diverse amount of information, ranging\nfrom daily-life stories to the latest global and local events and news.\nTwitter, especially, allows a fast spread of events happening real time, and\nenables individuals and organizations to stay informed of the events happening\nnow. Event detection from social media data poses different challenges from\ntraditional text and is a research area that has attracted much attention in\nrecent years. In this paper, we survey a wide range of event detection methods\nfor Twitter data stream, helping readers understand the recent development in\nthis area. We present the datasets available to the public. Furthermore, a few\nresearch opportunities",
        "translated": ""
    },
    {
        "title": "Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual\n  Question Answering",
        "url": "http://arxiv.org/abs/2306.16478v1",
        "pub_date": "2023-06-28",
        "summary": "This paper studies a category of visual question answering tasks, in which\naccessing external knowledge is necessary for answering the questions. This\ncategory is called outside-knowledge visual question answering (OK-VQA). A\nmajor step in developing OK-VQA systems is to retrieve relevant documents for\nthe given multi-modal query. Current state-of-the-art asymmetric dense\nretrieval model for this task uses an architecture with a multi-modal query\nencoder and a uni-modal document encoder. Such an architecture requires a large\namount of training data for effective performance. We propose an automatic data\ngeneration pipeline for pre-training passage retrieval models for OK-VQA tasks.\nThe proposed approach leads to 26.9% Precision@5 improvements compared to the\ncurrent state-of-the-art asymmetric architecture. Additionally, the proposed\npre-training approach exhibits a good ability in zero-shot retrieval scenarios.",
        "translated": ""
    },
    {
        "title": "Precision Anti-Cancer Drug Selection via Neural Ranking",
        "url": "http://arxiv.org/abs/2306.17771v1",
        "pub_date": "2023-06-30",
        "summary": "Personalized cancer treatment requires a thorough understanding of complex\ninteractions between drugs and cancer cell lines in varying genetic and\nmolecular contexts. To address this, high-throughput screening has been used to\ngenerate large-scale drug response data, facilitating data-driven computational\nmodels. Such models can capture complex drug-cell line interactions across\nvarious contexts in a fully data-driven manner. However, accurately\nprioritizing the most sensitive drugs for each cell line still remains a\nsignificant challenge. To address this, we developed neural ranking approaches\nthat leverage large-scale drug response data across multiple cell lines from\ndiverse cancer types. Unlike existing approaches that primarily utilize\nregression and classification techniques for drug response prediction, we\nformulated the objective of drug selection and prioritization as a drug ranking\nproblem. In this work, we proposed two neural listwise ranking methods that\nlearn latent representations of drugs and cell lines, and then use those\nrepresentations to score drugs in each cell line via a learnable scoring\nfunction. Specifically, we developed a neural listwise ranking method,\nList-One, on top of the existing method ListNet. Additionally, we proposed a\nnovel listwise ranking method, List-All, that focuses on all the sensitive\ndrugs instead of the top sensitive drug, unlike List-One. Our results\ndemonstrate that List-All outperforms the best baseline with significant\nimprovements of as much as 8.6% in hit@20 across 50% test cell lines.\nFurthermore, our analyses suggest that the learned latent spaces from our\nproposed methods demonstrate informative clustering structures and capture\nrelevant underlying biological features. Moreover, our comprehensive empirical\nevaluation provides a thorough and objective comparison of the performance of\ndifferent methods (including our proposed ones).",
        "translated": ""
    },
    {
        "title": "Outcome-based Evaluation of Systematic Review Automation",
        "url": "http://arxiv.org/abs/2306.17614v1",
        "pub_date": "2023-06-30",
        "summary": "Current methods of evaluating search strategies and automated citation\nscreening for systematic literature reviews typically rely on counting the\nnumber of relevant and not relevant publications. This established practice,\nhowever, does not accurately reflect the reality of conducting a systematic\nreview, because not all included publications have the same influence on the\nfinal outcome of the systematic review. More specifically, if an important\npublication gets excluded or included, this might significantly change the\noverall review outcome, while not including or excluding less influential\nstudies may only have a limited impact. However, in terms of evaluation\nmeasures, all inclusion and exclusion decisions are treated equally and,\ntherefore, failing to retrieve publications with little to no impact on the\nreview outcome leads to the same decrease in recall as failing to retrieve\ncrucial publications. We propose a new evaluation framework that takes into\naccount the impact of the reported study on the overall systematic review\noutcome. We demonstrate the framework by extracting review meta-analysis data\nand estimating outcome effects using predictions from ranking runs on\nsystematic reviews of interventions from CLEF TAR 2019 shared task. We further\nmeasure how closely the obtained outcomes are to the outcomes of the original\nreview if the arbitrary rankings were used. We evaluate 74 runs using the\nproposed framework and compare the results with those obtained using standard\nIR measures. We find that accounting for the difference in review outcomes\nleads to a different assessment of the quality of a system than if traditional\nevaluation measures were used. Our analysis provides new insights into the\nevaluation of retrieval results in the context of systematic review automation,\nemphasising the importance of assessing the usefulness of each document beyond\nbinary relevance.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting",
        "url": "http://arxiv.org/abs/2306.17563v1",
        "pub_date": "2023-06-30",
        "summary": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, there has been limited success so far, as researchers have\nfound it difficult to outperform fine-tuned baseline rankers on benchmark\ndatasets. We analyze pointwise and listwise ranking prompts used by existing\nmethods and argue that off-the-shelf LLMs do not fully understand these ranking\nformulations, possibly due to the nature of how LLMs are trained. In this\npaper, we propose to significantly reduce the burden on LLMs by using a new\ntechnique called Pairwise Ranking Prompting (PRP). Our results are the first in\nthe literature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on\nthe Flan-UL2 model with 20B parameters outperforms the previous best approach\nin the literature, which is based on the blackbox commercial GPT-4 that has 50x\n(estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only\ninferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while\noutperforming other existing solutions, such as InstructGPT which has 175B\nparameters, by over 10% for nearly all ranking metrics. Furthermore, we propose\nseveral variants of PRP to improve efficiency and show that it is possible to\nachieve competitive results even with linear complexity. We also discuss other\nbenefits of PRP, such as supporting both generation and scoring LLM APIs, as\nwell as being insensitive to input ordering.",
        "translated": ""
    },
    {
        "title": "Leveraging Watch-time Feedback for Short-Video Recommendations: A Causal\n  Labeling Framework",
        "url": "http://arxiv.org/abs/2306.17426v1",
        "pub_date": "2023-06-30",
        "summary": "With the proliferation of short video applications, the significance of short\nvideo recommendations has vastly increased. Unlike other recommendation\nscenarios, short video recommendation systems heavily rely on feedback from\nwatch time. Existing approaches simply treat watch time as a direct label,\nfailing to effectively harness its extensive semantics and introduce bias,\nthereby limiting the potential for modeling user interests based on watch time.\nTo overcome this challenge, we propose a framework named Debiasied\nMultiple-semantics-extracting Labeling (DML). DML constructs labels that\nencompass various semantics by utilizing quantiles derived from the\ndistribution of watch time, prioritizing relative order rather than absolute\nlabel values. This approach facilitates easier model learning while aligning\nwith the ranking objective of recommendations. Furthermore, we introduce a\nmethod inspired by causal adjustment to refine label definitions, thereby\nreducing the impact of bias on the label and directly mitigating bias at the\nlabel level. We substantiate the effectiveness of our DML framework through\nboth online and offline experiments. Extensive results demonstrate that our DML\ncould effectively leverage watch time to discover users' real interests,\nenhancing their engagement in our application.",
        "translated": ""
    },
    {
        "title": "Audio Embeddings as Teachers for Music Classification",
        "url": "http://arxiv.org/abs/2306.17424v1",
        "pub_date": "2023-06-30",
        "summary": "Music classification has been one of the most popular tasks in the field of\nmusic information retrieval. With the development of deep learning models, the\nlast decade has seen impressive improvements in a wide range of classification\ntasks. However, the increasing model complexity makes both training and\ninference computationally expensive. In this paper, we integrate the ideas of\ntransfer learning and feature-based knowledge distillation and systematically\ninvestigate using pre-trained audio embeddings as teachers to guide the\ntraining of low-complexity student networks. By regularizing the feature space\nof the student networks with the pre-trained embeddings, the knowledge in the\nteacher embeddings can be transferred to the students. We use various\npre-trained audio embeddings and test the effectiveness of the method on the\ntasks of musical instrument classification and music auto-tagging. Results show\nthat our method significantly improves the results in comparison to the\nidentical model trained without the teacher's knowledge. This technique can\nalso be combined with classical knowledge distillation approaches to further\nimprove the model's performance.",
        "translated": ""
    },
    {
        "title": "DeepTagger: Knowledge Enhanced Named Entity Recognition for Web-Based\n  Ads Queries",
        "url": "http://arxiv.org/abs/2306.17413v1",
        "pub_date": "2023-06-30",
        "summary": "Named entity recognition (NER) is a crucial task for online advertisement.\nState-of-the-art solutions leverage pre-trained language models for this task.\nHowever, three major challenges remain unresolved: web queries differ from\nnatural language, on which pre-trained models are trained; web queries are\nshort and lack contextual information; and labeled data for NER is scarce. We\npropose DeepTagger, a knowledge-enhanced NER model for web-based ads queries.\nThe proposed knowledge enhancement framework leverages both model-free and\nmodel-based approaches. For model-free enhancement, we collect unlabeled web\nqueries to augment domain knowledge; and we collect web search results to\nenrich the information of ads queries. We further leverage effective prompting\nmethods to automatically generate labels using large language models such as\nChatGPT. Additionally, we adopt a model-based knowledge enhancement method\nbased on adversarial data augmentation. We employ a three-stage training\nframework to train DeepTagger models. Empirical results in various NER tasks\ndemonstrate the effectiveness of the proposed framework.",
        "translated": ""
    },
    {
        "title": "Towards Personalized Cold-Start Recommendation with Prompts",
        "url": "http://arxiv.org/abs/2306.17256v1",
        "pub_date": "2023-06-29",
        "summary": "Recommender systems play a crucial role in helping users discover information\nthat aligns with their interests based on their past behaviors. However,\ndeveloping personalized recommendation systems becomes challenging when\nhistorical records of user-item interactions are unavailable, leading to what\nis known as the system cold-start recommendation problem. This issue is\nparticularly prominent in start-up businesses or platforms with insufficient\nuser engagement history. Previous studies focus on user or item cold-start\nscenarios, where systems could make recommendations for new users or items but\nare still trained with historical user-item interactions in the same domain,\nwhich cannot solve our problem. To bridge the gap, our research introduces an\ninnovative and effective approach, capitalizing on the capabilities of\npre-trained language models. We transform the recommendation process into\nsentiment analysis of natural languages containing information of user profiles\nand item attributes, where the sentiment polarity is predicted with prompt\nlearning. By harnessing the extensive knowledge housed within language models,\nthe prediction can be made without historical user-item interaction records. A\nbenchmark is also introduced to evaluate the proposed method under the\ncold-start setting, and the results demonstrate the effectiveness of our\nmethod. To the best of our knowledge, this is the first study to tackle the\nsystem cold-start recommendation problem. The benchmark and implementation of\nthe method are available at https://github.com/JacksonWuxs/PromptRec.",
        "translated": ""
    },
    {
        "title": "ChatGPT vs. Google: A Comparative Study of Search Performance and User\n  Experience",
        "url": "http://arxiv.org/abs/2307.01135v1",
        "pub_date": "2023-07-03",
        "summary": "The advent of ChatGPT, a large language model-powered chatbot, has prompted\nquestions about its potential implications for traditional search engines. In\nthis study, we investigate the differences in user behavior when employing\nsearch engines and chatbot tools for information-seeking tasks. We carry out a\nrandomized online experiment, dividing participants into two groups: one using\na ChatGPT-like tool and the other using a Google Search-like tool. Our findings\nreveal that the ChatGPT group consistently spends less time on all tasks, with\nno significant difference in overall task performance between the groups.\nNotably, ChatGPT levels user search performance across different education\nlevels and excels in answering straightforward questions and providing general\nsolutions but falls short in fact-checking tasks. Users perceive ChatGPT's\nresponses as having higher information quality compared to Google Search,\ndespite displaying a similar level of trust in both tools. Furthermore,\nparticipants using ChatGPT report significantly better user experiences in\nterms of usefulness, enjoyment, and satisfaction, while perceived ease of use\nremains comparable between the two tools. However, ChatGPT may also lead to\noverreliance and generate or replicate misinformation, yielding inconsistent\nresults. Our study offers valuable insights for search engine management and\nhighlights opportunities for integrating chatbot technologies into search\nengine designs.",
        "translated": ""
    },
    {
        "title": "OpenSiteRec: An Open Dataset for Site Recommendation",
        "url": "http://arxiv.org/abs/2307.00856v1",
        "pub_date": "2023-07-03",
        "summary": "As a representative information retrieval task, site recommendation, which\naims at predicting the optimal sites for a brand or an institution to open new\nbranches in an automatic data-driven way, is beneficial and crucial for brand\ndevelopment in modern business. However, there is no publicly available dataset\nso far and most existing approaches are limited to an extremely small scope of\nbrands, which seriously hinders the research on site recommendation. Therefore,\nwe collect, construct and release an open comprehensive dataset, namely\nOpenSiteRec, to facilitate and promote the research on site recommendation.\nSpecifically, OpenSiteRec leverages a heterogeneous graph schema to represent\nvarious types of real-world entities and relations in four international\nmetropolises. To evaluate the performance of the existing general methods on\nthe site recommendation task, we conduct benchmarking experiments of several\nrepresentative recommendation models on OpenSiteRec. Furthermore, we also\nhighlight the potential application directions to demonstrate the wide\napplicability of OpenSiteRec. We believe that our OpenSiteRec dataset is\nsignificant and anticipated to encourage the development of advanced methods\nfor site recommendation. OpenSiteRec is available online at\nhttps://OpenSiteRec.github.io/.",
        "translated": ""
    },
    {
        "title": "Looks Can Be Deceiving: Linking User-Item Interactions and User's\n  Propensity Towards Multi-Objective Recommendations",
        "url": "http://arxiv.org/abs/2307.00654v1",
        "pub_date": "2023-07-02",
        "summary": "Multi-objective recommender systems (MORS) provide suggestions to users\naccording to multiple (and possibly conflicting) goals. When a system optimizes\nits results at the individual-user level, it tailors them on a user's\npropensity towards the different objectives. Hence, the capability to\nunderstand users' fine-grained needs towards each goal is crucial. In this\npaper, we present the results of a user study in which we monitored the way\nusers interacted with recommended items, as well as their self-proclaimed\npropensities towards relevance, novelty and diversity objectives. The study was\ndivided into several sessions, where users evaluated recommendation lists\noriginating from a relevance-only single-objective baseline as well as MORS. We\nshow that despite MORS-based recommendations attracted less selections, its\npresence in the early sessions is crucial for users' satisfaction in the later\nstages. Surprisingly, the self-proclaimed willingness of users to interact with\nnovel and diverse items is not always reflected in the recommendations they\naccept. Post-study questionnaires provide insights on how to deal with this\nmatter, suggesting that MORS-based results should be accompanied by elements\nthat allow users to understand the recommendations, so as to facilitate their\nacceptance.",
        "translated": ""
    },
    {
        "title": "BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed\n  Search Logs for Zero-shot Biomedical Information Retrieval",
        "url": "http://arxiv.org/abs/2307.00589v1",
        "pub_date": "2023-07-02",
        "summary": "Information retrieval (IR) is essential in biomedical knowledge acquisition\nand clinical decision support. While recent progress has shown that language\nmodel encoders perform better semantic retrieval, training such models requires\nabundant query-article annotations that are difficult to obtain in biomedicine.\nAs a result, most biomedical IR systems only conduct lexical matching. In\nresponse, we introduce BioCPT, a first-of-its-kind Contrastively Pre-trained\nTransformer model for zero-shot biomedical IR. To train BioCPT, we collected an\nunprecedented scale of 255 million user click logs from PubMed. With such data,\nwe use contrastive learning to train a pair of closely-integrated retriever and\nre-ranker. Experimental results show that BioCPT sets new state-of-the-art\nperformance on five biomedical IR tasks, outperforming various baselines\nincluding much larger models such as GPT-3-sized cpt-text-XL. In addition,\nBioCPT also generates better biomedical article and sentence representations\nfor semantic evaluations. As such, BioCPT can be readily applied to various\nreal-world biomedical IR tasks. BioCPT API and code are publicly available at\nhttps://github.com/ncbi/BioCPT.",
        "translated": ""
    },
    {
        "title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text",
        "url": "http://arxiv.org/abs/2307.00509v1",
        "pub_date": "2023-07-02",
        "summary": "The task of textual geolocation - retrieving the coordinates of a place based\non a free-form language description - calls for not only grounding but also\nnatural language understanding and geospatial reasoning. Even though there are\nquite a few datasets in English used for geolocation, they are currently based\non open-source data (Wikipedia and Twitter), where the location of the\ndescribed place is mostly implicit, such that the location retrieval resolution\nis limited. Furthermore, there are no datasets available for addressing the\nproblem of textual geolocation in morphologically rich and resource-poor\nlanguages, such as Hebrew. In this paper, we present the Hebrew Geo-Location\n(HeGeL) corpus, designed to collect literal place descriptions and analyze\nlingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place\ndescriptions of various place types in three cities in Israel. Qualitative and\nempirical analysis show that the data exhibits abundant use of geospatial\nreasoning and requires a novel environmental representation.",
        "translated": ""
    },
    {
        "title": "Text based Large Language Model for Recommendation",
        "url": "http://arxiv.org/abs/2307.00457v1",
        "pub_date": "2023-07-02",
        "summary": "In recent years, large language models (LLM) have emerged as powerful tools\nfor diverse natural language processing tasks. However, their potential for\nrecommendation systems remains relatively unexplored. This paper presents an\ninnovative approach to recommendation systems using large language models\n(LLMs) based on text data. In this paper, we present a novel text-based large\nlanguage model for recommendation (TBLLMR) that utilized the expressive power\nof LLM to generate personalized recommendation. TBLLMR uses LLM's understanding\nability to interpret context, learn user preferences, and generate relevant\nrecommendation. Our proposed approach leverages the vast knowledge encoded in\nlarge language models to accomplish recommendation tasks. We first we formulate\nspecialized prompts to enhance the ability of LLM to comprehend recommendation\ntasks. Subsequently, we use these prompts to fine-tune the model on a dataset\nof user-item interactions, represented by textual data, to capture user\npreferences and item characteristics. Our research underscores the potential of\ntext-based LLMs in revolutionizing the domain of recommendation systems and\noffers a foundational framework for future explorations in this field. We\nconduct extensive experiments on benchmark datasets, and the experiments shows\nthat our TBLLMR has significant better results on large dataset.",
        "translated": ""
    },
    {
        "title": "One Copy Is All You Need: Resource-Efficient Streaming of Medical\n  Imaging Data at Scale",
        "url": "http://arxiv.org/abs/2307.00438v1",
        "pub_date": "2023-07-01",
        "summary": "Large-scale medical imaging datasets have accelerated development of\nartificial intelligence tools for clinical decision support. However, the large\nsize of these datasets is a bottleneck for users with limited storage and\nbandwidth. Many users may not even require such large datasets as AI models are\noften trained on lower resolution images. If users could directly download at\ntheir desired resolution, storage and bandwidth requirements would\nsignificantly decrease. However, it is impossible to anticipate every users'\nrequirements and impractical to store the data at multiple resolutions. What if\nwe could store images at a single resolution but send them at different ones?\nWe propose MIST, an open-source framework to operationalize progressive\nresolution for streaming medical images at multiple resolutions from a single\nhigh-resolution copy. We demonstrate that MIST can dramatically reduce imaging\ninfrastructure inefficiencies for hosting and streaming medical images by &gt;90%,\nwhile maintaining diagnostic quality for deep learning applications.",
        "translated": ""
    },
    {
        "title": "Effective Matching of Patients to Clinical Trials using Entity\n  Extraction and Neural Re-ranking",
        "url": "http://arxiv.org/abs/2307.00381v1",
        "pub_date": "2023-07-01",
        "summary": "Clinical trials (CTs) often fail due to inadequate patient recruitment. This\npaper tackles the challenges of CT retrieval by presenting an approach that\naddresses the patient-to-trials paradigm. Our approach involves two key\ncomponents in a pipeline-based model: (i) a data enrichment technique for\nenhancing both queries and documents during the first retrieval stage, and (ii)\na novel re-ranking schema that uses a Transformer network in a setup adapted to\nthis task by leveraging the structure of the CT documents. We use named entity\nrecognition and negation detection in both patient description and the\neligibility section of CTs. We further classify patient descriptions and CT\neligibility criteria into current, past, and family medical conditions. This\nextracted information is used to boost the importance of disease and drug\nmentions in both query and index for lexical retrieval. Furthermore, we propose\na two-step training schema for the Transformer network used to re-rank the\nresults from the lexical retrieval. The first step focuses on matching patient\ninformation with the descriptive sections of trials, while the second step aims\nto determine eligibility by matching patient information with the criteria\nsection. Our findings indicate that the inclusion criteria section of the CT\nhas a great influence on the relevance score in lexical models, and that the\nenrichment techniques for queries and documents improve the retrieval of\nrelevant trials. The re-ranking strategy, based on our training schema,\nconsistently enhances CT retrieval and shows improved performance by 15\\% in\nterms of precision at retrieving eligible trials. The results of our\nexperiments suggest the benefit of making use of extracted entities. Moreover,\nour proposed re-ranking schema shows promising effectiveness compared to larger\nneural models, even with limited training data.",
        "translated": ""
    },
    {
        "title": "Improving Text Matching in E-Commerce Search with A Rationalizable,\n  Intervenable and Fast Entity-Based Relevance Model",
        "url": "http://arxiv.org/abs/2307.00370v1",
        "pub_date": "2023-07-01",
        "summary": "Discovering the intended items of user queries from a massive repository of\nitems is one of the main goals of an e-commerce search system. Relevance\nprediction is essential to the search system since it helps improve\nperformance. When online serving a relevance model, the model is required to\nperform fast and accurate inference. Currently, the widely used models such as\nBi-encoder and Cross-encoder have their limitations in accuracy or inference\nspeed respectively. In this work, we propose a novel model called the\nEntity-Based Relevance Model (EBRM). We identify the entities contained in an\nitem and decompose the QI (query-item) relevance problem into multiple QE\n(query-entity) relevance problems; we then aggregate their results to form the\nQI prediction using a soft logic formulation. The decomposition allows us to\nuse a Cross-encoder QE relevance module for high accuracy as well as cache QE\npredictions for fast online inference. Utilizing soft logic makes the\nprediction procedure interpretable and intervenable. We also show that\npretraining the QE module with auto-generated QE data from user logs can\nfurther improve the overall performance. The proposed method is evaluated on\nlabeled data from e-commerce websites. Empirical results show that it achieves\npromising improvements with computation efficiency.",
        "translated": ""
    },
    {
        "title": "Improving Multitask Retrieval by Promoting Task Specialization",
        "url": "http://arxiv.org/abs/2307.00342v1",
        "pub_date": "2023-07-01",
        "summary": "In multitask retrieval, a single retriever is trained to retrieve relevant\ncontexts for multiple tasks. Despite its practical appeal, naive multitask\nretrieval lags behind task-specific retrieval in which a separate retriever is\ntrained for each task. We show that it is possible to train a multitask\nretriever that outperforms task-specific retrievers by promoting task\nspecialization. The main ingredients are: (1) a better choice of pretrained\nmodel (one that is explicitly optimized for multitasking) along with compatible\nprompting, and (2) a novel adaptive learning method that encourages each\nparameter to specialize in a particular task. The resulting multitask retriever\nis highly performant on the KILT benchmark. Upon analysis, we find that the\nmodel indeed learns parameters that are more task-specialized compared to naive\nmultitasking without prompting or adaptive learning.",
        "translated": ""
    },
    {
        "title": "MultiVENT: Multilingual Videos of Events with Aligned Natural Text",
        "url": "http://arxiv.org/abs/2307.03153v1",
        "pub_date": "2023-07-06",
        "summary": "Everyday news coverage has shifted from traditional broadcasts towards a wide\nrange of presentation formats such as first-hand, unedited video footage.\nDatasets that reflect the diverse array of multimodal, multilingual news\nsources available online could be used to teach models to benefit from this\nshift, but existing news video datasets focus on traditional news broadcasts\nproduced for English-speaking audiences. We address this limitation by\nconstructing MultiVENT, a dataset of multilingual, event-centric videos\ngrounded in text documents across five target languages. MultiVENT includes\nboth news broadcast videos and non-professional event footage, which we use to\nanalyze the state of online news videos and how they can be leveraged to build\nrobust, factually accurate models. Finally, we provide a model for complex,\nmultilingual video retrieval to serve as a baseline for information retrieval\nusing MultiVENT.",
        "translated": ""
    },
    {
        "title": "Track Mix Generation on Music Streaming Services using Transformers",
        "url": "http://arxiv.org/abs/2307.03045v1",
        "pub_date": "2023-07-06",
        "summary": "This paper introduces Track Mix, a personalized playlist generation system\nreleased in 2022 on the music streaming service Deezer. Track Mix automatically\ngenerates \"mix\" playlists inspired by initial music tracks, allowing users to\ndiscover music similar to their favorite content. To generate these mixes, we\nconsider a Transformer model trained on millions of track sequences from user\nplaylists. In light of the growing popularity of Transformers in recent years,\nwe analyze the advantages, drawbacks, and technical challenges of using such a\nmodel for mix generation on the service, compared to a more traditional\ncollaborative filtering approach. Since its release, Track Mix has been\ngenerating playlists for millions of users daily, enhancing their music\ndiscovery experience on Deezer.",
        "translated": ""
    },
    {
        "title": "Improving Retrieval-Augmented Large Language Models via Data Importance\n  Learning",
        "url": "http://arxiv.org/abs/2307.03027v1",
        "pub_date": "2023-07-06",
        "summary": "Retrieval augmentation enables large language models to take advantage of\nexternal knowledge, for example on tasks like question answering and data\nimputation. However, the performance of such retrieval-augmented models is\nlimited by the data quality of their underlying retrieval corpus. In this\npaper, we propose an algorithm based on multilinear extension for evaluating\nthe data importance of retrieved data points. There are exponentially many\nterms in the multilinear extension, and one key contribution of this paper is a\npolynomial time algorithm that computes exactly, given a retrieval-augmented\nmodel with an additive utility function and a validation set, the data\nimportance of data points in the retrieval corpus using the multilinear\nextension of the model's utility function. We further proposed an even more\nefficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental\nresults illustrate that we can enhance the performance of large language models\nby only pruning or reweighting the retrieval corpus, without requiring further\ntraining. For some tasks, this even allows a small model (e.g., GPT-JT),\naugmented with a search engine API, to outperform GPT-3.5 (without retrieval\naugmentation). Moreover, we show that weights based on multilinear extension\ncan be computed efficiently in practice (e.g., in less than ten minutes for a\ncorpus with 100 million elements).",
        "translated": ""
    },
    {
        "title": "A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System\n  Ranking Consistency and Discriminative Power",
        "url": "http://arxiv.org/abs/2307.02936v1",
        "pub_date": "2023-07-06",
        "summary": "Recently, Moffat et al. proposed an analytic framework, namely C/W/L/A, for\noffline evaluation metrics. This framework allows information retrieval (IR)\nresearchers to design evaluation metrics through the flexible combination of\nuser browsing models and user gain aggregations. However, the statistical\nstability of C/W/L/A metrics with different aggregations is not yet\ninvestigated. In this study, we investigate the statistical stability of\nC/W/L/A metrics from the perspective of: (1) the system ranking similarity\namong aggregations, (2) the system ranking consistency of aggregations and (3)\nthe discriminative power of aggregations. More specifically, we combined\nvarious aggregation functions with the browsing model of Precision, Discounted\nCumulative Gain (DCG), Rank-Biased Precision (RBP), INST, Average Precision\n(AP) and Expected Reciprocal Rank (ERR), examing their performances in terms of\nsystem ranking similarity, system ranking consistency and discriminative power\non two offline test collections. Our experimental result suggests that, in\nterms of system ranking consistency and discriminative power, the aggregation\nfunction of expected rate of gain (ERG) has an outstanding performance while\nthe aggregation function of maximum relevance usually has an insufficient\nperformance. The result also suggests that Precision, DCG, RBP, INST and AP\nwith their canonical aggregation all have favourable performances in system\nranking consistency and discriminative power; but for ERR, replacing its\ncanonical aggregation with ERG can further strengthen the discriminative power\nwhile obtaining a system ranking list similar to the canonical version at the\nsame time.",
        "translated": ""
    },
    {
        "title": "PLIERS: a Popularity-Based Recommender System for Content Dissemination\n  in Online Social Networks",
        "url": "http://arxiv.org/abs/2307.02865v1",
        "pub_date": "2023-07-06",
        "summary": "In this paper, we propose a novel tag-based recommender system called PLIERS,\nwhich relies on the assumption that users are mainly interested in items and\ntags with similar popularity to those they already own. PLIERS is aimed at\nreaching a good tradeoff between algorithmic complexity and the level of\npersonalization of recommended items. To evaluate PLIERS, we performed a set of\nexperiments on real OSN datasets, demonstrating that it outperforms\nstate-of-the-art solutions in terms of personalization, relevance, and novelty\nof recommendations.",
        "translated": ""
    },
    {
        "title": "BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by\n  Eliminating Ideological Segregation in Knowledge-based Recommendations",
        "url": "http://arxiv.org/abs/2307.02797v1",
        "pub_date": "2023-07-06",
        "summary": "In the realm of personalized recommendation systems, the increasing concern\nis the amplification of belief imbalance and user biases, a phenomenon\nprimarily attributed to the filter bubble. Addressing this critical issue, we\nintroduce an innovative intermediate agency (BHEISR) between users and existing\nrecommendation systems to attenuate the negative repercussions of the filter\nbubble effect in extant recommendation systems. The main objective is to strike\na belief balance for users while minimizing the detrimental influence caused by\nfilter bubbles. The BHEISR model amalgamates principles from nudge theory while\nupholding democratic and transparent principles. It harnesses user-specific\ncategory information to stimulate curiosity, even in areas users might\ninitially deem uninteresting. By progressively stimulating interest in novel\ncategories, the model encourages users to broaden their belief horizons and\nexplore the information they typically overlook. Our model is time-sensitive\nand operates on a user feedback loop. It utilizes the existing recommendation\nalgorithm of the model and incorporates user feedback from the prior time\nframe. This approach endeavors to transcend the constraints of the filter\nbubble, enrich recommendation diversity, and strike a belief balance among\nusers while also catering to user preferences and system-specific business\nrequirements. To validate the effectiveness and reliability of the BHEISR\nmodel, we conducted a series of comprehensive experiments with real-world\ndatasets. These experiments compared the performance of the BHEISR model\nagainst several baseline models using nearly 200 filter bubble-impacted users\nas test subjects. Our experimental results conclusively illustrate the superior\nperformance of the BHEISR model in mitigating filter bubbles and balancing user\nperspectives.",
        "translated": ""
    },
    {
        "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start\n  Recommendation",
        "url": "http://arxiv.org/abs/2307.02761v1",
        "pub_date": "2023-07-06",
        "summary": "Multimedia recommendation aims to fuse the multi-modal information of items\nfor feature enrichment to improve the recommendation performance. However,\nexisting methods typically introduce multi-modal information based on\ncollaborative information to improve the overall recommendation precision,\nwhile failing to explore its cold-start recommendation performance. Meanwhile,\nthese above methods are only applicable when such multi-modal data is\navailable. To address this problem, this paper proposes a recommendation\nframework, named Cross-modal Content Inference and Feature Enrichment\nRecommendation (CIERec), which exploits the multi-modal information to improve\nits cold-start recommendation performance. Specifically, CIERec first\nintroduces image annotation as the privileged information to help guide the\nmapping of unified features from the visual space to the semantic space in the\ntraining phase. And then CIERec enriches the content representation with the\nfusion of collaborative, visual, and cross-modal inferred representations, so\nas to improve its cold-start recommendation performance. Experimental results\non two real-world datasets show that the content representations learned by\nCIERec are able to achieve superior cold-start recommendation performance over\nexisting visually-aware recommendation algorithms. More importantly, CIERec can\nconsistently achieve significant improvements with different conventional\nvisually-aware backbones, which verifies its universality and effectiveness.",
        "translated": ""
    },
    {
        "title": "Knowledge Graph Self-Supervised Rationalization for Recommendation",
        "url": "http://arxiv.org/abs/2307.02759v1",
        "pub_date": "2023-07-06",
        "summary": "In this paper, we introduce a new self-supervised rationalization method,\ncalled KGRec, for knowledge-aware recommender systems. To effectively identify\ninformative knowledge connections, we propose an attentive knowledge\nrationalization mechanism that generates rational scores for knowledge\ntriplets. With these scores, KGRec integrates generative and contrastive\nself-supervised tasks for recommendation through rational masking. To highlight\nrationales in the knowledge graph, we design a novel generative task in the\nform of masking-reconstructing. By masking important knowledge with high\nrational scores, KGRec is trained to rebuild and highlight useful knowledge\nconnections that serve as rationales. To further rationalize the effect of\ncollaborative interactions on knowledge graph learning, we introduce a\ncontrastive learning task that aligns signals from knowledge and user-item\ninteraction views. To ensure noise-resistant contrasting, potential noisy edges\nin both graphs judged by the rational scores are masked. Extensive experiments\non three real-world datasets demonstrate that KGRec outperforms\nstate-of-the-art methods. We also provide the implementation codes for our\napproach at https://github.com/HKUDS/KGRec.",
        "translated": ""
    },
    {
        "title": "Dense Retrieval Adaptation using Target Domain Description",
        "url": "http://arxiv.org/abs/2307.02740v1",
        "pub_date": "2023-07-06",
        "summary": "In information retrieval (IR), domain adaptation is the process of adapting a\nretrieval model to a new domain whose data distribution is different from the\nsource domain. Existing methods in this area focus on unsupervised domain\nadaptation where they have access to the target document collection or\nsupervised (often few-shot) domain adaptation where they additionally have\naccess to (limited) labeled data in the target domain. There also exists\nresearch on improving zero-shot performance of retrieval models with no\nadaptation. This paper introduces a new category of domain adaptation in IR\nthat is as-yet unexplored. Here, similar to the zero-shot setting, we assume\nthe retrieval model does not have access to the target document collection. In\ncontrast, it does have access to a brief textual description that explains the\ntarget domain. We define a taxonomy of domain attributes in retrieval tasks to\nunderstand different properties of a source domain that can be adapted to a\ntarget domain. We introduce a novel automatic data construction pipeline that\nproduces a synthetic document collection, query set, and pseudo relevance\nlabels, given a textual domain description. Extensive experiments on five\ndiverse target domains show that adapting dense retrieval models using the\nconstructed synthetic data leads to effective retrieval performance on the\ntarget domain.",
        "translated": ""
    },
    {
        "title": "Improving Address Matching using Siamese Transformer Networks",
        "url": "http://arxiv.org/abs/2307.02300v1",
        "pub_date": "2023-07-05",
        "summary": "Matching addresses is a critical task for companies and post offices involved\nin the processing and delivery of packages. The ramifications of incorrectly\ndelivering a package to the wrong recipient are numerous, ranging from harm to\nthe company's reputation to economic and environmental costs. This research\nintroduces a deep learning-based model designed to increase the efficiency of\naddress matching for Portuguese addresses. The model comprises two parts: (i) a\nbi-encoder, which is fine-tuned to create meaningful embeddings of Portuguese\npostal addresses, utilized to retrieve the top 10 likely matches of the\nun-normalized target address from a normalized database, and (ii) a\ncross-encoder, which is fine-tuned to accurately rerank the 10 addresses\nobtained by the bi-encoder. The model has been tested on a real-case scenario\nof Portuguese addresses and exhibits a high degree of accuracy, exceeding 95%\nat the door level. When utilized with GPU computations, the inference speed is\nabout 4.5 times quicker than other traditional approaches such as BM25. An\nimplementation of this system in a real-world scenario would substantially\nincrease the effectiveness of the distribution process. Such an implementation\nis currently under investigation.",
        "translated": ""
    },
    {
        "title": "A Network Resource Allocation Recommendation Method with An Improved\n  Similarity Measure",
        "url": "http://arxiv.org/abs/2307.03399v1",
        "pub_date": "2023-07-07",
        "summary": "Recommender systems have been acknowledged as efficacious tools for managing\ninformation overload. Nevertheless, conventional algorithms adopted in such\nsystems primarily emphasize precise recommendations and, consequently, overlook\nother vital aspects like the coverage, diversity, and novelty of items. This\napproach results in less exposure for long-tail items. In this paper, to\npersonalize the recommendations and allocate recommendation resources more\npurposively, a method named PIM+RA is proposed. This method utilizes a\nbipartite network that incorporates self-connecting edges and weights.\nFurthermore, an improved Pearson correlation coefficient is employed for better\nredistribution. The evaluation of PIM+RA demonstrates a significant enhancement\nnot only in accuracy but also in coverage, diversity, and novelty of the\nrecommendation. It leads to a better balance in recommendation frequency by\nproviding effective exposure to long-tail items, while allowing customized\nparameters to adjust the recommendation list bias.",
        "translated": ""
    },
    {
        "title": "InfoSync: Information Synchronization across Multilingual\n  Semi-structured Tables",
        "url": "http://arxiv.org/abs/2307.03313v1",
        "pub_date": "2023-07-06",
        "summary": "Information Synchronization of semi-structured data across languages is\nchallenging. For instance, Wikipedia tables in one language should be\nsynchronized across languages. To address this problem, we introduce a new\ndataset InfoSyncC and a two-step method for tabular synchronization. InfoSync\ncontains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages,\nof which a subset (3.5K pairs) are manually annotated. The proposed method\nincludes 1) Information Alignment to map rows and 2) Information Update for\nupdating missing/outdated information for aligned tables across multilingual\ntables. When evaluated on InfoSync, information alignment achieves an F1 score\nof 87.91 (en &lt;-&gt; non-en). To evaluate information updation, we perform\nhuman-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach\nobtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of\nthe proposed method.",
        "translated": ""
    },
    {
        "title": "Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph\n  Reasoning",
        "url": "http://arxiv.org/abs/2307.03591v1",
        "pub_date": "2023-07-06",
        "summary": "Multimodal knowledge graphs (MKGs), which intuitively organize information in\nvarious modalities, can benefit multiple practical downstream tasks, such as\nrecommendation systems, and visual question answering. However, most MKGs are\nstill far from complete, which motivates the flourishing of MKG reasoning\nmodels. Recently, with the development of general artificial architectures, the\npretrained transformer models have drawn increasing attention, especially for\nmultimodal scenarios. However, the research of multimodal pretrained\ntransformer (MPT) for knowledge graph reasoning (KGR) is still at an early\nstage. As the biggest difference between MKG and other multimodal data, the\nrich structural information underlying the MKG still cannot be fully leveraged\nin existing MPT models. Most of them only utilize the graph structure as a\nretrieval map for matching images and texts connected with the same entity.\nThis manner hinders their reasoning performances. To this end, we propose the\ngraph Structure Guided Multimodal Pretrained Transformer for knowledge graph\nreasoning, termed SGMPT. Specifically, the graph structure encoder is adopted\nfor structural feature encoding. Then, a structure-guided fusion module with\ntwo different strategies, i.e., weighted summation and alignment constraint, is\nfirst designed to inject the structural information into both the textual and\nvisual features. To the best of our knowledge, SGMPT is the first MPT model for\nmultimodal KGR, which mines the structural information underlying the knowledge\ngraph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that\nour SGMPT outperforms existing state-of-the-art models, and prove the\neffectiveness of the designed strategies.",
        "translated": ""
    },
    {
        "title": "Undecimated Wavelet Transform for Word Embedded Semantic Marginal\n  Autoencoder in Security improvement and Denoising different Languages",
        "url": "http://arxiv.org/abs/2307.03679v1",
        "pub_date": "2023-07-06",
        "summary": "By combining the undecimated wavelet transform within a Word Embedded\nSemantic Marginal Autoencoder (WESMA), this research study provides a novel\nstrategy for improving security measures and denoising multiple languages. The\nincorporation of these strategies is intended to address the issues of\nrobustness, privacy, and multilingualism in data processing applications. The\nundecimated wavelet transform is used as a feature extraction tool to identify\nprominent language patterns and structural qualities in the input data. The\nproposed system may successfully capture significant information while\npreserving the temporal and geographical links within the data by employing\nthis transform. This improves security measures by increasing the system's\nability to detect abnormalities, discover hidden patterns, and distinguish\nbetween legitimate content and dangerous threats. The Word Embedded Semantic\nMarginal Autoencoder also functions as an intelligent framework for\ndimensionality and noise reduction. The autoencoder effectively learns the\nunderlying semantics of the data and reduces noise components by exploiting\nword embeddings and semantic context. As a result, data quality and accuracy\nare increased in following processing stages. The suggested methodology is\ntested using a diversified dataset that includes several languages and security\nscenarios. The experimental results show that the proposed approach is\neffective in attaining security enhancement and denoising capabilities across\nmultiple languages. The system is strong in dealing with linguistic variances,\nproducing consistent outcomes regardless of the language used. Furthermore,\nincorporating the undecimated wavelet transform considerably improves the\nsystem's ability to efficiently address complex security concerns",
        "translated": ""
    },
    {
        "title": "Fairness and Diversity in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2307.04644v1",
        "pub_date": "2023-07-10",
        "summary": "Recommender systems are effective tools for mitigating information overload\nand have seen extensive applications across various domains. However, the\nsingle focus on utility goals proves to be inadequate in addressing real-world\nconcerns, leading to increasing attention to fairness-aware and diversity-aware\nrecommender systems. While most existing studies explore fairness and diversity\nindependently, we identify strong connections between these two domains. In\nthis survey, we first discuss each of them individually and then dive into\ntheir connections. Additionally, motivated by the concepts of user-level and\nitem-level fairness, we broaden the understanding of diversity to encompass not\nonly the item level but also the user level. With this expanded perspective on\nuser and item-level diversity, we re-interpret fairness studies from the\nviewpoint of diversity. This fresh perspective enhances our understanding of\nfairness-related work and paves the way for potential future research\ndirections. Papers discussed in this survey along with public code links are\navailable at\nhttps://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems .",
        "translated": ""
    },
    {
        "title": "InPars Toolkit: A Unified and Reproducible Synthetic Data Generation\n  Pipeline for Neural Information Retrieval",
        "url": "http://arxiv.org/abs/2307.04601v1",
        "pub_date": "2023-07-10",
        "summary": "Recent work has explored Large Language Models (LLMs) to overcome the lack of\ntraining data for Information Retrieval (IR) tasks. The generalization\nabilities of these models have enabled the creation of synthetic in-domain data\nby providing instructions and a few examples on a prompt. InPars and\nPromptagator have pioneered this approach and both methods have demonstrated\nthe potential of using LLMs as synthetic data generators for IR tasks. This\nmakes them an attractive solution for IR tasks that suffer from a lack of\nannotated data. However, the reproducibility of these methods was limited,\nbecause InPars' training scripts are based on TPUs -- which are not widely\naccessible -- and because the code for Promptagator was not released and its\nproprietary LLM is not publicly accessible. To fully realize the potential of\nthese methods and make their impact more widespread in the research community,\nthe resources need to be accessible and easy to reproduce by researchers and\npractitioners. Our main contribution is a unified toolkit for end-to-end\nreproducible synthetic data generation research, which includes generation,\nfiltering, training and evaluation. Additionally, we provide an interface to IR\nlibraries widely used by the community and support for GPU. Our toolkit not\nonly reproduces the InPars method and partially reproduces Promptagator, but\nalso provides a plug-and-play functionality allowing the use of different LLMs,\nexploring filtering methods and finetuning various reranker models on the\ngenerated data. We also made available all the synthetic data generated in this\nwork for the 18 different datasets in the BEIR benchmark which took more than\n2,000 GPU hours to be generated as well as the reranker models finetuned on the\nsynthetic data. Code and data are available at\nhttps://github.com/zetaalphavector/InPars",
        "translated": ""
    },
    {
        "title": "A Semi-Automated Solution Approach Selection Tool for Any Use Case via\n  Scopus and OpenAI: a Case Study for AI/ML in Oncology",
        "url": "http://arxiv.org/abs/2307.04573v1",
        "pub_date": "2023-07-10",
        "summary": "In today's vast literature landscape, a manual review is very time-consuming.\nTo address this challenge, this paper proposes a semi-automated tool for\nsolution method review and selection. It caters to researchers, practitioners,\nand decision-makers while serving as a benchmark for future work. The tool\ncomprises three modules: (1) paper selection and scoring, using a keyword\nselection scheme to query Scopus API and compute relevancy; (2) solution method\nextraction in papers utilizing OpenAI API; (3) sensitivity analysis and\npost-analyzes. It reveals trends, relevant papers, and methods. AI in the\noncology case study and several use cases are presented with promising results,\ncomparing the tool to manual ground truth.",
        "translated": ""
    },
    {
        "title": "Alleviating Matthew Effect of Offline Reinforcement Learning in\n  Interactive Recommendation",
        "url": "http://arxiv.org/abs/2307.04571v1",
        "pub_date": "2023-07-10",
        "summary": "Offline reinforcement learning (RL), a technology that offline learns a\npolicy from logged data without the need to interact with online environments,\nhas become a favorable choice in decision-making processes like interactive\nrecommendation. Offline RL faces the value overestimation problem. To address\nit, existing methods employ conservatism, e.g., by constraining the learned\npolicy to be close to behavior policies or punishing the rarely visited\nstate-action pairs. However, when applying such offline RL to recommendation,\nit will cause a severe Matthew effect, i.e., the rich get richer and the poor\nget poorer, by promoting popular items or categories while suppressing the less\npopular ones. It is a notorious issue that needs to be addressed in practical\nrecommender systems.\n  In this paper, we aim to alleviate the Matthew effect in offline RL-based\nrecommendation. Through theoretical analyses, we find that the conservatism of\nexisting methods fails in pursuing users' long-term satisfaction. It inspires\nus to add a penalty term to relax the pessimism on states with high entropy of\nthe logging policy and indirectly penalizes actions leading to less diverse\nstates. This leads to the main technical contribution of the work: Debiased\nmodel-based Offline RL (DORL) method. Experiments show that DORL not only\ncaptures user interests well but also alleviates the Matthew effect. The\nimplementation is available via https://github.com/chongminggao/DORL-codes.",
        "translated": ""
    },
    {
        "title": "Counterfactual Explanation for Fairness in Recommendation",
        "url": "http://arxiv.org/abs/2307.04386v1",
        "pub_date": "2023-07-10",
        "summary": "Fairness-aware recommendation eliminates discrimination issues to build\ntrustworthy recommendation systems.Explaining the causes of unfair\nrecommendations is critical, as it promotes fairness diagnostics, and thus\nsecures users' trust in recommendation models. Existing fairness explanation\nmethods suffer high computation burdens due to the large-scale search space and\nthe greedy nature of the explanation search process. Besides, they perform\nscore-based optimizations with continuous values, which are not applicable to\ndiscrete attributes such as gender and race. In this work, we adopt the novel\nparadigm of counterfactual explanation from causal inference to explore how\nminimal alterations in explanations change model fairness, to abandon the\ngreedy search for explanations. We use real-world attributes from Heterogeneous\nInformation Networks (HINs) to empower counterfactual reasoning on discrete\nattributes. We propose a novel Counterfactual Explanation for Fairness\n(CFairER) that generates attribute-level counterfactual explanations from HINs\nfor recommendation fairness. Our CFairER conducts off-policy reinforcement\nlearning to seek high-quality counterfactual explanations, with an attentive\naction pruning reducing the search space of candidate counterfactuals. The\ncounterfactual explanations help to provide rational and proximate explanations\nfor model fairness, while the attentive action pruning narrows the search space\nof attributes. Extensive experiments demonstrate our proposed model can\ngenerate faithful explanations while maintaining favorable recommendation\nperformance.",
        "translated": ""
    },
    {
        "title": "Causal Neural Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2307.04384v1",
        "pub_date": "2023-07-10",
        "summary": "Graph collaborative filtering (GCF) has gained considerable attention in\nrecommendation systems by leveraging graph learning techniques to enhance\ncollaborative filtering (CF) models. One classical approach in GCF is to learn\nuser and item embeddings by modeling complex graph relations and utilizing\nthese embeddings for CF models. However, the quality of the embeddings\nsignificantly impacts the recommendation performance of GCF models. In this\npaper, we argue that existing graph learning methods are insufficient in\ngenerating satisfactory embeddings for CF models. This is because they\naggregate neighboring node messages directly, which can result in incorrect\nestimations of user-item correlations. To overcome this limitation, we propose\na novel approach that incorporates causal modeling to explicitly encode the\ncausal effects of neighboring nodes on the target node. This approach enables\nus to identify spurious correlations and uncover the root causes of user\npreferences. We introduce Causal Neural Graph Collaborative Filtering (CNGCF),\nthe first causality-aware graph learning framework for CF. CNGCF integrates\ncausal modeling into the graph representation learning process, explicitly\ncoupling causal effects between node pairs into the core message-passing\nprocess of graph learning. As a result, CNGCF yields causality-aware embeddings\nthat promote robust recommendations. Our extensive experiments demonstrate that\nCNGCF provides precise recommendations that align with user preferences.\nTherefore, our proposed framework can address the limitations of existing GCF\nmodels and offer a more effective solution for recommendation systems.",
        "translated": ""
    },
    {
        "title": "Graph Contrastive Learning with Multi-Objective for Personalized Product\n  Retrieval in Taobao Search",
        "url": "http://arxiv.org/abs/2307.04322v1",
        "pub_date": "2023-07-10",
        "summary": "In e-commerce search, personalized retrieval is a crucial technique for\nimproving user shopping experience. Recent works in this domain have achieved\nsignificant improvements by the representation learning paradigm, e.g.,\nembedding-based retrieval (EBR) and collaborative filtering (CF). EBR methods\ndo not sufficiently exploit the useful collaborative signal and are difficult\nto learn the representations of long-tail item well. Graph-based CF methods\nimprove personalization by modeling collaborative signal within the user click\ngraph. However, existing Graph-based methods ignore user's multiple behaviours,\nsuch as click/purchase and the relevance constraint between user behaviours and\nitems.In this paper, we propose a Graph Contrastive Learning with\nMulti-Objective (GCL-MO) collaborative filtering model, which solves the\nproblems of weak relevance and incomplete personalization in e-commerce search.\nSpecifically, GCL-MO builds a homogeneous graph of items and then optimizes a\nmulti-objective function of personalization and relevance. Moreover, we propose\na modified contrastive loss for multi-objectives graph learning, which avoids\nthe mutual suppression among positive samples and thus improves the\ngeneralization and robustness of long-tail item representations. These learned\nitem embeddings are then used for personalized retrieval by constructing an\nefficient offline-to-online inverted table. GCL-MO outperforms the online\ncollaborative filtering baseline in both offline/online experimental metrics\nand shows a significant improvement in the online A/B testing of Taobao search.",
        "translated": ""
    },
    {
        "title": "DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge\n  Graphs",
        "url": "http://arxiv.org/abs/2307.04090v1",
        "pub_date": "2023-07-09",
        "summary": "Recent work within the Argument Mining community has shown the applicability\nof Natural Language Processing systems for solving problems found within\ncompetitive debate. One of the most important tasks within competitive debate\nis for debaters to create high quality debate cases. We show that effective\ndebate cases can be constructed using constrained shortest path traversals on\nArgumentative Semantic Knowledge Graphs. We study this potential in the context\nof a type of American Competitive Debate, called Policy Debate, which already\nhas a large scale dataset targeting it called DebateSum. We significantly\nimprove upon DebateSum by introducing 53180 new examples, as well as further\nuseful metadata for every example, to the dataset. We leverage the txtai\nsemantic search and knowledge graph toolchain to produce and contribute 9\nsemantic knowledge graphs built on this dataset. We create a unique method for\nevaluating which knowledge graphs are better in the context of producing policy\ndebate cases. A demo which automatically generates debate cases, along with all\nother code and the Knowledge Graphs, are open-sourced and made available to the\npublic here: https://github.com/Hellisotherpeople/DebateKG",
        "translated": ""
    },
    {
        "title": "Fairness-Aware Graph Neural Networks: A Survey",
        "url": "http://arxiv.org/abs/2307.03929v1",
        "pub_date": "2023-07-08",
        "summary": "Graph Neural Networks (GNNs) have become increasingly important due to their\nrepresentational power and state-of-the-art predictive performance on many\nfundamental learning tasks. Despite this success, GNNs suffer from fairness\nissues that arise as a result of the underlying graph data and the fundamental\naggregation mechanism that lies at the heart of the large class of GNN models.\nIn this article, we examine and categorize fairness techniques for improving\nthe fairness of GNNs. Previous work on fair GNN models and techniques are\ndiscussed in terms of whether they focus on improving fairness during a\npreprocessing step, during training, or in a post-processing phase.\nFurthermore, we discuss how such techniques can be used together whenever\nappropriate, and highlight the advantages and intuition as well. We also\nintroduce an intuitive taxonomy for fairness evaluation metrics including\ngraph-level fairness, neighborhood-level fairness, embedding-level fairness,\nand prediction-level fairness metrics. In addition, graph datasets that are\nuseful for benchmarking the fairness of GNN models are summarized succinctly.\nFinally, we highlight key open problems and challenges that remain to be\naddressed.",
        "translated": ""
    },
    {
        "title": "Embedding Mental Health Discourse for Community Recommendation",
        "url": "http://arxiv.org/abs/2307.03892v1",
        "pub_date": "2023-07-08",
        "summary": "Our paper investigates the use of discourse embedding techniques to develop a\ncommunity recommendation system that focuses on mental health support groups on\nsocial media. Social media platforms provide a means for users to anonymously\nconnect with communities that cater to their specific interests. However, with\nthe vast number of online communities available, users may face difficulties in\nidentifying relevant groups to address their mental health concerns. To address\nthis challenge, we explore the integration of discourse information from\nvarious subreddit communities using embedding techniques to develop an\neffective recommendation system. Our approach involves the use of content-based\nand collaborative filtering techniques to enhance the performance of the\nrecommendation system. Our findings indicate that the proposed approach\noutperforms the use of each technique separately and provides interpretability\nin the recommendation process.",
        "translated": ""
    },
    {
        "title": "Duncode Characters Shorter",
        "url": "http://arxiv.org/abs/2307.05414v1",
        "pub_date": "2023-07-11",
        "summary": "This paper investigates the employment of various encoders in text\ntransformation, converting characters into bytes. It discusses local encoders\nsuch as ASCII and GB-2312, which encode specific characters into shorter bytes,\nand universal encoders like UTF-8 and UTF-16, which can encode the complete\nUnicode set with greater space requirements and are gaining widespread\nacceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,\nhowever, lack self-synchronizing capabilities. Duncode is introduced as an\ninnovative encoding method that aims to encode the entire Unicode character set\nwith high space efficiency, akin to local encoders. It has the potential to\ncompress multiple characters of a string into a Duncode unit using fewer bytes.\nDespite offering less self-synchronizing identification information, Duncode\nsurpasses UTF8 in terms of space efficiency. The application is available at\n\\url{https://github.com/laohur/duncode}. Additionally, we have developed a\nbenchmark for evaluating character encoders across different languages. It\nencompasses 179 languages and can be accessed at\n\\url{https://github.com/laohur/wiki2txt}.",
        "translated": ""
    },
    {
        "title": "Temporal Graphs Anomaly Emergence Detection: Benchmarking For Social\n  Media Interactions",
        "url": "http://arxiv.org/abs/2307.05268v1",
        "pub_date": "2023-07-11",
        "summary": "Temporal graphs have become an essential tool for analyzing complex dynamic\nsystems with multiple agents. Detecting anomalies in temporal graphs is crucial\nfor various applications, including identifying emerging trends, monitoring\nnetwork security, understanding social dynamics, tracking disease outbreaks,\nand understanding financial dynamics. In this paper, we present a comprehensive\nbenchmarking study that compares 12 data-driven methods for anomaly detection\nin temporal graphs. We conduct experiments on two temporal graphs extracted\nfrom Twitter and Facebook, aiming to identify anomalies in group interactions.\nSurprisingly, our study reveals an unclear pattern regarding the best method\nfor such tasks, highlighting the complexity and challenges involved in anomaly\nemergence detection in large and dynamic systems. The results underscore the\nneed for further research and innovative approaches to effectively detect\nemerging anomalies in dynamic systems represented as temporal graphs.",
        "translated": ""
    },
    {
        "title": "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion",
        "url": "http://arxiv.org/abs/2307.05260v1",
        "pub_date": "2023-07-11",
        "summary": "The task of Prior Case Retrieval (PCR) in the legal domain is about\nautomatically citing relevant (based on facts and precedence) prior legal cases\nin a given query case. To further promote research in PCR, in this paper, we\npropose a new large benchmark (in English) for the PCR task: IL-PCR (Indian\nLegal Prior Case Retrieval) corpus. Given the complex nature of case relevance\nand the long size of legal documents, BM25 remains a strong baseline for\nranking the cited prior documents. In this work, we explore the role of events\nin legal case retrieval and propose an unsupervised retrieval method-based\npipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find\nthat the proposed unsupervised retrieval method significantly increases\nperformance compared to BM25 and makes retrieval faster by a considerable\nmargin, making it applicable to real-time case retrieval systems. Our proposed\nsystem is generic, we show that it generalizes across two different legal\nsystems (Indian and Canadian), and it shows state-of-the-art performance on the\nbenchmarks for both the legal systems (IL-PCR and COLIEE corpora).",
        "translated": ""
    },
    {
        "title": "Generative Contrastive Graph Learning for Recommendation",
        "url": "http://arxiv.org/abs/2307.05100v1",
        "pub_date": "2023-07-11",
        "summary": "By treating users' interactions as a user-item graph, graph learning models\nhave been widely deployed in Collaborative Filtering(CF) based recommendation.\nRecently, researchers have introduced Graph Contrastive Learning(GCL)\ntechniques into CF to alleviate the sparse supervision issue, which first\nconstructs contrastive views by data augmentations and then provides\nself-supervised signals by maximizing the mutual information between\ncontrastive views. Despite the effectiveness, we argue that current GCL-based\nrecommendation models are still limited as current data augmentation\ntechniques, either structure augmentation or feature augmentation. First,\nstructure augmentation randomly dropout nodes or edges, which is easy to\ndestroy the intrinsic nature of the user-item graph. Second, feature\naugmentation imposes the same scale noise augmentation on each node, which\nneglects the unique characteristics of nodes on the graph. To tackle the above\nlimitations, we propose a novel Variational Graph Generative-Contrastive\nLearning(VGCL) framework for recommendation. Specifically, we leverage\nvariational graph reconstruction to estimate a Gaussian distribution of each\nnode, then generate multiple contrastive views through multiple samplings from\nthe estimated distributions, which builds a bridge between generative and\ncontrastive learning. Besides, the estimated variances are tailored to each\nnode, which regulates the scale of contrastive loss for each node on\noptimization. Considering the similarity of the estimated distributions, we\npropose a cluster-aware twofold contrastive learning, a node-level to encourage\nconsistency of a node's contrastive views and a cluster-level to encourage\nconsistency of nodes in a cluster. Finally, extensive experimental results on\nthree public datasets clearly demonstrate the effectiveness of the proposed\nmodel.",
        "translated": ""
    },
    {
        "title": "Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with\n  Sample-aware Prompting and Dynamic Revision Chain",
        "url": "http://arxiv.org/abs/2307.05074v1",
        "pub_date": "2023-07-11",
        "summary": "Text-to-SQL aims at generating SQL queries for the given natural language\nquestions and thus helping users to query databases. Prompt learning with large\nlanguage models (LLMs) has emerged as a recent approach, which designs prompts\nto lead LLMs to understand the input question and generate the corresponding\nSQL. However, it faces challenges with strict SQL syntax requirements. Existing\nwork prompts the LLMs with a list of demonstration examples (i.e. question-SQL\npairs) to generate SQL, but the fixed prompts can hardly handle the scenario\nwhere the semantic gap between the retrieved demonstration and the input\nquestion is large. In this paper, we propose a retrieval-augmented prompting\nmethod for a LLM-based Text-to-SQL framework, involving sample-aware prompting\nand a dynamic revision chain. Our approach incorporates sample-aware\ndemonstrations, which include the composition of SQL operators and fine-grained\ninformation related to the given question. To retrieve questions sharing\nsimilar intents with input questions, we propose two strategies for assisting\nretrieval. Firstly, we leverage LLMs to simplify the original questions,\nunifying the syntax and thereby clarifying the users' intentions. To generate\nexecutable and accurate SQLs without human intervention, we design a dynamic\nrevision chain which iteratively adapts fine-grained feedback from the\npreviously generated SQL. Experimental results on three Text-to-SQL benchmarks\ndemonstrate the superiority of our method over strong baseline models.",
        "translated": ""
    },
    {
        "title": "Mining for Unknown Unknowns",
        "url": "http://arxiv.org/abs/2307.05071v1",
        "pub_date": "2023-07-11",
        "summary": "Unknown unknowns are future relevant contingencies that lack an ex ante\ndescription. While there are numerous retrospective accounts showing that\nsignificant gains or losses might have been achieved or avoided had such\ncontingencies been previously uncovered, getting hold of unknown unknowns still\nremains elusive, both in practice and conceptually. Using Formal Concept\nAnalysis (FCA) - a subfield of lattice theory which is increasingly applied for\nmining and organizing data - this paper introduces a simple framework to\nsystematically think out of the box and direct the search for unknown unknowns.",
        "translated": ""
    },
    {
        "title": "Neural-Symbolic Recommendation with Graph-Enhanced Information",
        "url": "http://arxiv.org/abs/2307.05036v1",
        "pub_date": "2023-07-11",
        "summary": "The recommendation system is not only a problem of inductive statistics from\ndata but also a cognitive task that requires reasoning ability. The most\nadvanced graph neural networks have been widely used in recommendation systems\nbecause they can capture implicit structured information from graph-structured\ndata. However, like most neural network algorithms, they only learn matching\npatterns from a perception perspective. Some researchers use user behavior for\nlogic reasoning to achieve recommendation prediction from the perspective of\ncognitive reasoning, but this kind of reasoning is a local one and ignores\nimplicit information on a global scale. In this work, we combine the advantages\nof graph neural networks and propositional logic operations to construct a\nneuro-symbolic recommendation model with both global implicit reasoning ability\nand local explicit logic reasoning ability. We first build an item-item graph\nbased on the principle of adjacent interaction and use graph neural networks to\ncapture implicit information in global data. Then we transform user behavior\ninto propositional logic expressions to achieve recommendations from the\nperspective of cognitive reasoning. Extensive experiments on five public\ndatasets show that our proposed model outperforms several state-of-the-art\nmethods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].",
        "translated": ""
    },
    {
        "title": "Empowering recommender systems using automatically generated Knowledge\n  Graphs and Reinforcement Learning",
        "url": "http://arxiv.org/abs/2307.04996v1",
        "pub_date": "2023-07-11",
        "summary": "Personalized recommendations have a growing importance in direct marketing,\nwhich motivates research to enhance customer experiences by knowledge graph\n(KG) applications. For example, in financial services, companies may benefit\nfrom providing relevant financial articles to their customers to cultivate\nrelationships, foster client engagement and promote informed financial\ndecisions. While several approaches center on KG-based recommender systems for\nimproved content, in this study we focus on interpretable KG-based recommender\nsystems for decision making.To this end, we present two knowledge graph-based\napproaches for personalized article recommendations for a set of customers of a\nlarge multinational financial services company. The first approach employs\nReinforcement Learning and the second approach uses the XGBoost algorithm for\nrecommending articles to the customers. Both approaches make use of a KG\ngenerated from both structured (tabular data) and unstructured data (a large\nbody of text data).Using the Reinforcement Learning-based recommender system we\ncould leverage the graph traversal path leading to the recommendation as a way\nto generate interpretations (Path Directed Reasoning (PDR)). In the\nXGBoost-based approach, one can also provide explainable results using post-hoc\nmethods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I\nam Five).Importantly, our approach offers explainable results, promoting better\ndecision-making. This study underscores the potential of combining advanced\nmachine learning techniques with KG-driven insights to bolster experience in\ncustomer relationship management.",
        "translated": ""
    },
    {
        "title": "Ranking with Long-Term Constraints",
        "url": "http://arxiv.org/abs/2307.04923v1",
        "pub_date": "2023-07-10",
        "summary": "The feedback that users provide through their choices (e.g., clicks,\npurchases) is one of the most common types of data readily available for\ntraining search and recommendation algorithms. However, myopically training\nsystems based on choice data may only improve short-term engagement, but not\nthe long-term sustainability of the platform and the long-term benefits to its\nusers, content providers, and other stakeholders. In this paper, we thus\ndevelop a new framework in which decision makers (e.g., platform operators,\nregulators, users) can express long-term goals for the behavior of the platform\n(e.g., fairness, revenue distribution, legal requirements). These goals take\nthe form of exposure or impact targets that go well beyond individual sessions,\nand we provide new control-based algorithms to achieve these goals. In\nparticular, the controllers are designed to achieve the stated long-term goals\nwith minimum impact on short-term engagement. Beyond the principled theoretical\nderivation of the controllers, we evaluate the algorithms on both synthetic and\nreal-world data. While all controllers perform well, we find that they provide\ninteresting trade-offs in efficiency, robustness, and the ability to plan\nahead.",
        "translated": ""
    },
    {
        "title": "Testing different Log Bases For Vector Model Weighting Technique",
        "url": "http://arxiv.org/abs/2307.06213v1",
        "pub_date": "2023-07-12",
        "summary": "Information retrieval systems retrieves relevant documents based on a query\nsubmitted by the user. The documents are initially indexed and the words in the\ndocuments are assigned weights using a weighting technique called TFIDF which\nis the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF\nrepresents the number of occurrences of a term in a document. IDF measures\nwhether the term is common or rare across all documents. It is computed by\ndividing the total number of documents in the system by the number of documents\ncontaining the term and then computing the logarithm of the quotient. By\ndefault, we use base 10 to calculate the logarithm. In this paper, we are going\nto test this weighting technique by using a range of log bases from 0.1 to\n100.0 to calculate the IDF. Testing different log bases for vector model\nweighting technique is to highlight the importance of understanding the\nperformance of the system at different weighting values. We use the documents\nof MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled\nexplicitly for experiments in data information retrieval systems.",
        "translated": ""
    },
    {
        "title": "DDNAS: Discretized Differentiable Neural Architecture Search for Text\n  Classification",
        "url": "http://arxiv.org/abs/2307.06005v1",
        "pub_date": "2023-07-12",
        "summary": "Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.",
        "translated": ""
    },
    {
        "title": "Contrastive Learning for Conversion Rate Prediction",
        "url": "http://arxiv.org/abs/2307.05974v1",
        "pub_date": "2023-07-12",
        "summary": "Conversion rate (CVR) prediction plays an important role in advertising\nsystems. Recently, supervised deep neural network-based models have shown\npromising performance in CVR prediction. However, they are data hungry and\nrequire an enormous amount of training data. In online advertising systems,\nalthough there are millions to billions of ads, users tend to click only a\nsmall set of them and to convert on an even smaller set. This data sparsity\nissue restricts the power of these deep models. In this paper, we propose the\nContrastive Learning for CVR prediction (CL4CVR) framework. It associates the\nsupervised CVR prediction task with a contrastive learning task, which can\nlearn better data representations exploiting abundant unlabeled data and\nimprove the CVR prediction performance. To tailor the contrastive learning task\nto the CVR prediction problem, we propose embedding masking (EM), rather than\nfeature masking, to create two views of augmented samples. We also propose a\nfalse negative elimination (FNE) component to eliminate samples with the same\nfeature as the anchor sample, to account for the natural property in user\nbehavior data. We further propose a supervised positive inclusion (SPI)\ncomponent to include additional positive samples for each anchor sample, in\norder to make full use of sparse but precious user conversion events.\nExperimental results on two real-world conversion datasets demonstrate the\nsuperior performance of CL4CVR. The source code is available at\nhttps://github.com/DongRuiHust/CL4CVR.",
        "translated": ""
    },
    {
        "title": "Relational Extraction on Wikipedia Tables using Convolutional and Memory\n  Networks",
        "url": "http://arxiv.org/abs/2307.05827v1",
        "pub_date": "2023-07-11",
        "summary": "Relation extraction (RE) is the task of extracting relations between entities\nin text. Most RE methods extract relations from free-form running text and\nleave out other rich data sources, such as tables. We explore RE from the\nperspective of applying neural methods on tabularly organized data. We\nintroduce a new model consisting of Convolutional Neural Network (CNN) and\nBidirectional-Long Short Term Memory (BiLSTM) network to encode entities and\nlearn dependencies among them, respectively. We evaluate our model on a large\nand recent dataset and compare results with previous neural methods.\nExperimental results show that our model consistently outperforms the previous\nmodel for the task of relation extraction on tabular data. We perform\ncomprehensive error analyses and ablation study to show the contribution of\nvarious components of our model. Finally, we discuss the usefulness and\ntrade-offs of our approach, and provide suggestions for fostering further\nresearch.",
        "translated": ""
    },
    {
        "title": "Parmesan: mathematical concept extraction for education",
        "url": "http://arxiv.org/abs/2307.06699v1",
        "pub_date": "2023-07-13",
        "summary": "Mathematics is a highly specialized domain with its own unique set of\nchallenges that has seen limited study in natural language processing. However,\nmathematics is used in a wide variety of fields and multidisciplinary research\nin many different domains often relies on an understanding of mathematical\nconcepts. To aid researchers coming from other fields, we develop a prototype\nsystem for searching for and defining mathematical concepts in context,\nfocusing on the field of category theory. This system, Parmesan, depends on\nnatural language processing components including concept extraction, relation\nextraction, definition extraction, and entity linking. In developing this\nsystem, we show that existing techniques cannot be applied directly to the\ncategory theory domain, and suggest hybrid techniques that do perform well,\nthough we expect the system to evolve over time. We also provide two cleaned\nmathematical corpora that power the prototype system, which are based on\njournal articles and wiki pages, respectively. The corpora have been annotated\nwith dependency trees, lemmas, and part-of-speech tags.",
        "translated": ""
    },
    {
        "title": "Going Beyond Local: Global Graph-Enhanced Personalized News\n  Recommendations",
        "url": "http://arxiv.org/abs/2307.06576v1",
        "pub_date": "2023-07-13",
        "summary": "Precisely recommending candidate news articles to users has always been a\ncore challenge for personalized news recommendation systems. Most recent works\nprimarily focus on using advanced natural language processing techniques to\nextract semantic information from rich textual data, employing content-based\nmethods derived from local historical news. However, this approach lacks a\nglobal perspective, failing to account for users' hidden motivations and\nbehaviors beyond semantic information. To address this challenge, we propose a\nnovel model called GLORY (Global-LOcal news Recommendation sYstem), which\ncombines global representations learned from other users with local\nrepresentations to enhance personalized recommendation systems. We accomplish\nthis by constructing a Global-aware Historical News Encoder, which includes a\nglobal news graph and employs gated graph neural networks to enrich news\nrepresentations, thereby fusing historical news representations by a historical\nnews aggregator. Similarly, we extend this approach to a Global Candidate News\nEncoder, utilizing a global entity graph and a candidate news aggregator to\nenhance candidate news representation. Evaluation results on two public news\ndatasets demonstrate that our method outperforms existing approaches.\nFurthermore, our model offers more diverse recommendations.",
        "translated": ""
    },
    {
        "title": "Assessing the Ability of ChatGPT to Screen Articles for Systematic\n  Reviews",
        "url": "http://arxiv.org/abs/2307.06464v1",
        "pub_date": "2023-07-12",
        "summary": "By organizing knowledge within a research field, Systematic Reviews (SR)\nprovide valuable leads to steer research. Evidence suggests that SRs have\nbecome first-class artifacts in software engineering. However, the tedious\nmanual effort associated with the screening phase of SRs renders these studies\na costly and error-prone endeavor. While screening has traditionally been\nconsidered not amenable to automation, the advent of generative AI-driven\nchatbots, backed with large language models is set to disrupt the field. In\nthis report, we propose an approach to leverage these novel technological\ndevelopments for automating the screening of SRs. We assess the consistency,\nclassification performance, and generalizability of ChatGPT in screening\narticles for SRs and compare these figures with those of traditional\nclassifiers used in SR automation. Our results indicate that ChatGPT is a\nviable option to automate the SR processes, but requires careful considerations\nfrom developers when integrating ChatGPT into their SR tools.",
        "translated": ""
    },
    {
        "title": "Streaming CTR Prediction: Rethinking Recommendation Task for Real-World\n  Streaming Data",
        "url": "http://arxiv.org/abs/2307.07509v1",
        "pub_date": "2023-07-14",
        "summary": "The Click-Through Rate (CTR) prediction task is critical in industrial\nrecommender systems, where models are usually deployed on dynamic streaming\ndata in practical applications. Such streaming data in real-world recommender\nsystems face many challenges, such as distribution shift, temporal\nnon-stationarity, and systematic biases, which bring difficulties to the\ntraining and utilizing of recommendation models. However, most existing studies\napproach the CTR prediction as a classification task on static datasets,\nassuming that the train and test sets are independent and identically\ndistributed (a.k.a, i.i.d. assumption). To bridge this gap, we formulate the\nCTR prediction problem in streaming scenarios as a Streaming CTR Prediction\ntask. Accordingly, we propose dedicated benchmark settings and metrics to\nevaluate and analyze the performance of the models in streaming data. To better\nunderstand the differences compared to traditional CTR prediction tasks, we\ndelve into the factors that may affect the model performance, such as parameter\nscale, normalization, regularization, etc. The results reveal the existence of\nthe ''streaming learning dilemma'', whereby the same factor may have different\neffects on model performance in the static and streaming scenarios. Based on\nthe findings, we propose two simple but inspiring methods (i.e., tuning key\nparameters and exemplar replay) that significantly improve the effectiveness of\nthe CTR models in the new streaming scenario. We hope our work will inspire\nfurther research on streaming CTR prediction and help improve the robustness\nand adaptability of recommender systems.",
        "translated": ""
    },
    {
        "title": "PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language\n  Pre-training via Prompting",
        "url": "http://arxiv.org/abs/2307.07341v1",
        "pub_date": "2023-07-14",
        "summary": "Vision-language (VL) Pre-training (VLP) has shown to well generalize VL\nmodels over a wide range of VL downstream tasks, especially for cross-modal\nretrieval. However, it hinges on a huge amount of image-text pairs, which\nrequires tedious and costly curation. On the contrary, weakly-supervised VLP\n(W-VLP) explores means with object tags generated by a pre-trained object\ndetector (OD) from images. Yet, they still require paired information, i.e.\nimages and object-level annotations, as supervision to train an OD.\n  To further reduce the amount of supervision, we propose Prompts-in-The-Loop\n(PiTL) that prompts knowledge from large language models (LLMs) to describe\nimages. Concretely, given a category label of an image, e.g. refinery, the\nknowledge, e.g. a refinery could be seen with large storage tanks, pipework,\nand ..., extracted by LLMs is used as the language counterpart. The knowledge\nsupplements, e.g. the common relations among entities most likely appearing in\na scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of\n14K categories from ImageNet21K with PiTL. Empirically, the VL models\npre-trained with PiTL-generated pairs are strongly favored over other W-VLP\nworks on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with less\nsupervision. The results reveal the effectiveness of PiTL-generated pairs for\nVLP.",
        "translated": ""
    },
    {
        "title": "Hybrid moderation in the newsroom: Recommending featured posts to\n  content moderators",
        "url": "http://arxiv.org/abs/2307.07317v1",
        "pub_date": "2023-07-14",
        "summary": "Online news outlets are grappling with the moderation of user-generated\ncontent within their comment section. We present a recommender system based on\nranking class probabilities to support and empower the moderator in choosing\nfeatured posts, a time-consuming task. By combining user and textual content\nfeatures we obtain an optimal classification F1-score of 0.44 on the test set.\nFurthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of\nvalidation articles. As an expert evaluation, content moderators assessed the\noutput of a random selection of articles by choosing comments to feature based\non the recommendations, which resulted in a NDCG score of 0.83. We conclude\nthat first, adding text features yields the best score and second, while\nchoosing featured content remains somewhat subjective, content moderators found\nsuitable comments in all but one evaluated recommendations. We end the paper by\nanalyzing our best-performing model, a step towards transparency and\nexplainability in hybrid content moderation.",
        "translated": ""
    },
    {
        "title": "Learning to Retrieve In-Context Examples for Large Language Models",
        "url": "http://arxiv.org/abs/2307.07164v1",
        "pub_date": "2023-07-14",
        "summary": "Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of 30 tasks demonstrate that our framework significantly\nenhances in-context learning performance. Furthermore, we show the\ngeneralization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes.",
        "translated": ""
    },
    {
        "title": "Digital Health Discussion Through Articles Published Until the Year\n  2021: A Digital Topic Modeling Approach",
        "url": "http://arxiv.org/abs/2307.07130v1",
        "pub_date": "2023-07-14",
        "summary": "The digital health industry has grown in popularity since the 2010s, but\nthere has been limited analysis of the topics discussed in the field across\nacademic disciplines. This study aims to analyze the research trends of digital\nhealth-related articles published on the Web of Science until 2021, in order to\nunderstand the concentration, scope, and characteristics of the research.\n15,950 digital health-related papers from the top 10 academic fields were\nanalyzed using the Web of Science. The papers were grouped into three domains:\npublic health, medicine, and electrical engineering and computer science\n(EECS). Two time periods (2012-2016 and 2017-2021) were compared using Latent\nDirichlet Allocation (LDA) for topic modeling. The number of topics was\ndetermined based on coherence score, and topic compositions were compared using\na homogeneity test. The number of optimal topics varied across domains and time\nperiods. For public health, the first and second halves had 13 and 19 topics,\nrespectively. Medicine had 14 and 25 topics, and EECS had 7 and 21 topics. Text\nanalysis revealed shared topics among the domains, but with variations in\ncomposition. The homogeneity test confirmed significant differences between the\ngroups (p&lt;2.2e-16). Six dominant themes emerged, including journal article\nmethodology, information technology, medical issues, population demographics,\nsocial phenomena, and healthcare. Digital health research is expanding and\nevolving, particularly in relation to Covid-19, where topics such as depression\nand mental disorders, education, and physical activity have gained prominence.\nThere was no bias in topic composition among the three domains, but other\nfields like kinesiology or psychology could contribute to future digital health\nresearch. Exploring expanded topics that reflect people's needs for digital\nhealth over time will be crucial.",
        "translated": ""
    },
    {
        "title": "Making the Most Out of the Limited Context Length: Predictive Power\n  Varies with Clinical Note Type and Note Section",
        "url": "http://arxiv.org/abs/2307.07051v1",
        "pub_date": "2023-07-13",
        "summary": "Recent advances in large language models have led to renewed interest in\nnatural language processing in healthcare using the free text of clinical\nnotes. One distinguishing characteristic of clinical notes is their long time\nspan over multiple long documents. The unique structure of clinical notes\ncreates a new design choice: when the context length for a language model\npredictor is limited, which part of clinical notes should we choose as the\ninput? Existing studies either choose the inputs with domain knowledge or\nsimply truncate them. We propose a framework to analyze the sections with high\npredictive power. Using MIMIC-III, we show that: 1) predictive power\ndistribution is different between nursing notes and discharge notes and 2)\ncombining different types of notes could improve performance when the context\nlength is large. Our findings suggest that a carefully selected sampling\nfunction could enable more efficient information extraction from clinical\nnotes.",
        "translated": ""
    },
    {
        "title": "Towards Populating Generalizable Engineering Design Knowledge",
        "url": "http://arxiv.org/abs/2307.06985v1",
        "pub_date": "2023-07-13",
        "summary": "Aiming to populate generalizable engineering design knowledge, we propose a\nmethod to extract facts of the form head entity :: relationship :: tail entity\nfrom sentences found in patent documents. These facts could be combined within\nand across patent documents to form knowledge graphs that serve as schemes for\nrepresenting as well as storing design knowledge. Existing methods in\nengineering design literature often utilise a set of predefined relationships\nto populate triples that are statistical approximations rather than facts. In\nour method, we train a tagger to identify both entities and relationships from\na sentence. Given a pair of entities thus identified, we train another tagger\nto identify the relationship tokens that specifically denote the relationship\nbetween the pair. For training these taggers, we manually construct a dataset\nof 44,227 sentences and corresponding facts. We also compare the performance of\nthe method against typically recommended approaches, wherein, we predict the\nedges among tokens by pairing the tokens independently and as part of a graph.\nWe apply our method to sentences found in patents related to fan systems and\nbuild a domain knowledge base. Upon providing an overview of the knowledge\nbase, we search for solutions relevant to some key issues prevailing in fan\nsystems. We organize the responses into knowledge graphs and hold a comparative\ndiscussion against the opinions from ChatGPT.",
        "translated": ""
    },
    {
        "title": "NS4AR: A new, focused on sampling areas sampling method in graphical\n  recommendation Systems",
        "url": "http://arxiv.org/abs/2307.07321v1",
        "pub_date": "2023-07-13",
        "summary": "The effectiveness of graphical recommender system depends on the quantity and\nquality of negative sampling. This paper selects some typical recommender\nsystem models, as well as some latest negative sampling strategies on the\nmodels as baseline. Based on typical graphical recommender model, we divide\nsample region into assigned-n areas and use AdaSim to give different weight to\nthese areas to form positive set and negative set. Because of the volume and\nsignificance of negative items, we also proposed a subset selection model to\nnarrow the core negative samples.",
        "translated": ""
    },
    {
        "title": "Leveraging Recommender Systems to Reduce Content Gaps on Peer Production\n  Platforms",
        "url": "http://arxiv.org/abs/2307.08669v1",
        "pub_date": "2023-07-17",
        "summary": "Peer production platforms like Wikipedia commonly suffer from content gaps.\nPrior research suggests recommender systems can help solve this problem, by\nguiding editors towards underrepresented topics. However, it remains unclear\nwhether this approach would result in less relevant recommendations, leading to\nreduced overall engagement with recommended items. To answer this question, we\nfirst conducted offline analyses (Study 1) on SuggestBot, a task-routing\nrecommender system for Wikipedia, then did a three-month controlled experiment\n(Study 2). Our results show that presenting users with articles from\nunderrepresented topics increased the proportion of work done on those articles\nwithout significantly reducing overall recommendation uptake. We discuss the\nimplications of our results, including how ignoring the article discovery\nprocess can artificially narrow recommendations. We draw parallels between this\nphenomenon and the common issue of ``filter bubbles'' to show how any platform\nthat employs recommender systems is susceptible to it.",
        "translated": ""
    },
    {
        "title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2307.08303v1",
        "pub_date": "2023-07-17",
        "summary": "Dense retrieval (DR) converts queries and documents into dense embeddings and\nmeasures the similarity between queries and documents in vector space. One of\nthe challenges in DR is the lack of domain-specific training data. While DR\nmodels can learn from large-scale public datasets like MS MARCO through\ntransfer learning, evidence shows that not all DR models and domains can\nbenefit from transfer learning equally. Recently, some researchers have\nresorted to large language models (LLMs) to improve the zero-shot and few-shot\nDR models. However, the hard prompts or human-written prompts utilized in these\nworks cannot guarantee the good quality of generated weak queries. To tackle\nthis, we propose soft prompt tuning for augmenting DR (SPTAR): For each task,\nwe leverage soft prompt-tuning to optimize a task-specific soft prompt on\nlimited ground truth data and then prompt the LLMs to tag unlabeled documents\nwith weak queries, yielding enough weak document-query pairs to train\ntask-specific dense retrievers. We design a filter to select high-quality\nexample document-query pairs in the prompt to further improve the quality of\nweak tagged queries. To the best of our knowledge, there is no prior work\nutilizing soft prompt tuning to augment DR models. The experiments demonstrate\nthat SPTAR outperforms the unsupervised baselines BM25 and the recently\nproposed LLMs-based augmentation method for DR.",
        "translated": ""
    },
    {
        "title": "Measuring Item Global Residual Value for Fair Recommendation",
        "url": "http://arxiv.org/abs/2307.08259v1",
        "pub_date": "2023-07-17",
        "summary": "In the era of information explosion, numerous items emerge every day,\nespecially in feed scenarios. Due to the limited system display slots and user\nbrowsing attention, various recommendation systems are designed not only to\nsatisfy users' personalized information needs but also to allocate items'\nexposure. However, recent recommendation studies mainly focus on modeling user\npreferences to present satisfying results and maximize user interactions, while\npaying little attention to developing item-side fair exposure mechanisms for\nrational information delivery. This may lead to serious resource allocation\nproblems on the item side, such as the Snowball Effect. Furthermore, unfair\nexposure mechanisms may hurt recommendation performance. In this paper, we call\nfor a shift of attention from modeling user preferences to developing fair\nexposure mechanisms for items. We first conduct empirical analyses of feed\nscenarios to explore exposure problems between items with distinct uploaded\ntimes. This points out that unfair exposure caused by the time factor may be\nthe major cause of the Snowball Effect. Then, we propose to explicitly model\nitem-level customized timeliness distribution, Global Residual Value (GRV), for\nfair resource allocation. This GRV module is introduced into recommendations\nwith the designed Timeliness-aware Fair Recommendation Framework (TaFR).\nExtensive experiments on two datasets demonstrate that TaFR achieves consistent\nimprovements with various backbone recommendation models. By modeling item-side\ncustomized Global Residual Value, we achieve a fairer distribution of resources\nand, at the same time, improve recommendation performance.",
        "translated": ""
    },
    {
        "title": "Data Discovery for the SDGs: A Systematic Rule-based Approach",
        "url": "http://arxiv.org/abs/2307.07983v1",
        "pub_date": "2023-07-16",
        "summary": "In 2015, the United Nations put forward 17 Sustainable Development Goals\n(SDGs) to be achieved by 2030, where data has been promoted as a focus to\ninnovating sustainable development and as a means to measuring progress towards\nachieving the SDGs. In this study, we propose a systematic approach towards\ndiscovering data types and sources that can be used for SDG research. The\nproposed method integrates a systematic mapping approach using manual\nqualitative coding over a corpus of SDG-related research literature followed by\nan automated process that applies rules to perform data entity extraction\ncomputationally. This approach is exemplified by an analysis of literature\nrelating to SDG 7, the results of which are also presented in this paper. The\npaper concludes with a discussion of the approach and suggests future work to\nextend the method with more advance NLP and machine learning techniques.",
        "translated": ""
    },
    {
        "title": "Opinion mining using Double Channel CNN for Recommender System",
        "url": "http://arxiv.org/abs/2307.07798v1",
        "pub_date": "2023-07-15",
        "summary": "Much unstructured data has been produced with the growth of the Internet and\nsocial media. A significant volume of textual data includes users' opinions\nabout products in online stores and social media. By exploring and categorizing\nthem, helpful information can be acquired, including customer satisfaction,\nuser feedback about a particular event, predicting the sale of a specific\nproduct, and other similar cases. In this paper, we present an approach for\nsentiment analysis with a deep learning model and use it to recommend products.\nA two-channel convolutional neural network model has been used for opinion\nmining, which has five layers and extracts essential features from the data. We\nincreased the number of comments by applying the SMOTE algorithm to the initial\ndataset and balanced the data. Then we proceed to cluster the aspects. We also\nassign a weight to each cluster using tensor decomposition algorithms that\nimprove the recommender system's performance. Our proposed method has reached\n91.6% accuracy, significantly improved compared to previous aspect-based\napproaches.",
        "translated": ""
    },
    {
        "title": "Improving Trace Link Recommendation by Using Non-Isotropic Distances and\n  Combinations",
        "url": "http://arxiv.org/abs/2307.07781v1",
        "pub_date": "2023-07-15",
        "summary": "The existence of trace links between artifacts of the software development\nlife cycle can improve the efficiency of many activities during software\ndevelopment, maintenance and operations. Unfortunately, the creation and\nmaintenance of trace links is time-consuming and error-prone. Research efforts\nhave been spent to automatically compute trace links and lately gained\nmomentum, e.g., due to the availability of powerful tools in the area of\nnatural language processing. In this paper, we report on some observations that\nwe made during studying non-linear similarity measures for computing trace\nlinks. We argue, that taking a geometric viewpoint on semantic similarity can\nbe helpful for future traceability research. We evaluated our observations on a\ndataset of four open source projects and two industrial projects. We\nfurthermore point out that our findings are more general and can build the\nbasis for other information retrieval problems as well.",
        "translated": ""
    },
    {
        "title": "Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model",
        "url": "http://arxiv.org/abs/2307.07740v1",
        "pub_date": "2023-07-15",
        "summary": "Sentiment analysis is the process of identifying and categorizing people's\nemotions or opinions regarding various topics. The analysis of Twitter\nsentiment has become an increasingly popular topic in recent years. In this\npaper, we present several machine learning and a deep learning model to\nanalysis sentiment of Persian political tweets. Our analysis was conducted\nusing Bag of Words and ParsBERT for word representation. We applied Gaussian\nNaive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random\nForests, as well as a combination of CNN and LSTM to classify the polarities of\ntweets. The results of this study indicate that deep learning with ParsBERT\nembedding performs better than machine learning. The CNN-LSTM model had the\nhighest classification accuracy with 89 percent on the first dataset with three\nclasses and 71 percent on the second dataset with seven classes. Due to the\ncomplexity of Persian, it was a difficult task to achieve this level of\nefficiency.",
        "translated": ""
    },
    {
        "title": "On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit\n  Mechanisms",
        "url": "http://arxiv.org/abs/2307.07675v1",
        "pub_date": "2023-07-15",
        "summary": "Efficient learning in multi-armed bandit mechanisms such as pay-per-click\n(PPC) auctions typically involves three challenges: 1) inducing truthful\nbidding behavior (incentives), 2) using personalization in the users (context),\nand 3) circumventing manipulations in click patterns (corruptions). Each of\nthese challenges has been studied orthogonally in the literature; incentives\nhave been addressed by a line of work on truthful multi-armed bandit\nmechanisms, context has been extensively tackled by contextual bandit\nalgorithms, while corruptions have been discussed via a recent line of work on\nbandits with adversarial corruptions. Since these challenges co-exist, it is\nimportant to understand the robustness of each of these approaches in\naddressing the other challenges, provide algorithms that can handle all\nsimultaneously, and highlight inherent limitations in this combination. In this\nwork, we show that the most prominent contextual bandit algorithm,\n$\\epsilon$-greedy can be extended to handle the challenges introduced by\nstrategic arms in the contextual multi-arm bandit mechanism setting. We further\nshow that $\\epsilon$-greedy is inherently robust to adversarial data corruption\nattacks and achieves performance that degrades linearly with the amount of\ncorruption.",
        "translated": ""
    },
    {
        "title": "Deep Neural Aggregation for Recommending Items to Group of Users",
        "url": "http://arxiv.org/abs/2307.09447v1",
        "pub_date": "2023-07-18",
        "summary": "Modern society devotes a significant amount of time to digital interaction.\nMany of our daily actions are carried out through digital means. This has led\nto the emergence of numerous Artificial Intelligence tools that assist us in\nvarious aspects of our lives. One key tool for the digital society is\nRecommender Systems, intelligent systems that learn from our past actions to\npropose new ones that align with our interests. Some of these systems have\nspecialized in learning from the behavior of user groups to make\nrecommendations to a group of individuals who want to perform a joint task. In\nthis article, we analyze the current state of Group Recommender Systems and\npropose two new models that use emerging Deep Learning architectures.\nExperimental results demonstrate the improvement achieved by employing the\nproposed models compared to the state-of-the-art models using four different\ndatasets. The source code of the models, as well as that of all the experiments\nconducted, is available in a public repository.",
        "translated": ""
    },
    {
        "title": "Zero-shot Query Reformulation for Conversational Search",
        "url": "http://arxiv.org/abs/2307.09384v1",
        "pub_date": "2023-07-18",
        "summary": "As the popularity of voice assistants continues to surge, conversational\nsearch has gained increased attention in Information Retrieval. However, data\nsparsity issues in conversational search significantly hinder the progress of\nsupervised conversational search methods. Consequently, researchers are\nfocusing more on zero-shot conversational search approaches. Nevertheless,\nexisting zero-shot methods face three primary limitations: they are not\nuniversally applicable to all retrievers, their effectiveness lacks sufficient\nexplainability, and they struggle to resolve common conversational ambiguities\ncaused by omission. To address these limitations, we introduce a novel\nZero-shot Query Reformulation (ZeQR) framework that reformulates queries based\non previous dialogue contexts without requiring supervision from conversational\nsearch data. Specifically, our framework utilizes language models designed for\nmachine reading comprehension tasks to explicitly resolve two common\nambiguities: coreference and omission, in raw queries. In comparison to\nexisting zero-shot methods, our approach is universally applicable to any\nretriever without additional adaptation or indexing. It also provides greater\nexplainability and effectively enhances query intent understanding because\nambiguities are explicitly and proactively resolved. Through extensive\nexperiments on four TREC conversational datasets, we demonstrate the\neffectiveness of our method, which consistently outperforms state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via\n  Parameter Constraint",
        "url": "http://arxiv.org/abs/2307.09193v1",
        "pub_date": "2023-07-18",
        "summary": "Large-scale online recommender system spreads all over the Internet being in\ncharge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion\nRate (CVR) estimations. However, traditional CVR estimators suffer from\nwell-known Sample Selection Bias and Data Sparsity issues. Entire space models\nwere proposed to address the two issues via tracing the decision-making path of\n\"exposure_click_purchase\". Further, some researchers observed that there are\npurchase-related behaviors between click and purchase, which can better draw\nthe user's decision-making intention and improve the recommendation\nperformance. Thus, the decision-making path has been extended to\n\"exposure_click_in-shop action_purchase\" and can be modeled with conditional\nprobability approach. Nevertheless, we observe that the chain rule of\nconditional probability does not always hold. We report Probability Space\nConfusion (PSC) issue and give a derivation of difference between ground-truth\nand estimation mathematically. We propose a novel Entire Space Multi-Task Model\nfor Post-Click Conversion Rate via Parameter Constraint (ESMC) and two\nalternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and\nEntire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.\nSpecifically, we handle \"exposure_click_in-shop action\" and \"in-shop\naction_purchase\" separately in the light of characteristics of in-shop action.\nThe first path is still treated with conditional probability while the second\none is treated with parameter constraint strategy. Experiments on both offline\nand online environments in a large-scale recommendation system illustrate the\nsuperiority of our proposed methods over state-of-the-art models. The\nreal-world datasets will be released.",
        "translated": ""
    },
    {
        "title": "Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance\n  Detection and Feature Matching for Image Retrieval for Arguments",
        "url": "http://arxiv.org/abs/2307.09172v1",
        "pub_date": "2023-07-18",
        "summary": "Participating in the shared task \"Image Retrieval for arguments\", we used\ndifferent pipelines for image retrieval containing Image Generation, Stance\nDetection, Preselection and Feature Matching. We submitted four different runs\nwith different pipeline layout and compare them to given baseline. Our\npipelines perform similarly to the baseline.",
        "translated": ""
    },
    {
        "title": "Modeling Orders of User Behaviors via Differentiable Sorting: A\n  Multi-task Framework to Predicting User Post-click Conversion",
        "url": "http://arxiv.org/abs/2307.09089v1",
        "pub_date": "2023-07-18",
        "summary": "User post-click conversion prediction is of high interest to researchers and\ndevelopers. Recent studies employ multi-task learning to tackle the selection\nbias and data sparsity problem, two severe challenges in post-click behavior\nprediction, by incorporating click data. However, prior works mainly focused on\npointwise learning and the orders of labels (i.e., click and post-click) are\nnot well explored, which naturally poses a listwise learning problem. Inspired\nby recent advances on differentiable sorting, in this paper, we propose a novel\nmulti-task framework that leverages orders of user behaviors to predict user\npost-click conversion in an end-to-end approach. Specifically, we define an\naggregation operator to combine predicted outputs of different tasks to a\nunified score, then we use the computed scores to model the label relations via\ndifferentiable sorting. Extensive experiments on public and industrial datasets\nshow the superiority of our proposed model against competitive baselines.",
        "translated": ""
    },
    {
        "title": "GraphCL-DTA: a graph contrastive learning with molecular semantics for\n  drug-target binding affinity prediction",
        "url": "http://arxiv.org/abs/2307.08989v1",
        "pub_date": "2023-07-18",
        "summary": "Drug-target binding affinity prediction plays an important role in the early\nstages of drug discovery, which can infer the strength of interactions between\nnew drugs and new targets. However, the performance of previous computational\nmodels is limited by the following drawbacks. The learning of drug\nrepresentation relies only on supervised data, without taking into account the\ninformation contained in the molecular graph itself. Moreover, most previous\nstudies tended to design complicated representation learning module, while\nuniformity, which is used to measure representation quality, is ignored. In\nthis study, we propose GraphCL-DTA, a graph contrastive learning with molecular\nsemantics for drug-target binding affinity prediction. In GraphCL-DTA, we\ndesign a graph contrastive learning framework for molecular graphs to learn\ndrug representations, so that the semantics of molecular graphs are preserved.\nThrough this graph contrastive framework, a more essential and effective drug\nrepresentation can be learned without additional supervised data. Next, we\ndesign a new loss function that can be directly used to smoothly adjust the\nuniformity of drug and target representations. By directly optimizing the\nuniformity of representations, the representation quality of drugs and targets\ncan be improved. The effectiveness of the above innovative elements is verified\non two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA\non the above datasets suggests its superiority to the state-of-the-art model.",
        "translated": ""
    },
    {
        "title": "Sharpness-Aware Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2307.08910v1",
        "pub_date": "2023-07-18",
        "summary": "Graph Neural Networks (GNNs) have achieved impressive performance in\ncollaborative filtering. However, GNNs tend to yield inferior performance when\nthe distributions of training and test data are not aligned well. Also,\ntraining GNNs requires optimizing non-convex neural networks with an abundance\nof local and global minima, which may differ widely in their performance at\ntest time. Thus, it is essential to choose the minima carefully. Here we\npropose an effective training schema, called {gSAM}, under the principle that\nthe \\textit{flatter} minima has a better generalization ability than the\n\\textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of\nthe weight loss landscape by forming a bi-level optimization: the outer problem\nconducts the standard model training while the inner problem helps the model\njump out of the sharp minima. Experimental results show the superiority of our\ngSAM.",
        "translated": ""
    },
    {
        "title": "An Admissible Shift-Consistent Method for Recommender Systems",
        "url": "http://arxiv.org/abs/2307.08857v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper, we propose a new constraint, called shift-consistency, for\nsolving matrix/tensor completion problems in the context of recommender\nsystems. Our method provably guarantees several key mathematical properties:\n(1) satisfies a recently established admissibility criterion for recommender\nsystems; (2) satisfies a definition of fairness that eliminates a specific\nclass of potential opportunities for users to maliciously influence system\nrecommendations; and (3) offers robustness by exploiting provable uniqueness of\nmissing-value imputation. We provide a rigorous mathematical description of the\nmethod, including its generalization from matrix to tensor form to permit\nrepresentation and exploitation of complex structural relationships among sets\nof user and product attributes. We argue that our analysis suggests a\nstructured means for defining latent-space projections that can permit provable\nperformance properties to be established for machine learning methods.",
        "translated": ""
    },
    {
        "title": "An Exploration Study of Mixed-initiative Query Reformulation in\n  Conversational Passage Retrieval",
        "url": "http://arxiv.org/abs/2307.08803v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper, we report our methods and experiments for the TREC\nConversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce\nmulti-stage retrieval pipelines and explore one of the potential benefits of\ninvolving mixed-initiative interaction in conversational passage retrieval\nscenarios: reformulating raw queries. Before the first ranking stage of a\nmulti-stage retrieval pipeline, we propose a mixed-initiative query\nreformulation module, which achieves query reformulation based on the\nmixed-initiative interaction between the users and the system, as the\nreplacement for the neural reformulation method. Specifically, we design an\nalgorithm to generate appropriate questions related to the ambiguities in raw\nqueries, and another algorithm to reformulate raw queries by parsing users'\nfeedback and incorporating it into the raw query. For the first ranking stage\nof our multi-stage pipelines, we adopt a sparse ranking function: BM25, and a\ndense retrieval method: TCT-ColBERT. For the second-ranking step, we adopt a\npointwise reranker: MonoT5, and a pairwise reranker: DuoT5. Experiments on both\nTREC CAsT 2021 and TREC CAsT 2022 datasets show the effectiveness of our\nmixed-initiative-based query reformulation method on improving retrieval\nperformance compared with two popular reformulators: a neural reformulator:\nCANARD-T5 and a rule-based reformulator: historical query reformulator(HQE).",
        "translated": ""
    },
    {
        "title": "Imposing Consistency Properties on Blackbox Systems with Applications to\n  SVD-Based Recommender Systems",
        "url": "http://arxiv.org/abs/2307.08760v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper we discuss pre- and post-processing methods to induce desired\nconsistency and/or invariance properties in blackbox systems, e.g., AI-based.\nWe demonstrate our approach in the context of blackbox SVD-based\nmatrix-completion methods commonly used in recommender system (RS)\napplications. We provide empirical results showing that enforcement of\nunit-consistency and shift-consistency, which have provable RS-relevant\nproperties relating to robustness and fairness, also lead to improved\nperformance according to generic RMSE and MAE performance metrics, irrespective\nof the initial chosen hyperparameter.",
        "translated": ""
    },
    {
        "title": "UniMatch: A Unified User-Item Matching Framework for the Multi-purpose\n  Merchant Marketing",
        "url": "http://arxiv.org/abs/2307.09989v1",
        "pub_date": "2023-07-19",
        "summary": "When doing private domain marketing with cloud services, the merchants\nusually have to purchase different machine learning models for the multiple\nmarketing purposes, leading to a very high cost. We present a unified user-item\nmatching framework to simultaneously conduct item recommendation and user\ntargeting with just one model. We empirically demonstrate that the above\nconcurrent modeling is viable via modeling the user-item interaction matrix\nwith the multinomial distribution, and propose a bidirectional bias-corrected\nNCE loss for the implementation. The proposed loss function guides the model to\nlearn the user-item joint probability $p(u,i)$ instead of the conditional\nprobability $p(i|u)$ or $p(u|i)$ through correcting both the users and items'\nbiases caused by the in-batch negative sampling. In addition, our framework is\nmodel-agnostic enabling a flexible adaptation of different model architectures.\nExtensive experiments demonstrate that our framework results in significant\nperformance gains in comparison with the state-of-the-art methods, with greatly\nreduced cost on computing resources and daily maintenance.",
        "translated": ""
    },
    {
        "title": "Our Model Achieves Excellent Performance on MovieLens: What Does it\n  Mean?",
        "url": "http://arxiv.org/abs/2307.09985v1",
        "pub_date": "2023-07-19",
        "summary": "A typical benchmark dataset for recommender system (RecSys) evaluation\nconsists of user-item interactions generated on a platform within a time\nperiod. The interaction generation mechanism partially explains why a user\ninteracts with (e.g.,like, purchase, rate) an item, and the context of when a\nparticular interaction happened. In this study, we conduct a meticulous\nanalysis on the MovieLens dataset and explain the potential impact on using the\ndataset for evaluating recommendation algorithms. We make a few main findings\nfrom our analysis. First, there are significant differences in user\ninteractions at the different stages when a user interacts with the MovieLens\nplatform. The early interactions largely define the user portrait which affect\nthe subsequent interactions. Second, user interactions are highly affected by\nthe candidate movies that are recommended by the platform's internal\nrecommendation algorithm(s). Removal of interactions that happen nearer to the\nlast few interactions of a user leads to increasing difficulty in learning user\npreference, thus deteriorating recommendation accuracy. Third, changing the\norder of user interactions makes it more difficult for sequential algorithms to\ncapture the progressive interaction process. Based on these findings, we\nfurther discuss the discrepancy between the interaction generation mechanism\nthat is employed by the MovieLens system and that of typical real world\nrecommendation scenarios. In summary, models that achieve excellent\nrecommendation accuracy on the MovieLens dataset may not demonstrate superior\nperformance in practice for at least two kinds of differences: (i) the\ndifferences in the contexts of user-item interaction generation, and (ii) the\ndifferences in user knowledge about the item collections.",
        "translated": ""
    },
    {
        "title": "Who Provides the Largest Megaphone? The Role of Google News in Promoting\n  Russian State-Affiliated News Sources",
        "url": "http://arxiv.org/abs/2307.09834v1",
        "pub_date": "2023-07-19",
        "summary": "The Internet has not only digitized but also democratized information access\nacross the globe. This gradual but path-breaking move to online information\npropagation has resulted in search engines playing an increasingly prominent\nrole in shaping access to human knowledge. When an Internet user enters a\nquery, the search engine sorts through the hundreds of billions of possible\nwebpages to determine what to show. Google dominates the search engine market,\nwith Google Search surpassing 80% market share globally every year of the last\ndecade. Only in Russia and China do Google competitors claim more market share,\nwith approximately 60% of Internet users in Russia preferring Yandex (compared\nto 40% in favor of Google) and more than 80% of China's Internet users\naccessing Baidu as of 2022. Notwithstanding this long-standing regional\nvariation in Internet search providers, there is limited research showing how\nthese providers compare in terms of propagating state-sponsored information.\nOur study fills this research gap by focusing on Russian cyberspace and\nexamining how Google and Yandex's search algorithms rank content from Russian\nstate-controlled media (hereon, RSM) outlets. This question is timely and of\npractical interest given widespread reports indicating that RSM outlets have\nactively engaged in promoting Kremlin propaganda in the lead-up to, and in the\naftermath of, the Russian invasion of Ukraine in February 2022.",
        "translated": ""
    },
    {
        "title": "DisCover: Disentangled Music Representation Learning for Cover Song\n  Identification",
        "url": "http://arxiv.org/abs/2307.09775v1",
        "pub_date": "2023-07-19",
        "summary": "In the field of music information retrieval (MIR), cover song identification\n(CSI) is a challenging task that aims to identify cover versions of a query\nsong from a massive collection. Existing works still suffer from high\nintra-song variances and inter-song correlations, due to the entangled nature\nof version-specific and version-invariant factors in their modeling. In this\nwork, we set the goal of disentangling version-specific and version-invariant\nfactors, which could make it easier for the model to learn invariant music\nrepresentations for unseen query songs. We analyze the CSI task in a\ndisentanglement view with the causal graph technique, and identify the\nintra-version and inter-version effects biasing the invariant learning. To\nblock these effects, we propose the disentangled music representation learning\nframework (DisCover) for CSI. DisCover consists of two critical components: (1)\nKnowledge-guided Disentanglement Module (KDM) and (2) Gradient-based\nAdversarial Disentanglement Module (GADM), which block intra-version and\ninter-version biased effects, respectively. KDM minimizes the mutual\ninformation between the learned representations and version-variant factors\nthat are identified with prior domain knowledge. GADM identifies\nversion-variant factors by simulating the representation transitions between\nintra-song versions, and exploits adversarial distillation for effect blocking.\nExtensive comparisons with best-performing methods and in-depth analysis\ndemonstrate the effectiveness of DisCover and the and necessity of\ndisentanglement for CSI.",
        "translated": ""
    },
    {
        "title": "Information Retrieval Meets Large Language Models: A Strategic Report\n  from Chinese IR Community",
        "url": "http://arxiv.org/abs/2307.09751v1",
        "pub_date": "2023-07-19",
        "summary": "The research field of Information Retrieval (IR) has evolved significantly,\nexpanding beyond traditional search to meet diverse user information needs.\nRecently, Large Language Models (LLMs) have demonstrated exceptional\ncapabilities in text understanding, generation, and knowledge inference,\nopening up exciting avenues for IR research. LLMs not only facilitate\ngenerative retrieval but also offer improved solutions for user understanding,\nmodel evaluation, and user-system interactions. More importantly, the\nsynergistic relationship among IR models, LLMs, and humans forms a new\ntechnical paradigm that is more powerful for information seeking. IR models\nprovide real-time and relevant information, LLMs contribute internal knowledge,\nand humans play a central role of demanders and evaluators to the reliability\nof information services. Nevertheless, significant challenges exist, including\ncomputational costs, credibility concerns, domain-specific limitations, and\nethical considerations. To thoroughly discuss the transformative impact of LLMs\non IR research, the Chinese IR community conducted a strategic workshop in\nApril 2023, yielding valuable insights. This paper provides a summary of the\nworkshop's outcomes, including the rethinking of IR's core values, the mutual\nenhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and\nopen challenges.",
        "translated": ""
    },
    {
        "title": "Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for\n  Recommendation and Text Generation",
        "url": "http://arxiv.org/abs/2307.09688v1",
        "pub_date": "2023-07-19",
        "summary": "Modeling customer shopping intentions is a crucial task for e-commerce, as it\ndirectly impacts user experience and engagement. Thus, accurately understanding\ncustomer preferences is essential for providing personalized recommendations.\nSession-based recommendation, which utilizes customer session data to predict\ntheir next interaction, has become increasingly popular. However, existing\nsession datasets have limitations in terms of item attributes, user diversity,\nand dataset scale. As a result, they cannot comprehensively capture the\nspectrum of user behaviors and preferences. To bridge this gap, we present the\nAmazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It\nis the first multilingual dataset consisting of millions of user sessions from\nsix different locales, where the major languages of products are English,\nGerman, Japanese, French, Italian, and Spanish. Remarkably, the dataset can\nhelp us enhance personalization and understanding of user preferences, which\ncan benefit various existing tasks as well as enable new tasks. To test the\npotential of the dataset, we introduce three tasks in this work: (1)\nnext-product recommendation, (2) next-product recommendation with domain\nshifts, and (3) next-product title generation. With the above tasks, we\nbenchmark a range of algorithms on our proposed dataset, drawing new insights\nfor further research and practice. In addition, based on the proposed dataset\nand tasks, we hosted a competition in the KDD CUP 2023 and have attracted\nthousands of users and submissions. The winning solutions and the associated\nworkshop can be accessed at our website https://kddcup23.github.io/.",
        "translated": ""
    },
    {
        "title": "PubMed and Beyond: Recent Advances and Best Practices in Biomedical\n  Literature Search",
        "url": "http://arxiv.org/abs/2307.09683v1",
        "pub_date": "2023-07-18",
        "summary": "Biomedical research yields a wealth of information, much of which is only\naccessible through the literature. Consequently, literature search is an\nessential tool for building on prior knowledge in clinical and biomedical\nresearch. Although recent improvements in artificial intelligence have expanded\nfunctionality beyond keyword-based search, these advances may be unfamiliar to\nclinicians and researchers. In response, we present a survey of literature\nsearch tools tailored to both general and specific information needs in\nbiomedicine, with the objective of helping readers efficiently fulfill their\ninformation needs. We first examine the widely used PubMed search engine,\ndiscussing recent improvements and continued challenges. We then describe\nliterature search tools catering to five specific information needs: 1.\nIdentifying high-quality clinical research for evidence-based medicine. 2.\nRetrieving gene-related information for precision medicine and genomics. 3.\nSearching by meaning, including natural language questions. 4. Locating related\narticles with literature recommendation. 5. Mining literature to discover\nassociations between concepts such as diseases and genetic variants.\nAdditionally, we cover practical considerations and best practices for choosing\nand using these tools. Finally, we provide a perspective on the future of\nliterature search engines, considering recent breakthroughs in large language\nmodels such as ChatGPT. In summary, our survey provides a comprehensive view of\nbiomedical literature search functionalities with 36 publicly available tools.",
        "translated": ""
    },
    {
        "title": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation",
        "url": "http://arxiv.org/abs/2307.11019v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require\na substantial amount of factual knowledge and often rely on external\ninformation for assistance. Recently, large language models (LLMs) (e.g.,\nChatGPT), have demonstrated impressive prowess in solving a wide range of tasks\nwith world knowledge, including knowledge-intensive tasks. However, it remains\nunclear how well LLMs are able to perceive their factual knowledge boundaries,\nparticularly how they behave when incorporating retrieval augmentation. In this\nstudy, we present an initial analysis of the factual knowledge boundaries of\nLLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially,\nwe focus on three primary research questions and analyze them by examining QA\nperformance, priori judgement and posteriori judgement of LLMs. We show\nevidence that LLMs possess unwavering confidence in their capabilities to\nrespond to questions and the accuracy of their responses. Furthermore,\nretrieval augmentation proves to be an effective approach in enhancing LLMs'\nawareness of knowledge boundaries, thereby improving their judgemental\nabilities. Additionally, we also find that LLMs have a propensity to rely on\nthe provided retrieval results when formulating answers, while the quality of\nthese results significantly impacts their reliance. The code to reproduce this\nwork is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
        "translated": ""
    },
    {
        "title": "Enhancing Job Recommendation through LLM-based Generative Adversarial\n  Networks",
        "url": "http://arxiv.org/abs/2307.10747v1",
        "pub_date": "2023-07-20",
        "summary": "Recommending suitable jobs to users is a critical task in online recruitment\nplatforms, as it can enhance users' satisfaction and the platforms'\nprofitability. While existing job recommendation methods encounter challenges\nsuch as the low quality of users' resumes, which hampers their accuracy and\npractical effectiveness. With the rapid development of large language models\n(LLMs), utilizing the rich external knowledge encapsulated within them, as well\nas their powerful capabilities of text processing and reasoning, is a promising\nway to complete users' resumes for more accurate recommendations. However,\ndirectly leveraging LLMs to enhance recommendation results is not a\none-size-fits-all solution, as LLMs may suffer from fabricated generation and\nfew-shot problems, which degrade the quality of resume completion. In this\npaper, we propose a novel LLM-based approach for job recommendation. To\nalleviate the limitation of fabricated generation for LLMs, we extract accurate\nand valuable information beyond users' self-description, which helps the LLMs\nbetter profile users for resume completion. Specifically, we not only extract\nusers' explicit properties (e.g., skills, interests) from their\nself-description but also infer users' implicit characteristics from their\nbehaviors for more accurate and meaningful resume completion. Nevertheless,\nsome users still suffer from few-shot problems, which arise due to scarce\ninteraction records, leading to limited guidance for the models in generating\nhigh-quality resumes. To address this issue, we propose aligning unpaired\nlow-quality with high-quality generated resumes by Generative Adversarial\nNetworks (GANs), which can refine the resume representations for better\nrecommendation results. Extensive experiments on three large real-world\nrecruitment datasets demonstrate the effectiveness of our proposed method.",
        "translated": ""
    },
    {
        "title": "A Constraint-based Recommender System via RDF Knowledge Graphs",
        "url": "http://arxiv.org/abs/2307.10702v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge graphs, represented in RDF, are able to model entities and their\nrelations by means of ontologies. The use of knowledge graphs for information\nmodeling has attracted interest in recent years. In recommender systems, items\nand users can be mapped and integrated into the knowledge graph, which can\nrepresent more links and relationships between users and items.\nConstraint-based recommender systems are based on the idea of explicitly\nexploiting deep recommendation knowledge through constraints to identify\nrelevant recommendations. When combined with knowledge graphs, a\nconstraint-based recommender system gains several benefits in terms of\nconstraint sets. In this paper, we investigate and propose the construction of\na constraint-based recommender system via RDF knowledge graphs applied to the\nvehicle purchase/sale domain. The results of our experiments show that the\nproposed approach is able to efficiently identify recommendations in accordance\nwith user preferences.",
        "translated": ""
    },
    {
        "title": "A Personalized Recommender System Based-on Knowledge Graph Embeddings",
        "url": "http://arxiv.org/abs/2307.10680v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge graphs have proven to be effective for modeling entities and their\nrelationships through the use of ontologies. The recent emergence in interest\nfor using knowledge graphs as a form of information modeling has led to their\nincreased adoption in recommender systems. By incorporating users and items\ninto the knowledge graph, these systems can better capture the implicit\nconnections between them and provide more accurate recommendations. In this\npaper, we investigate and propose the construction of a personalized\nrecommender system via knowledge graphs embedding applied to the vehicle\npurchase/sale domain. The results of our experimentation demonstrate the\nefficacy of the proposed method in providing relevant recommendations that are\nconsistent with individual users.",
        "translated": ""
    },
    {
        "title": "Language-Enhanced Session-Based Recommendation with Decoupled\n  Contrastive Learning",
        "url": "http://arxiv.org/abs/2307.10650v1",
        "pub_date": "2023-07-20",
        "summary": "Session-based recommendation techniques aim to capture dynamic user behavior\nby analyzing past interactions. However, existing methods heavily rely on\nhistorical item ID sequences to extract user preferences, leading to challenges\nsuch as popular bias and cold-start problems. In this paper, we propose a\nhybrid multimodal approach for session-based recommendation to address these\nchallenges. Our approach combines different modalities, including textual\ncontent and item IDs, leveraging the complementary nature of these modalities\nusing CatBoost. To learn universal item representations, we design a language\nrepresentation-based item retrieval architecture that extracts features from\nthe textual content utilizing pre-trained language models. Furthermore, we\nintroduce a novel Decoupled Contrastive Learning method to enhance the\neffectiveness of the language representation. This technique decouples the\nsequence representation and item representation space, facilitating\nbidirectional alignment through dual-queue contrastive learning.\nSimultaneously, the momentum queue provides a large number of negative samples,\neffectively enhancing the effectiveness of contrastive learning. Our approach\nyielded competitive results, securing a 5th place ranking in KDD CUP 2023 Task\n1. We have released the source code and pre-trained models associated with this\nwork.",
        "translated": ""
    },
    {
        "title": "Improving Semantic Similarity Measure Within a Recommender System\n  Based-on RDF Graphs",
        "url": "http://arxiv.org/abs/2307.10639v1",
        "pub_date": "2023-07-20",
        "summary": "In today's era of information explosion, more users are becoming more reliant\nupon recommender systems to have better advice, suggestions, or inspire them.\nThe measure of the semantic relatedness or likeness between terms, words, or\ntext data plays an important role in different applications dealing with\ntextual data, as in a recommender system. Over the past few years, many\nontologies have been developed and used as a form of structured representation\nof knowledge bases for information systems. The measure of semantic similarity\nfrom ontology has developed by several methods. In this paper, we propose and\ncarry on an approach for the improvement of semantic similarity calculations\nwithin a recommender system based-on RDF graphs.",
        "translated": ""
    },
    {
        "title": "Detecting deceptive reviews using text classification",
        "url": "http://arxiv.org/abs/2307.10617v1",
        "pub_date": "2023-07-20",
        "summary": "In recent years, online reviews play a vital role for promoting any kind of\nproduct or services. Businesses may embed fake reviews in order to attract\ncustomers to purchase their products. They may even highlight the benefits of\ntheir own product or criticize the competition's product. Marketers,\nadvertisers, and other online business users have incentive to create fake\npositive reviews for products which they want to promote or give fake negative\nreviews for products which they really don't like. So now-a-days writing a\ndeceptive review is inevitable thing for promoting their own business or\ndegrading competitor's reputation. Thus, identifying deceptive reviews is an\nintense and on-going research area. This research paper proposes machine\nlearning model approach to identify deceptive reviews. The paper investigates\nthe performance of the several experiments done on a Deceptive Opinion Spam\nCorpus dataset of restaurants reviews. We developed a n-gram model and max\nfeatures to identify deceptive contents with a particular focus on fake\nreviews. Further, we conduct a benchmark study to investigate the performance\nof two different features extraction techniques and apply five machine learning\nclassification techniques. The experimental results show that passive\naggressive classifier outperforms other algorithms, and it reaches the highest\naccuracy not only in text classification but also to fake reviews. We also\nstudy the data augmentation and implement different deep learning techniques.",
        "translated": ""
    },
    {
        "title": "SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot\n  Neural Sparse Retrieval",
        "url": "http://arxiv.org/abs/2307.10488v1",
        "pub_date": "2023-07-19",
        "summary": "Traditionally, sparse retrieval systems relied on lexical representations to\nretrieve documents, such as BM25, dominated information retrieval tasks. With\nthe onset of pre-trained transformer models such as BERT, neural sparse\nretrieval has led to a new paradigm within retrieval. Despite the success,\nthere has been limited software supporting different sparse retrievers running\nin a unified, common environment. This hinders practitioners from fairly\ncomparing different sparse models and obtaining realistic evaluation results.\nAnother missing piece is, that a majority of prior work evaluates sparse\nretrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO.\nHowever, a key requirement in practical retrieval systems requires models that\ncan generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In\nthis work, we provide SPRINT, a unified Python toolkit based on Pyserini and\nLucene, supporting a common interface for evaluating neural sparse retrieval.\nThe toolkit currently includes five built-in models: uniCOIL, DeepImpact,\nSPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by\ndefining their term weighting method. Using our toolkit, we establish strong\nand reproducible zero-shot sparse retrieval baselines across the\nwell-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2\nachieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural\nsparse retrievers. In this work, we further uncover the reasons behind its\nperformance gain. We show that SPLADEv2 produces sparse representations with a\nmajority of tokens outside of the original query and document which is often\ncrucial for its performance gains, i.e. a limitation among its other sparse\ncounterparts. We provide our SPRINT toolkit, models, and data used in our\nexperiments publicly here at https://github.com/thakur-nandan/sprint.",
        "translated": ""
    },
    {
        "title": "Fast Approximate Nearest Neighbor Search with a Dynamic Exploration\n  Graph using Continuous Refinement",
        "url": "http://arxiv.org/abs/2307.10479v1",
        "pub_date": "2023-07-19",
        "summary": "For approximate nearest neighbor search, graph-based algorithms have shown to\noffer the best trade-off between accuracy and search time. We propose the\nDynamic Exploration Graph (DEG) which significantly outperforms existing\nalgorithms in terms of search and exploration efficiency by combining two new\nideas: First, a single undirected even regular graph is incrementally built by\npartially replacing existing edges to integrate new vertices and to update old\nneighborhoods at the same time. Secondly, an edge optimization algorithm is\nused to continuously improve the quality of the graph. Combining this ongoing\nrefinement with the graph construction process leads to a well-organized graph\nstructure at all times, resulting in: (1) increased search efficiency, (2)\npredictable index size, (3) guaranteed connectivity and therefore reachability\nof all vertices, and (4) a dynamic graph structure. In addition we investigate\nhow well existing graph-based search systems can handle indexed queries where\nthe seed vertex of a search is the query itself. Such exploration tasks,\ndespite their good starting point, are not necessarily easy. High efficiency in\napproximate nearest neighbor search (ANNS) does not automatically imply good\nperformance in exploratory search. Extensive experiments show that our new\nDynamic Exploration Graph outperforms existing algorithms significantly for\nindexed and unindexed queries.",
        "translated": ""
    },
    {
        "title": "Classification of Visualization Types and Perspectives in Patents",
        "url": "http://arxiv.org/abs/2307.10471v1",
        "pub_date": "2023-07-19",
        "summary": "Due to the swift growth of patent applications each year, information and\nmultimedia retrieval approaches that facilitate patent exploration and\nretrieval are of utmost importance. Different types of visualizations (e.g.,\ngraphs, technical drawings) and perspectives (e.g., side view, perspective) are\nused to visualize details of innovations in patents. The classification of\nthese images enables a more efficient search and allows for further analysis.\nSo far, datasets for image type classification miss some important\nvisualization types for patents. Furthermore, related work does not make use of\nrecent deep learning approaches including transformers. In this paper, we adopt\nstate-of-the-art deep learning methods for the classification of visualization\ntypes and perspectives in patent images. We extend the CLEF-IP dataset for\nimage type classification in patents to ten classes and provide manual ground\ntruth annotations. In addition, we derive a set of hierarchical classes from a\ndataset that provides weakly-labeled data for image perspectives. Experimental\nresults have demonstrated the feasibility of the proposed approaches. Source\ncode, models, and dataset will be made publicly available.",
        "translated": ""
    },
    {
        "title": "Alleviating the Long-Tail Problem in Conversational Recommender Systems",
        "url": "http://arxiv.org/abs/2307.11650v1",
        "pub_date": "2023-07-21",
        "summary": "Conversational recommender systems (CRS) aim to provide the recommendation\nservice via natural language conversations. To develop an effective CRS,\nhigh-quality CRS datasets are very crucial. However, existing CRS datasets\nsuffer from the long-tail issue, \\ie a large proportion of items are rarely (or\neven never) mentioned in the conversations, which are called long-tail items.\nAs a result, the CRSs trained on these datasets tend to recommend frequent\nitems, and the diversity of the recommended items would be largely reduced,\nmaking users easier to get bored.\n  To address this issue, this paper presents \\textbf{LOT-CRS}, a novel\nframework that focuses on simulating and utilizing a balanced CRS dataset (\\ie\ncovering all the items evenly) for improving \\textbf{LO}ng-\\textbf{T}ail\nrecommendation performance of CRSs. In our approach, we design two pre-training\ntasks to enhance the understanding of simulated conversation for long-tail\nitems, and adopt retrieval-augmented fine-tuning with label smoothness strategy\nto further improve the recommendation of long-tail items. Extensive experiments\non two public CRS datasets have demonstrated the effectiveness and\nextensibility of our approach, especially on long-tail recommendation.",
        "translated": ""
    },
    {
        "title": "Identifying document similarity using a fast estimation of the\n  Levenshtein Distance based on compression and signatures",
        "url": "http://arxiv.org/abs/2307.11496v1",
        "pub_date": "2023-07-21",
        "summary": "Identifying document similarity has many applications, e.g., source code\nanalysis or plagiarism detection. However, identifying similarities is not\ntrivial and can be time complex. For instance, the Levenshtein Distance is a\ncommon metric to define the similarity between two documents but has quadratic\nruntime which makes it impractical for large documents where large starts with\na few hundred kilobytes. In this paper, we present a novel concept that allows\nestimating the Levenshtein Distance: the algorithm first compresses documents\nto signatures (similar to hash values) using a user-defined compression ratio.\nSignatures can then be compared against each other (some constrains apply)\nwhere the outcome is the estimated Levenshtein Distance. Our evaluation shows\npromising results in terms of runtime efficiency and accuracy. In addition, we\nintroduce a significance score allowing examiners to set a threshold and\nidentify related documents.",
        "translated": ""
    },
    {
        "title": "Analysis of Elephant Movement in Sub-Saharan Africa: Ecological,\n  Climatic, and Conservation Perspectives",
        "url": "http://arxiv.org/abs/2307.11325v1",
        "pub_date": "2023-07-21",
        "summary": "The interaction between elephants and their environment has profound\nimplications for both ecology and conservation strategies. This study presents\nan analytical approach to decipher the intricate patterns of elephant movement\nin Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal\nvariations and rainfall patterns. Despite the complexities surrounding these\ninfluential factors, our analysis provides a holistic view of elephant\nmigratory behavior in the context of the dynamic African landscape. Our\ncomprehensive approach enables us to predict the potential impact of these\necological determinants on elephant migration, a critical step in establishing\ninformed conservation strategies. This projection is particularly crucial given\nthe impacts of global climate change on seasonal and rainfall patterns, which\ncould substantially influence elephant movements in the future. The findings of\nour work aim to not only advance the understanding of movement ecology but also\nfoster a sustainable coexistence of humans and elephants in Sub-Saharan Africa.\nBy predicting potential elephant routes, our work can inform strategies to\nminimize human-elephant conflict, effectively manage land use, and enhance\nanti-poaching efforts. This research underscores the importance of integrating\nmovement ecology and climatic variables for effective wildlife management and\nconservation planning.",
        "translated": ""
    },
    {
        "title": "Jina Embeddings: A Novel Set of High-Performance Sentence Embedding\n  Models",
        "url": "http://arxiv.org/abs/2307.11224v1",
        "pub_date": "2023-07-20",
        "summary": "Jina Embeddings constitutes a set of high-performance sentence embedding\nmodels adept at translating various textual inputs into numerical\nrepresentations, thereby capturing the semantic essence of the text. While\nthese models are not exclusively designed for text generation, they excel in\napplications such as dense retrieval and semantic textual similarity. This\npaper details the development of Jina Embeddings, starting with the creation of\na high-quality pairwise and triplet dataset. It underlines the crucial role of\ndata cleaning in dataset preparation, gives in-depth insights into the model\ntraining process, and concludes with a comprehensive performance evaluation\nusing the Massive Textual Embedding Benchmark (MTEB).",
        "translated": ""
    },
    {
        "title": "RCVaR: an Economic Approach to Estimate Cyberattacks Costs using Data\n  from Industry Reports",
        "url": "http://arxiv.org/abs/2307.11140v1",
        "pub_date": "2023-07-20",
        "summary": "Digitization increases business opportunities and the risk of companies being\nvictims of devastating cyberattacks. Therefore, managing risk exposure and\ncybersecurity strategies is essential for digitized companies that want to\nsurvive in competitive markets. However, understanding company-specific risks\nand quantifying their associated costs is not trivial. Current approaches fail\nto provide individualized and quantitative monetary estimations of\ncybersecurity impacts. Due to limited resources and technical expertise, SMEs\nand even large companies are affected and struggle to quantify their\ncyberattack exposure. Therefore, novel approaches must be placed to support the\nunderstanding of the financial loss due to cyberattacks. This article\nintroduces the Real Cyber Value at Risk (RCVaR), an economical approach for\nestimating cybersecurity costs using real-world information from public\ncybersecurity reports. RCVaR identifies the most significant cyber risk factors\nfrom various sources and combines their quantitative results to estimate\nspecific cyberattacks costs for companies. Furthermore, RCVaR extends current\nmethods to achieve cost and risk estimations based on historical real-world\ndata instead of only probability-based simulations. The evaluation of the\napproach on unseen data shows the accuracy and efficiency of the RCVaR in\npredicting and managing cyber risks. Thus, it shows that the RCVaR is a\nvaluable addition to cybersecurity planning and risk management processes.",
        "translated": ""
    },
    {
        "title": "HeteFedRec: Federated Recommender Systems with Model Heterogeneity",
        "url": "http://arxiv.org/abs/2307.12810v1",
        "pub_date": "2023-07-24",
        "summary": "Owing to the nature of privacy protection, federated recommender systems\n(FedRecs) have garnered increasing interest in the realm of on-device\nrecommender systems. However, most existing FedRecs only allow participating\nclients to collaboratively train a recommendation model of the same public\nparameter size. Training a model of the same size for all clients can lead to\nsuboptimal performance since clients possess varying resources. For example,\nclients with limited training data may prefer to train a smaller recommendation\nmodel to avoid excessive data consumption, while clients with sufficient data\nwould benefit from a larger model to achieve higher recommendation accuracy. To\naddress the above challenge, this paper introduces HeteFedRec, a novel FedRec\nframework that enables the assignment of personalized model sizes to\nparticipants. In HeteFedRec, we present a heterogeneous recommendation model\naggregation strategy, including a unified dual-task learning mechanism and a\ndimensional decorrelation regularization, to allow knowledge aggregation among\nrecommender models of different sizes. Additionally, a relation-based ensemble\nknowledge distillation method is proposed to effectively distil knowledge from\nheterogeneous item embeddings. Extensive experiments conducted on three\nreal-world recommendation datasets demonstrate the effectiveness and efficiency\nof HeteFedRec in training federated recommender systems under heterogeneous\nsettings.",
        "translated": ""
    },
    {
        "title": "RRAML: Reinforced Retrieval Augmented Machine Learning",
        "url": "http://arxiv.org/abs/2307.12798v1",
        "pub_date": "2023-07-24",
        "summary": "The emergence of large language models (LLMs) has revolutionized machine\nlearning and related fields, showcasing remarkable abilities in comprehending,\ngenerating, and manipulating human language. However, their conventional usage\nthrough API-based text prompt submissions imposes certain limitations in terms\nof context constraints and external source availability. To address these\nchallenges, we propose a novel framework called Reinforced Retrieval Augmented\nMachine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs\nwith supporting information retrieved by a purpose-built retriever from a vast\nuser-provided database. By leveraging recent advancements in reinforcement\nlearning, our method effectively addresses several critical challenges.\nFirstly, it circumvents the need for accessing LLM gradients. Secondly, our\nmethod alleviates the burden of retraining LLMs for specific tasks, as it is\noften impractical or impossible due to restricted access to the model and the\ncomputational intensity involved. Additionally we seamlessly link the\nretriever's task with the reasoner, mitigating hallucinations and reducing\nirrelevant, and potentially damaging retrieved documents. We believe that the\nresearch agenda outlined in this paper has the potential to profoundly impact\nthe field of AI, democratizing access to and utilization of LLMs for a wide\nrange of entities.",
        "translated": ""
    },
    {
        "title": "Unbiased Delayed Feedback Label Correction for Conversion Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2307.12756v1",
        "pub_date": "2023-07-24",
        "summary": "Conversion rate prediction is critical to many online applications such as\ndigital display advertising. To capture dynamic data distribution, industrial\nsystems often require retraining models on recent data daily or weekly.\nHowever, the delay of conversion behavior usually leads to incorrect labeling,\nwhich is called delayed feedback problem. Existing work may fail to introduce\nthe correct information about false negative samples due to data sparsity and\ndynamic data distribution. To directly introduce the correct feedback label\ninformation, we propose an Unbiased delayed feedback Label Correction framework\n(ULC), which uses an auxiliary model to correct labels for observed negative\nfeedback samples. Firstly, we theoretically prove that the label-corrected loss\nis an unbiased estimate of the oracle loss using true labels. Then, as there\nare no ready training data for label correction, counterfactual labeling is\nused to construct artificial training data. Furthermore, since counterfactual\nlabeling utilizes only partial training data, we design an embedding-based\nalternative training method to enhance performance. Comparative experiments on\nboth public and private datasets and detailed analyses show that our proposed\napproach effectively alleviates the delayed feedback problem and consistently\noutperforms the previous state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Self-refining of Pseudo Labels for Music Source Separation with Noisy\n  Labeled Data",
        "url": "http://arxiv.org/abs/2307.12576v1",
        "pub_date": "2023-07-24",
        "summary": "Music source separation (MSS) faces challenges due to the limited\navailability of correctly-labeled individual instrument tracks. With the push\nto acquire larger datasets to improve MSS performance, the inevitability of\nencountering mislabeled individual instrument tracks becomes a significant\nchallenge to address. This paper introduces an automated technique for refining\nthe labels in a partially mislabeled dataset. Our proposed self-refining\ntechnique, employed with a noisy-labeled dataset, results in only a 1% accuracy\ndegradation in multi-label instrument recognition compared to a classifier\ntrained on a clean-labeled dataset. The study demonstrates the importance of\nrefining noisy-labeled data in MSS model training and shows that utilizing the\nrefined dataset leads to comparable results derived from a clean-labeled\ndataset. Notably, upon only access to a noisy dataset, MSS models trained on a\nself-refined dataset even outperform those trained on a dataset refined with a\nclassifier trained on clean labels.",
        "translated": ""
    },
    {
        "title": "FaFCNN: A General Disease Classification Framework Based on Feature\n  Fusion Neural Networks",
        "url": "http://arxiv.org/abs/2307.12518v1",
        "pub_date": "2023-07-24",
        "summary": "There are two fundamental problems in applying deep learning/machine learning\nmethods to disease classification tasks, one is the insufficient number and\npoor quality of training samples; another one is how to effectively fuse\nmultiple source features and thus train robust classification models. To\naddress these problems, inspired by the process of human learning knowledge, we\npropose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which\nintroduces a feature-aware interaction module and a feature alignment module\nbased on domain adversarial learning. This is a general framework for disease\nclassification, and FaFCNN improves the way existing methods obtain sample\ncorrelation features. The experimental results show that training using\naugmented features obtained by pre-training gradient boosting decision tree\nyields more performance gains than random-forest based methods. On the\nlow-quality dataset with a large amount of missing data in our setup, FaFCNN\nobtains a consistently optimal performance compared to competitive baselines.\nIn addition, extensive experiments demonstrate the robustness of the proposed\nmethod and the effectiveness of each component of the model\\footnote{Accepted\nin IEEE SMC2023}.",
        "translated": ""
    },
    {
        "title": "Interface Design to Mitigate Inflation in Recommender Systems",
        "url": "http://arxiv.org/abs/2307.12424v1",
        "pub_date": "2023-07-23",
        "summary": "Recommendation systems rely on user-provided data to learn about item quality\nand provide personalized recommendations. An implicit assumption when\naggregating ratings into item quality is that ratings are strong indicators of\nitem quality. In this work, we test this assumption using data collected from a\nmusic discovery application. Our study focuses on two factors that cause rating\ninflation: heterogeneous user rating behavior and the dynamics of personalized\nrecommendations. We show that user rating behavior substantially varies by\nuser, leading to item quality estimates that reflect the users who rated an\nitem more than the item quality itself. Additionally, items that are more\nlikely to be shown via personalized recommendations can experience a\nsubstantial increase in their exposure and potential bias toward them. To\nmitigate these effects, we analyze the results of a randomized controlled trial\nin which the rating interface was modified. The test resulted in a substantial\nimprovement in user rating behavior and a reduction in item quality inflation.\nThese findings highlight the importance of carefully considering the\nassumptions underlying recommendation systems and designing interfaces that\nencourage accurate rating behavior.",
        "translated": ""
    },
    {
        "title": "RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC",
        "url": "http://arxiv.org/abs/2307.12301v1",
        "pub_date": "2023-07-23",
        "summary": "Image outlier detection (OD) is crucial for ensuring the quality and accuracy\nof image datasets used in computer vision tasks. The majority of OD algorithms,\nhowever, have not been targeted toward image data. Consequently, the results of\napplying such algorithms to images are often suboptimal. In this work, we\npropose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for\nimages. By comparing images in a RANSAC-based approach, our algorithm\nautomatically predicts the outlier score of each image without additional\ntraining or label information. We evaluate RANSAC-NN against state-of-the-art\nOD algorithms on 15 diverse datasets. Without any hyperparameter tuning,\nRANSAC-NN consistently performs favorably in contrast to other algorithms in\nalmost every dataset category. Furthermore, we provide a detailed analysis to\nunderstand each RANSAC-NN component, and we demonstrate its potential\napplications in image mislabeled detection. Code for RANSAC-NN is provided at\nhttps://github.com/mxtsai/ransac-nn",
        "translated": ""
    },
    {
        "title": "Conformal Group Recommender System",
        "url": "http://arxiv.org/abs/2307.12034v1",
        "pub_date": "2023-07-22",
        "summary": "Group recommender systems (GRS) are critical in discovering relevant items\nfrom a near-infinite inventory based on group preferences rather than\nindividual preferences, like recommending a movie, restaurant, or tourist\ndestination to a group of individuals. The traditional models of group\nrecommendation are designed to act like a black box with a strict focus on\nimproving recommendation accuracy, and most often, they place the onus on the\nusers to interpret recommendations. In recent years, the focus of Recommender\nSystems (RS) research has shifted away from merely improving recommendation\naccuracy towards value additions such as confidence and explanation. In this\nwork, we propose a conformal prediction framework that provides a measure of\nconfidence with prediction in conjunction with a group recommender system to\naugment the system-generated plain recommendations. In the context of group\nrecommender systems, we propose various nonconformity measures that play a\nvital role in the efficiency of the conformal framework. We also show that\ndefined nonconformity satisfies the exchangeability property. Experimental\nresults demonstrate the effectiveness of the proposed approach over several\nbenchmark datasets. Furthermore, our proposed approach also satisfies validity\nand efficiency properties.",
        "translated": ""
    },
    {
        "title": "XWalk: Random Walk Based Candidate Retrieval for Product Search",
        "url": "http://arxiv.org/abs/2307.12019v1",
        "pub_date": "2023-07-22",
        "summary": "In e-commerce, head queries account for the vast majority of gross\nmerchandise sales and improvements to head queries are highly impactful to the\nbusiness. While most supervised approaches to search perform better in head\nqueries vs. tail queries, we propose a method that further improves head query\nperformance dramatically. We propose XWalk, a random-walk based graph approach\nto candidate retrieval for product search that borrows from recommendation\nsystem techniques. XWalk is highly efficient to train and inference in a\nlarge-scale high traffic e-commerce setting, and shows substantial improvements\nin head query performance over state-of-the-art neural retreivers. Ensembling\nXWalk with a neural and/or lexical retriever combines the best of both worlds\nand the resulting retrieval system outperforms all other methods in both\noffline relevance-based evaluation and in online A/B tests.",
        "translated": ""
    },
    {
        "title": "HTP: Exploiting Holistic Temporal Patterns for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2307.11994v1",
        "pub_date": "2023-07-22",
        "summary": "Sequential recommender systems have demonstrated a huge success for next-item\nrecommendation by explicitly exploiting the temporal order of users' historical\ninteractions. In practice, user interactions contain more useful temporal\ninformation beyond order, as shown by some pioneering studies. In this paper,\nwe systematically investigate various temporal information for sequential\nrecommendation and identify three types of advantageous temporal patterns\nbeyond order, including absolute time information, relative item time intervals\nand relative recommendation time intervals. We are the first to explore\nitem-oriented absolute time patterns. While existing models consider only one\nor two of these three patterns, we propose a novel holistic temporal pattern\nbased neural network, named HTP, to fully leverage all these three patterns. In\nparticular, we introduce novel components to address the subtle correlations\nbetween relative item time intervals and relative recommendation time\nintervals, which render a major technical challenge. Extensive experiments on\nthree real-world benchmark datasets show that our HTP model consistently and\nsubstantially outperforms many state-of-the-art models. Our code is publically\navailable at https://github.com/623851394/HTP/tree/main/HTP-main",
        "translated": ""
    },
    {
        "title": "Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning",
        "url": "http://arxiv.org/abs/2307.13632v1",
        "pub_date": "2023-07-25",
        "summary": "Mainstream bias, where some users receive poor recommendations because their\npreferences are uncommon or simply because they are less active, is an\nimportant aspect to consider regarding fairness in recommender systems.\nExisting methods to mitigate mainstream bias do not explicitly model the\nimportance of these non-mainstream users or, when they do, it is in a way that\nis not necessarily compatible with the data and recommendation model at hand.\nIn contrast, we use the recommendation utility as a more generic and implicit\nproxy to quantify mainstreamness, and propose a simple user-weighting approach\nto incorporate it into the training process while taking the cost of potential\nrecommendation errors into account. We provide extensive experimental results\nshowing that quantifying mainstreamness via utility is better able at\nidentifying non-mainstream users, and that they are indeed better served when\ntraining the model in a cost-sensitive way. This is achieved with negligible or\nno loss in overall recommendation accuracy, meaning that the models learn a\nbetter balance across users. In addition, we show that research of this kind,\nwhich evaluates recommendation quality at the individual user level, may not be\nreliable if not using enough interactions when assessing model performance.",
        "translated": ""
    },
    {
        "title": "Gaussian Graph with Prototypical Contrastive Learning in E-Commerce\n  Bundle Recommendation",
        "url": "http://arxiv.org/abs/2307.13468v1",
        "pub_date": "2023-07-25",
        "summary": "Bundle recommendation aims to provide a bundle of items to satisfy the user\npreference on e-commerce platform. Existing successful solutions are based on\nthe contrastive graph learning paradigm where graph neural networks (GNNs) are\nemployed to learn representations from user-level and bundle-level graph views\nwith a contrastive learning module to enhance the cooperative association\nbetween different views. Nevertheless, they ignore the uncertainty issue which\nhas a significant impact in real bundle recommendation scenarios due to the\nlack of discriminative information caused by highly sparsity or diversity. We\nfurther suggest that their instancewise contrastive learning fails to\ndistinguish the semantically similar negatives (i.e., sampling bias issue),\nresulting in performance degradation. In this paper, we propose a novel\nGaussian Graph with Prototypical Contrastive Learning (GPCL) framework to\novercome these challenges. In particular, GPCL embeds each user/bundle/item as\na Gaussian distribution rather than a fixed vector. We further design a\nprototypical contrastive learning module to capture the contextual information\nand mitigate the sampling bias issue. Extensive experiments demonstrate that\nbenefiting from the proposed components, we achieve new state-of-the-art\nperformance compared to previous methods on several public datasets. Moreover,\nGPCL has been deployed on real-world e-commerce platform and achieved\nsubstantial improvements.",
        "translated": ""
    },
    {
        "title": "Comprehensive Review on Semantic Information Retrieval and Ontology\n  Engineering",
        "url": "http://arxiv.org/abs/2307.13427v1",
        "pub_date": "2023-07-25",
        "summary": "Situation awareness is a crucial cognitive skill that enables individuals to\nperceive, comprehend, and project the current state of their environment\naccurately. It involves being conscious of relevant information, understanding\nits meaning, and using that understanding to make well-informed decisions.\nAwareness systems often need to integrate new knowledge and adapt to changing\nenvironments. Ontology reasoning facilitates knowledge integration and\nevolution, allowing for seamless updates and expansions of the ontology. With\nthe consideration of above, we are providing a quick review on semantic\ninformation retrieval and ontology engineering to understand the emerging\nchallenges and future research. In the review we have found that the ontology\nreasoning addresses the limitations of traditional systems by providing a\nformal, flexible, and scalable framework for knowledge representation,\nreasoning, and inference.",
        "translated": ""
    },
    {
        "title": "An End-to-End Workflow using Topic Segmentation and Text Summarisation\n  Methods for Improved Podcast Comprehension",
        "url": "http://arxiv.org/abs/2307.13394v1",
        "pub_date": "2023-07-25",
        "summary": "The consumption of podcast media has been increasing rapidly. Due to the\nlengthy nature of podcast episodes, users often carefully select which ones to\nlisten to. Although episode descriptions aid users by providing a summary of\nthe entire podcast, they do not provide a topic-by-topic breakdown. This study\nexplores the combined application of topic segmentation and text summarisation\nmethods to investigate how podcast episode comprehension can be improved. We\nhave sampled 10 episodes from Spotify's English-Language Podcast Dataset and\nemployed TextTiling and TextSplit to segment them. Moreover, three text\nsummarisation models, namely T5, BART, and Pegasus, were applied to provide a\nvery short title for each segment. The segmentation part was evaluated using\nour annotated sample with the $P_k$ and WindowDiff ($WD$) metrics. A survey was\nalso rolled out ($N=25$) to assess the quality of the generated summaries. The\nTextSplit algorithm achieved the lowest mean for both evaluation metrics\n($\\bar{P_k}=0.41$ and $\\bar{WD}=0.41$), while the T5 model produced the best\nsummaries, achieving a relevancy score only $8\\%$ less to the one achieved by\nthe human-written titles.",
        "translated": ""
    },
    {
        "title": "Embedding Models for Supervised Automatic Extraction and Classification\n  of Named Entities in Scientific Acknowledgements",
        "url": "http://arxiv.org/abs/2307.13377v1",
        "pub_date": "2023-07-25",
        "summary": "Acknowledgments in scientific papers may give an insight into aspects of the\nscientific community, such as reward systems, collaboration patterns, and\nhidden research trends. The aim of the paper is to evaluate the performance of\ndifferent embedding models for the task of automatic extraction and\nclassification of acknowledged entities from the acknowledgment text in\nscientific papers. We trained and implemented a named entity recognition (NER)\ntask using the Flair NLP framework. The training was conducted using three\ndefault Flair NER models with four differently-sized corpora and different\nversions of the Flair NLP framework. The Flair Embeddings model trained on the\nmedium corpus with the latest FLAIR version showed the best accuracy of 0.79.\nExpanding the size of a training corpus from very small to medium size\nmassively increased the accuracy of all training algorithms, but further\nexpansion of the training corpus did not bring further improvement. Moreover,\nthe performance of the model slightly deteriorated. Our model is able to\nrecognize six entity types: funding agency, grant number, individuals,\nuniversity, corporation, and miscellaneous. The model works more precisely for\nsome entity types than for others; thus, individuals and grant numbers showed a\nvery good F1-Score over 0.9. Most of the previous works on acknowledgment\nanalysis were limited by the manual evaluation of data and therefore by the\namount of processed data. This model can be applied for the comprehensive\nanalysis of acknowledgment texts and may potentially make a great contribution\nto the field of automated acknowledgment analysis.",
        "translated": ""
    },
    {
        "title": "An Intent Taxonomy of Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2307.13298v1",
        "pub_date": "2023-07-25",
        "summary": "Legal case retrieval is a special Information Retrieval~(IR) task focusing on\nlegal case documents. Depending on the downstream tasks of the retrieved case\ndocuments, users' information needs in legal case retrieval could be\nsignificantly different from those in Web search and traditional ad-hoc\nretrieval tasks. While there are several studies that retrieve legal cases\nbased on text similarity, the underlying search intents of legal retrieval\nusers, as shown in this paper, are more complicated than that yet mostly\nunexplored. To this end, we present a novel hierarchical intent taxonomy of\nlegal case retrieval. It consists of five intent types categorized by three\ncriteria, i.e., search for Particular Case(s), Characterization, Penalty,\nProcedure, and Interest. The taxonomy was constructed transparently and\nevaluated extensively through interviews, editorial user studies, and query log\nanalysis. Through a laboratory user study, we reveal significant differences in\nuser behavior and satisfaction under different search intents in legal case\nretrieval. Furthermore, we apply the proposed taxonomy to various downstream\nlegal retrieval tasks, e.g., result ranking and satisfaction prediction, and\ndemonstrate its effectiveness. Our work provides important insights into the\nunderstanding of user intents in legal case retrieval and potentially leads to\nbetter retrieval techniques in the legal domain, such as intent-aware ranking\nstrategies and evaluation methodologies.",
        "translated": ""
    },
    {
        "title": "Investigating the Robustness of Sequential Recommender Systems Against\n  Training Data Perturbations: an Empirical Study",
        "url": "http://arxiv.org/abs/2307.13165v1",
        "pub_date": "2023-07-24",
        "summary": "Sequential Recommender Systems (SRSs) have been widely used to model user\nbehavior over time, but their robustness in the face of perturbations to\ntraining data is a critical issue. In this paper, we conduct an empirical study\nto investigate the effects of removing items at different positions within a\ntemporally ordered sequence. We evaluate two different SRS models on multiple\ndatasets, measuring their performance using Normalized Discounted Cumulative\nGain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that\nremoving items at the end of the sequence significantly impacts performance,\nwith NDCG decreasing up to 60\\%, while removing items from the beginning or\nmiddle has no significant effect. These findings highlight the importance of\nconsidering the position of the perturbed items in the training data and shall\ninform the design of more robust SRSs.",
        "translated": ""
    },
    {
        "title": "ChatGPT and Persuasive Technologies for the Management and Delivery of\n  Personalized Recommendations in Hotel Hospitality",
        "url": "http://arxiv.org/abs/2307.14298v1",
        "pub_date": "2023-07-26",
        "summary": "Recommender systems have become indispensable tools in the hotel hospitality\nindustry, enabling personalized and tailored experiences for guests. Recent\nadvancements in large language models (LLMs), such as ChatGPT, and persuasive\ntechnologies, have opened new avenues for enhancing the effectiveness of those\nsystems. This paper explores the potential of integrating ChatGPT and\npersuasive technologies for automating and improving hotel hospitality\nrecommender systems. First, we delve into the capabilities of ChatGPT, which\ncan understand and generate human-like text, enabling more accurate and\ncontext-aware recommendations. We discuss the integration of ChatGPT into\nrecommender systems, highlighting the ability to analyze user preferences,\nextract valuable insights from online reviews, and generate personalized\nrecommendations based on guest profiles. Second, we investigate the role of\npersuasive technology in influencing user behavior and enhancing the persuasive\nimpact of hotel recommendations. By incorporating persuasive techniques, such\nas social proof, scarcity and personalization, recommender systems can\neffectively influence user decision-making and encourage desired actions, such\nas booking a specific hotel or upgrading their room. To investigate the\nefficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment\nwith a case study involving a hotel recommender system. We aim to study the\nimpact of integrating ChatGPT and persua-sive techniques on user engagement,\nsatisfaction, and conversion rates. The preliminary results demonstrate the\npotential of these technologies in enhancing the overall guest experience and\nbusiness performance. Overall, this paper contributes to the field of hotel\nhospitality by exploring the synergistic relationship between LLMs and\npersuasive technology in recommender systems, ultimately influencing guest\nsatisfaction and hotel revenue.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Competitive Near Cold-start Recommenders for\n  Language- and Item-based Preferences",
        "url": "http://arxiv.org/abs/2307.14225v1",
        "pub_date": "2023-07-26",
        "summary": "Traditional recommender systems leverage users' item preference history to\nrecommend novel content that users may like. However, modern dialog interfaces\nthat allow users to express language-based preferences offer a fundamentally\ndifferent modality for preference input. Inspired by recent successes of\nprompting paradigms for large language models (LLMs), we study their use for\nmaking recommendations from both item-based and language-based preferences in\ncomparison to state-of-the-art item-based collaborative filtering (CF) methods.\nTo support this investigation, we collect a new dataset consisting of both\nitem-based and language-based preferences elicited from users along with their\nratings on a variety of (biased) recommended items and (unbiased) random items.\nAmong numerous experimental results, we find that LLMs provide competitive\nrecommendation performance for pure language-based preferences (no item\npreferences) in the near cold-start case in comparison to item-based CF\nmethods, despite having no supervised training for this specific task\n(zero-shot) or only a few labels (few-shot). This is particularly promising as\nlanguage-based preference representations are more explainable and scrutable\nthan item-based or vector-based representations.",
        "translated": ""
    },
    {
        "title": "A Probabilistic Position Bias Model for Short-Video Recommendation Feeds",
        "url": "http://arxiv.org/abs/2307.14059v1",
        "pub_date": "2023-07-26",
        "summary": "Modern web-based platforms show ranked lists of recommendations to users,\nattempting to maximise user satisfaction or business metrics. Typically, the\ngoal of such systems boils down to maximising the exposure probability for\nitems that are deemed \"reward-maximising\" according to a metric of interest.\nThis general framing comprises streaming applications, as well as e-commerce or\njob recommendations, and even web search. Position bias or user models can be\nused to estimate exposure probabilities for each use-case, specifically\ntailored to how users interact with the presented rankings. A unifying factor\nin these diverse problem settings is that typically only one or several items\nwill be engaged with (clicked, streamed,...) before a user leaves the ranked\nlist. Short-video feeds on social media platforms diverge from this general\nframing in several ways, most notably that users do not tend to leave the feed\nafter e.g. liking a post. Indeed, seemingly infinite feeds invite users to\nscroll further down the ranked list. For this reason, existing position bias or\nuser models tend to fall short in such settings, as they do not accurately\ncapture users' interaction modalities.\n  In this work, we propose a novel and probabilistically sound personalised\nposition bias model for feed recommendations. We focus on a 1st-level feed in a\nhierarchical structure, where users may enter a 2nd-level feed via any given\n1st-level item. We posit that users come to the platform with a scrolling\nbudget drawn according to some distribution, and show how the survival function\nof said distribution can be used to obtain closed-form estimates for\npersonalised exposure probabilities. Empirical insights from a large-scale\nsocial media platform show how our probabilistic position bias model more\naccurately captures empirical exposure than existing models, and paves the way\nfor unbiased evaluation and learning-to-rank.",
        "translated": ""
    },
    {
        "title": "Multi-view Hypergraph Contrastive Policy Learning for Conversational\n  Recommendation",
        "url": "http://arxiv.org/abs/2307.14024v1",
        "pub_date": "2023-07-26",
        "summary": "Conversational recommendation systems (CRS) aim to interactively acquire user\npreferences and accordingly recommend items to users. Accurately learning the\ndynamic user preferences is of crucial importance for CRS. Previous works learn\nthe user preferences with pairwise relations from the interactive conversation\nand item knowledge, while largely ignoring the fact that factors for a\nrelationship in CRS are multiplex. Specifically, the user likes/dislikes the\nitems that satisfy some attributes (Like/Dislike view). Moreover social\ninfluence is another important factor that affects user preference towards the\nitem (Social view), while is largely ignored by previous works in CRS. The user\npreferences from these three views are inherently different but also correlated\nas a whole. The user preferences from the same views should be more similar\nthan that from different views. The user preferences from Like View should be\nsimilar to Social View while different from Dislike View. To this end, we\npropose a novel model, namely Multi-view Hypergraph Contrastive Policy Learning\n(MHCPL). Specifically, MHCPL timely chooses useful social information according\nto the interactive history and builds a dynamic hypergraph with three types of\nmultiplex relations from different views. The multiplex relations in each view\nare successively connected according to their generation order.",
        "translated": ""
    },
    {
        "title": "Domain Disentanglement with Interpolative Data Augmentation for\n  Dual-Target Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2307.13910v1",
        "pub_date": "2023-07-26",
        "summary": "The conventional single-target Cross-Domain Recommendation (CDR) aims to\nimprove the recommendation performance on a sparser target domain by\ntransferring the knowledge from a source domain that contains relatively richer\ninformation. By contrast, in recent years, dual-target CDR has been proposed to\nimprove the recommendation performance on both domains simultaneously. However,\nto this end, there are two challenges in dual-target CDR: (1) how to generate\nboth relevant and diverse augmented user representations, and (2) how to\neffectively decouple domain-independent information from domain-specific\ninformation, in addition to domain-shared information, to capture comprehensive\nuser preferences. To address the above two challenges, we propose a\nDisentanglement-based framework with Interpolative Data Augmentation for\ndual-target Cross-Domain Recommendation, called DIDA-CDR. In DIDA-CDR, we first\npropose an interpolative data augmentation approach to generating both relevant\nand diverse augmented user representations to augment sparser domain and\nexplore potential user preferences. We then propose a disentanglement module to\neffectively decouple domain-specific and domain-independent information to\ncapture comprehensive user preferences. Both steps significantly contribute to\ncapturing more comprehensive user preferences, thereby improving the\nrecommendation performance on each domain. Extensive experiments conducted on\nfive real-world datasets show the significant superiority of DIDA-CDR over the\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "ClusterSeq: Enhancing Sequential Recommender Systems with Clustering\n  based Meta-Learning",
        "url": "http://arxiv.org/abs/2307.13766v1",
        "pub_date": "2023-07-25",
        "summary": "In practical scenarios, the effectiveness of sequential recommendation\nsystems is hindered by the user cold-start problem, which arises due to limited\ninteractions for accurately determining user preferences. Previous studies have\nattempted to address this issue by combining meta-learning with user and\nitem-side information. However, these approaches face inherent challenges in\nmodeling user preference dynamics, particularly for \"minor users\" who exhibit\ndistinct preferences compared to more common or \"major users.\" To overcome\nthese limitations, we present a novel approach called ClusterSeq, a\nMeta-Learning Clustering-Based Sequential Recommender System. ClusterSeq\nleverages dynamic information in the user sequence to enhance item prediction\naccuracy, even in the absence of side information. This model preserves the\npreferences of minor users without being overshadowed by major users, and it\ncapitalizes on the collective knowledge of users within the same cluster.\nExtensive experiments conducted on various benchmark datasets validate the\neffectiveness of ClusterSeq. Empirical results consistently demonstrate that\nClusterSeq outperforms several state-of-the-art meta-learning recommenders.\nNotably, compared to existing meta-learning methods, our proposed approach\nachieves a substantial improvement of 16-39% in Mean Reciprocal Rank (MRR).",
        "translated": ""
    },
    {
        "title": "On (Normalised) Discounted Cumulative Gain as an Offline Evaluation\n  Metric for Top-$n$ Recommendation",
        "url": "http://arxiv.org/abs/2307.15053v1",
        "pub_date": "2023-07-27",
        "summary": "Approaches to recommendation are typically evaluated in one of two ways: (1)\nvia a (simulated) online experiment, often seen as the gold standard, or (2)\nvia some offline evaluation procedure, where the goal is to approximate the\noutcome of an online experiment. Several offline evaluation metrics have been\nadopted in the literature, inspired by ranking metrics prevalent in the field\nof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one\nsuch metric that has seen widespread adoption in empirical studies, and higher\n(n)DCG values have been used to present new methods as the state-of-the-art in\ntop-$n$ recommendation for many years.\n  Our work takes a critical look at this approach, and investigates when we can\nexpect such metrics to approximate the gold standard outcome of an online\nexperiment. We formally present the assumptions that are necessary to consider\nDCG an unbiased estimator of online reward and provide a derivation for this\nmetric from first principles, highlighting where we deviate from its\ntraditional uses in IR. Importantly, we show that normalising the metric\nrenders it inconsistent, in that even when DCG is unbiased, ranking competing\nmethods by their normalised DCG can invert their relative order. Through a\ncorrelation analysis between off- and on-line experiments conducted on a\nlarge-scale recommendation platform, we show that our unbiased DCG estimates\nstrongly correlate with online reward, even when some of the metric's inherent\nassumptions are violated. This statement no longer holds for its normalised\nvariant, suggesting that nDCG's practical utility may be limited.",
        "translated": ""
    },
    {
        "title": "The Effect of Third Party Implementations on Reproducibility",
        "url": "http://arxiv.org/abs/2307.14956v1",
        "pub_date": "2023-07-27",
        "summary": "Reproducibility of recommender systems research has come under scrutiny\nduring recent years. Along with works focusing on repeating experiments with\ncertain algorithms, the research community has also started discussing various\naspects of evaluation and how these affect reproducibility. We add a novel\nangle to this discussion by examining how unofficial third-party\nimplementations could benefit or hinder reproducibility. Besides giving a\ngeneral overview, we thoroughly examine six third-party implementations of a\npopular recommender algorithm and compare them to the official version on five\npublic datasets. In the light of our alarming findings we aim to draw the\nattention of the research community to this neglected aspect of\nreproducibility.",
        "translated": ""
    },
    {
        "title": "Widespread Flaws in Offline Evaluation of Recommender Systems",
        "url": "http://arxiv.org/abs/2307.14951v1",
        "pub_date": "2023-07-27",
        "summary": "Even though offline evaluation is just an imperfect proxy of online\nperformance -- due to the interactive nature of recommenders -- it will\nprobably remain the primary way of evaluation in recommender systems research\nfor the foreseeable future, since the proprietary nature of production\nrecommenders prevents independent validation of A/B test setups and\nverification of online results. Therefore, it is imperative that offline\nevaluation setups are as realistic and as flawless as they can be.\nUnfortunately, evaluation flaws are quite common in recommender systems\nresearch nowadays, due to later works copying flawed evaluation setups from\ntheir predecessors without questioning their validity. In the hope of improving\nthe quality of offline evaluation of recommender systems, we discuss four of\nthese widespread flaws and why researchers should avoid them.",
        "translated": ""
    },
    {
        "title": "Scaling Session-Based Transformer Recommendations using Optimized\n  Negative Sampling and Loss Functions",
        "url": "http://arxiv.org/abs/2307.14906v1",
        "pub_date": "2023-07-27",
        "summary": "This work introduces TRON, a scalable session-based Transformer Recommender\nusing Optimized Negative-sampling. Motivated by the scalability and performance\nlimitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates\ntop-k negative sampling and listwise loss functions to enhance its\nrecommendation accuracy. Evaluations on relevant large-scale e-commerce\ndatasets show that TRON improves upon the recommendation quality of current\nmethods while maintaining training speeds similar to SASRec. A live A/B test\nyielded an 18.14% increase in click-through rate over SASRec, highlighting the\npotential of TRON in practical settings. For further research, we provide\naccess to our source code at https://github.com/otto-de/TRON and an anonymized\ndataset at https://github.com/otto-de/recsys-dataset.",
        "translated": ""
    },
    {
        "title": "Integrating Offline Reinforcement Learning with Transformers for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2307.14450v1",
        "pub_date": "2023-07-26",
        "summary": "We consider the problem of sequential recommendation, where the current\nrecommendation is made based on past interactions. This recommendation task\nrequires efficient processing of the sequential data and aims to provide\nrecommendations that maximize the long-term reward. To this end, we train a\nfarsighted recommender by using an offline RL algorithm with the policy network\nin our model architecture that has been initialized from a pre-trained\ntransformer model. The pre-trained model leverages the superb ability of the\ntransformer to process sequential information. Compared to prior works that\nrely on online interaction via simulation, we focus on implementing a fully\noffline RL framework that is able to converge in a fast and stable way. Through\nextensive experiments on public datasets, we show that our method is robust\nacross various recommendation regimes, including e-commerce and movie\nsuggestions. Compared to state-of-the-art supervised learning algorithms, our\nalgorithm yields recommendations of higher quality, demonstrating the clear\nadvantage of combining RL and transformers.",
        "translated": ""
    },
    {
        "title": "Measuring Americanization: A Global Quantitative Study of Interest in\n  American Topics on Wikipedia",
        "url": "http://arxiv.org/abs/2307.14401v1",
        "pub_date": "2023-07-26",
        "summary": "We conducted a global comparative analysis of the coverage of American topics\nin different language versions of Wikipedia, using over 90 million Wikidata\nitems and 40 million Wikipedia articles in 58 languages. Our study aimed to\ninvestigate whether Americanization is more or less dominant in different\nregions and cultures and to determine whether interest in American topics is\nuniversal.",
        "translated": ""
    },
    {
        "title": "Framework to Automatically Determine the Quality of Open Data Catalogs",
        "url": "http://arxiv.org/abs/2307.15464v1",
        "pub_date": "2023-07-28",
        "summary": "Data catalogs play a crucial role in modern data-driven organizations by\nfacilitating the discovery, understanding, and utilization of diverse data\nassets. However, ensuring their quality and reliability is complex, especially\nin open and large-scale data environments. This paper proposes a framework to\nautomatically determine the quality of open data catalogs, addressing the need\nfor efficient and reliable quality assessment mechanisms. Our framework can\nanalyze various core quality dimensions, such as accuracy, completeness,\nconsistency, scalability, and timeliness, offer several alternatives for the\nassessment of compatibility and similarity across such catalogs as well as the\nimplementation of a set of non-core quality dimensions such as provenance,\nreadability, and licensing. The goal is to empower data-driven organizations to\nmake informed decisions based on trustworthy and well-curated data assets. The\nsource code that illustrates our approach can be downloaded from\nhttps://www.github.com/jorge-martinez-gil/dataq/.",
        "translated": ""
    },
    {
        "title": "Toward Transparent Sequence Models with Model-Based Tree Markov Model",
        "url": "http://arxiv.org/abs/2307.15367v1",
        "pub_date": "2023-07-28",
        "summary": "In this study, we address the interpretability issue in complex, black-box\nMachine Learning models applied to sequence data. We introduce the Model-Based\ntree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model\naimed at detecting high mortality risk events and discovering hidden patterns\nassociated with the mortality risk in Intensive Care Units (ICU). This model\nleverages knowledge distilled from Deep Neural Networks (DNN) to enhance\npredictive performance while offering clear explanations. Our experimental\nresults indicate the improved performance of Model-Based trees (MOB trees) via\nemploying LSTM for learning sequential patterns, which are then transferred to\nMOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in\nthe MOB-HSMM enables uncovering potential and explainable sequences using\navailable information.",
        "translated": ""
    },
    {
        "title": "Staging E-Commerce Products for Online Advertising using Retrieval\n  Assisted Image Generation",
        "url": "http://arxiv.org/abs/2307.15326v1",
        "pub_date": "2023-07-28",
        "summary": "Online ads showing e-commerce products typically rely on the product images\nin a catalog sent to the advertising platform by an e-commerce platform. In the\nbroader ads industry such ads are called dynamic product ads (DPA). It is\ncommon for DPA catalogs to be in the scale of millions (corresponding to the\nscale of products which can be bought from the e-commerce platform). However,\nnot all product images in the catalog may be appealing when directly\nre-purposed as an ad image, and this may lead to lower click-through rates\n(CTRs). In particular, products just placed against a solid background may not\nbe as enticing and realistic as a product staged in a natural environment. To\naddress such shortcomings of DPA images at scale, we propose a generative\nadversarial network (GAN) based approach to generate staged backgrounds for\nun-staged product images. Generating the entire staged background is a\nchallenging task susceptible to hallucinations. To get around this, we\nintroduce a simpler approach called copy-paste staging using retrieval assisted\nGANs. In copy paste staging, we first retrieve (from the catalog) staged\nproducts similar to the un-staged input product, and then copy-paste the\nbackground of the retrieved product in the input image. A GAN based in-painting\nmodel is used to fill the holes left after this copy-paste operation. We show\nthe efficacy of our copy-paste staging method via offline metrics, and human\nevaluation. In addition, we show how our staging approach can enable animations\nof moving products leading to a video ad from a product image.",
        "translated": ""
    },
    {
        "title": "Reconciling the accuracy-diversity trade-off in recommendations",
        "url": "http://arxiv.org/abs/2307.15142v1",
        "pub_date": "2023-07-27",
        "summary": "In recommendation settings, there is an apparent trade-off between the goals\nof accuracy (to recommend items a user is most likely to want) and diversity\n(to recommend items representing a range of categories). As such, real-world\nrecommender systems often explicitly incorporate diversity separately from\naccuracy. This approach, however, leaves a basic question unanswered: Why is\nthere a trade-off in the first place?\n  We show how the trade-off can be explained via a user's consumption\nconstraints -- users typically only consume a few of the items they are\nrecommended. In a stylized model we introduce, objectives that account for this\nconstraint induce diverse recommendations, while objectives that do not account\nfor this constraint induce homogeneous recommendations. This suggests that\naccuracy and diversity appear misaligned because standard accuracy metrics do\nnot consider consumption constraints. Our model yields precise and\ninterpretable characterizations of diversity in different settings, giving\npractical insights into the design of diverse recommendations.",
        "translated": ""
    },
    {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative\n  Information-Seeking with Attribution",
        "url": "http://arxiv.org/abs/2307.16883v1",
        "pub_date": "2023-07-31",
        "summary": "The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.",
        "translated": ""
    },
    {
        "title": "Metric@CustomerN: Evaluating Metrics at a Customer Level in E-Commerce",
        "url": "http://arxiv.org/abs/2307.16832v1",
        "pub_date": "2023-07-31",
        "summary": "Accuracy measures such as Recall, Precision, and Hit Rate have been a\nstandard way of evaluating Recommendation Systems. The assumption is to use a\nfixed Top-N to represent them. We propose that median impressions viewed from\nhistorical sessions per diner be used as a personalized value for N. We present\npreliminary exploratory results and list future steps to improve upon and\nevaluate the efficacy of these personalized metrics.",
        "translated": ""
    },
    {
        "title": "Defense of Adversarial Ranking Attack in Text Retrieval: Benchmark and\n  Baseline via Detection",
        "url": "http://arxiv.org/abs/2307.16816v1",
        "pub_date": "2023-07-31",
        "summary": "Neural ranking models (NRMs) have undergone significant development and have\nbecome integral components of information retrieval (IR) systems.\nUnfortunately, recent research has unveiled the vulnerability of NRMs to\nadversarial document manipulations, potentially exploited by malicious search\nengine optimization practitioners. While progress in adversarial attack\nstrategies aids in identifying the potential weaknesses of NRMs before their\ndeployment, the defensive measures against such attacks, like the detection of\nadversarial documents, remain inadequately explored. To mitigate this gap, this\npaper establishes a benchmark dataset to facilitate the investigation of\nadversarial ranking defense and introduces two types of detection tasks for\nadversarial documents. A comprehensive investigation of the performance of\nseveral detection baselines is conducted, which involve examining the\nspamicity, perplexity, and linguistic acceptability, and utilizing supervised\nclassifiers. Experimental results demonstrate that a supervised classifier can\neffectively mitigate known attacks, but it performs poorly against unseen\nattacks. Furthermore, such classifier should avoid using query text to prevent\nlearning the classification on relevance, as it might lead to the inadvertent\ndiscarding of relevant documents.",
        "translated": ""
    },
    {
        "title": "Lexically-Accelerated Dense Retrieval",
        "url": "http://arxiv.org/abs/2307.16779v1",
        "pub_date": "2023-07-31",
        "summary": "Retrieval approaches that score documents based on learned dense vectors\n(i.e., dense retrieval) rather than lexical signals (i.e., conventional\nretrieval) are increasingly popular. Their ability to identify related\ndocuments that do not necessarily contain the same terms as those appearing in\nthe user's query (thereby improving recall) is one of their key advantages.\nHowever, to actually achieve these gains, dense retrieval approaches typically\nrequire an exhaustive search over the document collection, making them\nconsiderably more expensive at query-time than conventional lexical approaches.\nSeveral techniques aim to reduce this computational overhead by approximating\nthe results of a full dense retriever. Although these approaches reasonably\napproximate the top results, they suffer in terms of recall -- one of the key\nadvantages of dense retrieval. We introduce 'LADR' (Lexically-Accelerated Dense\nRetrieval), a simple-yet-effective approach that improves the efficiency of\nexisting dense retrieval models without compromising on retrieval\neffectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval\nexploration that uses a document proximity graph. We explore two variants of\nLADR: a proactive approach that expands the search space to the neighbors of\nall seed documents, and an adaptive approach that selectively searches the\ndocuments with the highest estimated relevance in an iterative fashion. Through\nextensive experiments across a variety of dense retrieval models, we find that\nLADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier\namong approximate k nearest neighbor techniques. Further, we find that when\ntuned to take around 8ms per query in retrieval latency on our hardware, LADR\nconsistently achieves both precision and recall that are on par with an\nexhaustive search on standard benchmarks.",
        "translated": ""
    },
    {
        "title": "AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of\n  Autism Spectrum Disorder",
        "url": "http://arxiv.org/abs/2307.16773v1",
        "pub_date": "2023-07-31",
        "summary": "To easily obtain the knowledge about autism spectrum disorder and help its\nearly screening and diagnosis, we create AsdKB, a Chinese knowledge base on\nautism spectrum disorder. The knowledge base is built on top of various\nsources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical\ndescriptions on mental and behavioural disorders, 2) the diagnostic knowledge\nfrom DSM-5 and different screening tools recommended by social organizations\nand medical institutes, and 3) the expert knowledge on professional physicians\nand hospitals from the Web. AsdKB contains both ontological and factual\nknowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The\npotential applications of AsdKB are question answering, auxiliary diagnosis,\nand expert recommendation, and we illustrate them with a prototype which can be\naccessed at http://asdkb.org.cn/.",
        "translated": ""
    },
    {
        "title": "NEON: Living Needs Prediction System in Meituan",
        "url": "http://arxiv.org/abs/2307.16644v1",
        "pub_date": "2023-07-31",
        "summary": "Living needs refer to the various needs in human's daily lives for survival\nand well-being, including food, housing, entertainment, etc. On life service\nplatforms that connect users to service providers, such as Meituan, the problem\nof living needs prediction is fundamental as it helps understand users and\nboost various downstream applications such as personalized recommendation.\nHowever, the problem has not been well explored and is faced with two critical\nchallenges. First, the needs are naturally connected to specific locations and\ntimes, suffering from complex impacts from the spatiotemporal context. Second,\nthere is a significant gap between users' actual living needs and their\nhistorical records on the platform. To address these two challenges, we design\na system of living NEeds predictiON named NEON, consisting of three phases:\nfeature mining, feature fusion, and multi-task prediction. In the feature\nmining phase, we carefully extract individual-level user features for\nspatiotemporal modeling, and aggregated-level behavioral features for enriching\ndata, which serve as the basis for addressing two challenges, respectively.\nFurther, in the feature fusion phase, we propose a neural network that\neffectively fuses two parts of features into the user representation. Moreover,\nwe design a multi-task prediction phase, where the auxiliary task of\nneeds-meeting way prediction can enhance the modeling of spatiotemporal\ncontext. Extensive offline evaluations verify that our NEON system can\neffectively predict users' living needs. Furthermore, we deploy NEON into\nMeituan's algorithm engine and evaluate how it enhances the three downstream\nprediction applications, via large-scale online A/B testing.",
        "translated": ""
    },
    {
        "title": "When Large Language Models Meet Personalization: Perspectives of\n  Challenges and Opportunities",
        "url": "http://arxiv.org/abs/2307.16376v1",
        "pub_date": "2023-07-31",
        "summary": "The advent of large language models marks a revolutionary breakthrough in\nartificial intelligence. With the unprecedented scale of training and model\nparameters, the capability of large language models has been dramatically\nimproved, leading to human-like performances in understanding, language\nsynthesizing, and common-sense reasoning, etc. Such a major leap-forward in\ngeneral AI capacity will change the pattern of how personalization is\nconducted. For one thing, it will reform the way of interaction between humans\nand personalization systems. Instead of being a passive medium of information\nfiltering, large language models present the foundation for active user\nengagement. On top of such a new foundation, user requests can be proactively\nexplored, and user's required information can be delivered in a natural and\nexplainable way. For another thing, it will also considerably expand the scope\nof personalization, making it grow from the sole function of collecting\npersonalized information to the compound function of providing personalized\nservices. By leveraging large language models as general-purpose interface, the\npersonalization systems may compile user requests into plans, calls the\nfunctions of external tools to execute the plans, and integrate the tools'\noutputs to complete the end-to-end personalization tasks. Today, large language\nmodels are still being developed, whereas the application in personalization is\nlargely unexplored. Therefore, we consider it to be the right time to review\nthe challenges in personalization and the opportunities to address them with\nLLMs. In particular, we dedicate this perspective paper to the discussion of\nthe following aspects: the development and challenges for the existing\npersonalization system, the newly emerged capabilities of large language\nmodels, and the potential ways of making use of large language models for\npersonalization.",
        "translated": ""
    },
    {
        "title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning",
        "url": "http://arxiv.org/abs/2307.16372v1",
        "pub_date": "2023-07-31",
        "summary": "Automatic music captioning, which generates natural language descriptions for\ngiven music tracks, holds significant potential for enhancing the understanding\nand organization of large volumes of musical data. Despite its importance,\nresearchers face challenges due to the costly and time-consuming collection\nprocess of existing music-language datasets, which are limited in size. To\naddress this data scarcity issue, we propose the use of large language models\n(LLMs) to artificially generate the description sentences from large-scale tag\ndatasets. This results in approximately 2.2M captions paired with 0.5M audio\nclips. We term it Large Language Model based Pseudo music caption dataset,\nshortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale\nmusic captioning dataset with various quantitative evaluation metrics used in\nthe field of natural language processing as well as human evaluation. In\naddition, we trained a transformer-based music captioning model with the\ndataset and evaluated it under zero-shot and transfer-learning settings. The\nresults demonstrate that our proposed approach outperforms the supervised\nbaseline model.",
        "translated": ""
    },
    {
        "title": "Workshop on Document Intelligence Understanding",
        "url": "http://arxiv.org/abs/2307.16369v1",
        "pub_date": "2023-07-31",
        "summary": "Document understanding and information extraction include different tasks to\nunderstand a document and extract valuable information automatically. Recently,\nthere has been a rising demand for developing document understanding among\ndifferent domains, including business, law, and medicine, to boost the\nefficiency of work that is associated with a large number of documents. This\nworkshop aims to bring together researchers and industry developers in the\nfield of document intelligence and understanding diverse document types to\nboost automatic document processing and understanding techniques. We also\nreleased a data challenge on the recently introduced document-level VQA\ndataset, PDFVQA. The PDFVQA challenge examines the structural and contextual\nunderstandings of proposed models on the natural full document level of\nmultiple consecutive document pages by including questions with a sequence of\nanswers extracted from multi-pages of the full document. This task helps to\nboost the document understanding step from the single-page level to the full\ndocument level understanding.",
        "translated": ""
    },
    {
        "title": "Time-Aware Item Weighting for the Next Basket Recommendations",
        "url": "http://arxiv.org/abs/2307.16297v1",
        "pub_date": "2023-07-30",
        "summary": "In this paper we study the next basket recommendation problem. Recent methods\nuse different approaches to achieve better performance. However, many of them\ndo not use information about the time of prediction and time intervals between\nbaskets. To fill this gap, we propose a novel method, Time-Aware Item-based\nWeighting (TAIW), which takes timestamps and intervals into account. We provide\nexperiments on three real-world datasets, and TAIW outperforms well-tuned\nstate-of-the-art baselines for next-basket recommendations. In addition, we\nshow the results of an ablation study and a case study of a few items.",
        "translated": ""
    },
    {
        "title": "TimePool: Visually Answer \"Which and When\" Questions On Univariate Time\n  Series",
        "url": "http://arxiv.org/abs/2308.00682v1",
        "pub_date": "2023-08-01",
        "summary": "When exploring time series datasets, analysts often pose \"which and when\"\nquestions. For example, with world life expectancy data over one hundred years,\nthey may inquire about the top 10 countries in life expectancy and the time\nperiod when they achieved this status, or which countries have had longer life\nexpectancy than Ireland and when. This paper proposes TimePool, a new\nvisualization prototype, to address this need for univariate time series\nanalysis. It allows users to construct interactive \"which and when\" queries and\nvisually explore the results for insights.",
        "translated": ""
    },
    {
        "title": "Explainable Graph Spectral Clustering of Text Documents",
        "url": "http://arxiv.org/abs/2308.00504v1",
        "pub_date": "2023-08-01",
        "summary": "Spectral clustering methods are known for their ability to represent clusters\nof diverse shapes, densities etc. However, results of such algorithms, when\napplied e.g. to text documents, are hard to explain to the user, especially due\nto embedding in the spectral space which has no obvious relation to document\ncontents. Therefore there is an urgent need to elaborate methods for explaining\nthe outcome of the clustering. This paper presents a contribution towards this\ngoal. We present a proposal of explanation of results of combinatorial\nLaplacian based graph spectral clustering. It is based on showing (approximate)\nequivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in\nthis paper) and term vector space embedding. Hence a bridge is constructed\nbetween the textual contents and the clustering results. We provide theoretical\nbackground for this approach. We performed experimental study showing that\n$K$-embedding approximates well Laplacian embedding under favourable block\nmatrix conditions and show that approximation is good enough under other\nconditions.",
        "translated": ""
    },
    {
        "title": "On the Effects of Regional Spelling Conventions in Retrieval Models",
        "url": "http://arxiv.org/abs/2308.00480v1",
        "pub_date": "2023-08-01",
        "summary": "One advantage of neural ranking models is that they are meant to generalise\nwell in situations of synonymity i.e. where two words have similar or identical\nmeanings. In this paper, we investigate and quantify how well various ranking\nmodels perform in a clear-cut case of synonymity: when words are simply\nexpressed in different surface forms due to regional differences in spelling\nconventions (e.g., color vs colour). We first explore the prevalence of\nAmerican and British English spelling conventions in datasets used for the\npre-training, training and evaluation of neural retrieval methods, and find\nthat American spelling conventions are far more prevalent. Despite these biases\nin the training data, we find that retrieval models often generalise well in\nthis case of synonymity. We explore the effect of document spelling\nnormalisation in retrieval and observe that all models are affected by\nnormalising the document's spelling. While they all experience a drop in\nperformance when normalised to a different spelling convention than that of the\nquery, we observe varied behaviour when the document is normalised to share the\nquery spelling convention: lexical models show improvements, dense retrievers\nremain unaffected, and re-rankers exhibit contradictory behaviour.",
        "translated": ""
    },
    {
        "title": "Generative Query Reformulation for Effective Adhoc Search",
        "url": "http://arxiv.org/abs/2308.00415v1",
        "pub_date": "2023-08-01",
        "summary": "Performing automatic reformulations of a user's query is a popular paradigm\nused in information retrieval (IR) for improving effectiveness -- as\nexemplified by the pseudo-relevance feedback approaches, which expand the query\nin order to alleviate the vocabulary mismatch problem. Recent advancements in\ngenerative language models have demonstrated their ability in generating\nresponses that are relevant to a given prompt. In light of this success, we\nseek to study the capacity of such models to perform query reformulation and\nhow they compare with long-standing query reformulation methods that use\npseudo-relevance feedback. In particular, we investigate two representative\nquery reformulation frameworks, GenQR and GenPRF. GenQR directly reformulates\nthe user's input query, while GenPRF provides additional context for the query\nby making use of pseudo-relevance feedback information. For each reformulation\nmethod, we leverage different techniques, including fine-tuning and direct\nprompting, to harness the knowledge of language models. The reformulated\nqueries produced by the generative models are demonstrated to markedly benefit\nthe effectiveness of a state-of-the-art retrieval pipeline on four TREC test\ncollections (varying from TREC 2004 Robust to the TREC 2019 Deep Learning).\nFurthermore, our results indicate that our studied generative models can\noutperform various statistical query expansion approaches while remaining\ncomparable to other existing complex neural query reformulation models, with\nthe added benefit of being simpler to implement.",
        "translated": ""
    },
    {
        "title": "Challenging the Myth of Graph Collaborative Filtering: a Reasoned and\n  Reproducibility-driven Analysis",
        "url": "http://arxiv.org/abs/2308.00404v1",
        "pub_date": "2023-08-01",
        "summary": "The success of graph neural network-based models (GNNs) has significantly\nadvanced recommender systems by effectively modeling users and items as a\nbipartite, undirected graph. However, many original graph-based works often\nadopt results from baseline papers without verifying their validity for the\nspecific configuration under analysis. Our work addresses this issue by\nfocusing on the replicability of results. We present a code that successfully\nreplicates results from six popular and recent graph recommendation models\n(NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark\ndatasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these\ngraph models with traditional collaborative filtering models that historically\nperformed well in offline evaluations. Furthermore, we extend our study to two\nnew datasets (Allrecipes and BookCrossing) that lack established setups in\nexisting literature. As the performance on these datasets differs from the\nprevious benchmarks, we analyze the impact of specific dataset characteristics\non recommendation accuracy. By investigating the information flow from users'\nneighborhoods, we aim to identify which models are influenced by intrinsic\nfeatures in the dataset structure. The code to reproduce our experiments is\navailable at: https://github.com/sisinflab/Graph-RSs-Reproducibility.",
        "translated": ""
    },
    {
        "title": "Masked and Swapped Sequence Modeling for Next Novel Basket\n  Recommendation in Grocery Shopping",
        "url": "http://arxiv.org/abs/2308.01308v1",
        "pub_date": "2023-08-02",
        "summary": "Next basket recommendation (NBR) is the task of predicting the next set of\nitems based on a sequence of already purchased baskets. It is a recommendation\ntask that has been widely studied, especially in the context of grocery\nshopping. In next basket recommendation (NBR), it is useful to distinguish\nbetween repeat items, i.e., items that a user has consumed before, and explore\nitems, i.e., items that a user has not consumed before. Most NBR work either\nignores this distinction or focuses on repeat items. We formulate the next\nnovel basket recommendation (NNBR) task, i.e., the task of recommending a\nbasket that only consists of novel items, which is valuable for both real-world\napplication and NBR evaluation. We evaluate how existing NBR methods perform on\nthe NNBR task and find that, so far, limited progress has been made w.r.t. the\nNNBR task. To address the NNBR task, we propose a simple bi-directional\ntransformer basket recommendation model (BTBR), which is focused on directly\nmodeling item-to-item correlations within and across baskets instead of\nlearning complex basket representations. To properly train BTBR, we propose and\ninvestigate several masking strategies and training objectives: (i) item-level\nrandom masking, (ii) item-level select masking, (iii) basket-level all masking,\n(iv) basket-level explore masking, and (v) joint masking. In addition, an\nitem-basket swapping strategy is proposed to enrich the item interactions\nwithin the same baskets. We conduct extensive experiments on three open\ndatasets with various characteristics. The results demonstrate the\neffectiveness of BTBR and our masking and swapping strategies for the NNBR\ntask. BTBR with a properly selected masking and swapping strategy can\nsubstantially improve NNBR performance.",
        "translated": ""
    },
    {
        "title": "A Survey on Popularity Bias in Recommender Systems",
        "url": "http://arxiv.org/abs/2308.01118v1",
        "pub_date": "2023-08-02",
        "summary": "Recommender systems help people find relevant content in a personalized way.\nOne main promise of such systems is that they are able to increase the\nvisibility of items in the long tail, i.e., the lesser-known items in a\ncatalogue. Existing research, however, suggests that in many situations today's\nrecommendation algorithms instead exhibit a popularity bias, meaning that they\noften focus on rather popular items in their recommendations. Such a bias may\nnot only lead to limited value of the recommendations for consumers and\nproviders in the short run, but it may also cause undesired reinforcement\neffects over time. In this paper, we discuss the potential reasons for\npopularity bias and we review existing approaches to detect, quantify and\nmitigate popularity bias in recommender systems. Our survey therefore includes\nboth an overview of the computational metrics used in the literature as well as\na review of the main technical approaches to reduce the bias. We furthermore\ncritically discuss today's literature, where we observe that the research is\nalmost entirely based on computational experiments and on certain assumptions\nregarding the practical effects of including long-tail items in the\nrecommendations.",
        "translated": ""
    },
    {
        "title": "Towards Better Query Classification with Multi-Expert Knowledge\n  Condensation in JD Ads Search",
        "url": "http://arxiv.org/abs/2308.01098v1",
        "pub_date": "2023-08-02",
        "summary": "Search query classification, as an effective way to understand user intents,\nis of great importance in real-world online ads systems. To ensure a lower\nlatency, a shallow model (e.g. FastText) is widely used for efficient online\ninference. However, the representation ability of the FastText model is\ninsufficient, resulting in poor classification performance, especially on some\nlow-frequency queries and tailed categories. Using a deeper and more complex\nmodel (e.g. BERT) is an effective solution, but it will cause a higher online\ninference latency and more expensive computing costs. Thus, how to juggle both\ninference efficiency and classification performance is obviously of great\npractical importance. To overcome this challenge, in this paper, we propose\nknowledge condensation (KC), a simple yet effective knowledge distillation\nframework to boost the classification performance of the online FastText model\nunder strict low latency constraints. Specifically, we propose to train an\noffline BERT model to retrieve more potentially relevant data. Benefiting from\nits powerful semantic representation, more relevant labels not exposed in the\nhistorical data will be added into the training set for better FastText model\ntraining. Moreover, a novel distribution-diverse multi-expert learning strategy\nis proposed to further improve the mining ability of relevant data. By training\nmultiple BERT models from different data distributions, it can respectively\nperform better at high, middle, and low-frequency search queries. The model\nensemble from multi-distribution makes its retrieval ability more powerful. We\nhave deployed two versions of this framework in JD search, and both offline\nexperiments and online A/B testing from multiple datasets have validated the\neffectiveness of the proposed approach.",
        "translated": ""
    },
    {
        "title": "Rethinking Similarity Search: Embracing Smarter Mechanisms over Smarter\n  Data",
        "url": "http://arxiv.org/abs/2308.00909v1",
        "pub_date": "2023-08-02",
        "summary": "In this vision paper, we propose a shift in perspective for improving the\neffectiveness of similarity search. Rather than focusing solely on enhancing\nthe data quality, particularly machine learning-generated embeddings, we\nadvocate for a more comprehensive approach that also enhances the underpinning\nsearch mechanisms. We highlight three novel avenues that call for a\nredefinition of the similarity search problem: exploiting implicit data\nstructures and distributions, engaging users in an iterative feedback loop, and\nmoving beyond a single query vector. These novel pathways have gained relevance\nin emerging applications such as large-scale language models, video clip\nretrieval, and data labeling. We discuss the corresponding research challenges\nposed by these new problem areas and share insights from our preliminary\ndiscoveries.",
        "translated": ""
    },
    {
        "title": "User-Controllable Recommendation via Counterfactual Retrospective and\n  Prospective Explanations",
        "url": "http://arxiv.org/abs/2308.00894v1",
        "pub_date": "2023-08-02",
        "summary": "Modern recommender systems utilize users' historical behaviors to generate\npersonalized recommendations. However, these systems often lack user\ncontrollability, leading to diminished user satisfaction and trust in the\nsystems. Acknowledging the recent advancements in explainable recommender\nsystems that enhance users' understanding of recommendation mechanisms, we\npropose leveraging these advancements to improve user controllability. In this\npaper, we present a user-controllable recommender system that seamlessly\nintegrates explainability and controllability within a unified framework. By\nproviding both retrospective and prospective explanations through\ncounterfactual reasoning, users can customize their control over the system by\ninteracting with these explanations.\n  Furthermore, we introduce and assess two attributes of controllability in\nrecommendation systems: the complexity of controllability and the accuracy of\ncontrollability. Experimental evaluations on MovieLens and Yelp datasets\nsubstantiate the effectiveness of our proposed framework. Additionally, our\nexperiments demonstrate that offering users control options can potentially\nenhance recommendation accuracy in the future. Source code and data are\navailable at \\url{https://github.com/chrisjtan/ucr}.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Contrastive BERT Fine-tuning for Fusion-based\n  Reviewed-Item Retrieval",
        "url": "http://arxiv.org/abs/2308.00762v1",
        "pub_date": "2023-08-01",
        "summary": "As natural language interfaces enable users to express increasingly complex\nnatural language queries, there is a parallel explosion of user review content\nthat can allow users to better find items such as restaurants, books, or movies\nthat match these expressive queries. While Neural Information Retrieval (IR)\nmethods have provided state-of-the-art results for matching queries to\ndocuments, they have not been extended to the task of Reviewed-Item Retrieval\n(RIR), where query-review scores must be aggregated (or fused) into item-level\nscores for ranking. In the absence of labeled RIR datasets, we extend Neural IR\nmethodology to RIR by leveraging self-supervised methods for contrastive\nlearning of BERT embeddings for both queries and reviews. Specifically,\ncontrastive learning requires a choice of positive and negative samples, where\nthe unique two-level structure of our item-review data combined with meta-data\naffords us a rich structure for the selection of these samples. For contrastive\nlearning in a Late Fusion scenario, we investigate the use of positive review\nsamples from the same item and/or with the same rating, selection of hard\npositive samples by choosing the least similar reviews from the same anchor\nitem, and selection of hard negative samples by choosing the most similar\nreviews from different items. We also explore anchor sub-sampling and\naugmenting with meta-data. For a more end-to-end Early Fusion approach, we\nintroduce contrastive item embedding learning to fuse reviews into single item\nembeddings. Experimental results show that Late Fusion contrastive learning for\nNeural RIR outperforms all other contrastive IR configurations, Neural IR, and\nsparse retrieval baselines, thus demonstrating the power of exploiting the\ntwo-level structure in Neural RIR approaches as well as the importance of\npreserving the nuance of individual review content via Late Fusion methods.",
        "translated": ""
    },
    {
        "title": "A Knowledge-Oriented Approach to Enhance Integration and Communicability\n  in the Polkadot Ecosystem",
        "url": "http://arxiv.org/abs/2308.00735v1",
        "pub_date": "2023-08-01",
        "summary": "The Polkadot ecosystem is a disruptive and highly complex multi-chain\narchitecture that poses challenges in terms of data analysis and\ncommunicability. Currently, there is a lack of standardized and holistic\napproaches to retrieve and analyze data across parachains and applications,\nmaking it difficult for general users and developers to access ecosystem data\nconsistently. This paper proposes a conceptual framework that includes a domain\nontology called POnto (a Polkadot Ontology) to address these challenges. POnto\nprovides a structured representation of the ecosystem's concepts and\nrelationships, enabling a formal understanding of the platform. The proposed\nknowledge-oriented approach enhances integration and communicability, enabling\na wider range of users to participate in the ecosystem and facilitating the\ndevelopment of AI-based applications. The paper presents a case study\nmethodology to validate the proposed framework, which includes expert feedback\nand insights from the Polkadot community. The POnto ontology and the roadmap\nfor a query engine based on a Controlled Natural Language using the ontology,\nprovide valuable contributions to the growth and adoption of the Polkadot\necosystem in heterogeneous socio-technical environments.",
        "translated": ""
    },
    {
        "title": "Adaptive Collaborative Filtering with Personalized Time Decay Functions\n  for Financial Product Recommendation",
        "url": "http://arxiv.org/abs/2308.01208v1",
        "pub_date": "2023-08-01",
        "summary": "Classical recommender systems often assume that historical data are\nstationary and fail to account for the dynamic nature of user preferences,\nlimiting their ability to provide reliable recommendations in time-sensitive\nsettings. This assumption is particularly problematic in finance, where\nfinancial products exhibit continuous changes in valuations, leading to\nfrequent shifts in client interests. These evolving interests, summarized in\nthe past client-product interactions, see their utility fade over time with a\ndegree that might differ from one client to another. To address this challenge,\nwe propose a time-dependent collaborative filtering algorithm that can\nadaptively discount distant client-product interactions using personalized\ndecay functions. Our approach is designed to handle the non-stationarity of\nfinancial data and produce reliable recommendations by modeling the dynamic\ncollaborative signals between clients and products. We evaluate our method\nusing a proprietary dataset from BNP Paribas and demonstrate significant\nimprovements over state-of-the-art benchmarks from relevant literature. Our\nfindings emphasize the importance of incorporating time explicitly in the model\nto enhance the accuracy of financial product recommendation.",
        "translated": ""
    },
    {
        "title": "MAP: A Model-agnostic Pretraining Framework for Click-through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2308.01737v1",
        "pub_date": "2023-08-03",
        "summary": "With the widespread application of personalized online services,\nclick-through rate (CTR) prediction has received more and more attention and\nresearch. The most prominent features of CTR prediction are its multi-field\ncategorical data format, and vast and daily-growing data volume. The large\ncapacity of neural models helps digest such massive amounts of data under the\nsupervised learning paradigm, yet they fail to utilize the substantial data to\nits full potential, since the 1-bit click signal is not sufficient to guide the\nmodel to learn capable representations of features and instances. The\nself-supervised learning paradigm provides a more promising pretrain-finetune\nsolution to better exploit the large amount of user click logs, and learn more\ngeneralized and effective representations. However, self-supervised learning\nfor CTR prediction is still an open question, since current works on this line\nare only preliminary and rudimentary. To this end, we propose a Model-agnostic\npretraining (MAP) framework that applies feature corruption and recovery on\nmulti-field categorical data, and more specifically, we derive two practical\nalgorithms: masked feature prediction (MFP) and replaced feature detection\n(RFD). MFP digs into feature interactions within each instance through masking\nand predicting a small portion of input features, and introduces noise\ncontrastive estimation (NCE) to handle large feature spaces. RFD further turns\nMFP into a binary classification mode through replacing and detecting changes\nin input features, making it even simpler and more effective for CTR\npretraining. Our extensive experiments on two real-world large-scale datasets\n(i.e., Avazu, Criteo) demonstrate the advantages of these two methods on\nseveral strong backbones (e.g., DCNv2, DeepFM), and achieve new\nstate-of-the-art performance in terms of both effectiveness and efficiency for\nCTR prediction.",
        "translated": ""
    },
    {
        "title": "Evaluating ChatGPT text-mining of clinical records for obesity\n  monitoring",
        "url": "http://arxiv.org/abs/2308.01666v1",
        "pub_date": "2023-08-03",
        "summary": "Background: Veterinary clinical narratives remain a largely untapped resource\nfor addressing complex diseases. Here we compare the ability of a large\nlanguage model (ChatGPT) and a previously developed regular expression (RegexT)\nto identify overweight body condition scores (BCS) in veterinary narratives.\nMethods: BCS values were extracted from 4,415 anonymised clinical narratives\nusing either RegexT or by appending the narrative to a prompt sent to ChatGPT\ncoercing the model to return the BCS information. Data were manually reviewed\nfor comparison. Results: The precision of RegexT was higher (100%, 95% CI\n94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall\nof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of\nRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is\nneeded to improve ChatGPT output. Conclusions: Large language models create\ndiverse opportunities and, whilst complex, present an intuitive interface to\ninformation but require careful implementation to avoid unpredictable errors.",
        "translated": ""
    },
    {
        "title": "Fast Slate Policy Optimization: Going Beyond Plackett-Luce",
        "url": "http://arxiv.org/abs/2308.01566v1",
        "pub_date": "2023-08-03",
        "summary": "An increasingly important building block of large scale machine learning\nsystems is based on returning slates; an ordered lists of items given a query.\nApplications of this technology include: search, information retrieval and\nrecommender systems. When the action space is large, decision systems are\nrestricted to a particular structure to complete online queries quickly. This\npaper addresses the optimization of these large scale decision systems given an\narbitrary reward function. We cast this learning problem in a policy\noptimization framework and propose a new class of policies, born from a novel\nrelaxation of decision functions. This results in a simple, yet efficient\nlearning algorithm that scales to massive action spaces. We compare our method\nto the commonly adopted Plackett-Luce policy class and demonstrate the\neffectiveness of our approach on problems with action space sizes in the order\nof millions.",
        "translated": ""
    },
    {
        "title": "Density Weighting for Multi-Interest Personalized Recommendation",
        "url": "http://arxiv.org/abs/2308.01563v1",
        "pub_date": "2023-08-03",
        "summary": "Using multiple user representations (MUR) to model user behavior instead of a\nsingle user representation (SUR) has been shown to improve personalization in\nrecommendation systems. However, the performance gains observed with MUR can be\nsensitive to the skewness in the item and/or user interest distribution. When\nthe data distribution is highly skewed, the gains observed by learning multiple\nrepresentations diminish since the model dominates on head items/interests,\nleading to poor performance on tail items. Robustness to data sparsity is\ntherefore essential for MUR-based approaches to achieve good performance for\nrecommendations. Yet, research in MUR and data imbalance have largely been done\nindependently. In this paper, we delve deeper into the shortcomings of MUR\ninferred from imbalanced data distributions. We make several contributions: (1)\nUsing synthetic datasets, we demonstrate the sensitivity of MUR with respect to\ndata imbalance, (2) To improve MUR for tail items, we propose an iterative\ndensity weighting scheme (IDW) with user tower calibration to mitigate the\neffect of training over long-tail distribution on personalization, and (3)\nThrough extensive experiments on three real-world benchmarks, we demonstrate\nIDW outperforms other alternatives that address data imbalance.",
        "translated": ""
    },
    {
        "title": "Adaptive Preferential Attached kNN Graph With Distribution-Awareness",
        "url": "http://arxiv.org/abs/2308.02442v1",
        "pub_date": "2023-08-04",
        "summary": "Graph-based kNN algorithms have garnered widespread popularity for machine\nlearning tasks, due to their simplicity and effectiveness. However, the\nconventional kNN graph's reliance on a fixed value of k can hinder its\nperformance, especially in scenarios involving complex data distributions.\nMoreover, like other classification models, the presence of ambiguous samples\nalong decision boundaries often presents a challenge, as they are more prone to\nincorrect classification. To address these issues, we propose the Preferential\nAttached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with\ndistribution-based graph construction. By incorporating distribution\ninformation, paNNG can significantly improve performance for ambiguous samples\nby \"pulling\" them towards their original classes and hence enable enhanced\noverall accuracy and generalization capability. Through rigorous evaluations on\ndiverse benchmark datasets, paNNG outperforms state-of-the-art algorithms,\nshowcasing its adaptability and efficacy across various real-world scenarios.",
        "translated": ""
    },
    {
        "title": "RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph\n  Classification",
        "url": "http://arxiv.org/abs/2308.02335v1",
        "pub_date": "2023-08-04",
        "summary": "Graph classification is a crucial task in many real-world multimedia\napplications, where graphs can represent various multimedia data types such as\nimages, videos, and social networks. Previous efforts have applied graph neural\nnetworks (GNNs) in balanced situations where the class distribution is\nbalanced. However, real-world data typically exhibit long-tailed class\ndistributions, resulting in a bias towards the head classes when using GNNs and\nlimited generalization ability over the tail classes. Recent approaches mainly\nfocus on re-balancing different classes during model training, which fails to\nexplicitly introduce new knowledge and sacrifices the performance of the head\nclasses. To address these drawbacks, we propose a novel framework called\nRetrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature\nextractor and an unbiased classifier in a decoupled manner. In the feature\nextractor training stage, we develop a graph retrieval module to search for\nrelevant graphs that directly enrich the intra-class diversity for the tail\nclasses. Moreover, we innovatively optimize a category-centered supervised\ncontrastive loss to obtain discriminative representations, which is more\nsuitable for long-tailed scenarios. In the classifier fine-tuning stage, we\nbalance the classifier weights with two weight regularization techniques, i.e.,\nMax-norm and weight decay. Experiments on various popular benchmarks verify the\nsuperiority of the proposed method against state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Learning to Select the Relevant History Turns in Conversational Question\n  Answering",
        "url": "http://arxiv.org/abs/2308.02294v1",
        "pub_date": "2023-08-04",
        "summary": "The increasing demand for the web-based digital assistants has given a rapid\nrise in the interest of the Information Retrieval (IR) community towards the\nfield of conversational question answering (ConvQA). However, one of the\ncritical aspects of ConvQA is the effective selection of conversational history\nturns to answer the question at hand. The dependency between relevant history\nselection and correct answer prediction is an intriguing but under-explored\narea. The selected relevant context can better guide the system so as to where\nexactly in the passage to look for an answer. Irrelevant context, on the other\nhand, brings noise to the system, thereby resulting in a decline in the model's\nperformance. In this paper, we propose a framework, DHS-ConvQA (Dynamic History\nSelection in Conversational Question Answering), that first generates the\ncontext and question entities for all the history turns, which are then pruned\non the basis of similarity they share in common with the question at hand. We\nalso propose an attention-based mechanism to re-rank the pruned terms based on\ntheir calculated weights of how useful they are in answering the question. In\nthe end, we further aid the model by highlighting the terms in the re-ranked\nconversational history using a binary classification task and keeping the\nuseful terms (predicted as 1) and ignoring the irrelevant terms (predicted as\n0). We demonstrate the efficacy of our proposed framework with extensive\nexperimental results on CANARD and QuAC -- the two popularly utilized datasets\nin ConvQA. We demonstrate that selecting relevant turns works better than\nrewriting the original question. We also investigate how adding the irrelevant\nhistory turns negatively impacts the model's performance and discuss the\nresearch challenges that demand more attention from the IR community.",
        "translated": ""
    },
    {
        "title": "Optimally Computing Compressed Indexing Arrays Based on the Compact\n  Directed Acyclic Word Graph",
        "url": "http://arxiv.org/abs/2308.02269v1",
        "pub_date": "2023-08-04",
        "summary": "In this paper, we present the first study of the computational complexity of\nconverting an automata-based text index structure, called the Compact Directed\nAcyclic Word Graph (CDAWG), of size $e$ for a text $T$ of length $n$ into other\ntext indexing structures for the same text, suitable for highly repetitive\ntexts: the run-length BWT of size $r$, the irreducible PLCP array of size $r$,\nand the quasi-irreducible LPF array of size $e$, as well as the lex-parse of\nsize $O(r)$ and the LZ77-parse of size $z$, where $r, z \\le e$. As main\nresults, we showed that the above structures can be optimally computed from\neither the CDAWG for $T$ stored in read-only memory or its self-index version\nof size $e$ without a text in $O(e)$ worst-case time and words of working\nspace. To obtain the above results, we devised techniques for enumerating a\nparticular subset of suffixes in the lexicographic and text orders using the\nforward and backward search on the CDAWG by extending the results by\nBelazzougui et al. in 2015.",
        "translated": ""
    },
    {
        "title": "Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song",
        "url": "http://arxiv.org/abs/2308.02249v1",
        "pub_date": "2023-08-04",
        "summary": "In this paper, we introduce a computational analysis of the field recording\ndataset of approximately 700 hours of Korean folk songs, which were recorded\naround 1980-90s. Because most of the songs were sung by non-expert musicians\nwithout accompaniment, the dataset provides several challenges. To address this\nchallenge, we utilized self-supervised learning with convolutional neural\nnetwork based on pitch contour, then analyzed how the musical concept of tori,\na classification system defined by a specific scale, ornamental notes, and an\nidiomatic melodic contour, is captured by the model. The experimental result\nshows that our approach can better capture the characteristics of tori compared\nto traditional pitch histograms. Using our approaches, we have examined how\nmusical discussions proposed in existing academia manifest in the actual field\nrecordings of Korean folk songs.",
        "translated": ""
    },
    {
        "title": "Towards Personalized Prompt-Model Retrieval for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.02205v1",
        "pub_date": "2023-08-04",
        "summary": "Recommender Systems are built to retrieve relevant items to satisfy users'\ninformation needs. The candidate corpus usually consists of a finite set of\nitems that are ready to be served, such as videos, products, or articles. With\nrecent advances in Generative AI such as GPT and Diffusion models, a new form\nof recommendation task is yet to be explored where items are to be created by\ngenerative models with personalized prompts. Taking image generation as an\nexample, with a single prompt from the user and access to a generative model,\nit is possible to generate hundreds of new images in a few minutes. How shall\nwe attain personalization in the presence of \"infinite\" items? In this\npreliminary study, we propose a two-stage framework, namely Prompt-Model\nRetrieval and Generated Item Ranking, to approach this new task formulation. We\nrelease GEMRec-18K, a prompt-model interaction dataset with 18K images\ngenerated by 200 publicly-available generative models paired with a diverse set\nof 90 textual prompts. Our findings demonstrate the promise of generative model\nrecommendation as a novel personalization problem and the limitations of\nexisting evaluation metrics. We highlight future directions for the RecSys\ncommunity to advance towards generative recommender systems. Our code and\ndataset are available at https://github.com/MAPS-research/GEMRec.",
        "translated": ""
    },
    {
        "title": "Incorporating Recklessness to Collaborative Filtering based Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2308.02058v1",
        "pub_date": "2023-08-03",
        "summary": "Recommender systems that include some reliability measure of their\npredictions tend to be more conservative in forecasting, due to their\nconstraint to preserve reliability. This leads to a significant drop in the\ncoverage and novelty that these systems can provide. In this paper, we propose\nthe inclusion of a new term in the learning process of matrix\nfactorization-based recommender systems, called recklessness, which enables the\ncontrol of the risk level desired when making decisions about the reliability\nof a prediction. Experimental results demonstrate that recklessness not only\nallows for risk regulation but also improves the quantity and quality of\npredictions provided by the recommender system.",
        "translated": ""
    },
    {
        "title": "Seasonality Based Reranking of E-commerce Autocomplete Using Natural\n  Language Queries",
        "url": "http://arxiv.org/abs/2308.02055v1",
        "pub_date": "2023-08-03",
        "summary": "Query autocomplete (QAC) also known as typeahead, suggests list of complete\nqueries as user types prefix in the search box. It is one of the key features\nof modern search engines specially in e-commerce. One of the goals of typeahead\nis to suggest relevant queries to users which are seasonally important. In this\npaper we propose a neural network based natural language processing (NLP)\nalgorithm to incorporate seasonality as a signal and present end to end\nevaluation of the QAC ranking model. Incorporating seasonality into\nautocomplete ranking model can improve autocomplete relevance and business\nmetric.",
        "translated": ""
    },
    {
        "title": "Domain specificity and data efficiency in typo tolerant spell checkers:\n  the case of search in online marketplaces",
        "url": "http://arxiv.org/abs/2308.01976v1",
        "pub_date": "2023-08-03",
        "summary": "Typographical errors are a major source of frustration for visitors of online\nmarketplaces. Because of the domain-specific nature of these marketplaces and\nthe very short queries users tend to search for, traditional spell cheking\nsolutions do not perform well in correcting typos. We present a data\naugmentation method to address the lack of annotated typo data and train a\nrecurrent neural network to learn context-limited domain-specific embeddings.\nThose embeddings are deployed in a real-time inferencing API for the Microsoft\nAppSource marketplace to find the closest match between a misspelled user query\nand the available product names. Our data efficient solution shows that\ncontrolled high quality synthetic data may be a powerful tool especially\nconsidering the current climate of large language models which rely on\nprohibitively huge and often uncontrolled datasets.",
        "translated": ""
    },
    {
        "title": "Randomized algorithms for precise measurement of differentially-private,\n  personalized recommendations",
        "url": "http://arxiv.org/abs/2308.03735v1",
        "pub_date": "2023-08-07",
        "summary": "Personalized recommendations form an important part of today's internet\necosystem, helping artists and creators to reach interested users, and helping\nusers to discover new and engaging content. However, many users today are\nskeptical of platforms that personalize recommendations, in part due to\nhistorically careless treatment of personal data and data privacy. Now,\nbusinesses that rely on personalized recommendations are entering a new\nparadigm, where many of their systems must be overhauled to be privacy-first.\nIn this article, we propose an algorithm for personalized recommendations that\nfacilitates both precise and differentially-private measurement. We consider\nadvertising as an example application, and conduct offline experiments to\nquantify how the proposed privacy-preserving algorithm affects key metrics\nrelated to user experience, advertiser value, and platform revenue compared to\nthe extremes of both (private) non-personalized and non-private, personalized\nimplementations.",
        "translated": ""
    },
    {
        "title": "Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity\n  Resolution",
        "url": "http://arxiv.org/abs/2308.03734v1",
        "pub_date": "2023-08-07",
        "summary": "The entity resolution problem requires finding pairs across datasets that\nbelong to different owners but refer to the same entity in the real world. To\ntrain and evaluate solutions (either rule-based or machine-learning-based) to\nthe entity resolution problem, generating a ground truth dataset with entity\npairs or clusters is needed. However, such a data annotation process involves\nhumans as domain oracles to review the plaintext data for all candidate record\npairs from different parties, which inevitably infringes the privacy of data\nowners, especially in privacy-sensitive cases like medical records. To the best\nof our knowledge, there is no prior work on privacy-preserving ground truth\ndataset generation, especially in the domain of entity resolution. We propose a\nnovel blind annotation protocol based on homomorphic encryption that allows\ndomain oracles to collaboratively label ground truths without sharing data in\nplaintext with other parties. In addition, we design a domain-specific\neasy-to-use language that hides the sophisticated underlying homomorphic\nencryption layer. Rigorous proof of the privacy guarantee is provided and our\nempirical experiments via an annotation simulator indicate the feasibility of\nour privacy-preserving protocol (f-measure on average achieves more than 90\\%\ncompared with the real ground truths).",
        "translated": ""
    },
    {
        "title": "Multi-View Graph Convolutional Network for Multimedia Recommendation",
        "url": "http://arxiv.org/abs/2308.03588v1",
        "pub_date": "2023-08-07",
        "summary": "Multimedia recommendation has received much attention in recent years. It\nmodels user preferences based on both behavior information and item multimodal\ninformation. Though current GCN-based methods achieve notable success, they\nsuffer from two limitations: (1) Modality noise contamination to the item\nrepresentations. Existing methods often mix modality features and behavior\nfeatures in a single view (e.g., user-item view) for propagation, the noise in\nthe modality features may be amplified and coupled with behavior features. In\nthe end, it leads to poor feature discriminability; (2) Incomplete user\npreference modeling caused by equal treatment of modality features. Users often\nexhibit distinct modality preferences when purchasing different items. Equally\nfusing each modality feature ignores the relative importance among different\nmodalities, leading to the suboptimal user preference modeling. To tackle the\nabove issues, we propose a novel Multi-View Graph Convolutional Network for the\nmultimedia recommendation. Specifically, to avoid modality noise contamination,\nthe modality features are first purified with the aid of item behavior\ninformation. Then, the purified modality features of items and behavior\nfeatures are enriched in separate views, including the user-item view and the\nitem-item view. In this way, the distinguishability of features is enhanced.\nMeanwhile, a behavior-aware fuser is designed to comprehensively model user\npreferences by adaptively learning the relative importance of different\nmodality features. Furthermore, we equip the fuser with a self-supervised\nauxiliary task. This task is expected to maximize the mutual information\nbetween the fused multimodal features and behavior features, so as to capture\ncomplementary and supplementary preference information simultaneously.\nExtensive experiments on three public datasets demonstrate the effectiveness of\nour methods.",
        "translated": ""
    },
    {
        "title": "TeraHAC: Hierarchical Agglomerative Clustering of Trillion-Edge Graphs",
        "url": "http://arxiv.org/abs/2308.03578v1",
        "pub_date": "2023-08-07",
        "summary": "We introduce TeraHAC, a $(1+\\epsilon)$-approximate hierarchical agglomerative\nclustering (HAC) algorithm which scales to trillion-edge graphs. Our algorithm\nis based on a new approach to computing $(1+\\epsilon)$-approximate HAC, which\nis a novel combination of the nearest-neighbor chain algorithm and the notion\nof $(1+\\epsilon)$-approximate HAC. Our approach allows us to partition the\ngraph among multiple machines and make significant progress in computing the\nclustering within each partition before any communication with other partitions\nis needed.\n  We evaluate TeraHAC on a number of real-world and synthetic graphs of up to 8\ntrillion edges. We show that TeraHAC requires over 100x fewer rounds compared\nto previously known approaches for computing HAC. It is up to 8.3x faster than\nSCC, the state-of-the-art distributed algorithm for hierarchical clustering,\nwhile achieving 1.16x higher quality. In fact, TeraHAC essentially retains the\nquality of the celebrated HAC algorithm while significantly improving the\nrunning time.",
        "translated": ""
    },
    {
        "title": "Global cognitive graph properties dynamics of hippocampal formation",
        "url": "http://arxiv.org/abs/2308.03563v1",
        "pub_date": "2023-08-07",
        "summary": "In the present study we have used a set of methods and metrics to build a\ngraph of relative neural connections in a hippocampus of a rodent. A set of\ngraphs was built on top of time-sequenced data and analyzed in terms of\ndynamics of a connection genesis. The analysis has shown that during the\nprocess of a rodent exploring a novel environment, the relations between\nneurons constantly change which indicates that globally memory is constantly\nupdated even for known areas of space. Even if some neurons gain cognitive\nspecialization, the global network though remains relatively stable.\nAdditionally we suggest a set of methods for building a graph of cognitive\nneural network.",
        "translated": ""
    },
    {
        "title": "Uncertainty-aware Consistency Learning for Cold-Start Item\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.03470v1",
        "pub_date": "2023-08-07",
        "summary": "Graph Neural Network (GNN)-based models have become the mainstream approach\nfor recommender systems. Despite the effectiveness, they are still suffering\nfrom the cold-start problem, i.e., recommend for few-interaction items.\nExisting GNN-based recommendation models to address the cold-start problem\nmainly focus on utilizing auxiliary features of users and items, leaving the\nuser-item interactions under-utilized. However, embeddings distributions of\ncold and warm items are still largely different, since cold items' embeddings\nare learned from lower-popularity interactions, while warm items' embeddings\nare from higher-popularity interactions. Thus, there is a seesaw phenomenon,\nwhere the recommendation performance for the cold and warm items cannot be\nimproved simultaneously. To this end, we proposed a Uncertainty-aware\nConsistency learning framework for Cold-start item recommendation (shorten as\nUCC) solely based on user-item interactions. Under this framework, we train the\nteacher model (generator) and student model (recommender) with consistency\nlearning, to ensure the cold items with additionally generated low-uncertainty\ninteractions can have similar distribution with the warm items. Therefore, the\nproposed framework improves the recommendation of cold and warm items at the\nsame time, without hurting any one of them. Extensive experiments on benchmark\ndatasets demonstrate that our proposed method significantly outperforms\nstate-of-the-art methods on both warm and cold items, with an average\nperformance improvement of 27.6%.",
        "translated": ""
    },
    {
        "title": "Doubly Robust Estimator for Off-Policy Evaluation with Large Action\n  Spaces",
        "url": "http://arxiv.org/abs/2308.03443v1",
        "pub_date": "2023-08-07",
        "summary": "We study Off-Policy Evaluation (OPE) in contextual bandit settings with large\naction spaces. The benchmark estimators suffer from severe bias and variance\ntradeoffs. Parametric approaches suffer from bias due to difficulty specifying\nthe correct model, whereas ones with importance weight suffer from variance. To\novercome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was\nproposed to mitigate the estimator's variance via embeddings of an action. To\nmake the estimator more accurate, we propose the doubly robust estimator of\nMIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical\nanalysis shows that the proposed estimator is unbiased under weaker assumptions\nthan MIPS while maintaining variance reduction against IPS, which was the main\nadvantage of MIPS. The empirical experiment verifies the supremacy of MDR\nagainst existing estimators.",
        "translated": ""
    },
    {
        "title": "Hierarchical Contrastive Learning with Multiple Augmentation for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.03400v1",
        "pub_date": "2023-08-07",
        "summary": "Sequential recommendation addresses the issue of preference drift by\npredicting the next item based on the user's previous behaviors. Recently, a\npromising approach using contrastive learning has emerged, demonstrating its\neffectiveness in recommending items under sparse user-item interactions.\nSignificantly, the effectiveness of combinations of various augmentation\nmethods has been demonstrated in different domains, particularly in computer\nvision. However, when it comes to augmentation within a contrastive learning\nframework in sequential recommendation, previous research has only focused on\nlimited conditions and simple structures. Thus, it is still possible to extend\nexisting approaches to boost the effects of augmentation methods by using\nprogressed structures with the combinations of multiple augmentation methods.\nIn this work, we propose a novel framework called Hierarchical Contrastive\nLearning with Multiple Augmentation for Sequential Recommendation(HCLRec) to\novercome the aforementioned limitation. Our framework leverages existing\naugmentation methods hierarchically to improve performance. By combining\naugmentation methods continuously, we generate low-level and high-level view\npairs. We employ a Transformers-based model to encode the input sequence\neffectively. Furthermore, we introduce additional blocks consisting of\nTransformers and position-wise feed-forward network(PFFN) layers to learn the\ninvariance of the original sequences from hierarchically augmented views. We\npass the input sequence to subsequent layers based on the number of increment\nlevels applied to the views to handle various augmentation levels. Within each\nlayer, we compute contrastive loss between pairs of views at the same level.\nExtensive experiments demonstrate that our proposed method outperforms\nstate-of-the-art approaches and that HCLRec is robust even when faced with the\nproblem of sparse interaction.",
        "translated": ""
    },
    {
        "title": "POSIT: Promotion of Semantic Item Tail via Adversarial Learning",
        "url": "http://arxiv.org/abs/2308.03366v1",
        "pub_date": "2023-08-07",
        "summary": "In many recommender problems, a handful of popular items (e.g. movies/TV\nshows, news etc.) can be dominant in recommendations for many users. However,\nwe know that in a large catalog of items, users are likely interested in more\nthan what is popular. The dominance of popular items may mean that users will\nnot see items they would likely enjoy. In this paper, we propose a technique to\novercome this problem using adversarial machine learning. We define a metric to\ntranslate user-level utility metric in terms of an advantage/disadvantage over\nitems. We subsequently use that metric in an adversarial learning framework to\nsystematically promote disadvantaged items. The resulting algorithm identifies\nsemantically meaningful items that get promoted in the learning algorithm. In\nthe empirical study, we evaluate the proposed technique on three publicly\navailable datasets and four competitive baselines. The result shows that our\nproposed method not only improves the coverage, but also, surprisingly,\nimproves the overall performance.",
        "translated": ""
    },
    {
        "title": "Heterogeneous Knowledge Fusion: A Novel Approach for Personalized\n  Recommendation via LLM",
        "url": "http://arxiv.org/abs/2308.03333v1",
        "pub_date": "2023-08-07",
        "summary": "The analysis and mining of user heterogeneous behavior are of paramount\nimportance in recommendation systems. However, the conventional approach of\nincorporating various types of heterogeneous behavior into recommendation\nmodels leads to feature sparsity and knowledge fragmentation issues. To address\nthis challenge, we propose a novel approach for personalized recommendation via\nLarge Language Model (LLM), by extracting and fusing heterogeneous knowledge\nfrom user heterogeneous behavior information. In addition, by combining\nheterogeneous knowledge and recommendation tasks, instruction tuning is\nperformed on LLM for personalized recommendations. The experimental results\ndemonstrate that our method can effectively integrate user heterogeneous\nbehavior and significantly improve recommendation performance.",
        "translated": ""
    },
    {
        "title": "Your Negative May not Be True Negative: Boosting Image-Text Matching\n  with False Negative Elimination",
        "url": "http://arxiv.org/abs/2308.04380v1",
        "pub_date": "2023-08-08",
        "summary": "Most existing image-text matching methods adopt triplet loss as the\noptimization objective, and choosing a proper negative sample for the triplet\nof &lt;anchor, positive, negative&gt; is important for effectively training the\nmodel, e.g., hard negatives make the model learn efficiently and effectively.\nHowever, we observe that existing methods mainly employ the most similar\nsamples as hard negatives, which may not be true negatives. In other words, the\nsamples with high similarity but not paired with the anchor may reserve\npositive semantic associations, and we call them false negatives. Repelling\nthese false negatives in triplet loss would mislead the semantic representation\nlearning and result in inferior retrieval performance. In this paper, we\npropose a novel False Negative Elimination (FNE) strategy to select negatives\nvia sampling, which could alleviate the problem introduced by false negatives.\nSpecifically, we first construct the distributions of positive and negative\nsamples separately via their similarities with the anchor, based on the\nfeatures extracted from image and text encoders. Then we calculate the false\nnegative probability of a given sample based on its similarity with the anchor\nand the above distributions via the Bayes' rule, which is employed as the\nsampling weight during negative sampling process. Since there may not exist any\nfalse negative in a small batch size, we design a memory module with momentum\nto retain a large negative buffer and implement our negative sampling strategy\nspanning over the buffer. In addition, to make the model focus on hard\nnegatives, we reassign the sampling weights for the simple negatives with a\ncut-down strategy. The extensive experiments are conducted on Flickr30K and\nMS-COCO, and the results demonstrate the superiority of our proposed false\nnegative elimination strategy. The code is available at\nhttps://github.com/LuminosityX/FNE.",
        "translated": ""
    },
    {
        "title": "Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval",
        "url": "http://arxiv.org/abs/2308.04343v1",
        "pub_date": "2023-08-08",
        "summary": "Most existing cross-modal retrieval methods employ two-stream encoders with\ndifferent architectures for images and texts, \\textit{e.g.}, CNN for images and\nRNN/Transformer for texts. Such discrepancy in architectures may induce\ndifferent semantic distribution spaces and limit the interactions between\nimages and texts, and further result in inferior alignment between images and\ntexts. To fill this research gap, inspired by recent advances of Transformers\nin vision tasks, we propose to unify the encoder architectures with\nTransformers for both modalities. Specifically, we design a cross-modal\nretrieval framework purely based on two-stream Transformers, dubbed\n\\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image\nTransformer, a text Transformer, and a hierarchical alignment module. With such\nidentical architectures, the encoders could produce representations with more\nsimilar characteristics for images and texts, and make the interactions and\nalignments between them much easier. Besides, to leverage the rich semantics,\nwe devise a hierarchical alignment scheme to explore multi-level\ncorrespondences of different layers between images and texts. To evaluate the\neffectiveness of the proposed HAT, we conduct extensive experiments on two\nbenchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that\nHAT outperforms SOTA baselines by a large margin. Specifically, on two key\ntasks, \\textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves\n7.6\\% and 16.7\\% relative score improvement of Recall@1 on MSCOCO, and 4.4\\%\nand 11.6\\% on Flickr30k respectively. The code is available at\n\\url{https://github.com/LuminosityX/HAT}.",
        "translated": ""
    },
    {
        "title": "Advancing Natural-Language Based Audio Retrieval with PaSST and Large\n  Audio-Caption Data Sets",
        "url": "http://arxiv.org/abs/2308.04258v1",
        "pub_date": "2023-08-08",
        "summary": "This work presents a text-to-audio-retrieval system based on pre-trained text\nand spectrogram transformers. Our method projects recordings and textual\ndescriptions into a shared audio-caption space in which related examples from\ndifferent modalities are close. Through a systematic analysis, we examine how\neach component of the system influences retrieval performance. As a result, we\nidentify two key components that play a crucial role in driving performance:\nthe self-attention-based audio encoder for audio embedding and the utilization\nof additional human-generated and synthetic data sets during pre-training. We\nfurther experimented with augmenting ClothoV2 captions with available keywords\nto increase their variety; however, this only led to marginal improvements. Our\nsystem ranked first in the 2023's DCASE Challenge, and it outperforms the\ncurrent state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.",
        "translated": ""
    },
    {
        "title": "UniRecSys: A Unified Framework for Personalized, Group, Package, and\n  Package-to-Group Recommendations",
        "url": "http://arxiv.org/abs/2308.04247v1",
        "pub_date": "2023-08-08",
        "summary": "Recommender systems aim to enhance the overall user experience by providing\ntailored recommendations for a variety of products and services. These systems\nhelp users make more informed decisions, leading to greater user satisfaction\nwith the platform. However, the implementation of these systems largely depends\non the context, which can vary from recommending an item or package to a user\nor a group. This requires careful exploration of several models during the\ndeployment, as there is no comprehensive and unified approach that deals with\nrecommendations at different levels. Furthermore, these individual models must\nbe closely attuned to their generated recommendations depending on the context\nto prevent significant variation in their generated recommendations. In this\npaper, we propose a novel unified recommendation framework that addresses all\nfour recommendation tasks, namely personalized, group, package, or\npackage-to-group recommendation, filling the gap in the current research\nlandscape. The proposed framework can be integrated with most of the\ntraditional matrix factorization-based collaborative filtering models. The idea\nis to enhance the formulation of the existing approaches by incorporating\ncomponents focusing on the exploitation of the group and package latent\nfactors. These components also help in exploiting a rich latent representation\nof the user/item by enforcing them to align closely with their corresponding\ngroup/package representation. We consider two prominent CF techniques,\nRegularized Matrix Factorization and Maximum Margin Matrix factorization, as\nthe baseline models and demonstrate their customization to various\nrecommendation tasks. Experiment results on two publicly available datasets are\nreported, comparing them to other baseline approaches that consider individual\nrating feedback for group or package recommendations.",
        "translated": ""
    },
    {
        "title": "OpinionConv: Conversational Product Search with Grounded Opinions",
        "url": "http://arxiv.org/abs/2308.04226v1",
        "pub_date": "2023-08-08",
        "summary": "When searching for products, the opinions of others play an important role in\nmaking informed decisions. Subjective experiences about a product can be a\nvaluable source of information. This is also true in sales conversations, where\na customer and a sales assistant exchange facts and opinions about products.\nHowever, training an AI for such conversations is complicated by the fact that\nlanguage models do not possess authentic opinions for their lack of real-world\nexperience. We address this problem by leveraging product reviews as a rich\nsource of product opinions to ground conversational AI in true subjective\nnarratives. With OpinionConv, we develop the first conversational AI for\nsimulating sales conversations. To validate the generated conversations, we\nconduct several user studies showing that the generated opinions are perceived\nas realistic. Our assessors also confirm the importance of opinions as an\ninformative basis for decision-making.",
        "translated": ""
    },
    {
        "title": "Understanding and Modeling Passive-Negative Feedback for Short-video\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.04086v1",
        "pub_date": "2023-08-08",
        "summary": "Sequential recommendation is one of the most important tasks in recommender\nsystems, which aims to recommend the next interacted item with historical\nbehaviors as input. Traditional sequential recommendation always mainly\nconsiders the collected positive feedback such as click, purchase, etc.\nHowever, in short-video platforms such as TikTok, video viewing behavior may\nnot always represent positive feedback. Specifically, the videos are played\nautomatically, and users passively receive the recommended videos. In this new\nscenario, users passively express negative feedback by skipping over videos\nthey do not like, which provides valuable information about their preferences.\nDifferent from the negative feedback studied in traditional recommender\nsystems, this passive-negative feedback can reflect users' interests and serve\nas an important supervision signal in extracting users' preferences. Therefore,\nit is essential to carefully design and utilize it in this novel recommendation\nscenario. In this work, we first conduct analyses based on a large-scale\nreal-world short-video behavior dataset and illustrate the significance of\nleveraging passive feedback. We then propose a novel method that deploys the\nsub-interest encoder, which incorporates positive feedback and passive-negative\nfeedback as supervision signals to learn the user's current active\nsub-interest. Moreover, we introduce an adaptive fusion layer to integrate\nvarious sub-interests effectively. To enhance the robustness of our model, we\nthen introduce a multi-task learning module to simultaneously optimize two\nkinds of feedback -- passive-negative feedback and traditional randomly-sampled\nnegative feedback. The experiments on two large-scale datasets verify that the\nproposed method can significantly outperform state-of-the-art approaches. The\ncode is released at https://github.com/tsinghua-fib-lab/RecSys2023-SINE.",
        "translated": ""
    },
    {
        "title": "Online Distillation-enhanced Multi-modal Transformer for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.04067v1",
        "pub_date": "2023-08-08",
        "summary": "Multi-modal recommendation systems, which integrate diverse types of\ninformation, have gained widespread attention in recent years. However,\ncompared to traditional collaborative filtering-based multi-modal\nrecommendation systems, research on multi-modal sequential recommendation is\nstill in its nascent stages. Unlike traditional sequential recommendation\nmodels that solely rely on item identifier (ID) information and focus on\nnetwork structure design, multi-modal recommendation models need to emphasize\nitem representation learning and the fusion of heterogeneous data sources. This\npaper investigates the impact of item representation learning on downstream\nrecommendation tasks and examines the disparities in information fusion at\ndifferent stages. Empirical experiments are conducted to demonstrate the need\nto design a framework suitable for collaborative learning and fusion of diverse\ninformation. Based on this, we propose a new model-agnostic framework for\nmulti-modal sequential recommendation tasks, called Online\nDistillation-enhanced Multi-modal Transformer (ODMT), to enhance feature\ninteraction and mutual learning among multi-source input (ID, text, and image),\nwhile avoiding conflicts among different features during training, thereby\nimproving recommendation accuracy. To be specific, we first introduce an\nID-aware Multi-modal Transformer module in the item representation learning\nstage to facilitate information interaction among different features. Secondly,\nwe employ an online distillation training strategy in the prediction\noptimization stage to make multi-source data learn from each other and improve\nprediction robustness. Experimental results on a video content recommendation\ndataset and three e-commerce recommendation datasets demonstrate the\neffectiveness of the proposed two modules, which is approximately 10%\nimprovement in performance compared to baseline models.",
        "translated": ""
    },
    {
        "title": "Adapting Foundation Models for Information Synthesis of Wireless\n  Communication Specifications",
        "url": "http://arxiv.org/abs/2308.04033v1",
        "pub_date": "2023-08-08",
        "summary": "Existing approaches to understanding, developing and researching modern\nwireless communication technologies involves time-intensive and arduous process\nof sifting through numerous webpages and technical specification documents,\ngathering the required information and synthesizing it. This paper presents\nNextGen Communications Copilot, a conversational artificial intelligence tool\nfor information synthesis of wireless communication specifications. The system\nbuilds on top of recent advancements in foundation models and consists of three\nkey additional components: a domain-specific database, a context extractor, and\na feedback mechanism. The system appends user queries with concise and\nquery-dependent contextual information extracted from a database of wireless\ntechnical specifications and incorporates tools for expert feedback and data\ncontributions. On evaluation using a benchmark dataset of queries and reference\nresponses created by subject matter experts, the system demonstrated more\nrelevant and accurate answers with an average BLEU score and BERTScore\nF1-measure of 0.37 and 0.79 respectively compared to the corresponding values\nof 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.",
        "translated": ""
    },
    {
        "title": "Top K Relevant Passage Retrieval for Biomedical Question Answering",
        "url": "http://arxiv.org/abs/2308.04028v1",
        "pub_date": "2023-08-08",
        "summary": "Question answering is a task that answers factoid questions using a large\ncollection of documents. It aims to provide precise answers in response to the\nuser's questions in natural language. Question answering relies on efficient\npassage retrieval to select candidate contexts, where traditional sparse vector\nspace models, such as TF-IDF or BM25, are the de facto method. On the web,\nthere is no single article that could provide all the possible answers\navailable on the internet to the question of the problem asked by the user. The\nexisting Dense Passage Retrieval model has been trained on Wikipedia dump from\nDec. 20, 2018, as the source documents for answering questions. Question\nanswering (QA) has made big strides with several open-domain and machine\ncomprehension systems built using large-scale annotated datasets. However, in\nthe clinical domain, this problem remains relatively unexplored. According to\nmultiple surveys, Biomedical Questions cannot be answered correctly from\nWikipedia Articles. In this work, we work on the existing DPR framework for the\nbiomedical domain and retrieve answers from the Pubmed articles which is a\nreliable source to answer medical questions. When evaluated on a BioASQ QA\ndataset, our fine-tuned dense retriever results in a 0.81 F1 score.",
        "translated": ""
    },
    {
        "title": "Exploring the Spatiotemporal Features of Online Food Recommendation\n  Service",
        "url": "http://arxiv.org/abs/2308.04019v1",
        "pub_date": "2023-08-08",
        "summary": "Online Food Recommendation Service (OFRS) has remarkable spatiotemporal\ncharacteristics and the advantage of being able to conveniently satisfy users'\nneeds in a timely manner. There have been a variety of studies that have begun\nto explore its spatiotemporal properties, but a comprehensive and in-depth\nanalysis of the OFRS spatiotemporal features is yet to be conducted. Therefore,\nthis paper studies the OFRS based on three questions: how spatiotemporal\nfeatures play a role; why self-attention cannot be used to model the\nspatiotemporal sequences of OFRS; and how to combine spatiotemporal features to\nimprove the efficiency of OFRS. Firstly, through experimental analysis, we\nsystemically extracted the spatiotemporal features of OFRS, identified the most\nvaluable features and designed an effective combination method. Secondly, we\nconducted a detailed analysis of the spatiotemporal sequences, which revealed\nthe shortcomings of self-attention in OFRS, and proposed a more optimized\nspatiotemporal sequence method for replacing self-attention. In addition, we\nalso designed a Dynamic Context Adaptation Model to further improve the\nefficiency and performance of OFRS. Through the offline experiments on two\nlarge datasets and online experiments for a week, the feasibility and\nsuperiority of our model were proven.",
        "translated": ""
    },
    {
        "title": "Dual Intents Graph Modeling for User-centric Group Discovery",
        "url": "http://arxiv.org/abs/2308.05013v1",
        "pub_date": "2023-08-09",
        "summary": "Online groups have become increasingly prevalent, providing users with space\nto share experiences and explore interests. Therefore, user-centric group\ndiscovery task, i.e., recommending groups to users can help both users' online\nexperiences and platforms' long-term developments. Existing recommender methods\ncan not deal with this task as modeling user-group participation into a\nbipartite graph overlooks their item-side interests. Although there exist a few\nworks attempting to address this task, they still fall short in fully\npreserving the social context and ensuring effective interest representation\nlearning.\n  In this paper, we focus on exploring the intents that motivate users to\nparticipate in groups, which can be categorized into different types, like the\nsocial-intent and the personal interest-intent. The former refers to users\njoining a group affected by their social links, while the latter relates to\nusers joining groups with like-minded people for self-enjoyment. To comprehend\ndifferent intents, we propose a novel model, DiRec, that first models each\nintent separately and then fuses them together for predictions. Specifically,\nfor social-intent, we introduce the hypergraph structure to model the\nrelationship between groups and members, leading to a richer understanding of\nthe social context. As for interest-intent, we employ novel structural\nrefinement on the interactive graph to uncover more intricate user behaviors\nand group interests, realizing better representation learning of interests.\nFurthermore, we also observe the intent overlapping in real-world scenarios and\ndevise a novel self-supervised learning loss that encourages such alignment for\nfinal recommendations. Extensive experiments on three public datasets show the\nsignificant improvement of DiRec over the state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction\n  Following",
        "url": "http://arxiv.org/abs/2308.04913v1",
        "pub_date": "2023-08-09",
        "summary": "E-commerce authoring involves creating attractive, abundant, and targeted\npromotional content to drive product sales. The emergence of large language\nmodels (LLMs) introduces an innovative paradigm, offering a unified solution to\naddress various authoring tasks within this scenario. However, mainstream LLMs\ntrained on general corpora with common sense knowledge reveal limitations in\nfitting complex and personalized features unique to e-commerce products and\ncustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,\nraising concerns about safeguarding voluminous customer privacy data during\ntransmission. This paper proposes the LLaMA-E, the unified and customized\ninstruction-following language models focusing on diverse e-commerce authoring\ntasks. Specifically, the domain experts create the seed instruction set from\nthe tasks of ads generation, query-enhanced product title rewriting, product\nclassification, purchase intent speculation, and general Q&amp;A. These tasks\nenable the models to comprehensively understand precise e-commerce authoring\nknowledge by interleaving features covering typical service aspects of\ncustomers, sellers, and platforms. The GPT-3.5 is introduced as a teacher\nmodel, which expands the seed instructions to form a training set for the\nLLaMA-E models with various scales. The experimental results show that the\nproposed LLaMA-E models achieve state-of-the-art results in quantitative and\nqualitative evaluations, also exhibiting the advantage in zero-shot scenes. To\nthe best of our knowledge, this study is the first to serve the LLMs to\nspecific e-commerce authoring scenarios.",
        "translated": ""
    },
    {
        "title": "Parallel Knowledge Enhancement based Framework for Multi-behavior\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.04807v1",
        "pub_date": "2023-08-09",
        "summary": "Multi-behavior recommendation algorithms aim to leverage the multiplex\ninteractions between users and items to learn users' latent preferences. Recent\nmulti-behavior recommendation frameworks contain two steps: fusion and\nprediction. In the fusion step, advanced neural networks are used to model the\nhierarchical correlations between user behaviors. In the prediction step,\nmultiple signals are utilized to jointly optimize the model with a multi-task\nlearning (MTL) paradigm. However, recent approaches have not addressed the\nissue caused by imbalanced data distribution in the fusion step, resulting in\nthe learned relationships being dominated by high-frequency behaviors. In the\nprediction step, the existing methods use a gate mechanism to directly\naggregate expert information generated by coupling input, leading to negative\ninformation transfer. To tackle these issues, we propose a Parallel Knowledge\nEnhancement Framework (PKEF) for multi-behavior recommendation. Specifically,\nwe enhance the hierarchical information propagation in the fusion step using\nparallel knowledge (PKF). Meanwhile, in the prediction step, we decouple the\nrepresentations to generate expert information and introduce a projection\nmechanism during aggregation to eliminate gradient conflicts and alleviate\nnegative transfer (PME). We conduct comprehensive experiments on three\nreal-world datasets to validate the effectiveness of our model. The results\nfurther demonstrate the rationality and effectiveness of the designed PKF and\nPME modules. The source code and datasets are available at\nhttps://github.com/MC-CV/PKEF.",
        "translated": ""
    },
    {
        "title": "DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels\n  from User Comments for Music",
        "url": "http://arxiv.org/abs/2308.04805v1",
        "pub_date": "2023-08-09",
        "summary": "Towards sufficient music searching, it is vital to form a complete set of\nlabels for each song. However, current solutions fail to resolve it as they\ncannot produce diverse enough mappings to make up for the information missed by\nthe gold labels. Based on the observation that such missing information may\nalready be presented in user comments, we propose to study the automated music\nlabeling in an essential but under-explored setting, where the model is\nrequired to harvest more diverse and valid labels from the users' comments\ngiven limited gold labels. To this end, we design an iterative framework (DiVa)\nto harvest more $\\underline{\\text{Di}}$verse and $\\underline{\\text{Va}}$lid\nlabels from user comments for music. The framework makes a classifier able to\nform complete sets of labels for songs via pseudo-labels inferred from\npre-trained classifiers and a novel joint score function. The experiment on a\ndensely annotated testing set reveals the superiority of the Diva over\nstate-of-the-art solutions in producing more diverse labels missed by the gold\nlabels. We hope our work can inspire future research on automated music\nlabeling.",
        "translated": ""
    },
    {
        "title": "Entire Space Cascade Delayed Feedback Modeling for Effective Conversion\n  Rate Prediction",
        "url": "http://arxiv.org/abs/2308.04768v1",
        "pub_date": "2023-08-09",
        "summary": "Conversion rate (CVR) prediction is an essential task for large-scale\ne-commerce platforms. However, refund behaviors frequently occur after\nconversion in online shopping systems, which drives us to pay attention to\neffective conversion for building healthier shopping services. This paper\ndefines the probability of item purchasing without any subsequent refund as an\neffective conversion rate (ECVR). A simple paradigm for ECVR prediction is to\ndecompose it into two sub-tasks: CVR prediction and post-conversion refund rate\n(RFR) prediction. However, RFR prediction suffers from data sparsity (DS) and\nsample selection bias (SSB) issues, as the refund behaviors are only available\nafter user purchase. Furthermore, there is delayed feedback in both conversion\nand refund events and they are sequentially dependent, named cascade delayed\nfeedback (CDF), which significantly harms data freshness for model training.\nPrevious studies mainly focus on tackling DS and SSB or delayed feedback for a\nsingle event. To jointly tackle these issues in ECVR prediction, we propose an\nEntire space CAscade Delayed feedback modeling (ECAD) method. Specifically,\nECAD deals with DS and SSB by constructing two tasks including CVR prediction\nand conversion \\&amp; refund rate (CVRFR) prediction using the entire space\nmodeling framework. In addition, it carefully schedules auxiliary tasks to\nleverage both conversion and refund time within data to alleviate CDF.\nExperimental results on the offline industrial dataset and online A/B testing\ndemonstrate the effectiveness of ECAD. In addition, ECAD has been deployed in\none of the recommender systems in Alibaba, contributing to a significant\nimprovement of ECVR.",
        "translated": ""
    },
    {
        "title": "Building Interpretable and Reliable Open Information Retriever for New\n  Domains Overnight",
        "url": "http://arxiv.org/abs/2308.04756v1",
        "pub_date": "2023-08-09",
        "summary": "Information retrieval (IR) or knowledge retrieval, is a critical component\nfor many down-stream tasks such as open-domain question answering (QA). It is\nalso very challenging, as it requires succinctness, completeness, and\ncorrectness. In recent works, dense retrieval models have achieved\nstate-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by\nrepresenting queries and knowledge passages with dense vectors and learning the\nlexical and semantic similarity. However, using single dense vectors and\nend-to-end supervision are not always optimal because queries may require\nattention to multiple aspects and event implicit knowledge. In this work, we\npropose an information retrieval pipeline that uses entity/event linking model\nand query decomposition model to focus more accurately on different information\nunits of the query. We show that, while being more interpretable and reliable,\nour proposed pipeline significantly improves passage coverages and denotation\naccuracies across five IR and QA benchmarks. It will be the go-to system to use\nfor applications that need to perform IR on a new domain without much dedicated\neffort, because of its superior interpretability and cross-domain performance.",
        "translated": ""
    },
    {
        "title": "Self-supervised Learning of Rotation-invariant 3D Point Set Features\n  using Transformer and its Self-distillation",
        "url": "http://arxiv.org/abs/2308.04725v1",
        "pub_date": "2023-08-09",
        "summary": "Invariance against rotations of 3D objects is an important property in\nanalyzing 3D point set data. Conventional 3D point set DNNs having rotation\ninvariance typically obtain accurate 3D shape features via supervised learning\nby using labeled 3D point sets as training samples. However, due to the rapid\nincrease in 3D point set data and the high cost of labeling, a framework to\nlearn rotation-invariant 3D shape features from numerous unlabeled 3D point\nsets is required. This paper proposes a novel self-supervised learning\nframework for acquiring accurate and rotation-invariant 3D point set features\nat object-level. Our proposed lightweight DNN architecture decomposes an input\n3D point set into multiple global-scale regions, called tokens, that preserve\nthe spatial layout of partial shapes composing the 3D object. We employ a\nself-attention mechanism to refine the tokens and aggregate them into an\nexpressive rotation-invariant feature per 3D point set. Our DNN is effectively\ntrained by using pseudo-labels generated by a self-distillation framework. To\nfacilitate the learning of accurate features, we propose to combine multi-crop\nand cut-mix data augmentation techniques to diversify 3D point sets for\ntraining. Through a comprehensive evaluation, we empirically demonstrate that,\n(1) existing rotation-invariant DNN architectures designed for supervised\nlearning do not necessarily learn accurate 3D shape features under a\nself-supervised learning scenario, and (2) our proposed algorithm learns\nrotation-invariant 3D point set features that are more accurate than those\nlearned by existing algorithms. Code will be available at\nhttps://github.com/takahikof/RIPT_SDMM",
        "translated": ""
    },
    {
        "title": "Pareto Invariant Representation Learning for Multimedia Recommendation",
        "url": "http://arxiv.org/abs/2308.04706v1",
        "pub_date": "2023-08-09",
        "summary": "Multimedia recommendation involves personalized ranking tasks, where\nmultimedia content is usually represented using a generic encoder. However,\nthese generic representations introduce spurious correlations that fail to\nreveal users' true preferences. Existing works attempt to alleviate this\nproblem by learning invariant representations, but overlook the balance between\nindependent and identically distributed (IID) and out-of-distribution (OOD)\ngeneralization. In this paper, we propose a framework called Pareto Invariant\nRepresentation Learning (PaInvRL) to mitigate the impact of spurious\ncorrelations from an IID-OOD multi-objective optimization perspective, by\nlearning invariant representations (intrinsic factors that attract user\nattention) and variant representations (other factors) simultaneously.\nSpecifically, PaInvRL includes three iteratively executed modules: (i)\nheterogeneous identification module, which identifies the heterogeneous\nenvironments to reflect distributional shifts for user-item interactions; (ii)\ninvariant mask generation module, which learns invariant masks based on the\nPareto-optimal solutions that minimize the adaptive weighted Invariant Risk\nMinimization (IRM) and Empirical Risk (ERM) losses; (iii) convert module, which\ngenerates both variant representations and item-invariant representations for\ntraining a multi-modal recommendation model that mitigates spurious\ncorrelations and balances the generalization performance within and cross the\nenvironmental distributions. We compare the proposed PaInvRL with\nstate-of-the-art recommendation models on three public multimedia\nrecommendation datasets (Movielens, Tiktok, and Kwai), and the experimental\nresults validate the effectiveness of PaInvRL for both within- and\ncross-environmental learning.",
        "translated": ""
    },
    {
        "title": "Evaluating and Optimizing the Effectiveness of Neural Machine\n  Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark",
        "url": "http://arxiv.org/abs/2308.04693v1",
        "pub_date": "2023-08-09",
        "summary": "Neural Machine Translation (NMT) is widely applied in software engineering\ntasks. The effectiveness of NMT for code retrieval relies on the ability to\nlearn from the sequence of tokens in the source language to the sequence of\ntokens in the target language. While NMT performs well in pseudocode-to-code\ntranslation, it might have challenges in learning to translate from natural\nlanguage query to source code in newly curated real-world code documentation/\nimplementation datasets. In this work, we analyze the performance of NMT in\nnatural language-to-code translation in the newly curated CAT benchmark that\nincludes the optimized versions of three Java datasets TLCodeSum,\nCodeSearchNet, Funcom, and a Python dataset PCSD. Our evaluation shows that NMT\nhas low accuracy, measured by CrystalBLEU and Meteor metrics in this task. To\nalleviate the duty of NMT in learning complex representation of source code, we\npropose ASTTrans Representation, a tailored representation of an Abstract\nSyntax Tree (AST) using a subset of non-terminal nodes. We show that the\nclassical approach NMT performs significantly better in learning ASTTrans\nRepresentation over code tokens with up to 36% improvement on Meteor score.\nMoreover, we leverage ASTTrans Representation to conduct combined code search\nprocesses from the state-of-the-art code search processes using GraphCodeBERT\nand UniXcoder. Our NMT models of learning ASTTrans Representation can boost the\nMean Reciprocal Rank of these state-of-the-art code search processes by up to\n3.08% and improve 23.08% of queries' results over the CAT benchmark.",
        "translated": ""
    },
    {
        "title": "web crawler strategies for web pages under robot.txt restriction",
        "url": "http://arxiv.org/abs/2308.04689v1",
        "pub_date": "2023-08-09",
        "summary": "In the present time, all know about World Wide Web and work over the Internet\ndaily. In this paper, we introduce the search engines working for keywords that\nare entered by users to find something. The search engine uses different search\nalgorithms for convenient results for providing to the net surfer. Net surfers\ngo with the top search results but how did the results of web pages get higher\nranks over search engines? how the search engine got that all the web pages in\nthe database? This paper gives the answers to all these kinds of basic\nquestions. Web crawlers working for search engines and robot exclusion protocol\nrules for web crawlers are also addressed in this research paper. Webmaster\nuses different restriction facts in robot.txt file to instruct web crawler,\nsome basic formats of robot.txt are also mentioned in this paper.",
        "translated": ""
    },
    {
        "title": "SSLRec: A Self-Supervised Learning Library for Recommendation",
        "url": "http://arxiv.org/abs/2308.05697v1",
        "pub_date": "2023-08-10",
        "summary": "Self-supervised learning (SSL) has gained significant interest in recent\nyears as a solution to address the challenges posed by sparse and noisy data in\nrecommender systems. Despite the growing number of SSL algorithms designed to\nprovide state-of-the-art performance in various recommendation scenarios (e.g.,\ngraph collaborative filtering, sequential recommendation, social\nrecommendation, KG-enhanced recommendation), there is still a lack of unified\nframeworks that integrate recommendation algorithms across different domains.\nSuch a framework could serve as the cornerstone for self-supervised\nrecommendation algorithms, unifying the validation of existing methods and\ndriving the design of new ones. To address this gap, we introduce SSLRec, a\nnovel benchmark platform that provides a standardized, flexible, and\ncomprehensive framework for evaluating various SSL-enhanced recommenders. The\nSSLRec library features a modular architecture that allows users to easily\nevaluate state-of-the-art models and a complete set of data augmentation and\nself-supervised toolkits to help create SSL recommendation models with specific\nneeds. Furthermore, SSLRec simplifies the process of training and evaluating\ndifferent recommendation models with consistent and fair settings. Our SSLRec\nplatform covers a comprehensive set of state-of-the-art SSL-enhanced\nrecommendation models across different scenarios, enabling researchers to\nevaluate these cutting-edge models and drive further innovation in the field.\nOur implemented SSLRec framework is available at the source code repository\nhttps://github.com/HKUDS/SSLRec.",
        "translated": ""
    },
    {
        "title": "Finding Already Debunked Narratives via Multistage Retrieval: Enabling\n  Cross-Lingual, Cross-Dataset and Zero-Shot Learning",
        "url": "http://arxiv.org/abs/2308.05680v1",
        "pub_date": "2023-08-10",
        "summary": "The task of retrieving already debunked narratives aims to detect stories\nthat have already been fact-checked. The successful detection of claims that\nhave already been debunked not only reduces the manual efforts of professional\nfact-checkers but can also contribute to slowing the spread of misinformation.\nMainly due to the lack of readily available data, this is an understudied\nproblem, particularly when considering the cross-lingual task, i.e. the\nretrieval of fact-checking articles in a language different from the language\nof the online post being checked. This paper fills this gap by (i) creating a\nnovel dataset to enable research on cross-lingual retrieval of already debunked\nnarratives, using tweets as queries to a database of fact-checking articles;\n(ii) presenting an extensive experiment to benchmark fine-tuned and\noff-the-shelf multilingual pre-trained Transformer models for this task; and\n(iii) proposing a novel multistage framework that divides this cross-lingual\ndebunk retrieval task into refinement and re-ranking stages. Results show that\nthe task of cross-lingual retrieval of already debunked narratives is\nchallenging and off-the-shelf Transformer models fail to outperform a strong\nlexical-based baseline (BM25). Nevertheless, our multistage retrieval framework\nis robust, outperforming BM25 in most scenarios and enabling cross-domain and\nzero-shot learning, without significantly harming the model's performance.",
        "translated": ""
    },
    {
        "title": "LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition",
        "url": "http://arxiv.org/abs/2308.05609v1",
        "pub_date": "2023-08-10",
        "summary": "Biomedical Natural Language Processing (NLP) tends to become cumbersome for\nmost researchers, frequently due to the amount and heterogeneity of text to be\nprocessed. To address this challenge, the industry is continuously developing\nhighly efficient tools and creating more flexible engineering solutions. This\nwork presents the integration between industry data engineering solutions for\nefficient data processing and academic systems developed for Named Entity\nRecognition (LasigeUnicage\\_NER) and Relation Extraction (BiOnt). Our design\nreflects an integration of those components with external knowledge in the form\nof additional training data from other datasets and biomedical ontologies. We\nused this pipeline in the 2022 LitCoin NLP Challenge, where our team\nLasigeUnicage was awarded the 7th Prize out of approximately 200 participating\nteams, reflecting a successful collaboration between the academia (LASIGE) and\nthe industry (Unicage). The software supporting this work is available at\n\\url{https://github.com/lasigeBioTM/Litcoin-Lasige_Unicage}.",
        "translated": ""
    },
    {
        "title": "Multi-domain Recommendation with Embedding Disentangling and Domain\n  Alignment",
        "url": "http://arxiv.org/abs/2308.05508v1",
        "pub_date": "2023-08-10",
        "summary": "Multi-domain recommendation (MDR) aims to provide recommendations for\ndifferent domains (e.g., types of products) with overlapping users/items and is\ncommon for platforms such as Amazon, Facebook, and LinkedIn that host multiple\nservices. Existing MDR models face two challenges: First, it is difficult to\ndisentangle knowledge that generalizes across domains (e.g., a user likes cheap\nitems) and knowledge specific to a single domain (e.g., a user likes blue\nclothing but not blue cars). Second, they have limited ability to transfer\nknowledge across domains with small overlaps. We propose a new MDR method named\nEDDA with two key components, i.e., embedding disentangling recommender and\ndomain alignment, to tackle the two challenges respectively. In particular, the\nembedding disentangling recommender separates both the model and embedding for\nthe inter-domain part and the intra-domain part, while most existing MDR\nmethods only focus on model-level disentangling. The domain alignment leverages\nrandom walks from graph processing to identify similar user/item pairs from\ndifferent domains and encourages similar user/item pairs to have similar\nembeddings, enhancing knowledge transfer. We compare EDDA with 12\nstate-of-the-art baselines on 3 real datasets. The results show that EDDA\nconsistently outperforms the baselines on all datasets and domains. All\ndatasets and codes are available at https://github.com/Stevenn9981/EDDA.",
        "translated": ""
    },
    {
        "title": "Bringing order into the realm of Transformer-based language models for\n  artificial intelligence and law",
        "url": "http://arxiv.org/abs/2308.05502v1",
        "pub_date": "2023-08-10",
        "summary": "Transformer-based language models (TLMs) have widely been recognized to be a\ncutting-edge technology for the successful development of deep-learning-based\nsolutions to problems and applications that require natural language processing\nand understanding. Like for other textual domains, TLMs have indeed pushed the\nstate-of-the-art of AI approaches for many tasks of interest in the legal\ndomain. Despite the first Transformer model being proposed about six years ago,\nthere has been a rapid progress of this technology at an unprecedented rate,\nwhereby BERT and related models represent a major reference, also in the legal\ndomain. This article provides the first systematic overview of TLM-based\nmethods for AI-driven problems and tasks in the legal sphere. A major goal is\nto highlight research advances in this field so as to understand, on the one\nhand, how the Transformers have contributed to the success of AI in supporting\nlegal processes, and on the other hand, what are the current limitations and\nopportunities for further research development.",
        "translated": ""
    },
    {
        "title": "Product Review Image Ranking for Fashion E-commerce",
        "url": "http://arxiv.org/abs/2308.05390v1",
        "pub_date": "2023-08-10",
        "summary": "In a fashion e-commerce platform where customers can't physically examine the\nproducts on their own, being able to see other customers' text and image\nreviews of the product is critical while making purchase decisions. Given the\nhigh reliance on these reviews, over the years we have observed customers\nproactively sharing their reviews. With an increase in the coverage of User\nGenerated Content (UGC), there has been a corresponding increase in the number\nof customer images. It is thus imperative to display the most relevant images\non top as it may influence users' online shopping choices and behavior. In this\npaper, we propose a simple yet effective training procedure for ranking\ncustomer images. We created a dataset consisting of Myntra (A Major Indian\nFashion e-commerce company) studio posts and highly engaged (upvotes/downvotes)\nUGC images as our starting point and used selected distortion techniques on the\nimages of the above dataset to bring their quality at par with those of bad UGC\nimages. We train our network to rank bad-quality images lower than high-quality\nones. Our proposed method outperforms the baseline models on two metrics,\nnamely correlation coefficient, and accuracy, by substantial margins.",
        "translated": ""
    },
    {
        "title": "Beyond Semantics: Learning a Behavior Augmented Relevance Model with\n  Self-supervised Learning",
        "url": "http://arxiv.org/abs/2308.05379v1",
        "pub_date": "2023-08-10",
        "summary": "Relevance modeling aims to locate desirable items for corresponding queries,\nwhich is crucial for search engines to ensure user experience. Although most\nconventional approaches address this problem by assessing the semantic\nsimilarity between the query and item, pure semantic matching is not\neverything. In reality, auxiliary query-item interactions extracted from user\nhistorical behavior data of the search log could provide hints to reveal users'\nsearch intents further. Drawing inspiration from this, we devise a novel\nBehavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that\nleverages neighbor queries of target item and neighbor items of target query to\ncomplement target query-item semantic matching. Specifically, our model builds\nmulti-level co-attention for distilling coarse-grained and fine-grained\nsemantic representations from both neighbor and target views. The model\nsubsequently employs neighbor-target self-supervised learning to improve the\naccuracy and robustness of BARL-ASe by strengthening representation and logit\nlearning. Furthermore, we discuss how to deal with the long-tail query-item\nmatching of the mini apps search scenario of Alipay practically. Experiments on\nreal-world industry data and online A/B testing demonstrate our proposal\nachieves promising performance with low latency.",
        "translated": ""
    },
    {
        "title": "Investigating disaster response through social media data and the\n  Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S.\n  wildfire season",
        "url": "http://arxiv.org/abs/2308.05281v1",
        "pub_date": "2023-08-10",
        "summary": "Effective disaster response is critical for affected communities. Responders\nand decision-makers would benefit from reliable, timely measures of the issues\nimpacting their communities during a disaster, and social media offers a\npotentially rich data source. Social media can reflect public concerns and\ndemands during a disaster, offering valuable insights for decision-makers to\nunderstand evolving situations and optimize resource allocation. We used\nBidirectional Encoder Representations from Transformers (BERT) topic modeling\nto cluster topics from Twitter data. Then, we conducted a temporal-spatial\nanalysis to examine the distribution of these topics across different regions\nduring the 2020 western U.S. wildfire season. Our results show that Twitter\nusers mainly focused on three topics:\"health impact,\" \"damage,\" and\n\"evacuation.\" We used the Susceptible-Infected-Recovered (SIR) theory to\nexplore the magnitude and velocity of topic diffusion on Twitter. The results\ndisplayed a clear relationship between topic trends and wildfire propagation\npatterns. The estimated parameters obtained from the SIR model in selected\ncities revealed that residents exhibited a high level of several concerns\nduring the wildfire. Our study details how the SIR model and topic modeling\nusing social media data can provide decision-makers with a quantitative\napproach to measure disaster response and support their decision-making\nprocesses.",
        "translated": ""
    },
    {
        "title": "A Large Language Model Enhanced Conversational Recommender System",
        "url": "http://arxiv.org/abs/2308.06212v1",
        "pub_date": "2023-08-11",
        "summary": "Conversational recommender systems (CRSs) aim to recommend high-quality items\nto users through a dialogue interface. It usually contains multiple sub-tasks,\nsuch as user preference elicitation, recommendation, explanation, and item\ninformation search. To develop effective CRSs, there are some challenges: 1)\nhow to properly manage sub-tasks; 2) how to effectively solve different\nsub-tasks; and 3) how to correctly generate responses that interact with users.\nRecently, Large Language Models (LLMs) have exhibited an unprecedented ability\nto reason and generate, presenting a new opportunity to develop more powerful\nCRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to\naddress the above challenges. For sub-task management, we leverage the\nreasoning ability of LLM to effectively manage sub-task. For sub-task solving,\nwe collaborate LLM with expert models of different sub-tasks to achieve the\nenhanced performance. For response generation, we utilize the generation\nability of LLM as a language interface to better interact with users.\nSpecifically, LLMCRS divides the workflow into four stages: sub-task detection,\nmodel matching, sub-task execution, and response generation. LLMCRS also\ndesigns schema-based instruction, demonstration-based instruction, dynamic\nsub-task and model matching, and summary-based generation to instruct LLM to\ngenerate desired results in the workflow. Finally, to adapt LLM to\nconversational recommendations, we also propose to fine-tune LLM with\nreinforcement learning from CRSs performance feedback, referred to as RLPF.\nExperimental results on benchmark datasets show that LLMCRS with RLPF\noutperforms the existing methods.",
        "translated": ""
    },
    {
        "title": "Identification of the Relevance of Comments in Codes Using Bag of Words\n  and Transformer Based Models",
        "url": "http://arxiv.org/abs/2308.06144v1",
        "pub_date": "2023-08-11",
        "summary": "The Forum for Information Retrieval (FIRE) started a shared task this year\nfor classification of comments of different code segments. This is binary text\nclassification task where the objective is to identify whether comments given\nfor certain code segments are relevant or not. The BioNLP-IISERB group at the\nIndian Institute of Science Education and Research Bhopal (IISERB) participated\nin this task and submitted five runs for five different models. The paper\npresents the overview of the models and other significant findings on the\ntraining corpus. The methods involve different feature engineering schemes and\ntext classification techniques. The performance of the classical bag of words\nmodel and transformer-based models were explored to identify significant\nfeatures from the given training corpus. We have explored different classifiers\nviz., random forest, support vector machine and logistic regression using the\nbag of words model. Furthermore, the pre-trained transformer based models like\nBERT, RoBERT and ALBERT were also used by fine-tuning them on the given\ntraining corpus. The performance of different such models over the training\ncorpus were reported and the best five models were implemented on the given\ntest corpus. The empirical results show that the bag of words model outperforms\nthe transformer based models, however, the performance of our runs are not\nreasonably well in both training and test corpus. This paper also addresses the\nlimitations of the models and scope for further improvement.",
        "translated": ""
    },
    {
        "title": "Toward a Better Understanding of Loss Functions for Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2308.06091v1",
        "pub_date": "2023-08-11",
        "summary": "Collaborative filtering (CF) is a pivotal technique in modern recommender\nsystems. The learning process of CF models typically consists of three\ncomponents: interaction encoder, loss function, and negative sampling. Although\nmany existing studies have proposed various CF models to design sophisticated\ninteraction encoders, recent work shows that simply reformulating the loss\nfunctions can achieve significant performance gains. This paper delves into\nanalyzing the relationship among existing loss functions. Our mathematical\nanalysis reveals that the previous loss functions can be interpreted as\nalignment and uniformity functions: (i) the alignment matches user and item\nrepresentations, and (ii) the uniformity disperses user and item distributions.\nInspired by this analysis, we propose a novel loss function that improves the\ndesign of alignment and uniformity considering the unique patterns of datasets\ncalled Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty\nof MAWU is two-fold: (i) margin-aware alignment (MA) mitigates\nuser/item-specific popularity biases, and (ii) weighted uniformity (WU) adjusts\nthe significance between user and item uniformities to reflect the inherent\ncharacteristics of datasets. Extensive experimental results show that MF and\nLightGCN equipped with MAWU are comparable or superior to state-of-the-art CF\nmodels with various loss functions on three public datasets.",
        "translated": ""
    },
    {
        "title": "Deep Context Interest Network for Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2308.06037v1",
        "pub_date": "2023-08-11",
        "summary": "Click-Through Rate (CTR) prediction, estimating the probability of a user\nclicking on an item, is essential in industrial applications, such as online\nadvertising. Many works focus on user behavior modeling to improve CTR\nprediction performance. However, most of those methods only model users'\npositive interests from users' click items while ignoring the context\ninformation, which is the display items around the clicks, resulting in\ninferior performance. In this paper, we highlight the importance of context\ninformation on user behavior modeling and propose a novel model named Deep\nContext Interest Network (DCIN), which integrally models the click and its\ndisplay context to learn users' context-aware interests. DCIN consists of three\nkey modules: 1) Position-aware Context Aggregation Module (PCAM), which\nperforms aggregation of display items with an attention mechanism; 2)\nFeedback-Context Fusion Module (FCFM), which fuses the representation of clicks\nand display contexts through non-linear feature interaction; 3) Interest\nMatching Module (IMM), which activates interests related with the target item.\nMoreover, we provide our hands-on solution to implement our DCIN model on\nlarge-scale industrial systems. The significant improvements in both offline\nand online evaluations demonstrate the superiority of our proposed DCIN method.\nNotably, DCIN has been deployed on our online advertising system serving the\nmain traffic, which brings 1.5% CTR and 1.5% RPM lift.",
        "translated": ""
    },
    {
        "title": "Designing a User Contextual Profile Ontology: A Focus on the Vehicle\n  Sales Domain",
        "url": "http://arxiv.org/abs/2308.06018v1",
        "pub_date": "2023-08-11",
        "summary": "In the digital age, it is crucial to understand and tailor experiences for\nusers interacting with systems and applications. This requires the creation of\nuser contextual profiles that combine user profiles with contextual\ninformation. However, there is a lack of research on the integration of\ncontextual information with different user profiles. This study aims to address\nthis gap by designing a user contextual profile ontology that considers both\nuser profiles and contextual information on each profile. Specifically, we\npresent a design and development of the user contextual profile ontology with a\nfocus on the vehicle sales domain. Our designed ontology serves as a structural\nfoundation for standardizing the representation of user profiles and contextual\ninformation, enhancing the system's ability to capture user preferences and\ncontextual information of the user accurately. Moreover, we illustrate a case\nstudy using the User Contextual Profile Ontology in generating personalized\nrecommendations for vehicle sales domain.",
        "translated": ""
    },
    {
        "title": "Augmented Negative Sampling for Collaborative Filtering",
        "url": "http://arxiv.org/abs/2308.05972v1",
        "pub_date": "2023-08-11",
        "summary": "Negative sampling is essential for implicit-feedback-based collaborative\nfiltering, which is used to constitute negative signals from massive unlabeled\ndata to guide supervised learning. The state-of-the-art idea is to utilize hard\nnegative samples that carry more useful information to form a better decision\nboundary. To balance efficiency and effectiveness, the vast majority of\nexisting methods follow the two-pass approach, in which the first pass samples\na fixed number of unobserved items by a simple static distribution and then the\nsecond pass selects the final negative items using a more sophisticated\nnegative sampling strategy. However, selecting negative samples from the\noriginal items is inherently restricted, and thus may not be able to contrast\npositive samples well. In this paper, we confirm this observation via\nexperiments and introduce two limitations of existing solutions: ambiguous trap\nand information discrimination. Our response to such limitations is to\nintroduce augmented negative samples. This direction renders a substantial\ntechnical challenge because constructing unconstrained negative samples may\nintroduce excessive noise that distorts the decision boundary. To this end, we\nintroduce a novel generic augmented negative sampling paradigm and provide a\nconcrete instantiation. First, we disentangle hard and easy factors of negative\nitems. Next, we generate new candidate negative samples by augmenting only the\neasy factors in a regulated manner: the direction and magnitude of the\naugmentation are carefully calibrated. Finally, we design an advanced negative\nsampling strategy to identify the final augmented negative samples, which\nconsiders not only the score function used in existing methods but also a new\nmetric called augmentation gain. Extensive experiments on real-world datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "LittleMu: Deploying an Online Virtual Teaching Assistant via\n  Heterogeneous Sources Integration and Chain of Teach Prompts",
        "url": "http://arxiv.org/abs/2308.05935v1",
        "pub_date": "2023-08-11",
        "summary": "Teaching assistants have played essential roles in the long history of\neducation. However, few MOOC platforms are providing human or virtual teaching\nassistants to support learning for massive online students due to the\ncomplexity of real-world online education scenarios and the lack of training\ndata. In this paper, we present a virtual MOOC teaching assistant, LittleMu\nwith minimum labeled training data, to provide question answering and chit-chat\nservices. Consisting of two interactive modules of heterogeneous retrieval and\nlanguage model prompting, LittleMu first integrates structural, semi- and\nunstructured knowledge sources to support accurate answers for a wide range of\nquestions. Then, we design delicate demonstrations named \"Chain of Teach\"\nprompts to exploit the large-scale pre-trained model to handle complex\nuncollected questions. Except for question answering, we develop other\neducational services such as knowledge-grounded chit-chat. We test the system's\nperformance via both offline evaluation and online deployment. Since May 2020,\nour LittleMu system has served over 80,000 users with over 300,000 queries from\nover 500 courses on XuetangX MOOC platform, which continuously contributes to a\nmore convenient and fair education. Our code, services, and dataset will be\navailable at https://github.com/THU-KEG/VTA.",
        "translated": ""
    },
    {
        "title": "LTP-MMF: Towards Long-term Provider Max-min Fairness Under\n  Recommendation Feedback Loops",
        "url": "http://arxiv.org/abs/2308.05902v1",
        "pub_date": "2023-08-11",
        "summary": "Multi-stakeholder recommender systems involve various roles, such as users,\nproviders. Previous work pointed out that max-min fairness (MMF) is a better\nmetric to support weak providers. However, when considering MMF, the features\nor parameters of these roles vary over time, how to ensure long-term provider\nMMF has become a significant challenge. We observed that recommendation\nfeedback loops (named RFL) will influence the provider MMF greatly in the long\nterm. RFL means that recommender system can only receive feedback on exposed\nitems from users and update recommender models incrementally based on this\nfeedback. When utilizing the feedback, the recommender model will regard\nunexposed item as negative. In this way, tail provider will not get the\nopportunity to be exposed, and its items will always be considered as negative\nsamples. Such phenomenons will become more and more serious in RFL. To\nalleviate the problem, this paper proposes an online ranking model named\nLong-Term Provider Max-min Fairness (named LTP-MMF). Theoretical analysis shows\nthat the long-term regret of LTP-MMF enjoys a sub-linear bound. Experimental\nresults on three public recommendation benchmarks demonstrated that LTP-MMF can\noutperform the baselines in the long term.",
        "translated": ""
    },
    {
        "title": "Cross-Attribute Matrix Factorization Model with Shared User Embedding",
        "url": "http://arxiv.org/abs/2308.07284v1",
        "pub_date": "2023-08-14",
        "summary": "Over the past few years, deep learning has firmly established its prowess\nacross various domains, including computer vision, speech recognition, and\nnatural language processing. Motivated by its outstanding success, researchers\nhave been directing their efforts towards applying deep learning techniques to\nrecommender systems. Neural collaborative filtering (NCF) and Neural Matrix\nFactorization (NeuMF) refreshes the traditional inner product in matrix\nfactorization with a neural architecture capable of learning complex and\ndata-driven functions. While these models effectively capture user-item\ninteractions, they overlook the specific attributes of both users and items.\nThis can lead to robustness issues, especially for items and users that belong\nto the \"long tail\". Such challenges are commonly recognized in recommender\nsystems as a part of the cold-start problem. A direct and intuitive approach to\naddress this issue is by leveraging the features and attributes of the items\nand users themselves. In this paper, we introduce a refined NeuMF model that\nconsiders not only the interaction between users and items, but also acrossing\nassociated attributes. Moreover, our proposed architecture features a shared\nuser embedding, seamlessly integrating with user embeddings to imporve the\nrobustness and effectively address the cold-start problem. Rigorous experiments\non both the Movielens and Pinterest datasets demonstrate the superiority of our\nCross-Attribute Matrix Factorization model, particularly in scenarios\ncharacterized by higher dataset sparsity.",
        "translated": ""
    },
    {
        "title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models",
        "url": "http://arxiv.org/abs/2308.07269v1",
        "pub_date": "2023-08-14",
        "summary": "Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to the outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners to apply knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub\nat https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and\ncomprehensive documentation for beginners to get started. Besides, we present\nan online system for real-time knowledge editing, and a demo video at\nhttp://knowlm.zjukg.cn/easyedit.mp4.",
        "translated": ""
    },
    {
        "title": "MM-GEF: Multi-modal representation meet collaborative filtering",
        "url": "http://arxiv.org/abs/2308.07222v1",
        "pub_date": "2023-08-14",
        "summary": "In modern e-commerce, item content features in various modalities offer\naccurate yet comprehensive information to recommender systems. The majority of\nprevious work either focuses on learning effective item representation during\nmodelling user-item interactions, or exploring item-item relationships by\nanalysing multi-modal features. Those methods, however, fail to incorporate the\ncollaborative item-user-item relationships into the multi-modal feature-based\nitem structure. In this work, we propose a graph-based item structure\nenhancement method MM-GEF: Multi-Modal recommendation with Graph Early-Fusion,\nwhich effectively combines the latent item structure underlying multi-modal\ncontents with the collaborative signals. Instead of processing the content\nfeature in different modalities separately, we show that the early-fusion of\nmulti-modal features provides significant improvement. MM-GEF learns refined\nitem representations by injecting structural information obtained from both\nmulti-modal and collaborative signals. Through extensive experiments on four\npublicly available datasets, we demonstrate systematical improvements of our\nmethod over state-of-the-art multi-modal recommendation methods.",
        "translated": ""
    },
    {
        "title": "gSASRec: Reducing Overconfidence in Sequential Recommendation Trained\n  with Negative Sampling",
        "url": "http://arxiv.org/abs/2308.07192v1",
        "pub_date": "2023-08-14",
        "summary": "A large catalogue size is one of the central challenges in training\nrecommendation models: a large number of items makes them memory and\ncomputationally inefficient to compute scores for all items during training,\nforcing these models to deploy negative sampling. However, negative sampling\nincreases the proportion of positive interactions in the training data, and\ntherefore models trained with negative sampling tend to overestimate the\nprobabilities of positive interactions a phenomenon we call overconfidence.\nWhile the absolute values of the predicted scores or probabilities are not\nimportant for the ranking of retrieved recommendations, overconfident models\nmay fail to estimate nuanced differences in the top-ranked items, resulting in\ndegraded performance. In this paper, we show that overconfidence explains why\nthe popular SASRec model underperforms when compared to BERT4Rec. This is\ncontrary to the BERT4Rec authors explanation that the difference in performance\nis due to the bi-directional attention mechanism. To mitigate overconfidence,\nwe propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) and\ntheoretically prove that it can mitigate overconfidence. We further propose the\ngSASRec model, an improvement over SASRec that deploys an increased number of\nnegatives and the gBCE loss. We show through detailed experiments on three\ndatasets that gSASRec does not exhibit the overconfidence problem. As a result,\ngSASRec can outperform BERT4Rec (e.g. +9.47% NDCG on the MovieLens-1M dataset),\nwhile requiring less training time (e.g. -73% training time on MovieLens-1M).\nMoreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets that\ncontain more than 1 million items.",
        "translated": ""
    },
    {
        "title": "Natural Language is All a Graph Needs",
        "url": "http://arxiv.org/abs/2308.07134v1",
        "pub_date": "2023-08-14",
        "summary": "The emergence of large-scale pre-trained language models, such as ChatGPT,\nhas revolutionized various research fields in artificial intelligence.\nTransformers-based large language models (LLMs) have gradually replaced CNNs\nand RNNs to unify fields of computer vision and natural language processing.\nCompared with the data that exists relatively independently such as images,\nvideos or texts, graph is a type of data that contains rich structural and\nrelational information. Meanwhile, natural language, as one of the most\nexpressive mediums, excels in describing complex structures. However, existing\nwork on incorporating graph learning problems into the generative language\nmodeling framework remains very limited. As the importance of language models\ncontinues to grow, it becomes essential to explore whether LLMs can also\nreplace GNNs as the foundational model for graphs. In this paper, we propose\nInstructGLM (Instruction-finetuned Graph Language Model), systematically design\nhighly scalable prompts based on natural language instructions, and use natural\nlanguage to describe the geometric structure and node features of the graph for\ninstruction tuning an LLMs to perform learning and inference on graphs in a\ngenerative manner. Our method exceeds all competitive GNN baselines on\nogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of\nour method and sheds light on generative language models replacing GNNs as the\nfoundation model for graph machine learning.",
        "translated": ""
    },
    {
        "title": "Large Language Models for Information Retrieval: A Survey",
        "url": "http://arxiv.org/abs/2308.07107v1",
        "pub_date": "2023-08-14",
        "summary": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions within\nthis expanding field.",
        "translated": ""
    },
    {
        "title": "UIPC-MF: User-Item Prototype Connection Matrix Factorization for\n  Explainable Collaborative Filtering",
        "url": "http://arxiv.org/abs/2308.07048v1",
        "pub_date": "2023-08-14",
        "summary": "Recommending items to potentially interested users has been an important\ncommercial task that faces two main challenges: accuracy and explainability.\nWhile most collaborative filtering models rely on statistical computations on a\nlarge scale of interaction data between users and items and can achieve high\nperformance, they often lack clear explanatory power. We propose UIPC-MF, a\nprototype-based matrix factorization method for explainable collaborative\nfiltering recommendations. In UIPC-MF, both users and items are associated with\nsets of prototypes, capturing general collaborative attributes. To enhance\nexplainability, UIPC-MF learns connection weights that reflect the associative\nrelations between user and item prototypes for recommendations. UIPC-MF\noutperforms other prototype-based baseline methods in terms of Hit Ratio and\nNormalized Discounted Cumulative Gain on three datasets, while also providing\nbetter transparency.",
        "translated": ""
    },
    {
        "title": "The Scientometrics and Reciprocality Underlying Co-Authorship Panels in\n  Google Scholar Profiles",
        "url": "http://arxiv.org/abs/2308.07001v1",
        "pub_date": "2023-08-14",
        "summary": "Online academic profiles are used by scholars to reflect a desired image to\ntheir online audience. In Google Scholar, scholars can select a subset of\nco-authors for presentation in a central location on their profile using a\nsocial feature called the Co-authroship panel. In this work, we examine whether\nscientometrics and reciprocality can explain the observed selections. To this\nend, we scrape and thoroughly analyze a novel set of 120,000 Google Scholar\nprofiles, ranging across four disciplines and various academic institutions.\nOur results suggest that scholars tend to favor co-authors with higher\nscientometrics over others for inclusion in their co-authorship panels.\nInterestingly, as one's own scientometrics are higher, the tendency to include\nco-authors with high scientometrics is diminishing. Furthermore, we find that\nreciprocality is central to explaining scholars' selections.",
        "translated": ""
    },
    {
        "title": "Discrete Conditional Diffusion for Reranking in Recommendation",
        "url": "http://arxiv.org/abs/2308.06982v1",
        "pub_date": "2023-08-14",
        "summary": "Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list to model interplay between items.\nConsidering the inherent challenges of reranking such as combinatorial\nsearching space, some previous studies have adopted the evaluator-generator\nparadigm, with a generator producing feasible sequences and a evaluator\nselecting the best one based on estimated listwise utility. Inspired by the\nremarkable success of diffusion generative models, this paper explores the\npotential of diffusion models for generating high-quality sequences in\nreranking. However, we argue that it is nontrivial to take diffusion models as\nthe generator in the context of recommendation. Firstly, diffusion models\nprimarily operate in continuous data space, differing from the discrete data\nspace of item permutations. Secondly, the recommendation task is different from\nconventional generation tasks as the purpose of recommender systems is to\nfulfill user interests. Lastly, real-life recommender systems require\nefficiency, posing challenges for the inference of diffusion models. To\novercome these challenges, we propose a novel Discrete Conditional Diffusion\nReranking (DCDR) framework for recommendation. DCDR extends traditional\ndiffusion models by introducing a discrete forward process with tractable\nposteriors, which adds noise to item sequences through step-wise discrete\noperations (e.g., swapping). Additionally, DCDR incorporates a conditional\nreverse process that generates item sequences conditioned on expected user\nresponses. Extensive offline experiments conducted on public datasets\ndemonstrate that DCDR outperforms state-of-the-art reranking methods.\nFurthermore, DCDR has been deployed in a real-world video app with over 300\nmillion daily active users, significantly enhancing online recommendation\nquality.",
        "translated": ""
    },
    {
        "title": "AutoAssign+: Automatic Shared Embedding Assignment in Streaming\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.06965v1",
        "pub_date": "2023-08-14",
        "summary": "In the domain of streaming recommender systems, conventional methods for\naddressing new user IDs or item IDs typically involve assigning initial ID\nembeddings randomly. However, this practice results in two practical\nchallenges: (i) Items or users with limited interactive data may yield\nsuboptimal prediction performance. (ii) Embedding new IDs or low-frequency IDs\nnecessitates consistently expanding the embedding table, leading to unnecessary\nmemory consumption. In light of these concerns, we introduce a reinforcement\nlearning-driven framework, namely AutoAssign+, that facilitates Automatic\nShared Embedding Assignment Plus. To be specific, AutoAssign+ utilizes an\nIdentity Agent as an actor network, which plays a dual role: (i) Representing\nlow-frequency IDs field-wise with a small set of shared embeddings to enhance\nthe embedding initialization, and (ii) Dynamically determining which ID\nfeatures should be retained or eliminated in the embedding table. The policy of\nthe agent is optimized with the guidance of a critic network. To evaluate the\neffectiveness of our approach, we perform extensive experiments on three\ncommonly used benchmark datasets. Our experiment results demonstrate that\nAutoAssign+ is capable of significantly enhancing recommendation performance by\nmitigating the cold-start problem. Furthermore, our framework yields a\nreduction in memory usage of approximately 20-30%, verifying its practical\neffectiveness and efficiency for streaming recommender systems.",
        "translated": ""
    },
    {
        "title": "Investigation Toward The Economic Feasibility of Personalized Medicine\n  For Healthcare Service Providers: The Case of Bladder Cancer",
        "url": "http://arxiv.org/abs/2308.07924v1",
        "pub_date": "2023-08-15",
        "summary": "In today's complex healthcare landscape, the pursuit of delivering optimal\npatient care while navigating intricate economic dynamics poses a significant\nchallenge for healthcare service providers (HSPs). In this already complex\ndynamics, the emergence of clinically promising personalized medicine based\ntreatment aims to revolutionize medicine. While personalized medicine holds\ntremendous potential for enhancing therapeutic outcomes, its integration within\nresource-constrained HSPs presents formidable challenges. In this study, we\ninvestigate the economic feasibility of implementing personalized medicine. The\ncentral objective is to strike a balance between catering to individual patient\nneeds and making economically viable decisions. Unlike conventional binary\napproaches to personalized treatment, we propose a more nuanced perspective by\ntreating personalization as a spectrum. This approach allows for greater\nflexibility in decision-making and resource allocation. To this end, we propose\na mathematical framework to investigate our proposal, focusing on Bladder\nCancer (BC) as a case study. Our results show that while it is feasible to\nintroduce personalized medicine, a highly efficient but highly expensive one\nwould be short-lived relative to its less effective but cheaper alternative as\nthe latter can be provided to a larger cohort of patients, optimizing the HSP's\nobjective better.",
        "translated": ""
    },
    {
        "title": "Synthesizing Political Zero-Shot Relation Classification via Codebook\n  Knowledge, NLI, and ChatGPT",
        "url": "http://arxiv.org/abs/2308.07876v1",
        "pub_date": "2023-08-15",
        "summary": "Recent supervised models for event coding vastly outperform pattern-matching\nmethods. However, their reliance solely on new annotations disregards the vast\nknowledge within expert databases, hindering their applicability to\nfine-grained classification. To address these limitations, we explore zero-shot\napproaches for political event ontology relation classification, by leveraging\nknowledge from established annotation codebooks. Our study encompasses both\nChatGPT and a novel natural language inference (NLI) based approach named ZSP.\nZSP adopts a tree-query framework that deconstructs the task into context,\nmodality, and class disambiguation levels. This framework improves\ninterpretability, efficiency, and adaptability to schema changes. By conducting\nextensive experiments on our newly curated datasets, we pinpoint the\ninstability issues within ChatGPT and highlight the superior performance of\nZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained\nRootcode classification. ZSP demonstrates competitive performance compared to\nsupervised BERT models, positioning it as a valuable tool for event record\nvalidation and ontology development. Our work underscores the potential of\nleveraging transfer learning and existing expertise to enhance the efficiency\nand scalability of research in the field.",
        "translated": ""
    },
    {
        "title": "Impression-Aware Recommender Systems",
        "url": "http://arxiv.org/abs/2308.07857v1",
        "pub_date": "2023-08-15",
        "summary": "Novel data sources bring new opportunities to improve the quality of\nrecommender systems. Impressions are a novel data source containing past\nrecommendations (shown items) and traditional interactions. Researchers may use\nimpressions to refine user preferences and overcome the current limitations in\nrecommender systems research. The relevance and interest of impressions have\nincreased over the years; hence, the need for a review of relevant work on this\ntype of recommenders. We present a systematic literature review on recommender\nsystems using impressions, focusing on three fundamental angles in research:\nrecommenders, datasets, and evaluation methodologies. We provide three\ncategorizations of papers describing recommenders using impressions, present\neach reviewed paper in detail, describe datasets with impressions, and analyze\nthe existing evaluation methodologies. Lastly, we present open questions and\nfuture directions of interest, highlighting aspects missing in the literature\nthat can be addressed in future works.",
        "translated": ""
    },
    {
        "title": "Dynamic Embedding Size Search with Minimum Regret for Streaming\n  Recommender System",
        "url": "http://arxiv.org/abs/2308.07760v1",
        "pub_date": "2023-08-15",
        "summary": "With the continuous increase of users and items, conventional recommender\nsystems trained on static datasets can hardly adapt to changing environments.\nThe high-throughput data requires the model to be updated in a timely manner\nfor capturing the user interest dynamics, which leads to the emergence of\nstreaming recommender systems. Due to the prevalence of deep learning-based\nrecommender systems, the embedding layer is widely adopted to represent the\ncharacteristics of users, items, and other features in low-dimensional vectors.\nHowever, it has been proved that setting an identical and static embedding size\nis sub-optimal in terms of recommendation performance and memory cost,\nespecially for streaming recommendations. To tackle this problem, we first\nrethink the streaming model update process and model the dynamic embedding size\nsearch as a bandit problem. Then, we analyze and quantify the factors that\ninfluence the optimal embedding sizes from the statistics perspective. Based on\nthis, we propose the \\textbf{D}ynamic \\textbf{E}mbedding \\textbf{S}ize\n\\textbf{S}earch (\\textbf{DESS}) method to minimize the embedding size selection\nregret on both user and item sides in a non-stationary manner. Theoretically,\nwe obtain a sublinear regret upper bound superior to previous methods.\nEmpirical results across two recommendation tasks on four public datasets also\ndemonstrate that our approach can achieve better streaming recommendation\nperformance with lower memory cost and higher time efficiency.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Dynamic Hypergraph Recommendation based on\n  Hyper-Relational Knowledge Graph",
        "url": "http://arxiv.org/abs/2308.07752v1",
        "pub_date": "2023-08-15",
        "summary": "Knowledge graphs (KGs) are commonly used as side information to enhance\ncollaborative signals and improve recommendation quality. In the context of\nknowledge-aware recommendation (KGR), graph neural networks (GNNs) have emerged\nas promising solutions for modeling factual and semantic information in KGs.\nHowever, the long-tail distribution of entities leads to sparsity in\nsupervision signals, which weakens the quality of item representation when\nutilizing KG enhancement. Additionally, the binary relation representation of\nKGs simplifies hyper-relational facts, making it challenging to model complex\nreal-world information. Furthermore, the over-smoothing phenomenon results in\nindistinguishable representations and information loss. To address these\nchallenges, we propose the SDK (Self-Supervised Dynamic Hypergraph\nRecommendation based on Hyper-Relational Knowledge Graph) framework. This\nframework establishes a cross-view hypergraph self-supervised learning\nmechanism for KG enhancement. Specifically, we model hyper-relational facts in\nKGs to capture interdependencies between entities under complete semantic\nconditions. With the refined representation, a hypergraph is dynamically\nconstructed to preserve features in the deep vector space, thereby alleviating\nthe over-smoothing problem. Furthermore, we mine external supervision signals\nfrom both the global perspective of the hypergraph and the local perspective of\ncollaborative filtering (CF) to guide the model prediction process. Extensive\nexperiments conducted on different datasets demonstrate the superiority of the\nSDK framework over state-of-the-art models. The results showcase its ability to\nalleviate the effects of over-smoothing and supervision signal sparsity.",
        "translated": ""
    },
    {
        "title": "SPM: Structured Pretraining and Matching Architectures for Relevance\n  Modeling in Meituan Search",
        "url": "http://arxiv.org/abs/2308.07711v1",
        "pub_date": "2023-08-15",
        "summary": "In e-commerce search, relevance between query and documents is an essential\nrequirement for satisfying user experience. Different from traditional\ne-commerce platforms that offer products, users search on life service\nplatforms such as Meituan mainly for product providers, which usually have\nabundant structured information, e.g. name, address, category, thousands of\nproducts. Modeling search relevance with these rich structured contents is\nchallenging due to the following issues: (1) there is language distribution\ndiscrepancy among different fields of structured document, making it difficult\nto directly adopt off-the-shelf pretrained language model based methods like\nBERT. (2) different fields usually have different importance and their length\nvary greatly, making it difficult to extract document information helpful for\nrelevance matching.\n  To tackle these issues, in this paper we propose a novel two-stage\npretraining and matching architecture for relevance matching with rich\nstructured documents. At pretraining stage, we propose an effective pretraining\nmethod that employs both query and multiple fields of document as inputs,\nincluding an effective information compression method for lengthy fields. At\nrelevance matching stage, a novel matching method is proposed by leveraging\ndomain knowledge in search query to generate more effective document\nrepresentations for relevance scoring. Extensive offline experiments and online\nA/B tests on millions of users verify that the proposed architectures\neffectively improve the performance of relevance modeling. The model has\nalready been deployed online, serving the search traffic of Meituan for over a\nyear.",
        "translated": ""
    },
    {
        "title": "Learning from All Sides: Diversified Positive Augmentation via\n  Self-distillation in Recommendation",
        "url": "http://arxiv.org/abs/2308.07629v1",
        "pub_date": "2023-08-15",
        "summary": "Personalized recommendation relies on user historical behaviors to provide\nuser-interested items, and thus seriously struggles with the data sparsity\nissue. A powerful positive item augmentation is beneficial to address the\nsparsity issue, while few works could jointly consider both the accuracy and\ndiversity of these augmented training labels. In this work, we propose a novel\nmodel-agnostic Diversified self-distillation guided positive augmentation\n(DivSPA) for accurate and diverse positive item augmentations. Specifically,\nDivSPA first conducts three types of retrieval strategies to collect\nhigh-quality and diverse positive item candidates according to users' overall\ninterests, short-term intentions, and similar users. Next, a self-distillation\nmodule is conducted to double-check and rerank these candidates as the final\npositive augmentations. Extensive offline and online evaluations verify the\neffectiveness of our proposed DivSPA on both accuracy and diversity. DivSPA is\nsimple and effective, which could be conveniently adapted to other base models\nand systems. Currently, DivSPA has been deployed on multiple widely-used\nreal-world recommender systems.",
        "translated": ""
    },
    {
        "title": "Delphic Costs and Benefits in Web Search: A utilitarian and historical\n  analysis",
        "url": "http://arxiv.org/abs/2308.07525v1",
        "pub_date": "2023-08-15",
        "summary": "We present a new framework to conceptualize and operationalize the total user\nexperience of search, by studying the entirety of a search journey from an\nutilitarian point of view.\n  Web search engines are widely perceived as \"free\". But search requires time\nand effort: in reality there are many intermingled non-monetary costs (e.g.\ntime costs, cognitive costs, interactivity costs) and the benefits may be\nmarred by various impairments, such as misunderstanding and misinformation.\nThis characterization of costs and benefits appears to be inherent to the human\nsearch for information within the pursuit of some larger task: most of the\ncosts and impairments can be identified in interactions with any web search\nengine, interactions with public libraries, and even in interactions with\nancient oracles. To emphasize this innate connection, we call these costs and\nbenefits Delphic, in contrast to explicitly financial costs and benefits.\n  Our main thesis is that the users' satisfaction with a search engine mostly\ndepends on their experience of Delphic cost and benefits, in other words on\ntheir utility. The consumer utility is correlated with classic measures of\nsearch engine quality, such as ranking, precision, recall, etc., but is not\ncompletely determined by them. To argue our thesis, we catalog the Delphic\ncosts and benefits and show how the development of search engines over the last\nquarter century, from classic Information Retrieval roots to the integration of\nLarge Language Models, was driven to a great extent by the quest of decreasing\nDelphic costs and increasing Delphic benefits.\n  We hope that the Delphic costs framework will engender new ideas and new\nresearch for evaluating and improving the web experience for everyone.",
        "translated": ""
    },
    {
        "title": "A Survey on Point-of-Interest Recommendations Leveraging Heterogeneous\n  Data",
        "url": "http://arxiv.org/abs/2308.07426v1",
        "pub_date": "2023-08-14",
        "summary": "Tourism is an important application domain for recommender systems. In this\ndomain, recommender systems are for example tasked with providing personalized\nrecommendations for transportation, accommodation, points-of-interest (POIs),\nor tourism services. Among these tasks, in particular the problem of\nrecommending POIs that are of likely interest to individual tourists has gained\ngrowing attention in recent years. Providing POI recommendations to tourists\n\\emph{during their trip} can however be especially challenging due to the\nvariability of the users' context. With the rapid development of the Web and\ntoday's multitude of online services, vast amounts of data from various sources\nhave become available, and these heterogeneous data sources represent a huge\npotential to better address the challenges of in-trip POI recommendation\nproblems. In this work, we provide a comprehensive survey of published research\non POI recommendation between 2017 and 2022 from the perspective of\nheterogeneous data sources. Specifically, we investigate which types of data\nare used in the literature and which technical approaches and evaluation\nmethods are predominant. Among other aspects, we find that today's research\nworks often focus on a narrow range of data sources, leaving great potential\nfor future works that better utilize heterogeneous data sources and diverse\ndata types for improved in-trip recommendations.",
        "translated": ""
    },
    {
        "title": "A Bi-Step Grounding Paradigm for Large Language Models in Recommendation\n  Systems",
        "url": "http://arxiv.org/abs/2308.08434v1",
        "pub_date": "2023-08-16",
        "summary": "As the focus on Large Language Models (LLMs) in the field of recommendation\nintensifies, the optimization of LLMs for recommendation purposes (referred to\nas LLM4Rec) assumes a crucial role in augmenting their effectiveness in\nproviding recommendations. However, existing approaches for LLM4Rec often\nassess performance using restricted sets of candidates, which may not\naccurately reflect the models' overall ranking capabilities. In this paper, our\nobjective is to investigate the comprehensive ranking capacity of LLMs and\npropose a two-step grounding framework known as BIGRec (Bi-step Grounding\nParadigm for Recommendation). It initially grounds LLMs to the recommendation\nspace by fine-tuning them to generate meaningful tokens for items and\nsubsequently identifies appropriate actual items that correspond to the\ngenerated tokens. By conducting extensive experiments on two datasets, we\nsubstantiate the superior performance, capacity for handling few-shot\nscenarios, and versatility across multiple domains exhibited by BIGRec.\nFurthermore, we observe that the marginal benefits derived from increasing the\nquantity of training samples are modest for BIGRec, implying that LLMs possess\nthe limited capability to assimilate statistical information, such as\npopularity and collaborative filtering, due to their robust semantic priors.\nThese findings also underline the efficacy of integrating diverse statistical\ninformation into the LLM4Rec framework, thereby pointing towards a potential\navenue for future research. Our code and data are available at\nhttps://github.com/SAI990323/Grounding4Rec.",
        "translated": ""
    },
    {
        "title": "Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value\n  Extraction",
        "url": "http://arxiv.org/abs/2308.08413v1",
        "pub_date": "2023-08-16",
        "summary": "Existing attribute-value extraction (AVE) models require large quantities of\nlabeled data for training. However, new products with new attribute-value pairs\nenter the market every day in real-world e-Commerce. Thus, we formulate AVE in\nmulti-label few-shot learning (FSL), aiming to extract unseen attribute value\npairs based on a small number of training examples. We propose a\nKnowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks,\nleveraging the generated label description and category information to learn\nmore discriminative prototypes. Besides, KEAF integrates with hybrid attention\nto reduce noise and capture more informative semantics for each class by\ncalculating the label-relevant and query-related weights. To achieve\nmulti-label inference, KEAF further learns a dynamic threshold by integrating\nthe semantic information from both the support set and the query set. Extensive\nexperiments with ablation studies conducted on two datasets demonstrate that\nKEAF outperforms other SOTA models for information extraction in FSL. The code\ncan be found at: https://github.com/gjiaying/KEAF",
        "translated": ""
    },
    {
        "title": "Content-based Recommendation Engine for Video Streaming Platform",
        "url": "http://arxiv.org/abs/2308.08406v1",
        "pub_date": "2023-08-16",
        "summary": "Recommendation engine suggest content, product or services to the user by\nusing machine learning algorithm. This paper proposed a content-based\nrecommendation engine for providing video suggestion to the user based on their\nprevious interests and choices. We will use TF-IDF text vectorization method to\ndetermine the relevance of words in a document. Then we will find out the\nsimilarity between each content by calculating cosine similarity between them.\nFinally, engine will recommend videos to the users based on the obtained\nsimilarity score value. In addition, we will measure the engine's performance\nby computing precision, recall, and F1 core of the proposed system.",
        "translated": ""
    },
    {
        "title": "Advancing continual lifelong learning in neural information retrieval:\n  definition, dataset, framework, and empirical evaluation",
        "url": "http://arxiv.org/abs/2308.08378v1",
        "pub_date": "2023-08-16",
        "summary": "Continual learning refers to the capability of a machine learning model to\nlearn and adapt to new information, without compromising its performance on\npreviously learned tasks. Although several studies have investigated continual\nlearning methods for information retrieval tasks, a well-defined task\nformulation is still lacking, and it is unclear how typical learning strategies\nperform in this context. To address this challenge, a systematic task\nformulation of continual neural information retrieval is presented, along with\na multiple-topic dataset that simulates continuous information retrieval. A\ncomprehensive continual neural information retrieval framework consisting of\ntypical retrieval models and continual learning strategies is then proposed.\nEmpirical evaluations illustrate that the proposed framework can successfully\nprevent catastrophic forgetting in neural information retrieval and enhance\nperformance on previously learned tasks. The results indicate that\nembedding-based retrieval models experience a decline in their continual\nlearning performance as the topic shift distance and dataset volume of new\ntasks increase. In contrast, pretraining-based models do not show any such\ncorrelation. Adopting suitable learning strategies can mitigate the effects of\ntopic shift and data augmentation.",
        "translated": ""
    },
    {
        "title": "Is Meta-Learning the Right Approach for the Cold-Start Problem in\n  Recommender Systems?",
        "url": "http://arxiv.org/abs/2308.08354v1",
        "pub_date": "2023-08-16",
        "summary": "Recommender systems have become fundamental building blocks of modern online\nproducts and services, and have a substantial impact on user experience. In the\npast few years, deep learning methods have attracted a lot of research, and are\nnow heavily used in modern real-world recommender systems. Nevertheless,\ndealing with recommendations in the cold-start setting, e.g., when a user has\ndone limited interactions in the system, is a problem that remains far from\nsolved. Meta-learning techniques, and in particular optimization-based\nmeta-learning, have recently become the most popular approaches in the academic\nresearch literature for tackling the cold-start problem in deep learning models\nfor recommender systems. However, current meta-learning approaches are not\npractical for real-world recommender systems, which have billions of users and\nitems, and strict latency requirements. In this paper we show that it is\npossible to obtaining similar, or higher, performance on commonly used\nbenchmarks for the cold-start problem without using meta-learning techniques.\nIn more detail, we show that, when tuned correctly, standard and widely adopted\ndeep learning models perform just as well as newer meta-learning models. We\nfurther show that an extremely simple modular approach using common\nrepresentation learning techniques, can perform comparably to meta-learning\ntechniques specifically designed for the cold-start setting while being much\nmore easily deployable in real-world applications.",
        "translated": ""
    },
    {
        "title": "Phase Retrieval with Background Information: Decreased References and\n  Efficient Methods",
        "url": "http://arxiv.org/abs/2308.08328v1",
        "pub_date": "2023-08-16",
        "summary": "Fourier phase retrieval(PR) is a severely ill-posed inverse problem that\narises in various applications. To guarantee a unique solution and relieve the\ndependence on the initialization, background information can be exploited as a\nstructural priors. However, the requirement for the background information may\nbe challenging when moving to the high-resolution imaging. At the same time,\nthe previously proposed projected gradient descent(PGD) method also demands\nmuch background information.\n  In this paper, we present an improved theoretical result about the demand for\nthe background information, along with two Douglas Rachford(DR) based methods.\nAnalytically, we demonstrate that the background required to ensure a unique\nsolution can be decreased by nearly $1/2$ for the 2-D signals compared to the\n1-D signals. By generalizing the results into $d$-dimension, we show that the\nlength of the background information more than $(2^{\\frac{d+1}{d}}-1)$ folds of\nthe signal is sufficient to ensure the uniqueness. At the same time, we also\nanalyze the stability and robustness of the model when measurements and\nbackground information are corrupted by the noise. Furthermore, two methods\ncalled Background Douglas-Rachford (BDR) and Convex Background Douglas-Rachford\n(CBDR) are proposed. BDR which is a kind of non-convex method is proven to have\nthe local R-linear convergence rate under mild assumptions. Instead, CBDR\nmethod uses the techniques of convexification and can be proven to own a global\nconvergence guarantee as long as the background information is sufficient. To\nsupport this, a new property called F-RIP is established. We test the\nperformance of the proposed methods through simulations as well as real\nexperimental measurements, and demonstrate that they achieve a higher recovery\nrate with less background information compared to the PGD method.",
        "translated": ""
    },
    {
        "title": "Pre-training with Large Language Model-based Document Expansion for\n  Dense Passage Retrieval",
        "url": "http://arxiv.org/abs/2308.08285v1",
        "pub_date": "2023-08-16",
        "summary": "In this paper, we systematically study the potential of pre-training with\nLarge Language Model(LLM)-based document expansion for dense passage retrieval.\nConcretely, we leverage the capabilities of LLMs for document expansion, i.e.\nquery generation, and effectively transfer expanded knowledge to retrievers\nusing pre-training strategies tailored for passage retrieval. These strategies\ninclude contrastive learning and bottlenecked query generation. Furthermore, we\nincorporate a curriculum learning strategy to reduce the reliance on LLM\ninferences. Experimental results demonstrate that pre-training with LLM-based\ndocument expansion significantly boosts the retrieval performance on\nlarge-scale web-search tasks. Our work shows strong zero-shot and out-of-domain\nretrieval abilities, making it more widely applicable for retrieval when\ninitializing with no human-labeled data.",
        "translated": ""
    },
    {
        "title": "Uncovering User Interest from Biased and Noised Watch Time in Video\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.08120v1",
        "pub_date": "2023-08-16",
        "summary": "In the video recommendation, watch time is commonly adopted as an indicator\nof user interest. However, watch time is not only influenced by the matching of\nusers' interests but also by other factors, such as duration bias and noisy\nwatching. Duration bias refers to the tendency for users to spend more time on\nvideos with longer durations, regardless of their actual interest level. Noisy\nwatching, on the other hand, describes users taking time to determine whether\nthey like a video or not, which can result in users spending time watching\nvideos they do not like. Consequently, the existence of duration bias and noisy\nwatching make watch time an inadequate label for indicating user interest.\nFurthermore, current methods primarily address duration bias and ignore the\nimpact of noisy watching, which may limit their effectiveness in uncovering\nuser interest from watch time. In this study, we first analyze the generation\nmechanism of users' watch time from a unified causal viewpoint. Specifically,\nwe considered the watch time as a mixture of the user's actual interest level,\nthe duration-biased watch time, and the noisy watch time. To mitigate both the\nduration bias and noisy watching, we propose Debiased and Denoised watch time\nCorrection (D$^2$Co), which can be divided into two steps: First, we employ a\nduration-wise Gaussian Mixture Model plus frequency-weighted moving average for\nestimating the bias and noise terms; then we utilize a sensitivity-controlled\ncorrection function to separate the user interest from the watch time, which is\nrobust to the estimation error of bias and noise terms. The experiments on two\npublic video recommendation datasets and online A/B testing indicate the\neffectiveness of the proposed method.",
        "translated": ""
    },
    {
        "title": "Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme\n  Detection",
        "url": "http://arxiv.org/abs/2308.08088v1",
        "pub_date": "2023-08-16",
        "summary": "Hateful meme detection is a challenging multimodal task that requires\ncomprehension of both vision and language, as well as cross-modal interactions.\nRecent studies have tried to fine-tune pre-trained vision-language models\n(PVLMs) for this task. However, with increasing model sizes, it becomes\nimportant to leverage powerful PVLMs more efficiently, rather than simply\nfine-tuning them. Recently, researchers have attempted to convert meme images\ninto textual captions and prompt language models for predictions. This approach\nhas shown good performance but suffers from non-informative image captions.\nConsidering the two factors mentioned above, we propose a probing-based\ncaptioning approach to leverage PVLMs in a zero-shot visual question answering\n(VQA) manner. Specifically, we prompt a frozen PVLM by asking hateful\ncontent-related questions and use the answers as image captions (which we call\nPro-Cap), so that the captions contain information critical for hateful content\ndetection. The good performance of models with Pro-Cap on three benchmarks\nvalidates the effectiveness and generalization of the proposed method.",
        "translated": ""
    },
    {
        "title": "Decentralized Graph Neural Network for Privacy-Preserving Recommendation",
        "url": "http://arxiv.org/abs/2308.08072v1",
        "pub_date": "2023-08-15",
        "summary": "Building a graph neural network (GNN)-based recommender system without\nviolating user privacy proves challenging. Existing methods can be divided into\nfederated GNNs and decentralized GNNs. But both methods have undesirable\neffects, i.e., low communication efficiency and privacy leakage. This paper\nproposes DGREC, a novel decentralized GNN for privacy-preserving\nrecommendations, where users can choose to publicize their interactions. It\nincludes three stages, i.e., graph construction, local gradient calculation,\nand global gradient passing. The first stage builds a local inner-item\nhypergraph for each user and a global inter-user graph. The second stage models\nuser preference and calculates gradients on each local device. The third stage\ndesigns a local differential privacy mechanism named secure gradient-sharing,\nwhich proves strong privacy-preserving of users' private data. We conduct\nextensive experiments on three public datasets to validate the consistent\nsuperiority of our framework.",
        "translated": ""
    },
    {
        "title": "MUSE: Music Recommender System with Shuffle Play Recommendation\n  Enhancement",
        "url": "http://arxiv.org/abs/2308.09649v1",
        "pub_date": "2023-08-18",
        "summary": "Recommender systems have become indispensable in music streaming services,\nenhancing user experiences by personalizing playlists and facilitating the\nserendipitous discovery of new music. However, the existing recommender systems\noverlook the unique challenges inherent in the music domain, specifically\nshuffle play, which provides subsequent tracks in a random sequence. Based on\nour observation that the shuffle play sessions hinder the overall training\nprocess of music recommender systems mainly due to the high unique transition\nrates of shuffle play sessions, we propose a Music Recommender System with\nShuffle Play Recommendation Enhancement (MUSE). MUSE employs the\nself-supervised learning framework that maximizes the agreement between the\noriginal session and the augmented session, which is augmented by our novel\nsession augmentation method, called transition-based augmentation. To further\nfacilitate the alignment of the representations between the two views, we\ndevise two fine-grained matching strategies, i.e., item- and similarity-based\nmatching strategies. Through rigorous experiments conducted across diverse\nenvironments, we demonstrate MUSE's efficacy over 12 baseline models on a\nlarge-scale Music Streaming Sessions Dataset (MSSD) from Spotify. The source\ncode of MUSE is available at \\url{https://github.com/yunhak0/MUSE}.",
        "translated": ""
    },
    {
        "title": "ReCon: Reducing Congestion in Job Recommendation using Optimal Transport",
        "url": "http://arxiv.org/abs/2308.09516v1",
        "pub_date": "2023-08-18",
        "summary": "Recommender systems may suffer from congestion, meaning that there is an\nunequal distribution of the items in how often they are recommended. Some items\nmay be recommended much more than others. Recommenders are increasingly used in\ndomains where items have limited availability, such as the job market, where\ncongestion is especially problematic: Recommending a vacancy -- for which\ntypically only one person will be hired -- to a large number of job seekers may\nlead to frustration for job seekers, as they may be applying for jobs where\nthey are not hired. This may also leave vacancies unfilled and result in job\nmarket inefficiency.\n  We propose a novel approach to job recommendation called ReCon, accounting\nfor the congestion problem. Our approach is to use an optimal transport\ncomponent to ensure a more equal spread of vacancies over job seekers, combined\nwith a job recommendation model in a multi-objective optimization problem. We\nevaluated our approach on two real-world job market datasets. The evaluation\nresults show that ReCon has good performance on both congestion-related (e.g.,\nCongestion) and desirability (e.g., NDCG) measures.",
        "translated": ""
    },
    {
        "title": "Attention Calibration for Transformer-based Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.09419v1",
        "pub_date": "2023-08-18",
        "summary": "Transformer-based sequential recommendation (SR) has been booming in recent\nyears, with the self-attention mechanism as its key component. Self-attention\nhas been widely believed to be able to effectively select those informative and\nrelevant items from a sequence of interacted items for next-item prediction via\nlearning larger attention weights for these items. However, this may not always\nbe true in reality. Our empirical analysis of some representative\nTransformer-based SR models reveals that it is not uncommon for large attention\nweights to be assigned to less relevant items, which can result in inaccurate\nrecommendations. Through further in-depth analysis, we find two factors that\nmay contribute to such inaccurate assignment of attention weights: sub-optimal\nposition encoding and noisy input. To this end, in this paper, we aim to\naddress this significant yet challenging gap in existing works. To be specific,\nwe propose a simple yet effective framework called Attention Calibration for\nTransformer-based Sequential Recommendation (AC-TSR). In AC-TSR, a novel\nspatial calibrator and adversarial calibrator are designed respectively to\ndirectly calibrates those incorrectly assigned attention weights. The former is\ndevised to explicitly capture the spatial relationships (i.e., order and\ndistance) among items for more precise calculation of attention weights. The\nlatter aims to redistribute the attention weights based on each item's\ncontribution to the next-item prediction. AC-TSR is readily adaptable and can\nbe seamlessly integrated into various existing transformer-based SR models.\nExtensive experimental results on four benchmark real-world datasets\ndemonstrate the superiority of our proposed ACTSR via significant\nrecommendation performance enhancements. The source code is available at\nhttps://github.com/AIM-SE/AC-TSR.",
        "translated": ""
    },
    {
        "title": "SHARK: A Lightweight Model Compression Approach for Large-scale\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2308.09395v1",
        "pub_date": "2023-08-18",
        "summary": "Increasing the size of embedding layers has shown to be effective in\nimproving the performance of recommendation models, yet gradually causing their\nsizes to exceed terabytes in industrial recommender systems, and hence the\nincrease of computing and storage costs. To save resources while maintaining\nmodel performances, we propose SHARK, the model compression practice we have\nsummarized in the recommender system of industrial scenarios. SHARK consists of\ntwo main components. First, we use the novel first-order component of Taylor\nexpansion as importance scores to prune the number of embedding tables (feature\nfields). Second, we introduce a new row-wise quantization method to apply\ndifferent quantization strategies to each embedding. We conduct extensive\nexperiments on both public and industrial datasets, demonstrating that each\ncomponent of our proposed SHARK framework outperforms previous approaches. We\nconduct A/B tests in multiple models on Kuaishou, such as short video,\ne-commerce, and advertising recommendation models. The results of the online\nA/B test showed SHARK can effectively reduce the memory footprint of the\nembedded layer. For the short-video scenarios, the compressed model without any\nperformance drop significantly saves 70% storage and thousands of machines,\nimproves 30\\% queries per second (QPS), and has been deployed to serve hundreds\nof millions of users and process tens of billions of requests every day.",
        "translated": ""
    },
    {
        "title": "How Discriminative Are Your Qrels? How To Study the Statistical\n  Significance of Document Adjudication Methods",
        "url": "http://arxiv.org/abs/2308.09340v1",
        "pub_date": "2023-08-18",
        "summary": "Creating test collections for offline retrieval evaluation requires human\neffort to judge documents' relevance. This expensive activity motivated much\nwork in developing methods for constructing benchmarks with fewer assessment\ncosts. In this respect, adjudication methods actively decide both which\ndocuments and the order in which experts review them, in order to better\nexploit the assessment budget or to lower it. Researchers evaluate the quality\nof those methods by measuring the correlation between the known gold ranking of\nsystems under the full collection and the observed ranking of systems under the\nlower-cost one. This traditional analysis ignores whether and how the low-cost\njudgements impact on the statistically significant differences among systems\nwith respect to the full collection. We fill this void by proposing a novel\nmethodology to evaluate how the low-cost adjudication methods preserve the\npairwise significant differences between systems as the full collection. In\nother terms, while traditional approaches look for stability in answering the\nquestion \"is system A better than system B?\", our proposed approach looks for\nstability in answering the question \"is system A significantly better than\nsystem B?\", which is the ultimate questions researchers need to answer to\nguarantee the generalisability of their results. Among other results, we found\nthat the best methods in terms of ranking of systems correlation do not always\nmatch those preserving statistical significance.",
        "translated": ""
    },
    {
        "title": "Meta-learning enhanced next POI recommendation by leveraging check-ins\n  from auxiliary cities",
        "url": "http://arxiv.org/abs/2308.09309v1",
        "pub_date": "2023-08-18",
        "summary": "Most existing point-of-interest (POI) recommenders aim to capture user\npreference by employing city-level user historical check-ins, thus facilitating\nusers' exploration of the city. However, the scarcity of city-level user\ncheck-ins brings a significant challenge to user preference learning. Although\nprior studies attempt to mitigate this challenge by exploiting various context\ninformation, e.g., spatio-temporal information, they ignore to transfer the\nknowledge (i.e., common behavioral pattern) from other relevant cities (i.e.,\nauxiliary cities). In this paper, we investigate the effect of knowledge\ndistilled from auxiliary cities and thus propose a novel Meta-learning Enhanced\nnext POI Recommendation framework (MERec). The MERec leverages the correlation\nof check-in behaviors among various cities into the meta-learning paradigm to\nhelp infer user preference in the target city, by holding the principle of\n\"paying more attention to more correlated knowledge\". Particularly, a\ncity-level correlation strategy is devised to attentively capture common\npatterns among cities, so as to transfer more relevant knowledge from more\ncorrelated cities. Extensive experiments verify the superiority of the proposed\nMERec against state-of-the-art algorithms.",
        "translated": ""
    },
    {
        "title": "Differentiable Retrieval Augmentation via Generative Language Modeling\n  for E-commerce Query Intent Classification",
        "url": "http://arxiv.org/abs/2308.09308v1",
        "pub_date": "2023-08-18",
        "summary": "Retrieval augmentation, which enhances downstream models by a knowledge\nretriever and an external corpus instead of by merely increasing the number of\nmodel parameters, has been successfully applied to many natural language\nprocessing (NLP) tasks such as text classification, question answering and so\non. However, existing methods that separately or asynchronously train the\nretriever and downstream model mainly due to the non-differentiability between\nthe two parts, usually lead to degraded performance compared to end-to-end\njoint training.",
        "translated": ""
    },
    {
        "title": "Graph-based Alignment and Uniformity for Recommendation",
        "url": "http://arxiv.org/abs/2308.09292v1",
        "pub_date": "2023-08-18",
        "summary": "Collaborative filtering-based recommender systems (RecSys) rely on learning\nrepresentations for users and items to predict preferences accurately.\nRepresentation learning on the hypersphere is a promising approach due to its\ndesirable properties, such as alignment and uniformity. However, the sparsity\nissue arises when it encounters RecSys. To address this issue, we propose a\nnovel approach, graph-based alignment and uniformity (GraphAU), that explicitly\nconsiders high-order connectivities in the user-item bipartite graph. GraphAU\naligns the user/item embedding to the dense vector representations of\nhigh-order neighbors using a neighborhood aggregator, eliminating the need to\ncompute the burdensome alignment to high-order neighborhoods individually. To\naddress the discrepancy in alignment losses, GraphAU includes a layer-wise\nalignment pooling module to integrate alignment losses layer-wise. Experiments\non four datasets show that GraphAU significantly alleviates the sparsity issue\nand achieves state-of-the-art performance. We open-source GraphAU at\nhttps://github.com/YangLiangwei/GraphAU.",
        "translated": ""
    },
    {
        "title": "A Model-Agnostic Framework for Recommendation via Interest-aware Item\n  Embeddings",
        "url": "http://arxiv.org/abs/2308.09202v1",
        "pub_date": "2023-08-17",
        "summary": "Item representation holds significant importance in recommendation systems,\nwhich encompasses domains such as news, retail, and videos. Retrieval and\nranking models utilise item representation to capture the user-item\nrelationship based on user behaviours. While existing representation learning\nmethods primarily focus on optimising item-based mechanisms, such as attention\nand sequential modelling. However, these methods lack a modelling mechanism to\ndirectly reflect user interests within the learned item representations.\nConsequently, these methods may be less effective in capturing user interests\nindirectly. To address this challenge, we propose a novel Interest-aware\nCapsule network (IaCN) recommendation model, a model-agnostic framework that\ndirectly learns interest-oriented item representations. IaCN serves as an\nauxiliary task, enabling the joint learning of both item-based and\ninterest-based representations. This framework adopts existing recommendation\nmodels without requiring substantial redesign. We evaluate the proposed\napproach on benchmark datasets, exploring various scenarios involving different\ndeep neural networks, behaviour sequence lengths, and joint learning ratios of\ninterest-oriented item representations. Experimental results demonstrate\nsignificant performance enhancements across diverse recommendation models,\nvalidating the effectiveness of our approach.",
        "translated": ""
    },
    {
        "title": "Identity-Aware Semi-Supervised Learning for Comic Character\n  Re-Identification",
        "url": "http://arxiv.org/abs/2308.09096v1",
        "pub_date": "2023-08-17",
        "summary": "Character re-identification, recognizing characters consistently across\ndifferent panels in comics, presents significant challenges due to limited\nannotated data and complex variations in character appearances. To tackle this\nissue, we introduce a robust semi-supervised framework that combines metric\nlearning with a novel 'Identity-Aware' self-supervision method by contrastive\nlearning of face and body pairs of characters. Our approach involves processing\nboth facial and bodily features within a unified network architecture,\nfacilitating the extraction of identity-aligned character embeddings that\ncapture individual identities while preserving the effectiveness of face and\nbody features. This integrated character representation enhances feature\nextraction and improves character re-identification compared to\nre-identification by face or body independently, offering a parameter-efficient\nsolution. By extensively validating our method using in-series and inter-series\nevaluation metrics, we demonstrate its effectiveness in consistently\nre-identifying comic characters. Compared to existing methods, our approach not\nonly addresses the challenge of character re-identification but also serves as\na foundation for downstream tasks since it can produce character embeddings\nwithout restrictions of face and body availability, enriching the comprehension\nof comic books. In our experiments, we leverage two newly curated datasets: the\n'Comic Character Instances Dataset', comprising over a million character\ninstances and the 'Comic Sequence Identity Dataset', containing annotations of\nidentities within more than 3000 sets of four consecutive comic panels that we\ncollected.",
        "translated": ""
    },
    {
        "title": "Leveraging Large Language Models for Pre-trained Recommender Systems",
        "url": "http://arxiv.org/abs/2308.10837v1",
        "pub_date": "2023-08-21",
        "summary": "Recent advancements in recommendation systems have shifted towards more\ncomprehensive and personalized recommendations by utilizing large language\nmodels (LLM). However, effectively integrating LLM's commonsense knowledge and\nreasoning abilities into recommendation systems remains a challenging problem.\nIn this paper, we propose RecSysLLM, a novel pre-trained recommendation model\nbased on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating\nrecommendation domain knowledge through unique designs of data, training, and\ninference. This allows RecSysLLM to leverage LLMs' capabilities for\nrecommendation tasks in an efficient, unified framework. We demonstrate the\neffectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM\nprovides a promising approach to developing unified recommendation systems by\nfully exploiting the power of pre-trained language models.",
        "translated": ""
    },
    {
        "title": "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
        "url": "http://arxiv.org/abs/2308.10835v1",
        "pub_date": "2023-08-21",
        "summary": "Recommendation systems aim to provide users with relevant suggestions, but\noften lack interpretability and fail to capture higher-level semantic\nrelationships between user behaviors and profiles. In this paper, we propose a\nnovel approach that leverages large language models (LLMs) to construct\npersonalized reasoning graphs. These graphs link a user's profile and\nbehavioral sequences through causal and logical inferences, representing the\nuser's interests in an interpretable way. Our approach, LLM reasoning graphs\n(LLMRG), has four components: chained graph reasoning, divergent extension,\nself-verification and scoring, and knowledge base self-improvement. The\nresulting reasoning graph is encoded using graph neural networks, which serves\nas additional input to improve conventional recommender systems, without\nrequiring extra user or item information. Our approach demonstrates how LLMs\ncan enable more logical and interpretable recommender systems through\npersonalized reasoning graphs. LLMRG allows recommendations to benefit from\nboth engineered recommendation systems and LLM-derived reasoning graphs. We\ndemonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios\nin enhancing base recommendation models.",
        "translated": ""
    },
    {
        "title": "DynED: Dynamic Ensemble Diversification in Data Stream Classification",
        "url": "http://arxiv.org/abs/2308.10807v1",
        "pub_date": "2023-08-21",
        "summary": "Ensemble methods are commonly used in classification due to their remarkable\nperformance. Achieving high accuracy in a data stream environment is a\nchallenging task considering disruptive changes in the data distribution, also\nknown as concept drift. A greater diversity of ensemble components is known to\nenhance prediction accuracy in such settings. Despite the diversity of\ncomponents within an ensemble, not all contribute as expected to its overall\nperformance. This necessitates a method for selecting components that exhibit\nhigh performance and diversity. We present a novel ensemble construction and\nmaintenance approach based on MMR (Maximal Marginal Relevance) that dynamically\ncombines the diversity and prediction accuracy of components during the process\nof structuring an ensemble. The experimental results on both four real and 11\nsynthetic datasets demonstrate that the proposed approach (DynED) provides a\nhigher average mean accuracy compared to the five state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "LSCPM: communities in massive real-world Link Streams by Clique\n  Percolation Method",
        "url": "http://arxiv.org/abs/2308.10801v1",
        "pub_date": "2023-08-21",
        "summary": "Community detection is a popular approach to understand the organization of\ninteractions in static networks. For that purpose, the Clique Percolation\nMethod (CPM), which involves the percolation of k-cliques, is a well-studied\ntechnique that offers several advantages. Besides, studying interactions that\noccur over time is useful in various contexts, which can be modeled by the link\nstream formalism. The Dynamic Clique Percolation Method (DCPM) has been\nproposed for extending CPM to temporal networks.\n  However, existing implementations are unable to handle massive datasets. We\npresent a novel algorithm that adapts CPM to link streams, which has the\nadvantage that it allows us to speed up the computation time with respect to\nthe existing DCPM method. We evaluate it experimentally on real datasets and\nshow that it scales to massive link streams. For example, it allows to obtain a\ncomplete set of communities in under twenty-five minutes for a dataset with\nthirty million links, what the state of the art fails to achieve even after a\nweek of computation. We further show that our method provides communities\nsimilar to DCPM, but slightly more aggregated. We exhibit the relevance of the\nobtained communities in real world cases, and show that they provide\ninformation on the importance of vertices in the link streams.",
        "translated": ""
    },
    {
        "title": "A Topology-aware Analysis of Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2308.10778v1",
        "pub_date": "2023-08-21",
        "summary": "The successful integration of graph neural networks into recommender systems\n(RSs) has led to a novel paradigm in collaborative filtering (CF), graph\ncollaborative filtering (graph CF). By representing user-item data as an\nundirected, bipartite graph, graph CF utilizes short- and long-range\nconnections to extract collaborative signals that yield more accurate user\npreferences than traditional CF methods. Although the recent literature\nhighlights the efficacy of various algorithmic strategies in graph CF, the\nimpact of datasets and their topological features on recommendation performance\nis yet to be studied. To fill this gap, we propose a topology-aware analysis of\ngraph CF. In this study, we (i) take some widely-adopted recommendation\ndatasets and use them to generate a large set of synthetic sub-datasets through\ntwo state-of-the-art graph sampling methods, (ii) measure eleven of their\nclassical and topological characteristics, and (iii) estimate the accuracy\ncalculated on the generated sub-datasets considering four popular and recent\ngraph-based RSs (i.e., LightGCN, DGCF, UltraGCN, and SVD-GCN). Finally, the\ninvestigation presents an explanatory framework that reveals the linear\nrelationships between characteristics and accuracy measures. The results,\nstatistically validated under different graph sampling settings, confirm the\nexistence of solid dependencies between topological characteristics and\naccuracy in the graph-based recommendation, offering a new perspective on how\nto interpret graph CF.",
        "translated": ""
    },
    {
        "title": "DepreSym: A Depression Symptom Annotated Corpus and the Role of LLMs as\n  Assessors of Psychological Markers",
        "url": "http://arxiv.org/abs/2308.10758v1",
        "pub_date": "2023-08-21",
        "summary": "Computational methods for depression detection aim to mine traces of\ndepression from online publications posted by Internet users. However,\nsolutions trained on existing collections exhibit limited generalisation and\ninterpretability. To tackle these issues, recent studies have shown that\nidentifying depressive symptoms can lead to more robust models. The eRisk\ninitiative fosters research on this area and has recently proposed a new\nranking task focused on developing search methods to find sentences related to\ndepressive symptoms. This search challenge relies on the symptoms specified by\nthe Beck Depression Inventory-II (BDI-II), a questionnaire widely used in\nclinical practice. Based on the participant systems' results, we present the\nDepreSym dataset, consisting of 21580 sentences annotated according to their\nrelevance to the 21 BDI-II symptoms. The labelled sentences come from a pool of\ndiverse ranking methods, and the final dataset serves as a valuable resource\nfor advancing the development of models that incorporate depressive markers\nsuch as clinical symptoms. Due to the complex nature of this relevance\nannotation, we designed a robust assessment methodology carried out by three\nexpert assessors (including an expert psychologist). Additionally, we explore\nhere the feasibility of employing recent Large Language Models (ChatGPT and\nGPT4) as potential assessors in this complex task. We undertake a comprehensive\nexamination of their performance, determine their main limitations and analyze\ntheir role as a complement or replacement for human annotators.",
        "translated": ""
    },
    {
        "title": "Contrastive Graph Prompt-tuning for Cross-domain Recommendation",
        "url": "http://arxiv.org/abs/2308.10685v1",
        "pub_date": "2023-08-21",
        "summary": "Recommender systems are frequently challenged by the data sparsity problem.\nOne approach to mitigate this issue is through cross-domain recommendation\ntechniques. In a cross-domain context, sharing knowledge between domains can\nenhance the effectiveness in the target domain. Recent cross-domain methods\nhave employed a pre-training approach, but we argue that these methods often\nresult in suboptimal fine-tuning, especially with large neural models. Modern\nlanguage models utilize prompts for efficient model tuning. Such prompts act as\na tunable latent vector, allowing for the freezing of the main model\nparameters. In our research, we introduce the Personalised Graph Prompt-based\nRecommendation (PGPRec) framework. This leverages the advantages of\nprompt-tuning. Within this framework, we formulate personalized graph prompts\nitem-wise, rooted in items that a user has previously engaged with.\nSpecifically, we employ Contrastive Learning (CL) to produce pre-trained\nembeddings that offer greater generalizability in the pre-training phase,\nensuring robust training during the tuning phase. Our evaluation of PGPRec in\ncross-domain scenarios involves comprehensive testing with the top-k\nrecommendation tasks and a cold-start analysis. Our empirical findings, based\non four Amazon Review datasets, reveal that the PGPRec framework can decrease\nthe tuned parameters by as much as 74%, maintaining competitive performance.\nRemarkably, there's an 11.41% enhancement in performance against the leading\nbaseline in cold-start situations.",
        "translated": ""
    },
    {
        "title": "Evaluating Temporal Persistence Using Replicability Measures",
        "url": "http://arxiv.org/abs/2308.10549v1",
        "pub_date": "2023-08-21",
        "summary": "In real-world Information Retrieval (IR) experiments, the Evaluation\nEnvironment (EE) is exposed to constant change. Documents are added, removed,\nor updated, and the information need and the search behavior of users is\nevolving. Simultaneously, IR systems are expected to retain a consistent\nquality. The LongEval Lab seeks to investigate the longitudinal persistence of\nIR systems, and in this work, we describe our participation. We submitted runs\nof five advanced retrieval systems, namely a Reciprocal Rank Fusion (RRF)\napproach, ColBERT, monoT5, Doc2Query, and E5, to both sub-tasks. Further, we\ncast the longitudinal evaluation as a replicability study to better understand\nthe temporal change observed. As a result, we quantify the persistence of the\nsubmitted runs and see great potential in this evaluation method.",
        "translated": ""
    },
    {
        "title": "DPAN: Dynamic Preference-based and Attribute-aware Network for Relevant\n  Recommendations",
        "url": "http://arxiv.org/abs/2308.10527v1",
        "pub_date": "2023-08-21",
        "summary": "In e-commerce platforms, the relevant recommendation is a unique scenario\nproviding related items for a trigger item that users are interested in.\nHowever, users' preferences for the similarity and diversity of recommendation\nresults are dynamic and vary under different conditions. Moreover, individual\nitem-level diversity is too coarse-grained since all recommended items are\nrelated to the trigger item. Thus, the two main challenges are to learn\nfine-grained representations of similarity and diversity and capture users'\ndynamic preferences for them under different conditions. To address these\nchallenges, we propose a novel method called the Dynamic Preference-based and\nAttribute-aware Network (DPAN) for predicting Click-Through Rate (CTR) in\nrelevant recommendations. Specifically, based on Attribute-aware Activation\nValues Generation (AAVG), Bi-dimensional Compression-based Re-expression (BCR)\nis designed to obtain similarity and diversity representations of user\ninterests and item information. Then Shallow and Deep Union-based Fusion (SDUF)\nis proposed to capture users' dynamic preferences for the diverse degree of\nrecommendation results according to various conditions. DPAN has demonstrated\nits effectiveness through extensive offline experiments and online A/B testing,\nresulting in a significant 7.62% improvement in CTR. Currently, DPAN has been\nsuccessfully deployed on our e-commerce platform serving the primary traffic\nfor relevant recommendations. The code of DPAN has been made publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Single-User Injection for Invisible Shilling Attack against Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2308.10467v1",
        "pub_date": "2023-08-21",
        "summary": "Recommendation systems (RS) are crucial for alleviating the information\noverload problem. Due to its pivotal role in guiding users to make decisions,\nunscrupulous parties are lured to launch attacks against RS to affect the\ndecisions of normal users and gain illegal profits. Among various types of\nattacks, shilling attack is one of the most subsistent and profitable attacks.\nIn shilling attack, an adversarial party injects a number of well-designed fake\nuser profiles into the system to mislead RS so that the attack goal can be\nachieved. Although existing shilling attack methods have achieved promising\nresults, they all adopt the attack paradigm of multi-user injection, where some\nfake user profiles are required. This paper provides the first study of\nshilling attack in an extremely limited scenario: only one fake user profile is\ninjected into the victim RS to launch shilling attacks (i.e., single-user\ninjection). We propose a novel single-user injection method SUI-Attack for\ninvisible shilling attack. SUI-Attack is a graph based attack method that\nmodels shilling attack as a node generation task over the user-item bipartite\ngraph of the victim RS, and it constructs the fake user profile by generating\nuser features and edges that link the fake user to items. Extensive experiments\ndemonstrate that SUI-Attack can achieve promising attack results in single-user\ninjection. In addition to its attack power, SUI-Attack increases the\nstealthiness of shilling attack and reduces the risk of being detected. We\nprovide our implementation at: https://github.com/KDEGroup/SUI-Attack.",
        "translated": ""
    },
    {
        "title": "Multi-event Video-Text Retrieval",
        "url": "http://arxiv.org/abs/2308.11551v1",
        "pub_date": "2023-08-22",
        "summary": "Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive\nvideo-text data on the Internet. A plethora of work characterized by using a\ntwo-stream Vision-Language model architecture that learns a joint\nrepresentation of video-text pairs has become a prominent approach for the VTR\ntask. However, these models operate under the assumption of bijective\nvideo-text correspondences and neglect a more practical scenario where video\ncontent usually encompasses multiple events, while texts like user queries or\nwebpage metadata tend to be specific and correspond to single events. This\nestablishes a gap between the previous training objective and real-world\napplications, leading to the potential performance degradation of earlier\nmodels during inference. In this study, we introduce the Multi-event Video-Text\nRetrieval (MeVTR) task, addressing scenarios in which each video contains\nmultiple different events, as a niche scenario of the conventional Video-Text\nRetrieval Task. We present a simple model, Me-Retriever, which incorporates key\nevent video representation and a new MeVTR loss for the MeVTR task.\nComprehensive experiments show that this straightforward framework outperforms\nother models in the Video-to-Text and Text-to-Video tasks, effectively\nestablishing a robust baseline for the MeVTR task. We believe this work serves\nas a strong foundation for future studies. Code is available at\nhttps://github.com/gengyuanmax/MeVTR.",
        "translated": ""
    },
    {
        "title": "L^2R: Lifelong Learning for First-stage Retrieval with\n  Backward-Compatible Representations",
        "url": "http://arxiv.org/abs/2308.11512v1",
        "pub_date": "2023-08-22",
        "summary": "First-stage retrieval is a critical task that aims to retrieve relevant\ndocument candidates from a large-scale collection. While existing retrieval\nmodels have achieved impressive performance, they are mostly studied on static\ndata sets, ignoring that in the real-world, the data on the Web is continuously\ngrowing with potential distribution drift. Consequently, retrievers trained on\nstatic old data may not suit new-coming data well and inevitably produce\nsub-optimal results. In this work, we study lifelong learning for first-stage\nretrieval, especially focusing on the setting where the emerging documents are\nunlabeled since relevance annotation is expensive and may not keep up with data\nemergence. Under this setting, we aim to develop model updating with two goals:\n(1) to effectively adapt to the evolving distribution with the unlabeled\nnew-coming data, and (2) to avoid re-inferring all embeddings of old documents\nto efficiently update the index each time the model is updated.\n  We first formalize the task and then propose a novel Lifelong Learning method\nfor the first-stage Retrieval, namely L^2R. L^2R adopts the typical memory\nmechanism for lifelong learning, and incorporates two crucial components: (1)\nselecting diverse support negatives for model training and memory updating for\neffective model adaptation, and (2) a ranking alignment objective to ensure the\nbackward-compatibility of representations to save the cost of index rebuilding\nwithout hurting the model performance. For evaluation, we construct two new\nbenchmarks from LoTTE and Multi-CPR datasets to simulate the document\ndistribution drift in realistic retrieval scenarios. Extensive experiments show\nthat L^2R significantly outperforms competitive lifelong learning baselines.",
        "translated": ""
    },
    {
        "title": "Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect\n  Dense Retrieval",
        "url": "http://arxiv.org/abs/2308.11474v1",
        "pub_date": "2023-08-22",
        "summary": "Grounded on pre-trained language models (PLMs), dense retrieval has been\nstudied extensively on plain text. In contrast, there has been little research\non retrieving data with multiple aspects using dense models. In the scenarios\nsuch as product search, the aspect information plays an essential role in\nrelevance matching, e.g., category: Electronics, Computers, and Pet Supplies. A\ncommon way of leveraging aspect information for multi-aspect retrieval is to\nintroduce an auxiliary classification objective, i.e., using item contents to\npredict the annotated value IDs of item aspects. However, by learning the value\nembeddings from scratch, this approach may not capture the various semantic\nsimilarities between the values sufficiently. To address this limitation, we\nleverage the aspect information as text strings rather than class IDs during\npre-training so that their semantic similarities can be naturally captured in\nthe PLMs. To facilitate effective retrieval with the aspect strings, we propose\nmutual prediction objectives between the text of the item aspect and content.\nIn this way, our model makes more sufficient use of aspect information than\nconducting undifferentiated masked language modeling (MLM) on the concatenated\ntext of aspects and content. Extensive experiments on two real-world datasets\n(product and mini-program search) show that our approach can outperform\ncompetitive baselines both treating aspect values as classes and conducting the\nsame MLM for aspect and content strings. Code and related dataset will be\navailable at the URL \\footnote{https://github.com/sunxiaojie99/ATTEMPT}.",
        "translated": ""
    },
    {
        "title": "On the Opportunities and Challenges of Offline Reinforcement Learning\n  for Recommender Systems",
        "url": "http://arxiv.org/abs/2308.11336v1",
        "pub_date": "2023-08-22",
        "summary": "Reinforcement learning serves as a potent tool for modeling dynamic user\ninterests within recommender systems, garnering increasing research attention\nof late. However, a significant drawback persists: its poor data efficiency,\nstemming from its interactive nature. The training of reinforcement\nlearning-based recommender systems demands expensive online interactions to\namass adequate trajectories, essential for agents to learn user preferences.\nThis inefficiency renders reinforcement learning-based recommender systems a\nformidable undertaking, necessitating the exploration of potential solutions.\nRecent strides in offline reinforcement learning present a new perspective.\nOffline reinforcement learning empowers agents to glean insights from offline\ndatasets and deploy learned policies in online settings. Given that recommender\nsystems possess extensive offline datasets, the framework of offline\nreinforcement learning aligns seamlessly. Despite being a burgeoning field,\nworks centered on recommender systems utilizing offline reinforcement learning\nremain limited. This survey aims to introduce and delve into offline\nreinforcement learning within recommender systems, offering an inclusive review\nof existing literature in this domain. Furthermore, we strive to underscore\nprevalent challenges, opportunities, and future pathways, poised to propel\nresearch in this evolving field.",
        "translated": ""
    },
    {
        "title": "Test Time Embedding Normalization for Popularity Bias Mitigation",
        "url": "http://arxiv.org/abs/2308.11288v1",
        "pub_date": "2023-08-22",
        "summary": "Popularity bias is a widespread problem in the field of recommender systems,\nwhere popular items tend to dominate recommendation results. In this work, we\npropose 'Test Time Embedding Normalization' as a simple yet effective strategy\nfor mitigating popularity bias, which surpasses the performance of the previous\nmitigation approaches by a significant margin. Our approach utilizes the\nnormalized item embedding during the inference stage to control the influence\nof embedding magnitude, which is highly correlated with item popularity.\nThrough extensive experiments, we show that our method combined with the\nsampled softmax loss effectively reduces popularity bias compare to previous\napproaches for bias mitigation. We further investigate the relationship between\nuser and item embeddings and find that the angular similarity between\nembeddings distinguishes preferable and non-preferable items regardless of\ntheir popularity. The analysis explains the mechanism behind the success of our\napproach in eliminating the impact of popularity bias. Our code is available at\nhttps://github.com/ml-postech/TTEN.",
        "translated": ""
    },
    {
        "title": "MISSRec: Pre-training and Transferring Multi-modal Interest-aware\n  Sequence Representation for Recommendation",
        "url": "http://arxiv.org/abs/2308.11175v1",
        "pub_date": "2023-08-22",
        "summary": "The goal of sequential recommendation (SR) is to predict a user's potential\ninterested items based on her/his historical interaction sequences. Most\nexisting sequential recommenders are developed based on ID features, which,\ndespite their widespread use, often underperform with sparse IDs and struggle\nwith the cold-start problem. Besides, inconsistent ID mappings hinder the\nmodel's transferability, isolating similar recommendation domains that could\nhave been co-optimized. This paper aims to address these issues by exploring\nthe potential of multi-modal information in learning robust and generalizable\nsequence representations. We propose MISSRec, a multi-modal pre-training and\ntransfer learning framework for SR. On the user side, we design a\nTransformer-based encoder-decoder model, where the contextual encoder learns to\ncapture the sequence-level multi-modal synergy while a novel interest-aware\ndecoder is developed to grasp item-modality-interest relations for better\nsequence representation. On the candidate item side, we adopt a dynamic fusion\nmodule to produce user-adaptive item representation, providing more precise\nmatching between users and items. We pre-train the model with contrastive\nlearning objectives and fine-tune it in an efficient manner. Extensive\nexperiments demonstrate the effectiveness and flexibility of MISSRec, promising\nan practical solution for real-world recommendation scenarios.",
        "translated": ""
    },
    {
        "title": "Towards Validating Long-Term User Feedbacks in Interactive\n  Recommendation Systems",
        "url": "http://arxiv.org/abs/2308.11137v1",
        "pub_date": "2023-08-22",
        "summary": "Interactive Recommender Systems (IRSs) have attracted a lot of attention, due\nto their ability to model interactive processes between users and recommender\nsystems. Numerous approaches have adopted Reinforcement Learning (RL)\nalgorithms, as these can directly maximize users' cumulative rewards. In IRS,\nresearchers commonly utilize publicly available review datasets to compare and\nevaluate algorithms. However, user feedback provided in public datasets merely\nincludes instant responses (e.g., a rating), with no inclusion of delayed\nresponses (e.g., the dwell time and the lifetime value). Thus, the question\nremains whether these review datasets are an appropriate choice to evaluate the\nlong-term effects of the IRS. In this work, we revisited experiments on IRS\nwith review datasets and compared RL-based models with a simple reward model\nthat greedily recommends the item with the highest one-step reward. Following\nextensive analysis, we can reveal three main findings: First, a simple greedy\nreward model consistently outperforms RL-based models in maximizing cumulative\nrewards. Second, applying higher weighting to long-term rewards leads to a\ndegradation of recommendation performance. Third, user feedbacks have mere\nlong-term effects on the benchmark datasets. Based on our findings, we conclude\nthat a dataset has to be carefully verified and that a simple greedy baseline\nshould be included for a proper evaluation of RL-based IRS approaches.",
        "translated": ""
    },
    {
        "title": "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential\n  Behavior Comprehension in Recommendation",
        "url": "http://arxiv.org/abs/2308.11131v1",
        "pub_date": "2023-08-22",
        "summary": "With large language models (LLMs) achieving remarkable breakthroughs in\nnatural language processing (NLP) domains, LLM-enhanced recommender systems\nhave received much attention and have been actively explored currently. In this\npaper, we focus on adapting and empowering a pure large language model for\nzero-shot and few-shot recommendation tasks. First and foremost, we identify\nand formulate the lifelong sequential behavior incomprehension problem for LLMs\nin recommendation domains, i.e., LLMs fail to extract useful information from a\ntextual context of long user behavior sequence, even if the length of context\nis far from reaching the context limitation of LLMs. To address such an issue\nand improve the recommendation performance of LLMs, we propose a novel\nframework, namely Retrieval-enhanced Large Language models (ReLLa) for\nrecommendation tasks in both zero-shot and few-shot settings. For zero-shot\nrecommendation, we perform semantic user behavior retrieval (SUBR) to improve\nthe data quality of testing samples, which greatly reduces the difficulty for\nLLMs to extract the essential knowledge from user behavior sequences. As for\nfew-shot recommendation, we further design retrieval-enhanced instruction\ntuning (ReiT) by adopting SUBR as a data augmentation technique for training\nsamples. Specifically, we develop a mixed training dataset consisting of both\nthe original data samples and their retrieval-enhanced counterparts. We conduct\nextensive experiments on a real-world public dataset (i.e., MovieLens-1M) to\ndemonstrate the superiority of ReLLa compared with existing baseline models, as\nwell as its capability for lifelong sequential behavior comprehension.",
        "translated": ""
    },
    {
        "title": "How Expressive are Graph Neural Networks in Recommendation?",
        "url": "http://arxiv.org/abs/2308.11127v1",
        "pub_date": "2023-08-22",
        "summary": "Graph Neural Networks (GNNs) have demonstrated superior performance on\nvarious graph learning tasks, including recommendation, where they leverage\nuser-item collaborative filtering signals in graphs. However, theoretical\nformulations of their capability are scarce, despite their empirical\neffectiveness in state-of-the-art recommender models. Recently, research has\nexplored the expressiveness of GNNs in general, demonstrating that message\npassing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that\nGNNs combined with random node initialization are universal. Nevertheless, the\nconcept of \"expressiveness\" for GNNs remains vaguely defined. Most existing\nworks adopt the graph isomorphism test as the metric of expressiveness, but\nthis graph-level task may not effectively assess a model's ability in\nrecommendation, where the objective is to distinguish nodes of different\ncloseness. In this paper, we provide a comprehensive theoretical analysis of\nthe expressiveness of GNNs in recommendation, considering three levels of\nexpressiveness metrics: graph isomorphism (graph-level), node automorphism\n(node-level), and topological closeness (link-level). We propose the\ntopological closeness metric to evaluate GNNs' ability to capture the\nstructural distance between nodes, which aligns closely with the objective of\nrecommendation. To validate the effectiveness of this new metric in evaluating\nrecommendation performance, we introduce a learning-less GNN algorithm that is\noptimal on the new metric and can be optimal on the node-level metric with\nsuitable modification. We conduct extensive experiments comparing the proposed\nalgorithm against various types of state-of-the-art GNN models to explore the\nexplainability of the new metric in the recommendation task. For\nreproducibility, implementation codes are available at\nhttps://github.com/HKUDS/GTE.",
        "translated": ""
    },
    {
        "title": "Anonymity at Risk? Assessing Re-Identification Capabilities of Large\n  Language Models",
        "url": "http://arxiv.org/abs/2308.11103v1",
        "pub_date": "2023-08-22",
        "summary": "Anonymity of both natural and legal persons in court rulings is a critical\naspect of privacy protection in the European Union and Switzerland. With the\nadvent of LLMs, concerns about large-scale re-identification of anonymized\npersons are growing. In accordance with the Federal Supreme Court of\nSwitzerland, we explore the potential of LLMs to re-identify individuals in\ncourt rulings by constructing a proof-of-concept using actual legal data from\nthe Swiss federal supreme court. Following the initial experiment, we\nconstructed an anonymized Wikipedia dataset as a more rigorous testing ground\nto further investigate the findings. With the introduction and application of\nthe new task of re-identifying people in texts, we also introduce new metrics\nto measure performance. We systematically analyze the factors that influence\nsuccessful re-identifications, identifying model size, input length, and\ninstruction tuning among the most critical determinants. Despite high\nre-identification rates on Wikipedia, even the best LLMs struggled with court\ndecisions. The complexity is attributed to the lack of test datasets, the\nnecessity for substantial training resources, and data sparsity in the\ninformation used for re-identification. In conclusion, this study demonstrates\nthat re-identification using LLMs may not be feasible for now, but as the\nproof-of-concept on Wikipedia showed, it might become possible in the future.\nWe hope that our system can help enhance the confidence in the security of\nanonymized decisions, thus leading to the courts being more confident to\npublish decisions.",
        "translated": ""
    },
    {
        "title": "Learning from Negative User Feedback and Measuring Responsiveness for\n  Sequential Recommenders",
        "url": "http://arxiv.org/abs/2308.12256v1",
        "pub_date": "2023-08-23",
        "summary": "Sequential recommenders have been widely used in industry due to their\nstrength in modeling user preferences. While these models excel at learning a\nuser's positive interests, less attention has been paid to learning from\nnegative user feedback. Negative user feedback is an important lever of user\ncontrol, and comes with an expectation that recommenders should respond quickly\nand reduce similar recommendations to the user. However, negative feedback\nsignals are often ignored in the training objective of sequential retrieval\nmodels, which primarily aim at predicting positive user interactions. In this\nwork, we incorporate explicit and implicit negative user feedback into the\ntraining objective of sequential recommenders in the retrieval stage using a\n\"not-to-recommend\" loss function that optimizes for the log-likelihood of not\nrecommending items with negative feedback. We demonstrate the effectiveness of\nthis approach using live experiments on a large-scale industrial recommender\nsystem. Furthermore, we address a challenge in measuring recommender\nresponsiveness to negative feedback by developing a counterfactual simulation\nframework to compare recommender responses between different user actions,\nshowing improved responsiveness from the modeling change.",
        "translated": ""
    },
    {
        "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task",
        "url": "http://arxiv.org/abs/2308.12241v1",
        "pub_date": "2023-08-23",
        "summary": "Recently, the fast development of Large Language Models (LLMs) such as\nChatGPT has significantly advanced NLP tasks by enhancing the capabilities of\nconversational models. However, the application of LLMs in the recommendation\ndomain has not been thoroughly investigated. To bridge this gap, we propose\nLLMRec, a LLM-based recommender system designed for benchmarking LLMs on\nvarious recommendation tasks. Specifically, we benchmark several popular\noff-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation\ntasks, including rating prediction, sequential recommendation, direct\nrecommendation, explanation generation, and review summarization. Furthermore,\nwe investigate the effectiveness of supervised finetuning to improve LLMs'\ninstruction compliance ability. The benchmark results indicate that LLMs\ndisplayed only moderate proficiency in accuracy-based tasks such as sequential\nand direct recommendation. However, they demonstrated comparable performance to\nstate-of-the-art methods in explainability-based tasks. We also conduct\nqualitative evaluations to further evaluate the quality of contents generated\nby different models, and the results show that LLMs can truly understand the\nprovided information and generate clearer and more reasonable results. We\naspire that this benchmark will serve as an inspiration for researchers to\ndelve deeper into the potential of LLMs in enhancing recommendation\nperformance. Our codes, processed data and benchmark results are available at\nhttps://github.com/williamliujl/LLMRec.",
        "translated": ""
    },
    {
        "title": "Counterfactual Graph Augmentation for Consumer Unfairness Mitigation in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2308.12083v1",
        "pub_date": "2023-08-23",
        "summary": "In recommendation literature, explainability and fairness are becoming two\nprominent perspectives to consider. However, prior works have mostly addressed\nthem separately, for instance by explaining to consumers why a certain item was\nrecommended or mitigating disparate impacts in recommendation utility. None of\nthem has leveraged explainability techniques to inform unfairness mitigation.\nIn this paper, we propose an approach that relies on counterfactual\nexplanations to augment the set of user-item interactions, such that using them\nwhile inferring recommendations leads to fairer outcomes. Modeling user-item\ninteractions as a bipartite graph, our approach augments the latter by\nidentifying new user-item edges that not only can explain the original\nunfairness by design, but can also mitigate it. Experiments on two public data\nsets show that our approach effectively leads to a better trade-off between\nfairness and recommendation utility compared with state-of-the-art mitigation\nprocedures. We further analyze the characteristics of added edges to highlight\nkey unfairness patterns. Source code available at\nhttps://github.com/jackmedda/RS-BGExplainer/tree/cikm2023.",
        "translated": ""
    },
    {
        "title": "Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep\n  Learning Track",
        "url": "http://arxiv.org/abs/2308.12039v1",
        "pub_date": "2023-08-23",
        "summary": "Large-scale text retrieval technology has been widely used in various\npractical business scenarios. This paper presents our systems for the TREC 2022\nDeep Learning Track. We explain the hybrid text retrieval and multi-stage text\nranking method adopted in our solution. The retrieval stage combined the two\nstructures of traditional sparse retrieval and neural dense retrieval. In the\nranking stage, in addition to the full interaction-based ranking model built on\nlarge pre-trained language model, we also proposes a lightweight sub-ranking\nmodule to further enhance the final text ranking performance. Evaluation\nresults demonstrate the effectiveness of our proposed approach. Our models\nachieve the 1st and 4th rank on the test set of passage ranking and document\nranking respectively.",
        "translated": ""
    },
    {
        "title": "LKPNR: LLM and KG for Personalized News Recommendation Framework",
        "url": "http://arxiv.org/abs/2308.12028v1",
        "pub_date": "2023-08-23",
        "summary": "Accurately recommending candidate news articles to users is a basic challenge\nfaced by personalized news recommendation systems. Traditional methods are\nusually difficult to grasp the complex semantic information in news texts,\nresulting in unsatisfactory recommendation results. Besides, these traditional\nmethods are more friendly to active users with rich historical behaviors.\nHowever, they can not effectively solve the \"long tail problem\" of inactive\nusers. To address these issues, this research presents a novel general\nframework that combines Large Language Models (LLM) and Knowledge Graphs (KG)\ninto semantic representations of traditional methods. In order to improve\nsemantic understanding in complex news texts, we use LLMs' powerful text\nunderstanding ability to generate news representations containing rich semantic\ninformation. In addition, our method combines the information about news\nentities and mines high-order structural information through multiple hops in\nKG, thus alleviating the challenge of long tail distribution. Experimental\nresults demonstrate that compared with various traditional models, the\nframework significantly improves the recommendation effect. The successful\nintegration of LLM and KG in our framework has established a feasible path for\nachieving more accurate personalized recommendations in the news field. Our\ncode is available at https://github.com/Xuan-ZW/LKPNR.",
        "translated": ""
    },
    {
        "title": "Economic Recommender Systems -- A Systematic Review",
        "url": "http://arxiv.org/abs/2308.11998v1",
        "pub_date": "2023-08-23",
        "summary": "Many of today's online services provide personalized recommendations to their\nusers. Such recommendations are typically designed to serve certain user needs,\ne.g., to quickly find relevant content in situations of information overload.\nCorrespondingly, the academic literature in the field largely focuses on the\nvalue of recommender systems for the end user. In this context, one underlying\nassumption is that the improved service that is achieved through the\nrecommendations will in turn positively impact the organization's goals, e.g.,\nin the form of higher customer retention or loyalty. However, in reality,\nrecommender systems can be used to target organizational economic goals more\ndirectly by incorporating monetary considerations such as price awareness and\nprofitability aspects into the underlying recommendation models. In this work,\nwe survey the existing literature on what we call Economic Recommender Systems\nbased on a systematic review approach that helped us identify 133 relevant\npapers. We first categorize existing works along different dimensions and then\nreview the most important technical approaches from the literature.\nFurthermore, we discuss common methodologies to evaluate such systems and\nfinally outline the limitations of today's research and future directions.",
        "translated": ""
    },
    {
        "title": "Integrating the Wikidata Taxonomy into YAGO",
        "url": "http://arxiv.org/abs/2308.11884v1",
        "pub_date": "2023-08-23",
        "summary": "Wikidata is one of the largest public general-purpose Knowledge Bases (KBs).\nYet, due to its collaborative nature, its schema and taxonomy have become\nconvoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from\nSchema.org, which reduced and cleaned up the taxonomy and constraints and made\nit possible to run automated reasoners on the data. However, it also cut away\nlarge parts of the Wikidata taxonomy. In this paper, we present our effort to\nmerge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay\nparticular attention to logical constraints and a careful distinction of\nclasses and instances. Our work creates YAGO 4.5, which adds a rich layer of\ninformative classes to YAGO, while at the same time keeping the KB logically\nconsistent.",
        "translated": ""
    },
    {
        "title": "CLIP Multi-modal Hashing: A new baseline CLIPMH",
        "url": "http://arxiv.org/abs/2308.11797v1",
        "pub_date": "2023-08-22",
        "summary": "The multi-modal hashing method is widely used in multimedia retrieval. It can\nfuse multi-source data to generate binary hash code. However, the current\nmulti-modal methods have the problem of low retrieval accuracy. The reason is\nthat the individual backbone networks have limited feature expression\ncapabilities and are not jointly pre-trained on large-scale unsupervised\nmulti-modal data. To solve this problem, we propose a new baseline CLIP\nMulti-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and\nimage features, and then fuse to generate hash code. CLIP improves the\nexpressiveness of each modal feature. In this way, it can greatly improve the\nretrieval performance of multi-modal hashing methods. In comparison to\nstate-of-the-art unsupervised and supervised multi-modal hashing methods,\nexperiments reveal that the proposed CLIPMH can significantly enhance\nperformance (Maximum increase of 8.38%). CLIP also has great advantages over\nthe text and visual backbone networks commonly used before.",
        "translated": ""
    },
    {
        "title": "Knowledge Graph Prompting for Multi-Document Question Answering",
        "url": "http://arxiv.org/abs/2308.11730v1",
        "pub_date": "2023-08-22",
        "summary": "The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has\nachieved remarkable success in open-domain question answering (OD-QA). However,\nfew works explore this paradigm in the scenario of multi-document question\nanswering (MD-QA), a task demanding a thorough understanding of the logical\nassociations among the contents and structures of different documents. To fill\nthis crucial gap, we propose a Knowledge Graph Prompting (KGP) method to\nformulate the right context in prompting LLMs for MD-QA, which consists of a\ngraph construction module and a graph traversal module. For graph construction,\nwe create a knowledge graph (KG) over multiple documents with nodes symbolizing\npassages or document structures (e.g., pages/tables), and edges denoting the\nsemantic/lexical similarity between passages or intra-document structural\nrelations. For graph traversal, we design an LM-guided graph traverser that\nnavigates across nodes and gathers supporting passages assisting LLMs in MD-QA.\nThe constructed graph serves as the global ruler that regulates the\ntransitional space among passages and reduces retrieval latency. Concurrently,\nthe LM-guided traverser acts as a local navigator that gathers pertinent\ncontext to progressively approach the question and guarantee retrieval quality.\nExtensive experiments underscore the efficacy of KGP for MD-QA, signifying the\npotential of leveraging graphs in enhancing the prompt design for LLMs. Our\ncode is at https://github.com/YuWVandy/KG-LLM-MDQA.",
        "translated": ""
    },
    {
        "title": "Invariant representation learning for sequential recommendation",
        "url": "http://arxiv.org/abs/2308.11728v1",
        "pub_date": "2023-08-22",
        "summary": "Sequential recommendation involves automatically recommending the next item\nto users based on their historical item sequence. While most prior research\nemploys RNN or transformer methods to glean information from the item\nsequence-generating probabilities for each user-item pair and recommending the\ntop items, these approaches often overlook the challenge posed by spurious\nrelationships. This paper specifically addresses these spurious relations. We\nintroduce a novel sequential recommendation framework named Irl4Rec. This\nframework harnesses invariant learning and employs a new objective that factors\nin the relationship between spurious variables and adjustment variables during\nmodel training. This approach aids in identifying spurious relations.\nComparative analyses reveal that our framework outperforms three typical\nmethods, underscoring the effectiveness of our model. Moreover, an ablation\nstudy further demonstrates the critical role our model plays in detecting\nspurious relations.",
        "translated": ""
    },
    {
        "title": "On Popularity Bias of Multimodal-aware Recommender Systems: a\n  Modalities-driven Analysis",
        "url": "http://arxiv.org/abs/2308.12911v1",
        "pub_date": "2023-08-24",
        "summary": "Multimodal-aware recommender systems (MRSs) exploit multimodal content (e.g.,\nproduct images or descriptions) as items' side information to improve\nrecommendation accuracy. While most of such methods rely on factorization\nmodels (e.g., MFBPR) as base architecture, it has been shown that MFBPR may be\naffected by popularity bias, meaning that it inherently tends to boost the\nrecommendation of popular (i.e., short-head) items at the detriment of niche\n(i.e., long-tail) items from the catalog. Motivated by this assumption, in this\nwork, we provide one of the first analyses on how multimodality in\nrecommendation could further amplify popularity bias. Concretely, we evaluate\nthe performance of four state-of-the-art MRSs algorithms (i.e., VBPR, MMGCN,\nGRCN, LATTICE) on three datasets from Amazon by assessing, along with\nrecommendation accuracy metrics, performance measures accounting for the\ndiversity of recommended items and the portion of retrieved niche items. To\nbetter investigate this aspect, we decide to study the separate influence of\neach modality (i.e., visual and textual) on popularity bias in different\nevaluation dimensions. Results, which demonstrate how the single modality may\naugment the negative effect of popularity bias, shed light on the importance to\nprovide a more rigorous analysis of the performance of such models.",
        "translated": ""
    },
    {
        "title": "Towards Communication-Efficient Model Updating for On-Device\n  Session-Based Recommendation",
        "url": "http://arxiv.org/abs/2308.12777v1",
        "pub_date": "2023-08-24",
        "summary": "On-device recommender systems recently have garnered increasing attention due\nto their advantages of providing prompt response and securing privacy. To stay\ncurrent with evolving user interests, cloud-based recommender systems are\nperiodically updated with new interaction data. However, on-device models\nstruggle to retrain themselves because of limited onboard computing resources.\nAs a solution, we consider the scenario where the model retraining occurs on\nthe server side and then the updated parameters are transferred to edge devices\nvia network communication. While this eliminates the need for local retraining,\nit incurs a regular transfer of parameters that significantly taxes network\nbandwidth. To mitigate this issue, we develop an efficient approach based on\ncompositional codes to compress the model update. This approach ensures the\non-device model is updated flexibly with minimal additional parameters whilst\nutilizing previous knowledge. The extensive experiments conducted on multiple\nsession-based recommendation models with distinctive architectures demonstrate\nthat the on-device model can achieve comparable accuracy to the retrained\nserver-side counterpart through transferring an update 60x smaller in size. The\ncodes are available at \\url{https://github.com/xiaxin1998/ODUpdate}.",
        "translated": ""
    },
    {
        "title": "On the Consistency of Average Embeddings for Item Recommendation",
        "url": "http://arxiv.org/abs/2308.12767v1",
        "pub_date": "2023-08-24",
        "summary": "A prevalent practice in recommender systems consists of averaging item\nembeddings to represent users or higher-level concepts in the same embedding\nspace. This paper investigates the relevance of such a practice. For this\npurpose, we propose an expected precision score, designed to measure the\nconsistency of an average embedding relative to the items used for its\nconstruction. We subsequently analyze the mathematical expression of this score\nin a theoretical setting with specific assumptions, as well as its empirical\nbehavior on real-world data from music streaming services. Our results\nemphasize that real-world averages are less consistent for recommendation,\nwhich paves the way for future research to better align real-world embeddings\nwith assumptions from our theoretical setting.",
        "translated": ""
    },
    {
        "title": "Video Recommendation Using Social Network Analysis and User Viewing\n  Patterns",
        "url": "http://arxiv.org/abs/2308.12743v1",
        "pub_date": "2023-08-24",
        "summary": "With the meteoric rise of video-on-demand (VOD) platforms, users face the\nchallenge of sifting through an expansive sea of content to uncover shows that\nclosely match their preferences. To address this information overload dilemma,\nVOD services have increasingly incorporated recommender systems powered by\nalgorithms that analyze user behavior and suggest personalized content.\nHowever, a majority of existing recommender systems depend on explicit user\nfeedback in the form of ratings and reviews, which can be difficult and\ntime-consuming to collect at scale. This presents a key research gap, as\nleveraging users' implicit feedback patterns could provide an alternative\navenue for building effective video recommendation models, circumventing the\nneed for explicit ratings. However, prior literature lacks sufficient\nexploration into implicit feedback-based recommender systems, especially in the\ncontext of modeling video viewing behavior. Therefore, this paper aims to\nbridge this research gap by proposing a novel video recommendation technique\nthat relies solely on users' implicit feedback in the form of their content\nviewing percentages.",
        "translated": ""
    },
    {
        "title": "Out of the Box Thinking: Improving Customer Lifetime Value Modelling via\n  Expert Routing and Game Whale Detection",
        "url": "http://arxiv.org/abs/2308.12729v1",
        "pub_date": "2023-08-24",
        "summary": "Customer lifetime value (LTV) prediction is essential for mobile game\npublishers trying to optimize the advertising investment for each user\nacquisition based on the estimated worth. In mobile games, deploying\nmicrotransactions is a simple yet effective monetization strategy, which\nattracts a tiny group of game whales who splurge on in-game purchases. The\npresence of such game whales may impede the practicality of existing LTV\nprediction models, since game whales' purchase behaviours always exhibit varied\ndistribution from general users. Consequently, identifying game whales can open\nup new opportunities to improve the accuracy of LTV prediction models. However,\nlittle attention has been paid to applying game whale detection in LTV\nprediction, and existing works are mainly specialized for the long-term LTV\nprediction with the assumption that the high-quality user features are\navailable, which is not applicable in the UA stage. In this paper, we propose\nExpLTV, a novel multi-task framework to perform LTV prediction and game whale\ndetection in a unified way. In ExpLTV, we first innovatively design a deep\nneural network-based game whale detector that can not only infer the intrinsic\norder in accordance with monetary value, but also precisely identify high\nspenders (i.e., game whales) and low spenders. Then, by treating the game whale\ndetector as a gating network to decide the different mixture patterns of LTV\nexperts assembling, we can thoroughly leverage the shared information and\nscenario-specific information (i.e., game whales modelling and low spenders\nmodelling). Finally, instead of separately designing a purchase rate estimator\nfor two tasks, we design a shared estimator that can preserve the inner task\nrelationships. The superiority of ExpLTV is further validated via extensive\nexperiments on three industrial datasets.",
        "translated": ""
    },
    {
        "title": "Laying foundations to quantify the \"Effort of Reproducibility\"",
        "url": "http://arxiv.org/abs/2308.12580v1",
        "pub_date": "2023-08-24",
        "summary": "Why are some research studies easy to reproduce while others are difficult?\nCasting doubt on the accuracy of scientific work is not fruitful, especially\nwhen an individual researcher cannot reproduce the claims made in the paper.\nThere could be many subjective reasons behind the inability to reproduce a\nscientific paper. The field of Machine Learning (ML) faces a reproducibility\ncrisis, and surveying a portion of published articles has resulted in a group\nrealization that although sharing code repositories would be appreciable, code\nbases are not the end all be all for determining the reproducibility of an\narticle. Various parties involved in the publication process have come forward\nto address the reproducibility crisis and solutions such as badging articles as\nreproducible, reproducibility checklists at conferences (\\textit{NeurIPS, ICML,\nICLR, etc.}), and sharing artifacts on \\textit{OpenReview} come across as\npromising solutions to the core problem. The breadth of literature on\nreproducibility focuses on measures required to avoid ir-reproducibility, and\nthere is not much research into the effort behind reproducing these articles.\nIn this paper, we investigate the factors that contribute to the easiness and\ndifficulty of reproducing previously published studies and report on the\nfoundational framework to quantify effort of reproducibility.",
        "translated": ""
    },
    {
        "title": "Exploring the Integration Strategies of Retriever and Large Language\n  Models",
        "url": "http://arxiv.org/abs/2308.12574v1",
        "pub_date": "2023-08-24",
        "summary": "The integration of retrieved passages and large language models (LLMs), such\nas ChatGPTs, has significantly contributed to improving open-domain question\nanswering. However, there is still a lack of exploration regarding the optimal\napproach for incorporating retrieved passages into the answer generation\nprocess. This paper aims to fill this gap by investigating different methods of\ncombining retrieved passages with LLMs to enhance answer generation. We begin\nby examining the limitations of a commonly-used concatenation approach.\nSurprisingly, this approach often results in generating \"unknown\" outputs, even\nwhen the correct document is among the top-k retrieved passages. To address\nthis issue, we explore four alternative strategies for integrating the\nretrieved passages with the LLMs. These strategies include two single-round\nmethods that utilize chain-of-thought reasoning and two multi-round strategies\nthat incorporate feedback loops. Through comprehensive analyses and\nexperiments, we provide insightful observations on how to effectively leverage\nretrieved passages to enhance the answer generation capability of LLMs.",
        "translated": ""
    },
    {
        "title": "Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature",
        "url": "http://arxiv.org/abs/2308.12420v1",
        "pub_date": "2023-08-23",
        "summary": "Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating\ncomprehensive insights into their diverse components. However, a systematic\nliterature review that emphasizes the Environmental, Sustainability, and\nGovernance (ESG) components of DLT remains lacking. To bridge this gap, we\nselected 107 seed papers to build a citation network of 63,083 references and\nrefined it to a corpus of 24,539 publications for analysis. Then, we labeled\nthe named entities in 46 papers according to twelve top-level categories\nderived from an established technology taxonomy and enhanced the taxonomy by\npinpointing DLT's ESG elements. Leveraging transformer-based language models,\nwe fine-tuned a pre-trained language model for a Named Entity Recognition (NER)\ntask using our labeled dataset. We used our fine-tuned language model to\ndistill the corpus to 505 key papers, facilitating a literature review via\nnamed entities and temporal graph analysis on DLT evolution in the context of\nESG. Our contributions are a methodology to conduct a machine learning-driven\nsystematic literature review in the DLT field, placing a special emphasis on\nESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed\nof 54,808 named entities, designed for DLT and ESG-related explorations.",
        "translated": ""
    },
    {
        "title": "On the Practicality of Dynamic Updates in Fast Searchable Encryption",
        "url": "http://arxiv.org/abs/2308.13486v1",
        "pub_date": "2023-08-25",
        "summary": "Searchable encrypted (SE) indexing systems are a useful tool for utilizing\ncloud services to store and manage sensitive information. However, much of the\nwork on SE systems to date has remained theoretical. In order to make them of\npractical use, more work is needed to develop optimal protocols and working\nmodels for them. This includes, in particular, the creation of a working update\nmodel in order to maintain an encrypted index of a dynamic document set such as\nan email inbox. I have created a working, real-world end-to-end SE\nimplementation that satisfies these needs, including the first empirical\nperformance evaluation of the dynamic SE update operation. In doing so, I show\na viable path to move from the theoretical concepts described by previous\nresearchers to a future production-worthy implementation and identify issues\nfor follow-on investigation.",
        "translated": ""
    },
    {
        "title": "Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability\n  of Language Models",
        "url": "http://arxiv.org/abs/2308.13467v1",
        "pub_date": "2023-08-25",
        "summary": "The Natural Language Processing(NLP) community has been using crowd sourcing\ntechniques to create benchmark datasets such as General Language Understanding\nand Evaluation(GLUE) for training modern Language Models such as BERT. GLUE\ntasks measure the reliability scores using inter annotator metrics i.e. Cohens\nKappa. However, the reliability aspect of LMs has often been overlooked. To\ncounter this problem, we explore a knowledge-guided LM ensembling approach that\nleverages reinforcement learning to integrate knowledge from ConceptNet and\nWikipedia as knowledge graph embeddings. This approach mimics human annotators\nresorting to external knowledge to compensate for information deficits in the\ndatasets. Across nine GLUE datasets, our research shows that ensembling\nstrengthens reliability and accuracy scores, outperforming state of the art.",
        "translated": ""
    },
    {
        "title": "A Bayesian Active Learning Approach to Comparative Judgement",
        "url": "http://arxiv.org/abs/2308.13292v1",
        "pub_date": "2023-08-25",
        "summary": "Assessment is a crucial part of education. Traditional marking is a source of\ninconsistencies and unconscious bias, placing a high cognitive load on the\nassessors. An approach to address these issues is comparative judgement (CJ).\nIn CJ, the assessor is presented with a pair of items and is asked to select\nthe better one. Following a series of comparisons, a rank is derived using a\nranking model, for example, the BTM, based on the results. While CJ is\nconsidered a reliable method for marking, there are concerns around\ntransparency, and the ideal number of pairwise comparisons to generate a\nreliable estimation of the rank order is not known. Additionally, there have\nbeen attempts to generate a method of selecting pairs that should be compared\nnext in an informative manner, but some existing methods are known to have\ncreated their own bias within results inflating the reliability metric used. As\na result, a random selection approach is usually deployed.\n  We propose a novel Bayesian approach to CJ (BCJ) for determining the ranks of\ncompared items alongside a new way to select the pairs to present to the\nmarker(s) using active learning (AL), addressing the key shortcomings of\ntraditional CJ. Furthermore, we demonstrate how the entire approach may provide\ntransparency by providing the user insights into how it is making its decisions\nand, at the same time, being more efficient. Results from our experiments\nconfirm that the proposed BCJ combined with entropy-driven AL pair-selection\nmethod is superior to other alternatives. We also find that the more\ncomparisons done, the more accurate BCJ becomes, which solves the issue the\ncurrent method has of the model deteriorating if too many comparisons are\nperformed. As our approach can generate the complete predicted rank\ndistribution for an item, we also show how this can be utilised in devising a\npredicted grade, guided by the assessor.",
        "translated": ""
    },
    {
        "title": "Learning and Optimization of Implicit Negative Feedback for Industrial\n  Short-video Recommender System",
        "url": "http://arxiv.org/abs/2308.13249v1",
        "pub_date": "2023-08-25",
        "summary": "Short-video recommendation is one of the most important recommendation\napplications in today's industrial information systems. Compared with other\nrecommendation tasks, the enormous amount of feedback is the most typical\ncharacteristic. Specifically, in short-video recommendation, the\neasiest-to-collect user feedback is from the skipping behaviors, which leads to\ntwo critical challenges for the recommendation model. First, the skipping\nbehavior reflects implicit user preferences, and thus it is challenging for\ninterest extraction. Second, the kind of special feedback involves multiple\nobjectives, such as total watching time, which is also very challenging. In\nthis paper, we present our industrial solution in Kuaishou, which serves\nbillion-level users every day. Specifically, we deploy a feedback-aware\nencoding module which well extracts user preference taking the impact of\ncontext into consideration. We further design a multi-objective prediction\nmodule which well distinguishes the relation and differences among different\nmodel objectives in the short-video recommendation. We conduct extensive online\nA/B testing, along with detailed and careful analysis, which verifies the\neffectiveness of our solution.",
        "translated": ""
    },
    {
        "title": "Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and\n  Ex-Post Fairness",
        "url": "http://arxiv.org/abs/2308.13242v1",
        "pub_date": "2023-08-25",
        "summary": "In learning-to-rank (LTR), optimizing only the relevance (or the expected\nranking utility) can cause representational harm to certain categories of\nitems. Moreover, if there is implicit bias in the relevance scores, LTR models\nmay fail to optimize for true relevance. Previous works have proposed efficient\nalgorithms to train stochastic ranking models that achieve fairness of exposure\nto the groups ex-ante (or, in expectation), which may not guarantee\nrepresentation fairness to the groups ex-post, that is, after realizing a\nranking from the stochastic ranking model. Typically, ex-post fairness is\nachieved by post-processing, but previous work does not train stochastic\nranking models that are aware of this post-processing.\n  In this paper, we propose a novel objective that maximizes expected relevance\nonly over those rankings that satisfy given representation constraints to\nensure ex-post fairness. Building upon recent work on an efficient sampler for\nex-post group-fair rankings, we propose a group-fair Plackett-Luce model and\nshow that it can be efficiently optimized for our objective in the LTR\nframework.\n  Experiments on three real-world datasets show that our group-fair algorithm\nguarantees fairness alongside usually having better relevance compared to the\nLTR baselines. In addition, our algorithm also achieves better relevance than\npost-processing baselines, which also ensures ex-post fairness. Further, when\nimplicit bias is injected into the training data, our algorithm typically\noutperforms existing LTR baselines in relevance.",
        "translated": ""
    },
    {
        "title": "MMBAttn: Max-Mean and Bit-wise Attention for CTR Prediction",
        "url": "http://arxiv.org/abs/2308.13187v1",
        "pub_date": "2023-08-25",
        "summary": "With the increasing complexity and scale of click-through rate (CTR)\nprediction tasks in online advertising and recommendation systems, accurately\nestimating the importance of features has become a critical aspect of\ndeveloping effective models. In this paper, we propose an attention-based\napproach that leverages max and mean pooling operations, along with a bit-wise\nattention mechanism, to enhance feature importance estimation in CTR\nprediction. Traditionally, pooling operations such as max and mean pooling have\nbeen widely used to extract relevant information from features. However, these\noperations can lead to information loss and hinder the accurate determination\nof feature importance. To address this challenge, we propose a novel attention\narchitecture that utilizes a bit-based attention structure that emphasizes the\nrelationships between all bits in features, together with maximum and mean\npooling. By considering the fine-grained interactions at the bit level, our\nmethod aims to capture intricate patterns and dependencies that might be\noverlooked by traditional pooling operations. To examine the effectiveness of\nthe proposed method, experiments have been conducted on three public datasets.\nThe experiments demonstrated that the proposed method significantly improves\nthe performance of the base models to achieve state-of-the-art results.",
        "translated": ""
    },
    {
        "title": "Multi-BERT for Embeddings for Recommendation System",
        "url": "http://arxiv.org/abs/2308.13050v1",
        "pub_date": "2023-08-24",
        "summary": "In this paper, we propose a novel approach for generating document embeddings\nusing a combination of Sentence-BERT (SBERT) and RoBERTa, two state-of-the-art\nnatural language processing models. Our approach treats sentences as tokens and\ngenerates embeddings for them, allowing the model to capture both\nintra-sentence and inter-sentence relations within a document. We evaluate our\nmodel on a book recommendation task and demonstrate its effectiveness in\ngenerating more semantically rich and accurate document embeddings. To assess\nthe performance of our approach, we conducted experiments on a book\nrecommendation task using the Goodreads dataset. We compared the document\nembeddings generated using our MULTI-BERT model to those generated using SBERT\nalone. We used precision as our evaluation metric to compare the quality of the\ngenerated embeddings. Our results showed that our model consistently\noutperformed SBERT in terms of the quality of the generated embeddings.\nFurthermore, we found that our model was able to capture more nuanced semantic\nrelations within documents, leading to more accurate recommendations. Overall,\nour results demonstrate the effectiveness of our approach and suggest that it\nis a promising direction for improving the performance of recommendation\nsystems",
        "translated": ""
    },
    {
        "title": "Financial News Analytics Using Fine-Tuned Llama 2 GPT Model",
        "url": "http://arxiv.org/abs/2308.13032v1",
        "pub_date": "2023-08-24",
        "summary": "The paper considers the possibility to fine-tune Llama 2 Large Language Model\n(LLM) for the multitask analysis of financial news. For fine-tuning, the\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\nthe following tasks: analysing a text from financial market perspectives,\nhighlighting main points of a text, summarizing a text and extracting named\nentities with appropriate sentiments. The obtained results show that the\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\nspecified structure of response, part of response can be a structured text and\nanother part of data can have JSON format for further processing. Extracted\nsentiments for named entities can be considered as predictive features in\nsupervised machine learning models with quantitative target variables.",
        "translated": ""
    },
    {
        "title": "TRIVEA: Transparent Ranking Interpretation using Visual Explanation of\n  Black-Box Algorithmic Rankers",
        "url": "http://arxiv.org/abs/2308.14622v1",
        "pub_date": "2023-08-28",
        "summary": "Ranking schemes drive many real-world decisions, like, where to study, whom\nto hire, what to buy, etc. Many of these decisions often come with high\nconsequences. For example, a university can be deemed less prestigious if not\nfeatured in a top-k list, and consumers might not even explore products that do\nnot get recommended to buyers. At the heart of most of these decisions are\nopaque ranking schemes, which dictate the ordering of data entities, but their\ninternal logic is inaccessible or proprietary. Drawing inferences about the\nranking differences is like a guessing game to the stakeholders, like, the\nrankees (i.e., the entities who are ranked, like product companies) and the\ndecision-makers (i.e., who use the rankings, like buyers). In this paper, we\naim to enable transparency in ranking interpretation by using algorithmic\nrankers that learn from available data and by enabling human reasoning about\nthe learned ranking differences using explainable AI (XAI) methods. To realize\nthis aim, we leverage the exploration-explanation paradigm of human-data\ninteraction to let human stakeholders explore subsets and groupings of complex\nmulti-attribute ranking data using visual explanations of model fit and\nattribute influence on rankings. We realize this explanation paradigm for\ntransparent ranking interpretation in TRIVEA, a visual analytic system that is\nfueled by: i) visualizations of model fit derived from algorithmic rankers that\nlearn the associations between attributes and rankings from available data and\nii) visual explanations derived from XAI methods that help abstract important\npatterns, like, the relative influence of attributes in different ranking\nranges. Using TRIVEA, end users not trained in data science have the agency to\ntransparently reason about the global and local behavior of the rankings\nwithout the need to open black-box ranking models and develop confidence in the\nresulting attribute-based inferences. We demonstrate the efficacy of TRIVEA\nusing multiple usage scenarios and subjective feedback from researchers with\ndiverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank,\nExplainable ML, Ranking",
        "translated": ""
    },
    {
        "title": "Fairness Through Domain Awareness: Mitigating Popularity Bias For Music\n  Discovery",
        "url": "http://arxiv.org/abs/2308.14601v1",
        "pub_date": "2023-08-28",
        "summary": "As online music platforms grow, music recommender systems play a vital role\nin helping users navigate and discover content within their vast musical\ndatabases. At odds with this larger goal, is the presence of popularity bias,\nwhich causes algorithmic systems to favor mainstream content over, potentially\nmore relevant, but niche items. In this work we explore the intrinsic\nrelationship between music discovery and popularity bias. To mitigate this\nissue we propose a domain-aware, individual fairness-based approach which\naddresses popularity bias in graph neural network (GNNs) based recommender\nsystems. Our approach uses individual fairness to reflect a ground truth\nlistening experience, i.e., if two songs sound similar, this similarity should\nbe reflected in their representations. In doing so, we facilitate meaningful\nmusic discovery that is robust to popularity bias and grounded in the music\ndomain. We apply our BOOST methodology to two discovery based tasks, performing\nrecommendations at both the playlist level and user level. Then, we ground our\nevaluation in the cold start setting, showing that our approach outperforms\nexisting fairness benchmarks in both performance and recommendation of\nlesser-known content. Finally, our analysis explains why our proposed\nmethodology is a novel and promising approach to mitigating popularity bias and\nimproving the discovery of new and niche content in music recommender systems.",
        "translated": ""
    },
    {
        "title": "Efficient and Accurate Tree Detection from 3D Point Clouds through Paid\n  Crowdsourcing",
        "url": "http://arxiv.org/abs/2308.14499v1",
        "pub_date": "2023-08-28",
        "summary": "Accurate tree detection is of growing importance in applications such as\nurban planning, forest inventory, and environmental monitoring. In this\narticle, we present an approach to creating tree maps by annotating them in 3D\npoint clouds. Point cloud representations allow the precise identification of\ntree positions, particularly stem locations, and their heights. Our method\nleverages human computational power through paid crowdsourcing, employing a web\ntool designed to enable even non-experts to effectively tackle the task. The\nprimary focus of this paper is to discuss the web tool's development and\nstrategies to ensure high-quality tree annotations despite encountering noise\nin the crowdsourced data. Following our methodology, we achieve quality\nmeasures surpassing 90% for various challenging test sets of diverse\ncomplexities. We emphasize that our tree map creation process, including\ninitial point cloud collection, can be completed within 1-2 days.",
        "translated": ""
    },
    {
        "title": "Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware\n  Pre-training for KBQA",
        "url": "http://arxiv.org/abs/2308.14436v1",
        "pub_date": "2023-08-28",
        "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with factual information such as entities and relations in KBs.\nHowever, traditional Pre-trained Language Models (PLMs) are directly\npre-trained on large-scale natural language corpus, which poses challenges for\nthem in understanding and representing complex subgraphs in structured KBs. To\nbridge the gap between texts and structured KBs, we propose a Structured\nKnowledge-aware Pre-training method (SKP). In the pre-training stage, we\nintroduce two novel structured knowledge-aware tasks, guiding the model to\neffectively learn the implicit relationship and better representations of\ncomplex subgraphs. In downstream KBQA task, we further design an efficient\nlinearization strategy and an interval attention mechanism, which assist the\nmodel to better encode complex subgraphs and shield the interference of\nirrelevant subgraphs during reasoning respectively. Detailed experiments and\nanalyses on WebQSP verify the effectiveness of SKP, especially the significant\nimprovement in subgraph retrieval (+4.08% H@10).",
        "translated": ""
    },
    {
        "title": "Can Transformer and GNN Help Each Other?",
        "url": "http://arxiv.org/abs/2308.14355v1",
        "pub_date": "2023-08-28",
        "summary": "Although Transformer has achieved great success in natural language process\nand computer vision, it has difficulty generalizing to medium and large-scale\ngraph data for two important reasons: (i) High complexity. (ii) Failing to\ncapture the complex and entangled structure information. In graph\nrepresentation learning, Graph Neural Networks(GNNs) can fuse the graph\nstructure and node attributes but have limited receptive fields. Therefore, we\nquestion whether can we combine Transformers and GNNs to help each other. In\nthis paper, we propose a new model named TransGNN where the Transformer layer\nand GNN layer are used alternately to improve each other. Specifically, to\nexpand the receptive field and disentangle the information aggregation from\nedges, we propose using Transformer to aggregate more relevant nodes'\ninformation to improve the message passing of GNNs. Besides, to capture the\ngraph structure information, we utilize positional encoding and make use of the\nGNN layer to fuse the structure into node attributes, which improves the\nTransformer in graph data. We also propose to sample the most relevant nodes\nfor Transformer and two efficient samples update strategies to lower the\ncomplexity. At last, we theoretically prove that TransGNN is more expressive\nthan GNNs only with extra linear complexity. The experiments on eight datasets\ncorroborate the effectiveness of TransGNN on node and graph classification\ntasks.",
        "translated": ""
    },
    {
        "title": "RecMind: Large Language Model Powered Agent For Recommendation",
        "url": "http://arxiv.org/abs/2308.14296v1",
        "pub_date": "2023-08-28",
        "summary": "Recent advancements in instructing Large Language Models (LLMs) to utilize\nexternal tools and execute multi-step plans have significantly enhanced their\nability to solve intricate tasks, ranging from mathematical problems to\ncreative writing. Yet, there remains a notable gap in studying the capacity of\nLLMs in responding to personalized queries such as a recommendation request. To\nbridge this gap, we have designed an LLM-powered autonomous recommender agent,\nRecMind, which is capable of providing precise personalized recommendations\nthrough careful planning, utilizing tools for obtaining external knowledge, and\nleveraging individual data. We propose a novel algorithm, Self-Inspiring, to\nimprove the planning ability of the LLM agent. At each intermediate planning\nstep, the LLM 'self-inspires' to consider all previously explored states to\nplan for next step. This mechanism greatly improves the model's ability to\ncomprehend and utilize historical planning information for recommendation. We\nevaluate RecMind's performance in various recommendation scenarios, including\nrating prediction, sequential recommendation, direct recommendation,\nexplanation generation, and review summarization. Our experiment shows that\nRecMind outperforms existing zero/few-shot LLM-based recommendation methods in\ndifferent recommendation tasks and achieves competitive performance to a recent\nmodel P5, which requires fully pre-train for the recommendation tasks.",
        "translated": ""
    },
    {
        "title": "Alleviating Video-Length Effect for Micro-video Recommendation",
        "url": "http://arxiv.org/abs/2308.14276v1",
        "pub_date": "2023-08-28",
        "summary": "Micro-videos platforms such as TikTok are extremely popular nowadays. One\nimportant feature is that users no longer select interested videos from a set,\ninstead they either watch the recommended video or skip to the next one. As a\nresult, the time length of users' watching behavior becomes the most important\nsignal for identifying preferences. However, our empirical data analysis has\nshown a video-length effect that long videos are easier to receive a higher\nvalue of average view time, thus adopting such view-time labels for measuring\nuser preferences can easily induce a biased model that favors the longer\nvideos. In this paper, we propose a Video Length Debiasing Recommendation\n(VLDRec) method to alleviate such an effect for micro-video recommendation.\nVLDRec designs the data labeling approach and the sample generation module that\nbetter capture user preferences in a view-time oriented manner. It further\nleverages the multi-task learning technique to jointly optimize the above\nsamples with original biased ones. Extensive experiments show that VLDRec can\nimprove the users' view time by 1.81% and 11.32% on two real-world datasets,\ngiven a recommendation list of a fixed overall video length, compared with the\nbest baseline method. Moreover, VLDRec is also more effective in matching\nusers' interests in terms of the video content.",
        "translated": ""
    },
    {
        "title": "Cross-Modal Retrieval: A Systematic Review of Methods and Future\n  Directions",
        "url": "http://arxiv.org/abs/2308.14263v1",
        "pub_date": "2023-08-28",
        "summary": "With the exponential surge in diverse multi-modal data, traditional uni-modal\nretrieval methods struggle to meet the needs of users demanding access to data\nfrom various modalities. To address this, cross-modal retrieval has emerged,\nenabling interaction across modalities, facilitating semantic matching, and\nleveraging complementarity and consistency between different modal data.\nAlthough prior literature undertook a review of the cross-modal retrieval\nfield, it exhibits numerous deficiencies pertaining to timeliness, taxonomy,\nand comprehensiveness. This paper conducts a comprehensive review of\ncross-modal retrieval's evolution, spanning from shallow statistical analysis\ntechniques to vision-language pre-training models. Commencing with a\ncomprehensive taxonomy grounded in machine learning paradigms, mechanisms, and\nmodels, the paper then delves deeply into the principles and architectures\nunderpinning existing cross-modal retrieval methods. Furthermore, it offers an\noverview of widely used benchmarks, metrics, and performances. Lastly, the\npaper probes the prospects and challenges that confront contemporary\ncross-modal retrieval, while engaging in a discourse on potential directions\nfor further progress in the field. To facilitate the research on cross-modal\nretrieval, we develop an open-source code repository at\nhttps://github.com/BMC-SDNU/Cross-Modal-Retrieval.",
        "translated": ""
    },
    {
        "title": "Distributional Off-Policy Evaluation for Slate Recommendations",
        "url": "http://arxiv.org/abs/2308.14165v1",
        "pub_date": "2023-08-27",
        "summary": "Recommendation strategies are typically evaluated by using previously logged\ndata, employing off-policy evaluation methods to estimate their expected\nperformance. However, for strategies that present users with slates of multiple\nitems, the resulting combinatorial action space renders many of these methods\nimpractical. Prior work has developed estimators that leverage the structure in\nslates to estimate the expected off-policy performance, but the estimation of\nthe entire performance distribution remains elusive. Estimating the complete\ndistribution allows for a more comprehensive evaluation of recommendation\nstrategies, particularly along the axes of risk and fairness that employ\nmetrics computable from the distribution. In this paper, we propose an\nestimator for the complete off-policy performance distribution for slates and\nestablish conditions under which the estimator is unbiased and consistent. This\nbuilds upon prior work on off-policy evaluation for slates and off-policy\ndistribution estimation in reinforcement learning. We validate the efficacy of\nour method empirically on synthetic data as well as on a slate recommendation\nsimulator constructed from real-world data (MovieLens-20M). Our results show a\nsignificant reduction in estimation variance and improved sample efficiency\nover prior work across a range of slate structures.",
        "translated": ""
    },
    {
        "title": "Only Encode Once: Making Content-based News Recommender Greener",
        "url": "http://arxiv.org/abs/2308.14155v1",
        "pub_date": "2023-08-27",
        "summary": "Large pretrained language models (PLM) have become de facto news encoders in\nmodern news recommender systems, due to their strong ability in comprehending\ntextual content. These huge Transformer-based architectures, when finetuned on\nrecommendation tasks, can greatly improve news recommendation performance.\nHowever, the PLM-based pretrain-finetune framework incurs high computational\ncost and energy consumption, primarily due to the extensive redundant\nprocessing of news encoding during each training epoch. In this paper, we\npropose the ``Only Encode Once'' framework for news recommendation (OLEO), by\ndecoupling news representation learning from downstream recommendation task\nlearning. The decoupled design makes content-based news recommender as green\nand efficient as id-based ones, leading to great reduction in computational\ncost and training resources. Extensive experiments show that our OLEO framework\ncan reduce carbon emissions by up to 13 times compared with the\nstate-of-the-art pretrain-finetune framework and maintain a competitive or even\nsuperior performance level. The source code is released for reproducibility.",
        "translated": ""
    },
    {
        "title": "Robust Long-Tailed Learning via Label-Aware Bounded CVaR",
        "url": "http://arxiv.org/abs/2308.15405v1",
        "pub_date": "2023-08-29",
        "summary": "Data in the real-world classification problems are always imbalanced or\nlong-tailed, wherein the majority classes have the most of the samples that\ndominate the model training. In such setting, the naive model tends to have\npoor performance on the minority classes. Previously, a variety of loss\nmodifications have been proposed to address the long-tailed leaning problem,\nwhile these methods either treat the samples in the same class\nindiscriminatingly or lack a theoretical guarantee. In this paper, we propose\ntwo novel approaches based on CVaR (Conditional Value at Risk) to improve the\nperformance of long-tailed learning with a solid theoretical ground.\nSpecifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss\nto overcome the pessimistic result of the original CVaR, and further design the\noptimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we\nadditionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to\nstabilize the optimization process, where we also offer the theoretical\nsupport. Extensive experiments on real-world datasets with long-tailed label\ndistributions verify the superiority of our proposed methods.",
        "translated": ""
    },
    {
        "title": "A Multi-Perspective Learning to Rank Approach to Support Children's\n  Information Seeking in the Classroom",
        "url": "http://arxiv.org/abs/2308.15265v1",
        "pub_date": "2023-08-29",
        "summary": "We introduce a novel re-ranking model that aims to augment the functionality\nof standard search engines to support classroom search activities for children\n(ages 6 to 11). This model extends the known listwise learning-to-rank\nframework by balancing risk and reward. Doing so enables the model to\nprioritize Web resources of high educational alignment, appropriateness, and\nadequate readability by analyzing the URLs, snippets, and page titles of Web\nresources retrieved by a given mainstream search engine. Experimental results,\nincluding an ablation study and comparisons with existing baselines, showcase\nthe correctness of the proposed model. The outcomes of this work demonstrate\nthe value of considering multiple perspectives inherent to the classroom\nsetting, e.g., educational alignment, readability, and objectionability, when\napplied to the design of algorithms that can better support children's\ninformation discovery.",
        "translated": ""
    },
    {
        "title": "Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation",
        "url": "http://arxiv.org/abs/2308.15244v1",
        "pub_date": "2023-08-29",
        "summary": "Since Knowledge Graphs (KGs) contain rich semantic information, recently\nthere has been an influx of KG-enhanced recommendation methods. Most of\nexisting methods are entirely designed based on euclidean space without\nconsidering curvature. However, recent studies have revealed that a tremendous\ngraph-structured data exhibits highly non-euclidean properties. Motivated by\nthese observations, in this work, we propose a knowledge-based multiple\nadaptive spaces fusion method for recommendation, namely MCKG. Unlike existing\nmethods that solely adopt a specific manifold, we introduce the unified space\nthat is compatible with hyperbolic, euclidean and spherical spaces.\nFurthermore, we fuse the multiple unified spaces in an attention manner to\nobtain the high-quality embeddings for better knowledge propagation. In\naddition, we propose a geometry-aware optimization strategy which enables the\npull and push processes benefited from both hyperbolic and spherical spaces.\nSpecifically, in hyperbolic space, we set smaller margins in the area near to\nthe origin, which is conducive to distinguishing between highly similar\npositive items and negative ones. At the same time, we set larger margins in\nthe area far from the origin to ensure the model has sufficient error\ntolerance. The similar manner also applies to spherical spaces. Extensive\nexperiments on three real-world datasets demonstrate that the MCKG has a\nsignificant improvement over state-of-the-art recommendation methods. Further\nablation experiments verify the importance of multi-space fusion and\ngeometry-aware optimization strategy, justifying the rationality and\neffectiveness of MCKG.",
        "translated": ""
    },
    {
        "title": "Classification-Aware Neural Topic Model Combined With Interpretable\n  Analysis -- For Conflict Classification",
        "url": "http://arxiv.org/abs/2308.15232v1",
        "pub_date": "2023-08-29",
        "summary": "A large number of conflict events are affecting the world all the time. In\norder to analyse such conflict events effectively, this paper presents a\nClassification-Aware Neural Topic Model (CANTM-IA) for Conflict Information\nClassification and Topic Discovery. The model provides a reliable\ninterpretation of classification results and discovered topics by introducing\ninterpretability analysis. At the same time, interpretation is introduced into\nthe model architecture to improve the classification performance of the model\nand to allow interpretation to focus further on the details of the data.\nFinally, the model architecture is optimised to reduce the complexity of the\nmodel.",
        "translated": ""
    },
    {
        "title": "Providing Previously Unseen Users Fair Recommendations Using Variational\n  Autoencoders",
        "url": "http://arxiv.org/abs/2308.15230v1",
        "pub_date": "2023-08-29",
        "summary": "An emerging definition of fairness in machine learning requires that models\nare oblivious to demographic user information, e.g., a user's gender or age\nshould not influence the model. Personalized recommender systems are\nparticularly prone to violating this definition through their explicit user\nfocus and user modelling. Explicit user modelling is also an aspect that makes\nmany recommender systems incapable of providing hitherto unseen users with\nrecommendations. We propose novel approaches for mitigating discrimination in\nVariational Autoencoder-based recommender systems by limiting the encoding of\ndemographic information. The approaches are capable of, and evaluated on,\nproviding users that are not represented in the training data with fair\nrecommendations.",
        "translated": ""
    },
    {
        "title": "CAGRA: Highly Parallel Graph Construction and Approximate Nearest\n  Neighbor Search for GPUs",
        "url": "http://arxiv.org/abs/2308.15136v1",
        "pub_date": "2023-08-29",
        "summary": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in various\ndisciplines spanning data mining and artificial intelligence, from information\nretrieval and computer vision to natural language processing and recommender\nsystems. Data volumes have soared in recent years and the computational cost of\nan exhaustive exact nearest neighbor search is often prohibitive, necessitating\nthe adoption of approximate techniques. The balanced performance and recall of\ngraph-based approaches have more recently garnered significant attention in\nANNS algorithms, however, only a few studies have explored harnessing the power\nof GPUs and multi-core processors despite the widespread use of massively\nparallel and general-purpose computing. To bridge this gap, we introduce a\nnovel parallel computing hardware-based proximity graph and search algorithm.\nBy leveraging the high-performance capabilities of modern hardware, our\napproach achieves remarkable efficiency gains. In particular, our method\nsurpasses existing CPU and GPU-based methods in constructing the proximity\ngraph, demonstrating higher throughput in both large- and small-batch searches\nwhile maintaining compatible accuracy. In graph construction time, our method,\nCAGRA, is 2.2~27x faster than HNSW, which is one of the CPU SOTA\nimplementations. In large-batch query throughput in the 90% to 95% recall\nrange, our method is 33~77x faster than HNSW, and is 3.8~8.8x faster than the\nSOTA implementations for GPU. For a single query, our method is 3.4~53x faster\nthan HNSW at 95% recall.",
        "translated": ""
    },
    {
        "title": "Killing two birds with one stone: Can an audio captioning system also be\n  used for audio-text retrieval?",
        "url": "http://arxiv.org/abs/2308.15090v1",
        "pub_date": "2023-08-29",
        "summary": "Automated Audio Captioning (AAC) aims to develop systems capable of\ndescribing an audio recording using a textual sentence. In contrast, Audio-Text\nRetrieval (ATR) systems seek to find the best matching audio recording(s) for a\ngiven textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks\nrequire different types of systems: AAC employs a sequence-to-sequence model,\nwhile ATR utilizes a ranking model that compares audio and text representations\nwithin a shared projection subspace. However, this work investigates the\nrelationship between AAC and ATR by exploring the ATR capabilities of an\nunmodified AAC system, without fine-tuning for the new task. Our AAC system\nconsists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio\ntagging, and a transformer decoder responsible for generating sentences. For\nAAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on\nAudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss\nvalues obtained for any audio/caption pair. Experimental results on the Clotho\nand AudioCaps datasets demonstrate decent recall values using this simple\napproach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for\nAu-dioCaps, which is above the current state-of-the-art method without external\ndata. Interestingly, we observe that normalizing the loss values was necessary\nfor Audio-to-Text retrieval.",
        "translated": ""
    },
    {
        "title": "STEC: See-Through Transformer-based Encoder for CTR Prediction",
        "url": "http://arxiv.org/abs/2308.15033v1",
        "pub_date": "2023-08-29",
        "summary": "Click-Through Rate (CTR) prediction holds a pivotal place in online\nadvertising and recommender systems since CTR prediction performance directly\ninfluences the overall satisfaction of the users and the revenue generated by\ncompanies. Even so, CTR prediction is still an active area of research since it\ninvolves accurately modelling the preferences of users based on sparse and\nhigh-dimensional features where the higher-order interactions of multiple\nfeatures can lead to different outcomes. Most CTR prediction models have relied\non a single fusion and interaction learning strategy. The few CTR prediction\nmodels that have utilized multiple interaction modelling strategies have\ntreated each interaction to be self-contained. In this paper, we propose a\nnovel model named STEC that reaps the benefits of multiple interaction learning\napproaches in a single unified architecture. Additionally, our model introduces\nresidual connections from different orders of interactions which boosts the\nperformance by allowing lower level interactions to directly affect the\npredictions. Through extensive experiments on four real-world datasets, we\ndemonstrate that STEC outperforms existing state-of-the-art approaches for CTR\nprediction thanks to its greater expressive capabilities.",
        "translated": ""
    },
    {
        "title": "Improving Neural Ranking Models with Traditional IR Methods",
        "url": "http://arxiv.org/abs/2308.15027v1",
        "pub_date": "2023-08-29",
        "summary": "Neural ranking methods based on large transformer models have recently gained\nsignificant attention in the information retrieval community, and have been\nadopted by major commercial solutions. Nevertheless, they are computationally\nexpensive to create, and require a great deal of labeled data for specialized\ncorpora. In this paper, we explore a low resource alternative which is a\nbag-of-embedding model for document retrieval and find that it is competitive\nwith large transformer models fine tuned on information retrieval tasks. Our\nresults show that a simple combination of TF-IDF, a traditional keyword\nmatching method, with a shallow embedding model provides a low cost path to\ncompete well with the performance of complex neural ranking models on 3\ndatasets. Furthermore, adding TF-IDF measures improves the performance of\nlarge-scale fine tuned models on these tasks.",
        "translated": ""
    },
    {
        "title": "CAPS: A Practical Partition Index for Filtered Similarity Search",
        "url": "http://arxiv.org/abs/2308.15014v1",
        "pub_date": "2023-08-29",
        "summary": "With the surging popularity of approximate near-neighbor search (ANNS),\ndriven by advances in neural representation learning, the ability to serve\nqueries accompanied by a set of constraints has become an area of intense\ninterest. While the community has recently proposed several algorithms for\nconstrained ANNS, almost all of these methods focus on integration with\ngraph-based indexes, the predominant class of algorithms achieving\nstate-of-the-art performance in latency-recall tradeoffs. In this work, we take\na different approach and focus on developing a constrained ANNS algorithm via\nspace partitioning as opposed to graphs. To that end, we introduce Constrained\nApproximate Partitioned Search (CAPS), an index for ANNS with filters via space\npartitions that not only retains the benefits of a partition-based algorithm\nbut also outperforms state-of-the-art graph-based constrained search techniques\nin recall-latency tradeoffs, with only 10% of the index size.",
        "translated": ""
    },
    {
        "title": "Adaptive Multi-Modalities Fusion in Sequential Recommendation Systems",
        "url": "http://arxiv.org/abs/2308.15980v1",
        "pub_date": "2023-08-30",
        "summary": "In sequential recommendation, multi-modal information (e.g., text or image)\ncan provide a more comprehensive view of an item's profile. The optimal stage\n(early or late) to fuse modality features into item representations is still\ndebated. We propose a graph-based approach (named MMSR) to fuse modality\nfeatures in an adaptive order, enabling each modality to prioritize either its\ninherent sequential nature or its interplay with other modalities. MMSR\nrepresents each user's history as a graph, where the modality features of each\nitem in a user's history sequence are denoted by cross-linked nodes. The edges\nbetween homogeneous nodes represent intra-modality sequential relationships,\nand the ones between heterogeneous nodes represent inter-modality\ninterdependence relationships. During graph propagation, MMSR incorporates dual\nattention, differentiating homogeneous and heterogeneous neighbors. To\nadaptively assign nodes with distinct fusion orders, MMSR allows each node's\nrepresentation to be asynchronously updated through an update gate. In\nscenarios where modalities exhibit stronger sequential relationships, the\nupdate gate prioritizes updates among homogeneous nodes. Conversely, when the\ninterdependent relationships between modalities are more pronounced, the update\ngate prioritizes updates among heterogeneous nodes. Consequently, MMSR\nestablishes a fusion order that spans a spectrum from early to late modality\nfusion. In experiments across six datasets, MMSR consistently outperforms\nstate-of-the-art models, and our graph propagation methods surpass other graph\nneural networks. Additionally, MMSR naturally manages missing modalities.",
        "translated": ""
    },
    {
        "title": "Denoising Attention for Query-aware User Modeling in Personalized Search",
        "url": "http://arxiv.org/abs/2308.15968v1",
        "pub_date": "2023-08-30",
        "summary": "The personalization of search results has gained increasing attention in the\npast few years, thanks to the development of Neural Networks-based approaches\nfor Information Retrieval and the importance of personalization in many search\nscenarios. Recent works have proposed to build user models at query time by\nleveraging the Attention mechanism, which allows weighing the contribution of\nthe user-related information w.r.t. the current query. This approach allows\ntaking into account the diversity of the user's interests by giving more\nimportance to those related to the current search performed by the user.\n  In this paper, we first discuss some shortcomings of the standard Attention\nformulation when employed for personalization. In particular, we focus on\nissues related to its normalization mechanism and its inability to entirely\nfilter out noisy user-related information. Then, we introduce the Denoising\nAttention mechanism: an Attention variant that directly tackles the above\nshortcomings by adopting a robust normalization scheme and introducing a\nfiltering mechanism. The reported experimental evaluation shows the benefits of\nthe proposed approach over other Attention-based variants.",
        "translated": ""
    },
    {
        "title": "DRGame: Diversified Recommendation for Multi-category Video Games with\n  Balanced Implicit Preferences",
        "url": "http://arxiv.org/abs/2308.15823v1",
        "pub_date": "2023-08-30",
        "summary": "The growing popularity of subscription services in video game consumption has\nemphasized the importance of offering diversified recommendations. Providing\nusers with a diverse range of games is essential for ensuring continued\nengagement and fostering long-term subscriptions. However, existing\nrecommendation models face challenges in effectively handling highly imbalanced\nimplicit feedback in gaming interactions. Additionally, they struggle to take\ninto account the distinctive characteristics of multiple categories and the\nlatent user interests associated with these categories. In response to these\nchallenges, we propose a novel framework, named DRGame, to obtain diversified\nrecommendation. It is centered on multi-category video games, consisting of two\n{components}: Balance-driven Implicit Preferences Learning for data\npre-processing and Clustering-based Diversified Recommendation {Module} for\nfinal prediction. The first module aims to achieve a balanced representation of\nimplicit feedback in game time, thereby discovering a comprehensive view of\nplayer interests across different categories. The second module adopts\ncategory-aware representation learning to cluster and select players and games\nbased on balanced implicit preferences, and then employs asymmetric neighbor\naggregation to achieve diversified recommendations. Experimental results on a\nreal-world dataset demonstrate the superiority of our proposed method over\nexisting approaches in terms of game diversity recommendations.",
        "translated": ""
    },
    {
        "title": "Knowledge-grounded Natural Language Recommendation Explanation",
        "url": "http://arxiv.org/abs/2308.15813v1",
        "pub_date": "2023-08-30",
        "summary": "Explanations accompanied by a recommendation can assist users in\nunderstanding the decision made by recommendation systems, which in turn\nincreases a user's confidence and trust in the system. Recently, research has\nfocused on generating natural language explanations in a human-readable format.\nThus far, the proposed approaches leverage item reviews written by users, which\nare often subjective, sparse in language, and unable to account for new items\nthat have not been purchased or reviewed before. Instead, we aim to generate\nfact-grounded recommendation explanations that are objectively described with\nitem features while implicitly considering a user's preferences, based on the\nuser's purchase history. To achieve this, we propose a knowledge graph (KG)\napproach to natural language explainable recommendation. Our approach draws on\nuser-item features through a novel collaborative filtering-based KG\nrepresentation to produce fact-grounded, personalized explanations, while\njointly learning user-item representations for recommendation scoring.\nExperimental results show that our approach consistently outperforms previous\nstate-of-the-art models on natural language explainable recommendation.",
        "translated": ""
    },
    {
        "title": "Fragment and Integrate Network (FIN): A Novel Spatial-Temporal Modeling\n  Based on Long Sequential Behavior for Online Food Ordering Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2308.15703v1",
        "pub_date": "2023-08-30",
        "summary": "Spatial-temporal information has been proven to be of great significance for\nclick-through rate prediction tasks in online Location-Based Services (LBS),\nespecially in mainstream food ordering platforms such as DoorDash, Uber Eats,\nMeituan, and Ele.me. Modeling user spatial-temporal preferences with sequential\nbehavior data has become a hot topic in recommendation systems and online\nadvertising. However, most of existing methods either lack the representation\nof rich spatial-temporal information or only handle user behaviors with limited\nlength, e.g. 100. In this paper, we tackle these problems by designing a new\nspatial-temporal modeling paradigm named Fragment and Integrate Network (FIN).\nFIN consists of two networks: (i) Fragment Network (FN) extracts Multiple\nSub-Sequences (MSS) from lifelong sequential behavior data, and captures the\nspecific spatial-temporal representation by modeling each MSS respectively.\nHere both a simplified attention and a complicated attention are adopted to\nbalance the performance gain and resource consumption. (ii) Integrate Network\n(IN) builds a new integrated sequence by utilizing spatial-temporal interaction\non MSS and captures the comprehensive spatial-temporal representation by\nmodeling the integrated sequence with a complicated attention. Both public\ndatasets and production datasets have demonstrated the accuracy and scalability\nof FIN. Since 2022, FIN has been fully deployed in the recommendation\nadvertising system of Ele.me, one of the most popular online food ordering\nplatforms in China, obtaining 5.7% improvement on Click-Through Rate (CTR) and\n7.3% increase on Revenue Per Mille (RPM).",
        "translated": ""
    },
    {
        "title": "A Survey on Multi-Behavior Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.15701v1",
        "pub_date": "2023-08-30",
        "summary": "Recommender systems is set up to address the issue of information overload in\ntraditional information retrieval systems, which is focused on recommending\ninformation that is of most interest to users from massive information.\nGenerally, there is a sequential nature and heterogeneity to the behavior of a\nperson interacting with a system, leading to the proposal of multi-behavior\nsequential recommendation (MBSR). MBSR is a relatively new and worthy direction\nfor in-depth research, which can achieve state-of-the-art recommendation\nthrough suitable modeling, and some related works have been proposed. This\nsurvey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in\ndetail, including its problem definition, application scenarios and challenges\nfaced. Secondly, we detail the classification of MBSR, including\nneighborhood-based methods, matrix factorization-based methods and deep\nlearning-based methods, where we further classify the deep learning-based\nmethods into different learning architectures based on RNN, GNN, Transformer,\nand generic architectures as well as architectures that integrate hybrid\ntechniques. In each method, we present related works based on the data\nperspective and the modeling perspective, as well as analyze the strengths,\nweaknesses and features of these works. Finally, we discuss some promising\nfuture research directions to address the challenges and improve the current\nstatus of MBSR.",
        "translated": ""
    },
    {
        "title": "Ensuring User-side Fairness in Dynamic Recommender Systems",
        "url": "http://arxiv.org/abs/2308.15651v1",
        "pub_date": "2023-08-29",
        "summary": "User-side group fairness is crucial for modern recommender systems, as it\naims to alleviate performance disparity between groups of users defined by\nsensitive attributes such as gender, race, or age. We find that the disparity\ntends to persist or even increase over time. This calls for effective ways to\naddress user-side fairness in a dynamic environment, which has been\ninfrequently explored in the literature. However, fairness-constrained\nre-ranking, a typical method to ensure user-side fairness (i.e., reducing\nperformance disparity), faces two fundamental challenges in the dynamic\nsetting: (1) non-differentiability of the ranking-based fairness constraint,\nwhich hinders the end-to-end training paradigm, and (2) time-inefficiency,\nwhich impedes quick adaptation to changes in user preferences. In this paper,\nwe propose FAir Dynamic rEcommender (FADE), an end-to-end framework with\nfine-tuning strategy to dynamically alleviate performance disparity. To tackle\nthe above challenges, FADE uses a novel fairness loss designed to be\ndifferentiable and lightweight to fine-tune model parameters to ensure both\nuser-side fairness and high-quality recommendations. Via extensive experiments\non the real-world dataset, we empirically demonstrate that FADE effectively and\nefficiently reduces performance disparity, and furthermore, FADE improves\noverall recommendation quality over time compared to not using any new data.",
        "translated": ""
    },
    {
        "title": "Dimensionality Reduction Using pseudo-Boolean polynomials For Cluster\n  Analysis",
        "url": "http://arxiv.org/abs/2308.15553v1",
        "pub_date": "2023-08-29",
        "summary": "We introduce usage of a reduction property of penalty-based formulation of\npseudo-Boolean polynomials as a mechanism for invariant dimensionality\nreduction in cluster analysis processes. In our experiments, we show that\nmultidimensional data, like 4-dimensional Iris Flower dataset can be reduced to\n2-dimensional space while the 30-dimensional Wisconsin Diagnostic Breast Cancer\n(WDBC) dataset can be reduced to 3-dimensional space, and by searching lines or\nplanes that lie between reduced samples we can extract clusters in a linear and\nunbiased manner with competitive accuracies, reproducibility and clear\ninterpretation.",
        "translated": ""
    },
    {
        "title": "Co-evolving Vector Quantization for ID-based Recommendation",
        "url": "http://arxiv.org/abs/2308.16761v1",
        "pub_date": "2023-08-31",
        "summary": "Category information plays a crucial role in enhancing the quality and\npersonalization of recommendations. Nevertheless, the availability of item\ncategory information is not consistently present, particularly in the context\nof ID-based recommendations. In this work, we propose an alternative approach\nto automatically learn and generate entity (i.e., user and item) categorical\ninformation at different levels of granularity, specifically for ID-based\nrecommendation. Specifically, we devise a co-evolving vector quantization\nframework, namely COVE, which enables the simultaneous learning and refinement\nof code representation and entity embedding in an end-to-end manner, starting\nfrom the randomly initialized states. With its high adaptability, COVE can be\neasily integrated into existing recommendation models. We validate the\neffectiveness of COVE on various recommendation tasks including list\ncompletion, collaborative filtering, and click-through rate prediction, across\ndifferent recommendation models. We will publish the code and data for other\nresearchers to reproduce our work.",
        "translated": ""
    },
    {
        "title": "Context Aware Query Rewriting for Text Rankers using LLM",
        "url": "http://arxiv.org/abs/2308.16753v1",
        "pub_date": "2023-08-31",
        "summary": "Query rewriting refers to an established family of approaches that are\napplied to underspecified and ambiguous queries to overcome the vocabulary\nmismatch problem in document ranking. Queries are typically rewritten during\nquery processing time for better query modelling for the downstream ranker.\nWith the advent of large-language models (LLMs), there have been initial\ninvestigations into using generative approaches to generate pseudo documents to\ntackle this inherent vocabulary gap. In this work, we analyze the utility of\nLLMs for improved query rewriting for text ranking tasks. We find that there\nare two inherent limitations of using LLMs as query re-writers -- concept drift\nwhen using only queries as prompts and large inference costs during query\nprocessing. We adopt a simple, yet surprisingly effective, approach called\ncontext aware query rewriting (CAR) to leverage the benefits of LLMs for query\nunderstanding. Firstly, we rewrite ambiguous training queries by context-aware\nprompting of LLMs, where we use only relevant documents as context.Unlike\nexisting approaches, we use LLM-based query rewriting only during the training\nphase. Eventually, a ranker is fine-tuned on the rewritten queries instead of\nthe original queries during training. In our extensive experiments, we find\nthat fine-tuning a ranker using re-written queries offers a significant\nimprovement of up to 33% on the passage ranking task and up to 28% on the\ndocument ranking task when compared to the baseline performance of using\noriginal queries.",
        "translated": ""
    },
    {
        "title": "Concentrating on the Impact: Consequence-based Explanations in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2308.16708v1",
        "pub_date": "2023-08-31",
        "summary": "Recommender systems assist users in decision-making, where the presentation\nof recommended items and their explanations are critical factors for enhancing\nthe overall user experience. Although various methods for generating\nexplanations have been proposed, there is still room for improvement,\nparticularly for users who lack expertise in a specific item domain. In this\nstudy, we introduce the novel concept of \\textit{consequence-based\nexplanations}, a type of explanation that emphasizes the individual impact of\nconsuming a recommended item on the user, which makes the effect of following\nrecommendations clearer. We conducted an online user study to examine our\nassumption about the appreciation of consequence-based explanations and their\nimpacts on different explanation aims in recommender systems. Our findings\nhighlight the importance of consequence-based explanations, which were\nwell-received by users and effectively improved user satisfaction in\nrecommender systems. These results provide valuable insights for designing\nengaging explanations that can enhance the overall user experience in\ndecision-making.",
        "translated": ""
    },
    {
        "title": "Towards Long-Tailed Recognition for Graph Classification via\n  Collaborative Experts",
        "url": "http://arxiv.org/abs/2308.16609v1",
        "pub_date": "2023-08-31",
        "summary": "Graph classification, aiming at learning the graph-level representations for\neffective class assignments, has received outstanding achievements, which\nheavily relies on high-quality datasets that have balanced class distribution.\nIn fact, most real-world graph data naturally presents a long-tailed form,\nwhere the head classes occupy much more samples than the tail classes, it thus\nis essential to study the graph-level classification over long-tailed data\nwhile still remaining largely unexplored. However, most existing long-tailed\nlearning methods in visions fail to jointly optimize the representation\nlearning and classifier training, as well as neglect the mining of the\nhard-to-classify classes. Directly applying existing methods to graphs may lead\nto sub-optimal performance, since the model trained on graphs would be more\nsensitive to the long-tailed distribution due to the complex topological\ncharacteristics. Hence, in this paper, we propose a novel long-tailed\ngraph-level classification framework via Collaborative Multi-expert Learning\n(CoMe) to tackle the problem. To equilibrate the contributions of head and tail\nclasses, we first develop balanced contrastive learning from the view of\nrepresentation learning, and then design an individual-expert classifier\ntraining based on hard class mining. In addition, we execute gated fusion and\ndisentangled knowledge distillation among the multiple experts to promote the\ncollaboration in a multi-expert framework. Comprehensive experiments are\nperformed on seven widely-used benchmark datasets to demonstrate the\nsuperiority of our method CoMe over state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Recommender AI Agent: Integrating Large Language Models for Interactive\n  Recommendations",
        "url": "http://arxiv.org/abs/2308.16505v1",
        "pub_date": "2023-08-31",
        "summary": "Recommender models excel at providing domain-specific item recommendations by\nleveraging extensive user behavior data. Despite their ability to act as\nlightweight domain experts, they struggle to perform versatile tasks such as\nproviding explanations and engaging in conversations. On the other hand, large\nlanguage models (LLMs) represent a significant step towards artificial general\nintelligence, showcasing remarkable capabilities in instruction comprehension,\ncommonsense reasoning, and human interaction. However, LLMs lack the knowledge\nof domain-specific item catalogs and behavioral patterns, particularly in areas\nthat diverge from general world knowledge, such as online e-commerce.\nFinetuning LLMs for each domain is neither economic nor efficient.\n  In this paper, we bridge the gap between recommender models and LLMs,\ncombining their respective strengths to create a versatile and interactive\nrecommender system. We introduce an efficient framework called RecAgent, which\nemploys LLMs as the brain and recommender models as tools. We first outline a\nminimal set of essential tools required to transform LLMs into RecAgent. We\nthen propose an efficient workflow within RecAgent for task execution,\nincorporating key components such as a memory bus, dynamic\ndemonstration-augmented task planning, and reflection. RecAgent enables\ntraditional recommender systems, such as those ID-based matrix factorization\nmodels, to become interactive systems with a natural language interface through\nthe integration of LLMs. Experimental results on several public datasets show\nthat RecAgent achieves satisfying performance as a conversational recommender\nsystem, outperforming general-purpose LLMs.",
        "translated": ""
    },
    {
        "title": "AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR\n  Prediction",
        "url": "http://arxiv.org/abs/2308.16437v1",
        "pub_date": "2023-08-31",
        "summary": "Click-through rate (CTR) prediction is a crucial issue in recommendation\nsystems. There has been an emergence of various public CTR datasets. However,\nexisting datasets primarily suffer from the following limitations. Firstly,\nusers generally click different types of items from multiple scenarios, and\nmodeling from multiple scenarios can provide a more comprehensive understanding\nof users. Existing datasets only include data for the same type of items from a\nsingle scenario. Secondly, multi-modal features are essential in multi-scenario\nprediction as they address the issue of inconsistent ID encoding between\ndifferent scenarios. The existing datasets are based on ID features and lack\nmulti-modal features. Third, a large-scale dataset can provide a more reliable\nevaluation of models, fully reflecting the performance differences between\nmodels. The scale of existing datasets is around 100 million, which is\nrelatively small compared to the real-world CTR prediction. To address these\nlimitations, we propose AntM$^{2}$C, a Multi-Scenario Multi-Modal CTR dataset\nbased on industrial data from Alipay. Specifically, AntM$^{2}$C provides the\nfollowing advantages: 1) It covers CTR data of 5 different types of items,\nproviding insights into the preferences of users for different items, including\nadvertisements, vouchers, mini-programs, contents, and videos. 2) Apart from\nID-based features, AntM$^{2}$C also provides 2 multi-modal features, raw text\nand image features, which can effectively establish connections between items\nwith different IDs. 3) AntM$^{2}$C provides 1 billion CTR data with 200\nfeatures, including 200 million users and 6 million items. It is currently the\nlargest-scale CTR dataset available. Based on AntM$^{2}$C, we construct several\ntypical CTR tasks and provide comparisons with baseline methods. The dataset\nhomepage is available at https://www.atecup.cn/home.",
        "translated": ""
    },
    {
        "title": "NeMig -- A Bilingual News Collection and Knowledge Graph about Migration",
        "url": "http://arxiv.org/abs/2309.00550v1",
        "pub_date": "2023-09-01",
        "summary": "News recommendation plays a critical role in shaping the public's worldviews\nthrough the way in which it filters and disseminates information about\ndifferent topics. Given the crucial impact that media plays in opinion\nformation, especially for sensitive topics, understanding the effects of\npersonalized recommendation beyond accuracy has become essential in today's\ndigital society. In this work, we present NeMig, a bilingual news collection on\nthe topic of migration, and corresponding rich user data. In comparison to\nexisting news recommendation datasets, which comprise a large variety of\nmonolingual news, NeMig covers articles on a single controversial topic,\npublished in both Germany and the US. We annotate the sentiment polarization of\nthe articles and the political leanings of the media outlets, in addition to\nextracting subtopics and named entities disambiguated through Wikidata. These\nfeatures can be used to analyze the effects of algorithmic news curation beyond\naccuracy-based performance, such as recommender biases and the creation of\nfilter bubbles. We construct domain-specific knowledge graphs from the news\ntext and metadata, thus encoding knowledge-level connections between articles.\nImportantly, while existing datasets include only click behavior, we collect\nuser socio-demographic and political information in addition to explicit click\nfeedback. We demonstrate the utility of NeMig through experiments on the tasks\nof news recommenders benchmarking, analysis of biases in recommenders, and news\ntrends analysis. NeMig aims to provide a useful resource for the news\nrecommendation community and to foster interdisciplinary research into the\nmultidimensional effects of algorithmic news curation.",
        "translated": ""
    },
    {
        "title": "General and Practical Tuning Method for Off-the-Shelf Graph-Based Index:\n  SISAP Indexing Challenge Report by Team UTokyo",
        "url": "http://arxiv.org/abs/2309.00472v1",
        "pub_date": "2023-09-01",
        "summary": "Despite the efficacy of graph-based algorithms for Approximate Nearest\nNeighbor (ANN) searches, the optimal tuning of such systems remains unclear.\nThis study introduces a method to tune the performance of off-the-shelf\ngraph-based indexes, focusing on the dimension of vectors, database size, and\nentry points of graph traversal. We utilize a black-box optimization algorithm\nto perform integrated tuning to meet the required levels of recall and Queries\nPer Second (QPS). We applied our approach to Task A of the SISAP 2023 Indexing\nChallenge and got second place in the 10M and 30M tracks. It improves\nperformance substantially compared to brute force methods. This research offers\na universally applicable tuning method for graph-based indexes, extending\nbeyond the specific conditions of the competition to broader uses.",
        "translated": ""
    },
    {
        "title": "Explainable Active Learning for Preference Elicitation",
        "url": "http://arxiv.org/abs/2309.00356v1",
        "pub_date": "2023-09-01",
        "summary": "Gaining insights into the preferences of new users and subsequently\npersonalizing recommendations necessitate managing user interactions\nintelligently, namely, posing pertinent questions to elicit valuable\ninformation effectively. In this study, our focus is on a specific scenario of\nthe cold-start problem, where the recommendation system lacks adequate user\npresence or access to other users' data is restricted, obstructing employing\nuser profiling methods utilizing existing data in the system. We employ Active\nLearning (AL) to solve the addressed problem with the objective of maximizing\ninformation acquisition with minimal user effort. AL operates for selecting\ninformative data from a large unlabeled set to inquire an oracle to label them\nand eventually updating a machine learning (ML) model. We operate AL in an\nintegrated process of unsupervised, semi-supervised, and supervised ML within\nan explanatory preference elicitation process. It harvests user feedback (given\nfor the system's explanations on the presented items) over informative samples\nto update an underlying ML model estimating user preferences. The designed user\ninteraction facilitates personalizing the system by incorporating user feedback\ninto the ML model and also enhances user trust by refining the system's\nexplanations on recommendations. We implement the proposed preference\nelicitation methodology for food recommendation. We conducted human experiments\nto assess its efficacy in the short term and also experimented with several AL\nstrategies over synthetic user profiles that we created for two food datasets,\naiming for long-term performance analysis. The experimental results demonstrate\nthe efficiency of the proposed preference elicitation with limited user-labeled\ndata while also enhancing user trust through accurate explanations.",
        "translated": ""
    },
    {
        "title": "Towards Contrastive Learning in Music Video Domain",
        "url": "http://arxiv.org/abs/2309.00347v1",
        "pub_date": "2023-09-01",
        "summary": "Contrastive learning is a powerful way of learning multimodal representations\nacross various domains such as image-caption retrieval and audio-visual\nrepresentation learning. In this work, we investigate if these findings\ngeneralize to the domain of music videos. Specifically, we create a dual\nen-coder for the audio and video modalities and train it using a bidirectional\ncontrastive loss. For the experiments, we use an industry dataset containing\n550 000 music videos as well as the public Million Song Dataset, and evaluate\nthe quality of learned representations on the downstream tasks of music tagging\nand genre classification. Our results indicate that pre-trained networks\nwithout contrastive fine-tuning outperform our contrastive learning approach\nwhen evaluated on both tasks. To gain a better understanding of the reasons\ncontrastive learning was not successful for music videos, we perform a\nqualitative analysis of the learned representations, revealing why contrastive\nlearning might have difficulties uniting embeddings from two modalities. Based\non these findings, we outline possible directions for future work. To\nfacilitate the reproducibility of our results, we share our code and the\npre-trained model.",
        "translated": ""
    },
    {
        "title": "Fairness of Exposure in Dynamic Recommendation",
        "url": "http://arxiv.org/abs/2309.02322v1",
        "pub_date": "2023-09-05",
        "summary": "Exposure bias is a well-known issue in recommender systems where the exposure\nis not fairly distributed among items in the recommendation results. This is\nespecially problematic when bias is amplified over time as a few items (e.g.,\npopular ones) are repeatedly over-represented in recommendation lists and\nusers' interactions with those items will amplify bias towards those items over\ntime resulting in a feedback loop. This issue has been extensively studied in\nthe literature in static recommendation environment where a single round of\nrecommendation result is processed to improve the exposure fairness. However,\nless work has been done on addressing exposure bias in a dynamic recommendation\nsetting where the system is operating over time, the recommendation model and\nthe input data are dynamically updated with ongoing user feedback on\nrecommended items at each round. In this paper, we study exposure bias in a\ndynamic recommendation setting. Our goal is to show that existing bias\nmitigation methods that are designed to operate in a static recommendation\nsetting are unable to satisfy fairness of exposure for items in long run. In\nparticular, we empirically study one of these methods and show that repeatedly\napplying this method fails to fairly distribute exposure among items in long\nrun. To address this limitation, we show how this method can be adapted to\neffectively operate in a dynamic recommendation setting and achieve exposure\nfairness for items in long run. Experiments on a real-world dataset confirm\nthat our solution is superior in achieving long-term exposure fairness for the\nitems while maintaining the recommendation accuracy.",
        "translated": ""
    },
    {
        "title": "STGIN: Spatial-Temporal Graph Interaction Network for Large-scale POI\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.02251v1",
        "pub_date": "2023-09-05",
        "summary": "In Location-Based Services, Point-Of-Interest(POI) recommendation plays a\ncrucial role in both user experience and business opportunities. Graph neural\nnetworks have been proven effective in providing personalized POI\nrecommendation services. However, there are still two critical challenges.\nFirst, existing graph models attempt to capture users' diversified interests\nthrough a unified graph, which limits their ability to express interests in\nvarious spatial-temporal contexts. Second, the efficiency limitations of graph\nconstruction and graph sampling in large-scale systems make it difficult to\nadapt quickly to new real-time interests. To tackle the above challenges, we\npropose a novel Spatial-Temporal Graph Interaction Network. Specifically, we\nconstruct subgraphs of spatial, temporal, spatial-temporal, and global views\nrespectively to precisely characterize the user's interests in various\ncontexts. In addition, we design an industry-friendly framework to track the\nuser's latest interests. Extensive experiments on the real-world dataset show\nthat our method outperforms state-of-the-art models. This work has been\nsuccessfully deployed in a large e-commerce platform, delivering a 1.1% CTR and\n6.3% RPM improvement.",
        "translated": ""
    },
    {
        "title": "TensorBank:Tensor Lakehouse for Foundation Model Training",
        "url": "http://arxiv.org/abs/2309.02094v1",
        "pub_date": "2023-09-05",
        "summary": "Storing and streaming high dimensional data for foundation model training\nbecame a critical requirement with the rise of foundation models beyond natural\nlanguage. In this paper we introduce TensorBank, a petabyte scale tensor\nlakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU\nmemory at wire speed based on complex relational queries. We use Hierarchical\nStatistical Indices (HSI) for query acceleration. Our architecture allows to\ndirectly address tensors on block level using HTTP range reads. Once in GPU\nmemory, data can be transformed using PyTorch transforms. We provide a generic\nPyTorch dataset type with a corresponding dataset factory translating\nrelational queries and requested transformations as an instance. By making use\nof the HSI, irrelevant blocks can be skipped without reading them as those\nindices contain statistics on their content at different hierarchical\nresolution levels. This is an opinionated architecture powered by open\nstandards and making heavy use of open-source technology. Although, hardened\nfor production use using geospatial-temporal data, this architecture\ngeneralizes to other use case like computer vision, computational neuroscience,\nbiological sequence analysis and more.",
        "translated": ""
    },
    {
        "title": "MvFS: Multi-view Feature Selection for Recommender System",
        "url": "http://arxiv.org/abs/2309.02064v1",
        "pub_date": "2023-09-05",
        "summary": "Feature selection, which is a technique to select key features in recommender\nsystems, has received increasing research attention. Recently, Adaptive Feature\nSelection (AdaFS) has shown remarkable performance by adaptively selecting\nfeatures for each data instance, considering that the importance of a given\nfeature field can vary significantly across data. However, this method still\nhas limitations in that its selection process could be easily biased to major\nfeatures that frequently occur. To address these problems, we propose\nMulti-view Feature Selection (MvFS), which selects informative features for\neach instance more effectively. Most importantly, MvFS employs a multi-view\nnetwork consisting of multiple sub-networks, each of which learns to measure\nthe feature importance of a part of data with different feature patterns. By\ndoing so, MvFS promotes a more balanced feature selection process mitigating\nthe bias problem towards dominant patterns. Moreover, MvFS adopts an effective\nimportance score modeling strategy which is applied independently to each field\nwithout incurring dependency among features. Experimental results on real-world\ndatasets demonstrate the effectiveness of MvFS compared to state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "Scenario-Aware Hierarchical Dynamic Network for Multi-Scenario\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.02061v1",
        "pub_date": "2023-09-05",
        "summary": "Click-Through Rate (CTR) prediction is a fundamental technique in\nrecommendation and advertising systems. Recent studies have shown that\nimplementing multi-scenario recommendations contributes to strengthening\ninformation sharing and improving overall performance. However, existing\nmulti-scenario models only consider coarse-grained explicit scenario modeling\nthat depends on pre-defined scenario identification from manual prior rules,\nwhich is biased and sub-optimal. To address these limitations, we propose a\nScenario-Aware Hierarchical Dynamic Network for Multi-Scenario Recommendations\n(HierRec), which perceives implicit patterns adaptively and conducts explicit\nand implicit scenario modeling jointly. In particular, HierRec designs a basic\nscenario-oriented module based on the dynamic weight to capture\nscenario-specific information. Then the hierarchical explicit and implicit\nscenario-aware modules are proposed to model hybrid-grained scenario\ninformation. The multi-head implicit modeling design contributes to perceiving\ndistinctive patterns from different perspectives. Our experiments on two public\ndatasets and real-world industrial applications on a mainstream online\nadvertising platform demonstrate that our HierRec outperforms existing models\nsignificantly.",
        "translated": ""
    },
    {
        "title": "Robust Recommender System: A Survey and Future Directions",
        "url": "http://arxiv.org/abs/2309.02057v1",
        "pub_date": "2023-09-05",
        "summary": "With the rapid growth of information, recommender systems have become\nintegral for providing personalized suggestions and overcoming information\noverload. However, their practical deployment often encounters \"dirty\" data,\nwhere noise or malicious information can lead to abnormal recommendations.\nResearch on improving recommender systems' robustness against such dirty data\nhas thus gained significant attention. This survey provides a comprehensive\nreview of recent work on recommender systems' robustness. We first present a\ntaxonomy to organize current techniques for withstanding malicious attacks and\nnatural noise. We then explore state-of-the-art methods in each category,\nincluding fraudster detection, adversarial training, certifiable robust\ntraining against malicious attacks, and regularization, purification,\nself-supervised learning against natural noise. Additionally, we summarize\nevaluation metrics and common datasets used to assess robustness. We discuss\nrobustness across varying recommendation scenarios and its interplay with other\nproperties like accuracy, interpretability, privacy, and fairness. Finally, we\ndelve into open issues and future research directions in this emerging field.\nOur goal is to equip readers with a holistic understanding of robust\nrecommender systems and spotlight pathways for future research and development.",
        "translated": ""
    },
    {
        "title": "Towards Individual and Multistakeholder Fairness in Tourism Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2309.02052v1",
        "pub_date": "2023-09-05",
        "summary": "This position paper summarizes our published review on individual and\nmultistakeholder fairness in Tourism Recommender Systems (TRS). Recently, there\nhas been growing attention to fairness considerations in recommender systems\n(RS). It has been acknowledged in research that fairness in RS is often closely\ntied to the presence of multiple stakeholders, such as end users, item\nproviders, and platforms, as it raises concerns for the fair treatment of all\nparties involved. Hence, fairness in RS is a multi-faceted concept that\nrequires consideration of the perspectives and needs of the different\nstakeholders to ensure fair outcomes for them. However, there may often be\ninstances where achieving the goals of one stakeholder could conflict with\nthose of another, resulting in trade-offs.\n  In this paper, we emphasized addressing the unique challenges of ensuring\nfairness in RS within the tourism domain. We aimed to discuss potential\nstrategies for mitigating the aforementioned challenges and examine the\napplicability of solutions from other domains to tackle fairness issues in\ntourism. By exploring cross-domain approaches and strategies for incorporating\nS-Fairness, we can uncover valuable insights and determine how these solutions\ncan be adapted and implemented effectively in the context of tourism to enhance\nfairness in RS.",
        "translated": ""
    },
    {
        "title": "DiscoverPath: A Knowledge Refinement and Retrieval System for\n  Interdisciplinarity on Biomedical Research",
        "url": "http://arxiv.org/abs/2309.01808v1",
        "pub_date": "2023-09-04",
        "summary": "The exponential growth in scholarly publications necessitates advanced tools\nfor efficient article retrieval, especially in interdisciplinary fields where\ndiverse terminologies are used to describe similar research. Traditional\nkeyword-based search engines often fall short in assisting users who may not be\nfamiliar with specific terminologies. To address this, we present a knowledge\ngraph-based paper search engine for biomedical research to enhance the user\nexperience in discovering relevant queries and articles. The system, dubbed\nDiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS)\ntagging to extract terminologies and relationships from article abstracts to\ncreate a KG. To reduce information overload, DiscoverPath presents users with a\nfocused subgraph containing the queried entity and its neighboring nodes and\nincorporates a query recommendation system, enabling users to iteratively\nrefine their queries. The system is equipped with an accessible Graphical User\nInterface that provides an intuitive visualization of the KG, query\nrecommendations, and detailed article information, enabling efficient article\nretrieval, thus fostering interdisciplinary knowledge exploration. DiscoverPath\nis open-sourced at https://github.com/ynchuang/DiscoverPath.",
        "translated": ""
    },
    {
        "title": "CRUISE-Screening: Living Literature Reviews Toolbox",
        "url": "http://arxiv.org/abs/2309.01684v1",
        "pub_date": "2023-09-04",
        "summary": "Keeping up with research and finding related work is still a time-consuming\ntask for academics. Researchers sift through thousands of studies to identify a\nfew relevant ones. Automation techniques can help by increasing the efficiency\nand effectiveness of this task. To this end, we developed CRUISE-Screening, a\nweb-based application for conducting living literature reviews - a type of\nliterature review that is continuously updated to reflect the latest research\nin a particular field. CRUISE-Screening is connected to several search engines\nvia an API, which allows for updating the search results periodically.\nMoreover, it can facilitate the process of screening for relevant publications\nby using text classification and question answering models. CRUISE-Screening\ncan be used both by researchers conducting literature reviews and by those\nworking on automating the citation screening process to validate their\nalgorithms. The application is open-source:\nhttps://github.com/ProjectDoSSIER/cruise-screening, and a demo is available\nunder this URL: https://citation-screening.ec.tuwien.ac.at. We discuss the\nlimitations of our tool in Appendix A.",
        "translated": ""
    },
    {
        "title": "Fair Ranking under Disparate Uncertainty",
        "url": "http://arxiv.org/abs/2309.01610v1",
        "pub_date": "2023-09-04",
        "summary": "Ranking is a ubiquitous method for focusing the attention of human evaluators\non a manageable subset of options. Its use ranges from surfacing potentially\nrelevant products on an e-commerce site to prioritizing college applications\nfor human review. While ranking can make human evaluation far more effective by\nfocusing attention on the most promising options, we argue that it can\nintroduce unfairness if the uncertainty of the underlying relevance model\ndiffers between groups of options. Unfortunately, such disparity in uncertainty\nappears widespread, since the relevance estimates for minority groups tend to\nhave higher uncertainty due to a lack of data or appropriate features. To\novercome this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a\nnew fairness criterion for ranking that provably corrects for the disparity in\nuncertainty between groups. Furthermore, we present a practical algorithm for\ncomputing EOR rankings in time $O(n \\log(n))$ and prove its close approximation\nguarantee to the globally optimal solution. In a comprehensive empirical\nevaluation on synthetic data, a US Census dataset, and a real-world case study\nof Amazon search queries, we find that the algorithm reliably guarantees EOR\nfairness while providing effective rankings.",
        "translated": ""
    },
    {
        "title": "Impression-Informed Multi-Behavior Recommender System: A Hierarchical\n  Graph Attention Approach",
        "url": "http://arxiv.org/abs/2309.03169v1",
        "pub_date": "2023-09-06",
        "summary": "While recommender systems have significantly benefited from implicit\nfeedback, they have often missed the nuances of multi-behavior interactions\nbetween users and items. Historically, these systems either amalgamated all\nbehaviors, such as \\textit{impression} (formerly \\textit{view}),\n\\textit{add-to-cart}, and \\textit{buy}, under a singular 'interaction' label,\nor prioritized only the target behavior, often the \\textit{buy} action,\ndiscarding valuable auxiliary signals. Although recent advancements tried\naddressing this simplification, they primarily gravitated towards optimizing\nthe target behavior alone, battling with data scarcity. Additionally, they\ntended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these\ngaps, we introduce the \\textbf{H}ierarchical \\textbf{M}ulti-behavior\n\\textbf{G}raph Attention \\textbf{N}etwork (HMGN). This pioneering framework\nleverages attention mechanisms to discern information from both inter and\nintra-behaviors while employing a multi-task Hierarchical Bayesian Personalized\nRanking (HBPR) for optimization. Recognizing the need for scalability, our\napproach integrates a specialized multi-behavior sub-graph sampling technique.\nMoreover, the adaptability of HMGN allows for the seamless inclusion of\nknowledge metadata and time-series data. Empirical results attest to our\nmodel's prowess, registering a notable performance boost of up to 64\\% in\nNDCG@100 metrics over conventional graph neural network methods.",
        "translated": ""
    },
    {
        "title": "Helper Recommendation with seniority control in Online Health Community",
        "url": "http://arxiv.org/abs/2309.02978v1",
        "pub_date": "2023-09-06",
        "summary": "Online health communities (OHCs) are forums where patients with similar\nconditions communicate their experiences and provide moral support. Social\nsupport in OHCs plays a crucial role in easing and rehabilitating patients.\nHowever, many time-sensitive questions from patients often remain unanswered\ndue to the multitude of threads and the random nature of patient visits in\nOHCs. To address this issue, it is imperative to propose a recommender system\nthat assists solution seekers in finding appropriate problem helpers.\nNevertheless, developing a recommendation algorithm to enhance social support\nin OHCs remains an under-explored area. Traditional recommender systems cannot\nbe directly adapted due to the following obstacles. First, unlike user-item\nlinks in traditional recommender systems, it is hard to model the social\nsupport behind helper-seeker links in OHCs since they are formed based on\nvarious heterogeneous reasons. Second, it is difficult to distinguish the\nimpact of historical activities in characterizing patients. Third, it is\nsignificantly challenging to ensure that the recommended helpers possess\nsufficient expertise to assist the seekers. To tackle the aforementioned\nchallenges, we develop a Monotonically regularIzed diseNTangled Variational\nAutoencoders (MINT) model to strengthen social support in OHCs.",
        "translated": ""
    },
    {
        "title": "Prompt-based Effective Input Reformulation for Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2309.02962v1",
        "pub_date": "2023-09-06",
        "summary": "Legal case retrieval plays an important role for legal practitioners to\neffectively retrieve relevant cases given a query case. Most existing neural\nlegal case retrieval models directly encode the whole legal text of a case to\ngenerate a case representation, which is then utilised to conduct a nearest\nneighbour search for retrieval. Although these straightforward methods have\nachieved improvement over conventional statistical methods in retrieval\naccuracy, two significant challenges are identified in this paper: (1) Legal\nfeature alignment: the usage of the whole case text as the input will generally\nincorporate redundant and noisy information because, from the legal\nperspective, the determining factor of relevant cases is the alignment of key\nlegal features instead of whole text matching; (2) Legal context preservation:\nfurthermore, since the existing text encoding models usually have an input\nlength limit shorter than the case, the whole case text needs to be truncated\nor divided into paragraphs, which leads to the loss of the global context of\nlegal information. In this paper, a novel legal case retrieval framework,\nPromptCase, is proposed to tackle these challenges. Firstly, legal facts and\nlegal issues are identified and formally defined as the key features\nfacilitating legal case retrieval based on a thorough study of the definition\nof relevant cases from a legal perspective. Secondly, with the determining\nlegal features, a prompt-based encoding scheme is designed to conduct an\neffective encoding with language models. Extensive zero-shot experiments have\nbeen conducted on two benchmark datasets in legal case retrieval, which\ndemonstrate the superior retrieval effectiveness of the proposed PromptCase.\nThe code has been released on https://github.com/yanran-tang/PromptCase.",
        "translated": ""
    },
    {
        "title": "Tidying Up the Conversational Recommender Systems' Biases",
        "url": "http://arxiv.org/abs/2309.02550v1",
        "pub_date": "2023-09-05",
        "summary": "The growing popularity of language models has sparked interest in\nconversational recommender systems (CRS) within both industry and research\ncircles. However, concerns regarding biases in these systems have emerged.\nWhile individual components of CRS have been subject to bias studies, a\nliterature gap remains in understanding specific biases unique to CRS and how\nthese biases may be amplified or reduced when integrated into complex CRS\nmodels. In this paper, we provide a concise review of biases in CRS by\nsurveying recent literature. We examine the presence of biases throughout the\nsystem's pipeline and consider the challenges that arise from combining\nmultiple models. Our study investigates biases in classic recommender systems\nand their relevance to CRS. Moreover, we address specific biases in CRS,\nconsidering variations with and without natural language understanding\ncapabilities, along with biases related to dialogue systems and language\nmodels. Through our findings, we highlight the necessity of adopting a holistic\nperspective when dealing with biases in complex CRS models.",
        "translated": ""
    },
    {
        "title": "Extending Transductive Knowledge Graph Embedding Models for Inductive\n  Logical Relational Inference",
        "url": "http://arxiv.org/abs/2309.03773v1",
        "pub_date": "2023-09-07",
        "summary": "Many downstream inference tasks for knowledge graphs, such as relation\nprediction, have been handled successfully by knowledge graph embedding\ntechniques in the transductive setting. To address the inductive setting\nwherein new entities are introduced into the knowledge graph at inference time,\nmore recent work opts for models which learn implicit representations of the\nknowledge graph through a complex function of a network's subgraph structure,\noften parametrized by graph neural network architectures. These come at the\ncost of increased parametrization, reduced interpretability and limited\ngeneralization to other downstream inference tasks. In this work, we bridge the\ngap between traditional transductive knowledge graph embedding approaches and\nmore recent inductive relation prediction models by introducing a generalized\nform of harmonic extension which leverages representations learned through\ntransductive embedding methods to infer representations of new entities\nintroduced at inference time as in the inductive setting. This harmonic\nextension technique provides the best such approximation, can be implemented\nvia an efficient iterative scheme, and can be employed to answer a family of\nconjunctive logical queries over the knowledge graph, further expanding the\ncapabilities of transductive embedding methods. In experiments on a number of\nlarge-scale knowledge graph embedding benchmarks, we find that this approach\nfor extending the functionality of transductive knowledge graph embedding\nmodels to perform knowledge graph completion and answer logical queries in the\ninductive setting is competitive with--and in some scenarios\noutperforms--several state-of-the-art models derived explicitly for such\ninductive tasks.",
        "translated": ""
    },
    {
        "title": "VideolandGPT: A User Study on a Conversational Recommender System",
        "url": "http://arxiv.org/abs/2309.03645v1",
        "pub_date": "2023-09-07",
        "summary": "This paper investigates how large language models (LLMs) can enhance\nrecommender systems, with a specific focus on Conversational Recommender\nSystems that leverage user preferences and personalised candidate selections\nfrom existing ranking models. We introduce VideolandGPT, a recommender system\nfor a Video-on-Demand (VOD) platform, Videoland, which uses ChatGPT to select\nfrom a predetermined set of contents, considering the additional context\nindicated by users' interactions with a chat interface. We evaluate ranking\nmetrics, user experience, and fairness of recommendations, comparing a\npersonalised and a non-personalised version of the system, in a between-subject\nuser study. Our results indicate that the personalised version outperforms the\nnon-personalised in terms of accuracy and general user satisfaction, while both\nversions increase the visibility of items which are not in the top of the\nrecommendation lists. However, both versions present inconsistent behavior in\nterms of fairness, as the system may generate recommendations which are not\navailable on Videoland.",
        "translated": ""
    },
    {
        "title": "Evaluating ChatGPT as a Recommender System: A Rigorous Approach",
        "url": "http://arxiv.org/abs/2309.03613v1",
        "pub_date": "2023-09-07",
        "summary": "Recent popularity surrounds large AI language models due to their impressive\nnatural language capabilities. They contribute significantly to\nlanguage-related tasks, including prompt-based learning, making them valuable\nfor various specific tasks. This approach unlocks their full potential,\nenhancing precision and generalization. Research communities are actively\nexploring their applications, with ChatGPT receiving recognition. Despite\nextensive research on large language models, their potential in recommendation\nscenarios still needs to be explored. This study aims to fill this gap by\ninvestigating ChatGPT's capabilities as a zero-shot recommender system. Our\ngoals include evaluating its ability to use user preferences for\nrecommendations, reordering existing recommendation lists, leveraging\ninformation from similar users, and handling cold-start situations. We assess\nChatGPT's performance through comprehensive experiments using three datasets\n(MovieLens Small, Last.FM, and Facebook Book). We compare ChatGPT's performance\nagainst standard recommendation algorithms and other large language models,\nsuch as GPT-3.5 and PaLM-2. To measure recommendation effectiveness, we employ\nwidely-used evaluation metrics like Mean Average Precision (MAP), Recall,\nPrecision, F1, normalized Discounted Cumulative Gain (nDCG), Item Coverage,\nExpected Popularity Complement (EPC), Average Coverage of Long Tail (ACLT),\nAverage Recommendation Popularity (ARP), and Popularity-based Ranking-based\nEqual Opportunity (PopREO). Through thoroughly exploring ChatGPT's abilities in\nrecommender systems, our study aims to contribute to the growing body of\nresearch on the versatility and potential applications of large language\nmodels. Our experiment code is available on the GitHub repository:\nhttps://github.com/sisinflab/Recommender-ChatGPT",
        "translated": ""
    },
    {
        "title": "Learning Compact Compositional Embeddings via Regularized Pruning for\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.03518v1",
        "pub_date": "2023-09-07",
        "summary": "Latent factor models are the dominant backbones of contemporary recommender\nsystems (RSs) given their performance advantages, where a unique vector\nembedding with a fixed dimensionality (e.g., 128) is required to represent each\nentity (commonly a user/item). Due to the large number of users and items on\ne-commerce sites, the embedding table is arguably the least memory-efficient\ncomponent of RSs. For any lightweight recommender that aims to efficiently\nscale with the growing size of users/items or to remain applicable in\nresource-constrained settings, existing solutions either reduce the number of\nembeddings needed via hashing, or sparsify the full embedding table to switch\noff selected embedding dimensions. However, as hash collision arises or\nembeddings become overly sparse, especially when adapting to a tighter memory\nbudget, those lightweight recommenders inevitably have to compromise their\naccuracy. To this end, we propose a novel compact embedding framework for RSs,\nnamely Compositional Embedding with Regularized Pruning (CERP). Specifically,\nCERP represents each entity by combining a pair of embeddings from two\nindependent, substantially smaller meta-embedding tables, which are then\njointly pruned via a learnable element-wise threshold. In addition, we\ninnovatively design a regularized pruning mechanism in CERP, such that the two\nsparsified meta-embedding tables are encouraged to encode information that is\nmutually complementary. Given the compatibility with agnostic latent factor\nmodels, we pair CERP with two popular recommendation models for extensive\nexperiments, where results on two real-world datasets under different memory\nbudgets demonstrate its superiority against state-of-the-art baselines. The\ncodebase of CERP is available in https://github.com/xurong-liang/CERP.",
        "translated": ""
    },
    {
        "title": "Behind Recommender Systems: the Geography of the ACM RecSys Community",
        "url": "http://arxiv.org/abs/2309.03512v1",
        "pub_date": "2023-09-07",
        "summary": "The amount and dissemination rate of media content accessible online is\nnowadays overwhelming. Recommender Systems filter this information into\nmanageable streams or feeds, adapted to our personal needs or preferences. It\nis of utter importance that algorithms employed to filter information do not\ndistort or cut out important elements from our perspectives of the world. Under\nthis principle, it is essential to involve diverse views and teams from the\nearliest stages of their design and development. This has been highlighted, for\ninstance, in recent European Union regulations such as the Digital Services\nAct, via the requirement of risk monitoring, including the risk of\ndiscrimination, and the AI Act, through the requirement to involve people with\ndiverse backgrounds in the development of AI systems. We look into the\ngeographic diversity of the recommender systems research community,\nspecifically by analyzing the affiliation countries of the authors who\ncontributed to the ACM Conference on Recommender Systems (RecSys) during the\nlast 15 years. This study has been carried out in the framework of the\nDiversity in AI - DivinAI project, whose main objective is the long-term\nmonitoring of diversity in AI forums through a set of indexes.",
        "translated": ""
    },
    {
        "title": "Provider Fairness and Beyond-Accuracy Trade-offs in Recommender Systems",
        "url": "http://arxiv.org/abs/2309.04250v1",
        "pub_date": "2023-09-08",
        "summary": "Recommender systems, while transformative in online user experiences, have\nraised concerns over potential provider-side fairness issues. These systems may\ninadvertently favor popular items, thereby marginalizing less popular ones and\ncompromising provider fairness. While previous research has recognized\nprovider-side fairness issues, the investigation into how these biases affect\nbeyond-accuracy aspects of recommendation systems - such as diversity, novelty,\ncoverage, and serendipity - has been less emphasized. In this paper, we address\nthis gap by introducing a simple yet effective post-processing re-ranking model\nthat prioritizes provider fairness, while simultaneously maintaining user\nrelevance and recommendation quality. We then conduct an in-depth evaluation of\nthe model's impact on various aspects of recommendation quality across multiple\ndatasets. Specifically, we apply the post-processing algorithm to four distinct\nrecommendation models across four varied domain datasets, assessing the\nimprovement in each metric, encompassing both accuracy and beyond-accuracy\naspects. This comprehensive analysis allows us to gauge the effectiveness of\nour approach in mitigating provider biases. Our findings underscore the\neffectiveness of the adopted method in improving provider fairness and\nrecommendation quality. They also provide valuable insights into the trade-offs\ninvolved in achieving fairness in recommender systems, contributing to a more\nnuanced understanding of this complex issue.",
        "translated": ""
    },
    {
        "title": "Offline Recommender System Evaluation under Unobserved Confounding",
        "url": "http://arxiv.org/abs/2309.04222v1",
        "pub_date": "2023-09-08",
        "summary": "Off-Policy Estimation (OPE) methods allow us to learn and evaluate\ndecision-making policies from logged data. This makes them an attractive choice\nfor the offline evaluation of recommender systems, and several recent works\nhave reported successful adoption of OPE methods to this end. An important\nassumption that makes this work is the absence of unobserved confounders:\nrandom variables that influence both actions and rewards at data collection\ntime. Because the data collection policy is typically under the practitioner's\ncontrol, the unconfoundedness assumption is often left implicit, and its\nviolations are rarely dealt with in the existing literature.\n  This work aims to highlight the problems that arise when performing\noff-policy estimation in the presence of unobserved confounders, specifically\nfocusing on a recommendation use-case. We focus on policy-based estimators,\nwhere the logging propensities are learned from logged data. We characterise\nthe statistical bias that arises due to confounding, and show how existing\ndiagnostics are unable to uncover such cases. Because the bias depends directly\non the true and unobserved logging propensities, it is non-identifiable. As the\nunconfoundedness assumption is famously untestable, this becomes especially\nproblematic. This paper emphasises this common, yet often overlooked issue.\nThrough synthetic data, we empirically show how na\\\"ive propensity estimation\nunder confounding can lead to severely biased metric estimates that are allowed\nto fly under the radar. We aim to cultivate an awareness among researchers and\npractitioners of this important problem, and touch upon potential research\ndirections towards mitigating its effects.",
        "translated": ""
    },
    {
        "title": "Receiving an algorithmic recommendation based on documentary filmmaking\n  techniques",
        "url": "http://arxiv.org/abs/2309.04184v1",
        "pub_date": "2023-09-08",
        "summary": "This article analyzes the reception of a novel algorithmic recommendation of\ndocumentary films by a panel of moviegoers of the T{\\\"e}nk platform. In order\nto propose an alternative to recommendations based on a thematic\nclassification, the director or the production period, a set of metadata has\nbeen elaborated within the framework of this experimentation in order to\ncharacterize the great variety of ``documentary filmmaking dispositifs'' . The\ngoal is to investigate the different ways in which the platform's film lovers\nappropriate a personalized recommendation of 4 documentaries with similar or\nsimilar filmmaking dispositifs. To conclude, the contributions and limits of\nthis proof of concept are discussed in order to sketch out avenues of\nreflection for improving the instrumented mediation of documentary films.",
        "translated": ""
    },
    {
        "title": "A Long-Tail Friendly Representation Framework for Artist and Music\n  Similarity",
        "url": "http://arxiv.org/abs/2309.04182v1",
        "pub_date": "2023-09-08",
        "summary": "The investigation of the similarity between artists and music is crucial in\nmusic retrieval and recommendation, and addressing the challenge of the\nlong-tail phenomenon is increasingly important. This paper proposes a Long-Tail\nFriendly Representation Framework (LTFRF) that utilizes neural networks to\nmodel the similarity relationship. Our approach integrates music, user,\nmetadata, and relationship data into a unified metric learning framework, and\nemploys a meta-consistency relationship as a regular term to introduce the\nMulti-Relationship Loss. Compared to the Graph Neural Network (GNN), our\nproposed framework improves the representation performance in long-tail\nscenarios, which are characterized by sparse relationships between artists and\nmusic. We conduct experiments and analysis on the AllMusic dataset, and the\nresults demonstrate that our framework provides a favorable generalization of\nartist and music representation. Specifically, on similar artist/music\nrecommendation tasks, the LTFRF outperforms the baseline by 9.69%/19.42% in Hit\nRatio@10, and in long-tail cases, the framework achieves 11.05%/14.14% higher\nthan the baseline in Consistent@10.",
        "translated": ""
    },
    {
        "title": "PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded\n  Diffraction Patterns Phase Retrieval",
        "url": "http://arxiv.org/abs/2309.04171v1",
        "pub_date": "2023-09-08",
        "summary": "The problem of phase retrieval (PR) involves recovering an unknown image from\nlimited amplitude measurement data and is a challenge nonlinear inverse problem\nin computational imaging and image processing. However, many of the PR methods\nare based on black-box network models that lack interpretability and\nplug-and-play (PnP) frameworks that are computationally complex and require\ncareful parameter tuning. To address this, we have developed PRISTA-Net, a deep\nunfolding network (DUN) based on the first-order iterative shrinkage\nthresholding algorithm (ISTA). This network utilizes a learnable nonlinear\ntransformation to address the proximal-point mapping sub-problem associated\nwith the sparse priors, and an attention mechanism to focus on phase\ninformation containing image edges, textures, and structures. Additionally, the\nfast Fourier transform (FFT) is used to learn global features to enhance local\ninformation, and the designed logarithmic-based loss function leads to\nsignificant improvements when the noise level is low. All parameters in the\nproposed PRISTA-Net framework, including the nonlinear transformation,\nthreshold parameters, and step size, are learned end-to-end instead of being\nmanually set. This method combines the interpretability of traditional methods\nwith the fast inference ability of deep learning and is able to handle noise at\neach iteration during the unfolding stage, thus improving recovery quality.\nExperiments on Coded Diffraction Patterns (CDPs) measurements demonstrate that\nour approach outperforms the existing state-of-the-art methods in terms of\nqualitative and quantitative evaluations. Our source codes are available at\n\\emph{https://github.com/liuaxou/PRISTA-Net}.",
        "translated": ""
    },
    {
        "title": "D2WFP: A Novel Protocol for Forensically Identifying, Extracting, and\n  Analysing Deep and Dark Web Browsing Activities",
        "url": "http://arxiv.org/abs/2309.05537v1",
        "pub_date": "2023-09-11",
        "summary": "The use of the un-indexed web, commonly known as the deep web and dark web,\nto commit or facilitate criminal activity has drastically increased over the\npast decade. The dark web is an in-famously dangerous place where all kinds of\ncriminal activities take place [1-2], despite advances in web forensics\ntechniques, tools, and methodologies, few studies have formally tackled the\ndark and deep web forensics and the technical differences in terms of\ninvestigative techniques and artefacts identification and extraction. This\nresearch proposes a novel and comprehensive protocol to guide and assist\ndigital forensics professionals in investigating crimes committed on or via the\ndeep and dark web, The protocol named D2WFP establishes a new sequential\napproach for performing investigative activities by observing the order of\nvolatility and implementing a systemic approach covering all browsing related\nhives and artefacts which ultimately resulted into improv-ing the accuracy and\neffectiveness. Rigorous quantitative and qualitative research has been\nconducted by assessing D2WFP following a scientifically-sound and comprehensive\nprocess in different scenarios and the obtained results show an apparent\nincrease in the number of artefacts re-covered when adopting D2WFP which\noutperform any current industry or opensource browsing forensics tools. The\nsecond contribution of D2WFP is the robust formulation of artefact correlation\nand cross-validation within D2WFP which enables digital forensics professionals\nto better document and structure their analysis of host-based deep and dark web\nbrowsing artefacts.",
        "translated": ""
    },
    {
        "title": "Re-formalization of Individual Fairness",
        "url": "http://arxiv.org/abs/2309.05521v1",
        "pub_date": "2023-09-11",
        "summary": "The notion of individual fairness is a formalization of an ethical principle,\n\"Treating like cases alike,\" which has been argued such as by Aristotle. In a\nfairness-aware machine learning context, Dwork et al. firstly formalized the\nnotion. In their formalization, a similar pair of data in an unfair space\nshould be mapped to similar positions in a fair space. We propose to\nre-formalize individual fairness by the statistical independence conditioned by\nindividuals. This re-formalization has the following merits. First, our\nformalization is compatible with that of Dwork et al. Second, our formalization\nenables to combine individual fairness with the fairness notion, equalized odds\nor sufficiency, as well as statistical parity. Third, though their\nformalization implicitly assumes a pre-process approach for making fair\nprediction, our formalization is applicable to an in-process or post-process\napproach.",
        "translated": ""
    },
    {
        "title": "Towards Content-based Pixel Retrieval in Revisited Oxford and Paris",
        "url": "http://arxiv.org/abs/2309.05438v1",
        "pub_date": "2023-09-11",
        "summary": "This paper introduces the first two pixel retrieval benchmarks. Pixel\nretrieval is segmented instance retrieval. Like semantic segmentation extends\nclassification to the pixel level, pixel retrieval is an extension of image\nretrieval and offers information about which pixels are related to the query\nobject. In addition to retrieving images for the given query, it helps users\nquickly identify the query object in true positive images and exclude false\npositive images by denoting the correlated pixels. Our user study results show\npixel-level annotation can significantly improve the user experience.\n  Compared with semantic and instance segmentation, pixel retrieval requires a\nfine-grained recognition capability for variable-granularity targets. To this\nend, we propose pixel retrieval benchmarks named PROxford and PRParis, which\nare based on the widely used image retrieval datasets, ROxford and RParis.\nThree professional annotators label 5,942 images with two rounds of\ndouble-checking and refinement. Furthermore, we conduct extensive experiments\nand analysis on the SOTA methods in image search, image matching, detection,\nsegmentation, and dense matching using our pixel retrieval benchmarks. Results\nshow that the pixel retrieval task is challenging to these approaches and\ndistinctive from existing problems, suggesting that further research can\nadvance the content-based pixel-retrieval and thus user search experience. The\ndatasets can be downloaded from\n\\href{https://github.com/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval}{this\nlink}.",
        "translated": ""
    },
    {
        "title": "Formalizing Multimedia Recommendation through Multimodal Deep Learning",
        "url": "http://arxiv.org/abs/2309.05273v1",
        "pub_date": "2023-09-11",
        "summary": "Recommender systems (RSs) offer personalized navigation experiences on online\nplatforms, but recommendation remains a challenging task, particularly in\nspecific scenarios and domains. Multimodality can help tap into richer\ninformation sources and construct more refined user/item profiles for\nrecommendations. However, existing literature lacks a shared and universal\nschema for modeling and solving the recommendation problem through the lens of\nmultimodality. This work aims to formalize a general multimodal schema for\nmultimedia recommendation. It provides a comprehensive literature review of\nmultimodal approaches for multimedia recommendation from the last eight years,\noutlines the theoretical foundations of a multimodal pipeline, and demonstrates\nits rationale by applying it to selected state-of-the-art approaches. The work\nalso conducts a benchmarking analysis of recent algorithms for multimedia\nrecommendation within Elliot, a rigorous framework for evaluating recommender\nsystems. The main aim is to provide guidelines for designing and implementing\nthe next generation of multimodal approaches in multimedia recommendation.",
        "translated": ""
    },
    {
        "title": "Generating Natural Language Queries for More Effective Systematic Review\n  Screening Prioritisation",
        "url": "http://arxiv.org/abs/2309.05238v1",
        "pub_date": "2023-09-11",
        "summary": "Screening prioritisation in medical systematic reviews aims to rank the set\nof documents retrieved by complex Boolean queries. The goal is to prioritise\nthe most important documents so that subsequent review steps can be carried out\nmore efficiently and effectively. The current state of the art uses the final\ntitle of the review to rank documents using BERT-based neural neural rankers.\nHowever, the final title is only formulated at the end of the review process,\nwhich makes this approach impractical as it relies on ex post facto\ninformation. At the time of screening, only a rough working title is available,\nwith which the BERT-based ranker achieves is significantly worse than the final\ntitle. In this paper, we explore alternative sources of queries for screening\nprioritisation, such as the Boolean query used to retrieve the set of documents\nto be screened, and queries generated by instruction-based generative large\nlanguage models such as ChatGPT and Alpaca. Our best approach is not only\npractical based on the information available at screening time, but is similar\nin effectiveness with the final title.",
        "translated": ""
    },
    {
        "title": "Learning Personalized User Preference from Cold Start in Multi-turn\n  Conversations",
        "url": "http://arxiv.org/abs/2309.05127v1",
        "pub_date": "2023-09-10",
        "summary": "This paper presents a novel teachable conversation interaction system that is\ncapable of learning users preferences from cold start by gradually adapting to\npersonal preferences. In particular, the TAI system is able to automatically\nidentify and label user preference in live interactions, manage dialogue flows\nfor interactive teaching sessions, and reuse learned preference for preference\nelicitation. We develop the TAI system by leveraging BERT encoder models to\nencode both dialogue and relevant context information, and build action\nprediction (AP), argument filling (AF) and named entity recognition (NER)\nmodels to understand the teaching session. We adopt a seeker-provider\ninteraction loop mechanism to generate diverse dialogues from cold-start. TAI\nis capable of learning user preference, which achieves 0.9122 turn level\naccuracy on out-of-sample dataset, and has been successfully adopted in\nproduction.",
        "translated": ""
    },
    {
        "title": "Personalized Search Via Neural Contextual Semantic Relevance Ranking",
        "url": "http://arxiv.org/abs/2309.05113v1",
        "pub_date": "2023-09-10",
        "summary": "Existing neural relevance models do not give enough consideration for query\nand item context information which diversifies the search results to adapt for\npersonal preference. To bridge this gap, this paper presents a neural learning\nframework to personalize document ranking results by leveraging the signals to\ncapture how the document fits into users' context. In particular, it models the\nrelationships between document content and user query context using both\nlexical representations and semantic embeddings such that the user's intent can\nbe better understood by data enrichment of personalized query context\ninformation. Extensive experiments performed on the search dataset, demonstrate\nthe effectiveness of the proposed method.",
        "translated": ""
    },
    {
        "title": "Duplicate Question Retrieval and Confirmation Time Prediction in\n  Software Communities",
        "url": "http://arxiv.org/abs/2309.05035v1",
        "pub_date": "2023-09-10",
        "summary": "Community Question Answering (CQA) in different domains is growing at a large\nscale because of the availability of several platforms and huge shareable\ninformation among users. With the rapid growth of such online platforms, a\nmassive amount of archived data makes it difficult for moderators to retrieve\npossible duplicates for a new question and identify and confirm existing\nquestion pairs as duplicates at the right time. This problem is even more\ncritical in CQAs corresponding to large software systems like askubuntu where\nmoderators need to be experts to comprehend something as a duplicate. Note that\nthe prime challenge in such CQA platforms is that the moderators are themselves\nexperts and are therefore usually extremely busy with their time being\nextraordinarily expensive. To facilitate the task of the moderators, in this\nwork, we have tackled two significant issues for the askubuntu CQA platform:\n(1) retrieval of duplicate questions given a new question and (2) duplicate\nquestion confirmation time prediction. In the first task, we focus on\nretrieving duplicate questions from a question pool for a particular newly\nposted question. In the second task, we solve a regression problem to rank a\npair of questions that could potentially take a long time to get confirmed as\nduplicates. For duplicate question retrieval, we propose a Siamese neural\nnetwork based approach by exploiting both text and network-based features,\nwhich outperforms several state-of-the-art baseline techniques. Our method\noutperforms DupPredictor and DUPE by 5% and 7% respectively. For duplicate\nconfirmation time prediction, we have used both the standard machine learning\nmodels and neural network along with the text and graph-based features. We\nobtain Spearman's rank correlation of 0.20 and 0.213 (statistically\nsignificant) for text and graph based features respectively.",
        "translated": ""
    },
    {
        "title": "Streamlined Data Fusion: Unleashing the Power of Linear Combination with\n  Minimal Relevance Judgments",
        "url": "http://arxiv.org/abs/2309.04981v1",
        "pub_date": "2023-09-10",
        "summary": "Linear combination is a potent data fusion method in information retrieval\ntasks, thanks to its ability to adjust weights for diverse scenarios. However,\nachieving optimal weight training has traditionally required manual relevance\njudgments on a large percentage of documents, a labor-intensive and expensive\nprocess. In this study, we investigate the feasibility of obtaining\nnear-optimal weights using a mere 20\\%-50\\% of relevant documents. Through\nexperiments on four TREC datasets, we find that weights trained with multiple\nlinear regression using this reduced set closely rival those obtained with\nTREC's official \"qrels.\" Our findings unlock the potential for more efficient\nand affordable data fusion, empowering researchers and practitioners to reap\nits full benefits with significantly less effort.",
        "translated": ""
    },
    {
        "title": "Multi-modal Extreme Classification",
        "url": "http://arxiv.org/abs/2309.04961v1",
        "pub_date": "2023-09-10",
        "summary": "This paper develops the MUFIN technique for extreme classification (XC) tasks\nwith millions of labels where datapoints and labels are endowed with visual and\ntextual descriptors. Applications of MUFIN to product-to-product recommendation\nand bid query prediction over several millions of products are presented.\nContemporary multi-modal methods frequently rely on purely embedding-based\nmethods. On the other hand, XC methods utilize classifier architectures to\noffer superior accuracies than embedding only methods but mostly focus on\ntext-based categorization tasks. MUFIN bridges this gap by reformulating\nmulti-modal categorization as an XC problem with several millions of labels.\nThis presents the twin challenges of developing multi-modal architectures that\ncan offer embeddings sufficiently expressive to allow accurate categorization\nover millions of labels; and training and inference routines that scale\nlogarithmically in the number of labels. MUFIN develops an architecture based\non cross-modal attention and trains it in a modular fashion using pre-training\nand positive and negative mining. A novel product-to-product recommendation\ndataset MM-AmazonTitles-300K containing over 300K products was curated from\npublicly available amazon.com listings with each product endowed with a title\nand multiple images. On the all datasets MUFIN offered at least 3% higher\naccuracy than leading text-based, image-based and multi-modal techniques. Code\nfor MUFIN is available at https://github.com/Extreme-classification/MUFIN",
        "translated": ""
    },
    {
        "title": "Human Action Co-occurrence in Lifestyle Vlogs using Graph Link\n  Prediction",
        "url": "http://arxiv.org/abs/2309.06219v1",
        "pub_date": "2023-09-12",
        "summary": "We introduce the task of automatic human action co-occurrence identification,\ni.e., determine whether two human actions can co-occur in the same interval of\ntime. We create and make publicly available the ACE (Action Co-occurrencE)\ndataset, consisting of a large graph of ~12k co-occurring pairs of visual\nactions and their corresponding video clips. We describe graph link prediction\nmodels that leverage visual and textual information to automatically infer if\ntwo actions are co-occurring. We show that graphs are particularly well suited\nto capture relations between human actions, and the learned graph\nrepresentations are effective for our task and capture novel and relevant\ninformation across different data domains. The ACE dataset and the code\nintroduced in this paper are publicly available at\nhttps://github.com/MichiganNLP/vlog_action_co-occurrence.",
        "translated": ""
    },
    {
        "title": "HAMUR: Hyper Adapter for Multi-Domain Recommendation",
        "url": "http://arxiv.org/abs/2309.06217v1",
        "pub_date": "2023-09-12",
        "summary": "Multi-Domain Recommendation (MDR) has gained significant attention in recent\nyears, which leverages data from multiple domains to enhance their performance\nconcurrently.However, current MDR models are confronted with two limitations.\nFirstly, the majority of these models adopt an approach that explicitly shares\nparameters between domains, leading to mutual interference among them.\nSecondly, due to the distribution differences among domains, the utilization of\nstatic parameters in existing methods limits their flexibility to adapt to\ndiverse domains. To address these challenges, we propose a novel model Hyper\nAdapter for Multi-Domain Recommendation (HAMUR). Specifically, HAMUR consists\nof two components: (1). Domain-specific adapter, designed as a pluggable module\nthat can be seamlessly integrated into various existing multi-domain backbone\nmodels, and (2). Domain-shared hyper-network, which implicitly captures shared\ninformation among domains and dynamically generates the parameters for the\nadapter. We conduct extensive experiments on two public datasets using various\nbackbone networks. The experimental results validate the effectiveness and\nscalability of the proposed model.",
        "translated": ""
    },
    {
        "title": "Improving and Evaluating the Detection of Fragmentation in News\n  Recommendations with the Clustering of News Story Chains",
        "url": "http://arxiv.org/abs/2309.06192v1",
        "pub_date": "2023-09-12",
        "summary": "News recommender systems play an increasingly influential role in shaping\ninformation access within democratic societies. However, tailoring\nrecommendations to users' specific interests can result in the divergence of\ninformation streams. Fragmented access to information poses challenges to the\nintegrity of the public sphere, thereby influencing democracy and public\ndiscourse. The Fragmentation metric quantifies the degree of fragmentation of\ninformation streams in news recommendations. Accurate measurement of this\nmetric requires the application of Natural Language Processing (NLP) to\nidentify distinct news events, stories, or timelines. This paper presents an\nextensive investigation of various approaches for quantifying Fragmentation in\nnews recommendations. These approaches are evaluated both intrinsically, by\nmeasuring performance on news story clustering, and extrinsically, by assessing\nthe Fragmentation scores of different simulated news recommender scenarios. Our\nfindings demonstrate that agglomerative hierarchical clustering coupled with\nSentenceBERT text representation is substantially better at detecting\nFragmentation than earlier implementations. Additionally, the analysis of\nsimulated scenarios yields valuable insights and recommendations for\nstakeholders concerning the measurement and interpretation of Fragmentation.",
        "translated": ""
    },
    {
        "title": "AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity\n  Recognition and Linking",
        "url": "http://arxiv.org/abs/2309.06175v1",
        "pub_date": "2023-09-12",
        "summary": "This paper presents a novel approach to address the Entity Recognition and\nLinking Challenge at NLPCC 2015. The task involves extracting named entity\nmentions from short search queries and linking them to entities within a\nreference Chinese knowledge base. To tackle this problem, we first expand the\nexisting knowledge base and utilize external knowledge to identify candidate\nentities, thereby improving the recall rate. Next, we extract features from the\ncandidate entities and utilize Support Vector Regression and Multiple Additive\nRegression Tree as scoring functions to filter the results. Additionally, we\napply rules to further refine the results and enhance precision. Our method is\ncomputationally efficient and achieves an F1 score of 0.535.",
        "translated": ""
    },
    {
        "title": "Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning\n  Strategies are not Better than Random Selection",
        "url": "http://arxiv.org/abs/2309.06131v1",
        "pub_date": "2023-09-12",
        "summary": "Search methods based on Pretrained Language Models (PLM) have demonstrated\ngreat effectiveness gains compared to statistical and early neural ranking\nmodels. However, fine-tuning PLM-based rankers requires a great amount of\nannotated training data. Annotating data involves a large manual effort and\nthus is expensive, especially in domain specific tasks. In this paper we\ninvestigate fine-tuning PLM-based rankers under limited training data and\nbudget. We investigate two scenarios: fine-tuning a ranker from scratch, and\ndomain adaptation starting with a ranker already fine-tuned on general data,\nand continuing fine-tuning on a target dataset. We observe a great variability\nin effectiveness when fine-tuning on different randomly selected subsets of\ntraining data. This suggests that it is possible to achieve effectiveness gains\nby actively selecting a subset of the training data that has the most positive\neffect on the rankers. This way, it would be possible to fine-tune effective\nPLM rankers at a reduced annotation budget. To investigate this, we adapt\nexisting Active Learning (AL) strategies to the task of fine-tuning PLM rankers\nand investigate their effectiveness, also considering annotation and\ncomputational costs. Our extensive analysis shows that AL strategies do not\nsignificantly outperform random selection of training subsets in terms of\neffectiveness. We further find that gains provided by AL strategies come at the\nexpense of more assessments (thus higher annotation costs) and AL strategies\nunderperform random selection when comparing effectiveness given a fixed\nannotation cost. Our results highlight that ``optimal'' subsets of training\ndata that provide high effectiveness at low annotation cost do exist, but\ncurrent mainstream AL strategies applied to PLM rankers are not capable of\nidentifying them.",
        "translated": ""
    },
    {
        "title": "Characterizing Latent Perspectives of Media Houses Towards Public\n  Figures",
        "url": "http://arxiv.org/abs/2309.06112v1",
        "pub_date": "2023-09-12",
        "summary": "Media houses reporting on public figures, often come with their own biases\nstemming from their respective worldviews. A characterization of these\nunderlying patterns helps us in better understanding and interpreting news\nstories. For this, we need diverse or subjective summarizations, which may not\nbe amenable for classifying into predefined class labels. This work proposes a\nzero-shot approach for non-extractive or generative characterizations of person\nentities from a corpus using GPT-2. We use well-articulated articles from\nseveral well-known news media houses as a corpus to build a sound argument for\nthis approach. First, we fine-tune a GPT-2 pre-trained language model with a\ncorpus where specific person entities are characterized. Second, we further\nfine-tune this with demonstrations of person entity characterizations, created\nfrom a corpus of programmatically constructed characterizations. This twice\nfine-tuned model is primed with manual prompts consisting of entity names that\nwere not previously encountered in the second fine-tuning, to generate a simple\nsentence about the entity. The results were encouraging, when compared against\nactual characterizations from the corpus.",
        "translated": ""
    },
    {
        "title": "Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering\n  Trends across Diverse Platforms",
        "url": "http://arxiv.org/abs/2309.05961v1",
        "pub_date": "2023-09-12",
        "summary": "Community Question Answering (CQA) platforms steadily gain popularity as they\nprovide users with fast responses to their queries. The swiftness of these\nresponses is contingent on a mixture of query-specific and user-related\nelements. This paper scrutinizes these contributing factors within the context\nof six highly popular CQA platforms, identified through their standout\nanswering speed. Our investigation reveals a correlation between the time taken\nto yield the first response to a question and several variables: the metadata,\nthe formulation of the questions, and the level of interaction among users.\nAdditionally, by employing conventional machine learning models to analyze\nthese metadata and patterns of user interaction, we endeavor to predict which\nqueries will receive their initial responses promptly.",
        "translated": ""
    },
    {
        "title": "GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection",
        "url": "http://arxiv.org/abs/2309.05953v1",
        "pub_date": "2023-09-12",
        "summary": "Logs play a crucial role in system monitoring and debugging by recording\nvaluable system information, including events and states. Although various\nmethods have been proposed to detect anomalies in log sequences, they often\noverlook the significance of considering relations among system components,\nsuch as services and users, which can be identified from log contents.\nUnderstanding these relations is vital for detecting anomalies and their\nunderlying causes. To address this issue, we introduce GLAD, a Graph-based Log\nAnomaly Detection framework designed to detect relational anomalies in system\nlogs. GLAD incorporates log semantics, relational patterns, and sequential\npatterns into a unified framework for anomaly detection. Specifically, GLAD\nfirst introduces a field extraction module that utilizes prompt-based few-shot\nlearning to identify essential fields from log contents. Then GLAD constructs\ndynamic log graphs for sliding windows by interconnecting extracted fields and\nlog events parsed from the log parser. These graphs represent events and fields\nas nodes and their relations as edges. Subsequently, GLAD utilizes a\ntemporal-attentive graph edge anomaly detection model for identifying anomalous\nrelations in these dynamic log graphs. This model employs a Graph Neural\nNetwork (GNN)-based encoder enhanced with transformers to capture content,\nstructural and temporal features. We evaluate our proposed method on three\ndatasets, and the results demonstrate the effectiveness of GLAD in detecting\nanomalies indicated by varying relational patterns.",
        "translated": ""
    },
    {
        "title": "A Survey of Hallucination in Large Foundation Models",
        "url": "http://arxiv.org/abs/2309.05922v1",
        "pub_date": "2023-09-12",
        "summary": "Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large'' Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.",
        "translated": ""
    },
    {
        "title": "SAGE: Structured Attribute Value Generation for Billion-Scale Product\n  Catalogs",
        "url": "http://arxiv.org/abs/2309.05920v1",
        "pub_date": "2023-09-12",
        "summary": "We introduce SAGE; a Generative LLM for inferring attribute values for\nproducts across world-wide e-Commerce catalogs. We introduce a novel\nformulation of the attribute-value prediction problem as a Seq2Seq\nsummarization task, across languages, product types and target attributes. Our\nnovel modeling approach lifts the restriction of predicting attribute values\nwithin a pre-specified set of choices, as well as, the requirement that the\nsought attribute values need to be explicitly mentioned in the text. SAGE can\ninfer attribute values even when such values are mentioned implicitly using\nperiphrastic language, or not-at-all-as is the case for common-sense defaults.\nAdditionally, SAGE is capable of predicting whether an attribute is\ninapplicable for the product at hand, or non-obtainable from the available\ninformation. SAGE is the first method able to tackle all aspects of the\nattribute-value-prediction task as they arise in practical settings in\ne-Commerce catalogs. A comprehensive set of experiments demonstrates the\neffectiveness of the proposed approach, as well as, its superiority against\nstate-of-the-art competing alternatives. Moreover, our experiments highlight\nSAGE's ability to tackle the task of predicting attribute values in zero-shot\nsetting; thereby, opening up opportunities for significantly reducing the\noverall number of labeled examples required for training.",
        "translated": ""
    },
    {
        "title": "Résumé Parsing as Hierarchical Sequence Labeling: An Empirical Study",
        "url": "http://arxiv.org/abs/2309.07015v1",
        "pub_date": "2023-09-13",
        "summary": "Extracting information from r\\'esum\\'es is typically formulated as a\ntwo-stage problem, where the document is first segmented into sections and then\neach section is processed individually to extract the target entities. Instead,\nwe cast the whole problem as sequence labeling in two levels -- lines and\ntokens -- and study model architectures for solving both tasks simultaneously.\nWe build high-quality r\\'esum\\'e parsing corpora in English, French, Chinese,\nSpanish, German, Portuguese, and Swedish. Based on these corpora, we present\nexperimental results that demonstrate the effectiveness of the proposed models\nfor the information extraction task, outperforming approaches introduced in\nprevious work. We conduct an ablation study of the proposed architectures. We\nalso analyze both model performance and resource efficiency, and describe the\ntrade-offs for model deployment in the context of a production environment.",
        "translated": ""
    },
    {
        "title": "Modeling Dislocation Dynamics Data Using Semantic Web Technologies",
        "url": "http://arxiv.org/abs/2309.06930v1",
        "pub_date": "2023-09-13",
        "summary": "Research in the field of Materials Science and Engineering focuses on the\ndesign, synthesis, properties, and performance of materials. An important class\nof materials that is widely investigated are crystalline materials, including\nmetals and semiconductors. Crystalline material typically contains a distinct\ntype of defect called \"dislocation\". This defect significantly affects various\nmaterial properties, including strength, fracture toughness, and ductility.\nResearchers have devoted a significant effort in recent years to understanding\ndislocation behavior through experimental characterization techniques and\nsimulations, e.g., dislocation dynamics simulations. This paper presents how\ndata from dislocation dynamics simulations can be modeled using semantic web\ntechnologies through annotating data with ontologies. We extend the already\nexisting Dislocation Ontology by adding missing concepts and aligning it with\ntwo other domain-related ontologies (i.e., the Elementary Multi-perspective\nMaterial Ontology and the Materials Design Ontology) allowing for representing\nthe dislocation simulation data efficiently. Moreover, we show a real-world use\ncase by representing the discrete dislocation dynamics data as a knowledge\ngraph (DisLocKG) that illustrates the relationship between them. We also\ndeveloped a SPARQL endpoint that brings extensive flexibility to query\nDisLocKG.",
        "translated": ""
    },
    {
        "title": "Multi-behavior Recommendation with SVD Graph Neural Networks",
        "url": "http://arxiv.org/abs/2309.06912v1",
        "pub_date": "2023-09-13",
        "summary": "Graph Neural Networks (GNNs) has been extensively employed in the field of\nrecommender systems, offering users personalized recommendations and yielding\nremarkable outcomes. Recently, GNNs incorporating contrastive learning have\ndemonstrated promising performance in handling sparse data problem of\nrecommendation system. However, existing contrastive learning methods still\nhave limitations in addressing the cold-start problem and resisting noise\ninterference especially for multi-behavior recommendation. To mitigate the\naforementioned issues, the present research posits a GNNs based multi-behavior\nrecommendation model MB-SVD that utilizes Singular Value Decomposition (SVD)\ngraphs to enhance model performance. In particular, MB-SVD considers user\npreferences under different behaviors, improving recommendation effectiveness\nwhile better addressing the cold-start problem. Our model introduces an\ninnovative methodology, which subsume multi-behavior contrastive learning\nparadigm to proficiently discern the intricate interconnections among\nheterogeneous manifestations of user behavior and generates SVD graphs to\nautomate the distillation of crucial multi-behavior self-supervised information\nfor robust graph augmentation. Furthermore, the SVD based framework reduces the\nembedding dimensions and computational load. Thorough experimentation showcases\nthe remarkable performance of our proposed MB-SVD approach in multi-behavior\nrecommendation endeavors across diverse real-world datasets.",
        "translated": ""
    },
    {
        "title": "Towards the TopMost: A Topic Modeling System Toolkit",
        "url": "http://arxiv.org/abs/2309.06908v1",
        "pub_date": "2023-09-13",
        "summary": "Topic models have been proposed for decades with various applications and\nrecently refreshed by the neural variational inference. However, these topic\nmodels adopt totally distinct dataset, implementation, and evaluation settings,\nwhich hinders their quick utilization and fair comparisons. This greatly\nhinders the research progress of topic models. To address these issues, in this\npaper we propose a Topic Modeling System Toolkit (TopMost). Compared to\nexisting toolkits, TopMost stands out by covering a wider range of topic\nmodeling scenarios including complete lifecycles with dataset pre-processing,\nmodel training, testing, and evaluations. The highly cohesive and decoupled\nmodular design of TopMost enables quick utilization, fair comparisons, and\nflexible extensions of different topic models. This can facilitate the research\nand applications of topic models. Our code, tutorials, and documentation are\navailable at https://github.com/bobxwu/topmost.",
        "translated": ""
    },
    {
        "title": "ProMap: Datasets for Product Mapping in E-commerce",
        "url": "http://arxiv.org/abs/2309.06882v1",
        "pub_date": "2023-09-13",
        "summary": "The goal of product mapping is to decide, whether two listings from two\ndifferent e-shops describe the same products. Existing datasets of matching and\nnon-matching pairs of products, however, often suffer from incomplete product\ninformation or contain only very distant non-matching products. Therefore,\nwhile predictive models trained on these datasets achieve good results on them,\nin practice, they are unusable as they cannot distinguish very similar but\nnon-matching pairs of products. This paper introduces two new datasets for\nproduct mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn\nconsisting of 1,555 English product pairs of matching and non-matching products\nmanually scraped from two pairs of e-shops. The datasets contain both images\nand textual descriptions of the products, including their specifications,\nmaking them one of the most complete datasets for product mapping.\nAdditionally, the non-matching products were selected in two phases, creating\ntwo types of non-matches -- close non-matches and medium non-matches. Even the\nmedium non-matches are pairs of products that are much more similar than\nnon-matches in other datasets -- for example, they still need to have the same\nbrand and similar name and price. After simple data preprocessing, several\nmachine learning algorithms were trained on these and two the other datasets to\ndemonstrate the complexity and completeness of ProMap datasets. ProMap datasets\nare presented as a golden standard for further research of product mapping\nfilling the gaps in existing ones.",
        "translated": ""
    },
    {
        "title": "An Image Dataset for Benchmarking Recommender Systems with Raw Pixels",
        "url": "http://arxiv.org/abs/2309.06789v1",
        "pub_date": "2023-09-13",
        "summary": "Recommender systems (RS) have achieved significant success by leveraging\nexplicit identification (ID) features. However, the full potential of content\nfeatures, especially the pure image pixel features, remains relatively\nunexplored. The limited availability of large, diverse, and content-driven\nimage recommendation datasets has hindered the use of raw images as item\nrepresentations. In this regard, we present PixelRec, a massive image-centric\nrecommendation dataset that includes approximately 200 million user-image\ninteractions, 30 million users, and 400,000 high-quality cover images. By\nproviding direct access to raw image pixels, PixelRec enables recommendation\nmodels to learn item representation directly from them. To demonstrate its\nutility, we begin by presenting the results of several classical pure ID-based\nbaseline models, termed IDNet, trained on PixelRec. Then, to show the\neffectiveness of the dataset's image features, we substitute the itemID\nembeddings (from IDNet) with a powerful vision encoder that represents items\nusing their raw image pixels. This new model is dubbed PixelNet.Our findings\nindicate that even in standard, non-cold start recommendation settings where\nIDNet is recognized as highly effective, PixelNet can already perform equally\nwell or even better than IDNet. Moreover, PixelNet has several other notable\nadvantages over IDNet, such as being more effective in cold-start and\ncross-domain recommendation scenarios. These results underscore the importance\nof visual features in PixelRec. We believe that PixelRec can serve as a\ncritical resource and testing ground for research on recommendation models that\nemphasize image pixel content. The dataset, code, and leaderboard will be\navailable at https://github.com/website-pixelrec/PixelRec.",
        "translated": ""
    },
    {
        "title": "CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data\n  Generation",
        "url": "http://arxiv.org/abs/2309.06748v1",
        "pub_date": "2023-09-13",
        "summary": "Conversational search provides a natural interface for information retrieval\n(IR). Recent approaches have demonstrated promising results in applying dense\nretrieval to conversational IR. However, training dense retrievers requires\nlarge amounts of in-domain paired data. This hinders the development of\nconversational dense retrievers, as abundant in-domain conversations are\nexpensive to collect. In this paper, we propose CONVERSER, a framework for\ntraining conversational dense retrievers with at most 6 examples of in-domain\ndialogues. Specifically, we utilize the in-context learning capability of large\nlanguage models to generate conversational queries given a passage in the\nretrieval corpus. Experimental results on conversational retrieval benchmarks\nOR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable\nperformance to fully-supervised models, demonstrating the effectiveness of our\nproposed framework in few-shot conversational dense retrieval. All source code\nand generated datasets are available at https://github.com/MiuLab/CONVERSER",
        "translated": ""
    },
    {
        "title": "Hierarchical Multi-Task Learning Framework for Session-based\n  Recommendations",
        "url": "http://arxiv.org/abs/2309.06533v1",
        "pub_date": "2023-09-12",
        "summary": "While session-based recommender systems (SBRSs) have shown superior\nrecommendation performance, multi-task learning (MTL) has been adopted by SBRSs\nto enhance their prediction accuracy and generalizability further. Hierarchical\nMTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds\noutputs from auxiliary tasks to main tasks. This hierarchy leads to richer\ninput features for main tasks and higher interpretability of predictions,\ncompared to existing MTL frameworks. However, the H-MTL framework has not been\ninvestigated in SBRSs yet. In this paper, we propose HierSRec which\nincorporates the H-MTL architecture into SBRSs. HierSRec encodes a given\nsession with a metadata-aware Transformer and performs next-category prediction\n(i.e., auxiliary task) with the session encoding. Next, HierSRec conducts\nnext-item prediction (i.e., main task) with the category prediction result and\nsession encoding. For scalable inference, HierSRec creates a compact set of\ncandidate items (e.g., 4% of total items) per test example using the category\nprediction. Experiments show that HierSRec outperforms existing SBRSs as per\nnext-item prediction accuracy on two session-based recommendation datasets. The\naccuracy of HierSRec measured with the carefully-curated candidate items aligns\nwith the accuracy of HierSRec calculated with all items, which validates the\nusefulness of our candidate generation scheme via H-MTL.",
        "translated": ""
    },
    {
        "title": "Ambiguity-Aware In-Context Learning with Large Language Models",
        "url": "http://arxiv.org/abs/2309.07900v1",
        "pub_date": "2023-09-14",
        "summary": "In-context learning (ICL) i.e. showing LLMs only a few task-specific\ndemonstrations has led to downstream gains with no task-specific fine-tuning\nrequired. However, LLMs are sensitive to the choice of prompts, and therefore a\ncrucial research question is how to select good demonstrations for ICL. One\neffective strategy is leveraging semantic similarity between the ICL\ndemonstrations and test inputs by using a text retriever, which however is\nsub-optimal as that does not consider the LLM's existing knowledge about that\ntask. From prior work (Min et al., 2022), we already know that labels paired\nwith the demonstrations bias the model predictions. This leads us to our\nhypothesis whether considering LLM's existing knowledge about the task,\nespecially with respect to the output label space can help in a better\ndemonstration selection strategy. Through extensive experimentation on three\ntext classification tasks, we find that it is beneficial to not only choose\nsemantically similar ICL demonstrations but also to choose those demonstrations\nthat help resolve the inherent label ambiguity surrounding the test example.\nInterestingly, we find that including demonstrations that the LLM previously\nmis-classified and also fall on the test example's decision boundary, brings\nthe most performance gain.",
        "translated": ""
    },
    {
        "title": "NineRec: A Benchmark Dataset Suite for Evaluating Transferable\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.07705v1",
        "pub_date": "2023-09-14",
        "summary": "Learning a recommender system model from an item's raw modality features\n(such as image, text, audio, etc.), called MoRec, has attracted growing\ninterest recently. One key advantage of MoRec is that it can easily benefit\nfrom advances in other fields, such as natural language processing (NLP) and\ncomputer vision (CV). Moreover, it naturally supports transfer learning across\ndifferent systems through modality features, known as transferable recommender\nsystems, or TransRec.\n  However, so far, TransRec has made little progress, compared to\ngroundbreaking foundation models in the fields of NLP and CV. The lack of\nlarge-scale, high-quality recommendation datasets poses a major obstacle. To\nthis end, we introduce NineRec, a TransRec dataset suite that includes a\nlarge-scale source domain recommendation dataset and nine diverse target domain\nrecommendation datasets. Each item in NineRec is represented by a text\ndescription and a high-resolution cover image. With NineRec, we can implement\nTransRec models in an end-to-end training manner instead of using pre-extracted\ninvariant features. We conduct a benchmark study and empirical analysis of\nTransRec using NineRec, and our findings provide several valuable insights. To\nsupport further research, we make our code, datasets, benchmarks, and\nleaderboards publicly available at\nhttps://github.com/anonymous?ninerec/NineRec.",
        "translated": ""
    },
    {
        "title": "A Conversation is Worth A Thousand Recommendations: A Survey of Holistic\n  Conversational Recommender Systems",
        "url": "http://arxiv.org/abs/2309.07682v1",
        "pub_date": "2023-09-14",
        "summary": "Conversational recommender systems (CRS) generate recommendations through an\ninteractive process. However, not all CRS approaches use human conversations as\ntheir source of interaction data; the majority of prior CRS work simulates\ninteractions by exchanging entity-level information. As a result, claims of\nprior CRS work do not generalise to real-world settings where conversations\ntake unexpected turns, or where conversational and intent understanding is not\nperfect. To tackle this challenge, the research community has started to\nexamine holistic CRS, which are trained using conversational data collected\nfrom real-world scenarios. Despite their emergence, such holistic approaches\nare under-explored.\n  We present a comprehensive survey of holistic CRS methods by summarizing the\nliterature in a structured manner. Our survey recognises holistic CRS\napproaches as having three components: 1) a backbone language model, the\noptional use of 2) external knowledge, and/or 3) external guidance. We also\ngive a detailed analysis of CRS datasets and evaluation methods in real\napplication scenarios. We offer our insight as to the current challenges of\nholistic CRS and possible future trends.",
        "translated": ""
    },
    {
        "title": "Feature Engineering in Learning-to-Rank for Community Question Answering\n  Task",
        "url": "http://arxiv.org/abs/2309.07610v1",
        "pub_date": "2023-09-14",
        "summary": "Community question answering (CQA) forums are Internet-based platforms where\nusers ask questions about a topic and other expert users try to provide\nsolutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer,\nStackExchange exist with a lot of user-generated data. These data are leveraged\nin automated CQA ranking systems where similar questions (and answers) are\npresented in response to the query of the user. In this work, we empirically\ninvestigate a few aspects of this domain. Firstly, in addition to traditional\nfeatures like TF-IDF, BM25 etc., we introduce a BERT-based feature that\ncaptures the semantic similarity between the question and answer. Secondly,\nmost of the existing research works have focused on features extracted only\nfrom the question part; features extracted from answers have not been explored\nextensively. We combine both types of features in a linear fashion. Thirdly,\nusing our proposed concepts, we conduct an empirical investigation with\ndifferent rank-learning algorithms, some of which have not been used so far in\nCQA domain. On three standard CQA datasets, our proposed framework achieves\nstate-of-the-art performance. We also analyze importance of the features we use\nin our investigation. This work is expected to guide the practitioners to\nselect a better set of features for the CQA retrieval task.",
        "translated": ""
    },
    {
        "title": "Zero-shot Audio Topic Reranking using Large Language Models",
        "url": "http://arxiv.org/abs/2309.07606v1",
        "pub_date": "2023-09-14",
        "summary": "The Multimodal Video Search by Examples (MVSE) project investigates using\nvideo clips as the query term for information retrieval, rather than the more\ntraditional text query. This enables far richer search modalities such as\nimages, speaker, content, topic, and emotion. A key element for this process is\nhighly rapid, flexible, search to support large archives, which in MVSE is\nfacilitated by representing video attributes by embeddings. This work aims to\nmitigate any performance loss from this rapid archive search by examining\nreranking approaches. In particular, zero-shot reranking methods using large\nlanguage models are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking can achieve improved retrieval ranking without the need for any\ntask-specific training data.",
        "translated": ""
    },
    {
        "title": "Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?",
        "url": "http://arxiv.org/abs/2309.07602v1",
        "pub_date": "2023-09-14",
        "summary": "Recently sequential recommendations and next-item prediction task has become\nincreasingly popular in the field of recommender systems. Currently, two\nstate-of-the-art baselines are Transformer-based models SASRec and BERT4Rec.\nOver the past few years, there have been quite a few publications comparing\nthese two algorithms and proposing new state-of-the-art models. In most of the\npublications, BERT4Rec achieves better performance than SASRec. But BERT4Rec\nuses cross-entropy over softmax for all items, while SASRec uses negative\nsampling and calculates binary cross-entropy loss for one positive and one\nnegative item. In our work, we show that if both models are trained with the\nsame loss, which is used by BERT4Rec, then SASRec will significantly outperform\nBERT4Rec both in terms of quality and training speed. In addition, we show that\nSASRec could be effectively trained with negative sampling and still outperform\nBERT4Rec, but the number of negative examples should be much larger than one.",
        "translated": ""
    },
    {
        "title": "C-Pack: Packaged Resources To Advance General Chinese Embedding",
        "url": "http://arxiv.org/abs/2309.07597v1",
        "pub_date": "2023-09-14",
        "summary": "We introduce C-Pack, a package of resources that significantly advance the\nfield of general Chinese embeddings. C-Pack includes three critical resources.\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\nC-TEM is a family of embedding models covering multiple sizes. Our models\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\ntime of the release. We also integrate and optimize the entire suite of\ntraining methods for C-TEM. Along with our resources on general Chinese\nembedding, we release our data and models for English text embeddings. The\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\nmeanwhile, our released English data is 2 times larger than the Chinese data.\nAll these resources are made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.",
        "translated": ""
    },
    {
        "title": "Neuro-Symbolic Recommendation Model based on Logic Query",
        "url": "http://arxiv.org/abs/2309.07594v1",
        "pub_date": "2023-09-14",
        "summary": "A recommendation system assists users in finding items that are relevant to\nthem. Existing recommendation models are primarily based on predicting\nrelationships between users and items and use complex matching models or\nincorporate extensive external information to capture association patterns in\ndata. However, recommendation is not only a problem of inductive statistics\nusing data; it is also a cognitive task of reasoning decisions based on\nknowledge extracted from information. Hence, a logic system could naturally be\nincorporated for the reasoning in a recommendation task. However, although\nhard-rule approaches based on logic systems can provide powerful reasoning\nability, they struggle to cope with inconsistent and incomplete knowledge in\nreal-world tasks, especially for complex tasks such as recommendation.\nTherefore, in this paper, we propose a neuro-symbolic recommendation model,\nwhich transforms the user history interactions into a logic expression and then\ntransforms the recommendation prediction into a query task based on this logic\nexpression. The logic expressions are then computed based on the modular logic\noperations of the neural network. We also construct an implicit logic encoder\nto reasonably reduce the complexity of the logic computation. Finally, a user's\ninterest items can be queried in the vector space based on the computation\nresults. Experiments on three well-known datasets verified that our method\nperforms better compared to state of the art shallow, deep, session, and\nreasoning models.",
        "translated": ""
    },
    {
        "title": "MMEAD: MS MARCO Entity Annotations and Disambiguations",
        "url": "http://arxiv.org/abs/2309.07574v1",
        "pub_date": "2023-09-14",
        "summary": "MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for\nentity links for the MS MARCO datasets. We specify a format to store and share\nlinks for both document and passage collections of MS MARCO. Following this\nspecification, we release entity links to Wikipedia for documents and passages\nin both MS MARCO collections (v1 and v2). Entity links have been produced by\nthe REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing\nusers to load the link data and entity embeddings effortlessly. Using MMEAD\ntakes only a few lines of code. Finally, we show how MMEAD can be used for IR\nresearch that uses entity information. We show how to improve recall@1000 and\nMRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this\nresource. We also demonstrate how entity expansions can be used for interactive\nsearch applications.",
        "translated": ""
    },
    {
        "title": "When do Generative Query and Document Expansions Fail? A Comprehensive\n  Study Across Methods, Retrievers, and Datasets",
        "url": "http://arxiv.org/abs/2309.08541v1",
        "pub_date": "2023-09-15",
        "summary": "Using large language models (LMs) for query or document expansion can improve\ngeneralization in information retrieval. However, it is unknown whether these\ntechniques are universally beneficial or only effective in specific settings,\nsuch as for particular retrieval models, dataset domains, or query types. To\nanswer this, we conduct the first comprehensive analysis of LM-based expansion.\nWe find that there exists a strong negative correlation between retriever\nperformance and gains from expansion: expansion improves scores for weaker\nmodels, but generally harms stronger models. We show this trend holds across a\nset of eleven expansion techniques, twelve datasets with diverse distribution\nshifts, and twenty-four retrieval models. Through qualitative error analysis,\nwe hypothesize that although expansions provide extra information (potentially\nimproving recall), they add additional noise that makes it difficult to discern\nbetween the top relevant documents (thus introducing false positives). Our\nresults suggest the following recipe: use expansions for weaker models or when\nthe target dataset significantly differs from training corpus in format;\notherwise, avoid expansions to keep the relevance signal clear.",
        "translated": ""
    },
    {
        "title": "SilverRetriever: Advancing Neural Passage Retrieval for Polish Question\n  Answering",
        "url": "http://arxiv.org/abs/2309.08469v1",
        "pub_date": "2023-09-15",
        "summary": "Modern open-domain question answering systems often rely on accurate and\nefficient retrieval components to find passages containing the facts necessary\nto answer the question. Recently, neural retrievers have gained popularity over\nlexical alternatives due to their superior performance. However, most of the\nwork concerns popular languages such as English or Chinese. For others, such as\nPolish, few models are available. In this work, we present SilverRetriever, a\nneural retriever for Polish trained on a diverse collection of manually or\nweakly labeled datasets. SilverRetriever achieves much better results than\nother Polish models and is competitive with larger multilingual models.\nTogether with the model, we open-source five new passage retrieval datasets.",
        "translated": ""
    },
    {
        "title": "Explaining Search Result Stances to Opinionated People",
        "url": "http://arxiv.org/abs/2309.08460v1",
        "pub_date": "2023-09-15",
        "summary": "People use web search engines to find information before forming opinions,\nwhich can lead to practical decisions with different levels of impact. The\ncognitive effort of search can leave opinionated users vulnerable to cognitive\nbiases, e.g., the confirmation bias. In this paper, we investigate whether\nstance labels and their explanations can help users consume more diverse search\nresults. We automatically classify and label search results on three topics\n(i.e., intellectual property rights, school uniforms, and atheism) as against,\nneutral, and in favor, and generate explanations for these labels. In a user\nstudy (N =203), we then investigate whether search result stance bias (balanced\nvs biased) and the level of explanation (plain text, label only, label and\nexplanation) influence the diversity of search results clicked. We find that\nstance labels and explanations lead to a more diverse search result\nconsumption. However, we do not find evidence for systematic opinion change\namong users in this context. We believe these results can help designers of\nsearch engines to make more informed design decisions.",
        "translated": ""
    },
    {
        "title": "FedDCSR: Federated Cross-domain Sequential Recommendation via\n  Disentangled Representation Learning",
        "url": "http://arxiv.org/abs/2309.08420v1",
        "pub_date": "2023-09-15",
        "summary": "Cross-domain Sequential Recommendation (CSR) which leverages user sequence\ndata from multiple domains has received extensive attention in recent years.\nHowever, the existing CSR methods require sharing origin user data across\ndomains, which violates the General Data Protection Regulation (GDPR). Thus, it\nis necessary to combine federated learning (FL) and CSR to fully utilize\nknowledge from different domains while preserving data privacy. Nonetheless,\nthe sequence feature heterogeneity across different domains significantly\nimpacts the overall performance of FL. In this paper, we propose FedDCSR, a\nnovel federated cross-domain sequential recommendation framework via\ndisentangled representation learning. Specifically, to address the sequence\nfeature heterogeneity across domains, we introduce an approach called\ninter-intra domain sequence representation disentanglement (SRD) to disentangle\nthe user sequence features into domain-shared and domain-exclusive features. In\naddition, we design an intra domain contrastive infomax (CIM) strategy to learn\nricher domain-exclusive features of users by performing data augmentation on\nuser sequences. Extensive experiments on three real-world scenarios demonstrate\nthat FedDCSR achieves significant improvements over existing baselines.",
        "translated": ""
    },
    {
        "title": "Structural Self-Supervised Objectives for Transformers",
        "url": "http://arxiv.org/abs/2309.08272v1",
        "pub_date": "2023-09-15",
        "summary": "This thesis focuses on improving the pre-training of natural language models\nusing unsupervised raw data to make them more efficient and aligned with\ndownstream applications.\n  In the first part, we introduce three alternative pre-training objectives to\nBERT's Masked Language Modeling (MLM), namely Random Token Substitution (RTS),\nCluster-based Random Token Substitution (C-RTS), and Swapped Language Modeling\n(SLM). These objectives involve token swapping instead of masking, with RTS and\nC-RTS aiming to predict token originality and SLM predicting the original token\nvalues. Results show that RTS and C-RTS require less pre-training time while\nmaintaining performance comparable to MLM. Surprisingly, SLM outperforms MLM on\ncertain tasks despite using the same computational budget.\n  In the second part, we proposes self-supervised pre-training tasks that align\nstructurally with downstream applications, reducing the need for labeled data.\nWe use large corpora like Wikipedia and CC-News to train models to recognize if\ntext spans originate from the same paragraph or document in several ways. By\ndoing continuous pre-training, starting from existing models like RoBERTa,\nELECTRA, DeBERTa, BART, and T5, we demonstrate significant performance\nimprovements in tasks like Fact Verification, Answer Sentence Selection, and\nSummarization. These improvements are especially pronounced when limited\nannotation data is available. The proposed objectives also achieve\nstate-of-the-art results on various benchmark datasets, including FEVER (dev\nset), ASNQ, WikiQA, and TREC-QA, as well as enhancing the quality of summaries.\nImportantly, these techniques can be easily integrated with other methods\nwithout altering the internal structure of Transformer models, making them\nversatile for various NLP applications.",
        "translated": ""
    },
    {
        "title": "AdSEE: Investigating the Impact of Image Style Editing on Advertisement\n  Attractiveness",
        "url": "http://arxiv.org/abs/2309.08159v1",
        "pub_date": "2023-09-15",
        "summary": "Online advertisements are important elements in e-commerce sites, social\nmedia platforms, and search engines. With the increasing popularity of mobile\nbrowsing, many online ads are displayed with visual information in the form of\na cover image in addition to text descriptions to grab the attention of users.\nVarious recent studies have focused on predicting the click rates of online\nadvertisements aware of visual features or composing optimal advertisement\nelements to enhance visibility. In this paper, we propose Advertisement Style\nEditing and Attractiveness Enhancement (AdSEE), which explores whether semantic\nediting to ads images can affect or alter the popularity of online\nadvertisements. We introduce StyleGAN-based facial semantic editing and\ninversion to ads images and train a click rate predictor attributing GAN-based\nface latent representations in addition to traditional visual and textual\nfeatures to click rates. Through a large collected dataset named QQ-AD,\ncontaining 20,527 online ads, we perform extensive offline tests to study how\ndifferent semantic directions and their edit coefficients may impact click\nrates. We further design a Genetic Advertisement Editor to efficiently search\nfor the optimal edit directions and intensity given an input ad cover image to\nenhance its projected click rates. Online A/B tests performed over a period of\n5 days have verified the increased click-through rates of AdSEE-edited samples\nas compared to a control group of original ads, verifying the relation between\nimage styles and ad popularity. We open source the code for AdSEE research at\nhttps://github.com/LiyaoJiang1998/adsee.",
        "translated": ""
    },
    {
        "title": "Uncertainty-Aware Multi-View Visual Semantic Embedding",
        "url": "http://arxiv.org/abs/2309.08154v1",
        "pub_date": "2023-09-15",
        "summary": "The key challenge in image-text retrieval is effectively leveraging semantic\ninformation to measure the similarity between vision and language data.\nHowever, using instance-level binary labels, where each image is paired with a\nsingle text, fails to capture multiple correspondences between different\nsemantic units, leading to uncertainty in multi-modal semantic understanding.\nAlthough recent research has captured fine-grained information through more\ncomplex model structures or pre-training techniques, few studies have directly\nmodeled uncertainty of correspondence to fully exploit binary labels. To\naddress this issue, we propose an Uncertainty-Aware Multi-View Visual Semantic\nEmbedding (UAMVSE)} framework that decomposes the overall image-text matching\ninto multiple view-text matchings. Our framework introduce an uncertainty-aware\nloss function (UALoss) to compute the weighting of each view-text loss by\nadaptively modeling the uncertainty in each view-text correspondence. Different\nweightings guide the model to focus on different semantic information,\nenhancing the model's ability to comprehend the correspondence of images and\ntexts. We also design an optimized image-text matching strategy by normalizing\nthe similarity matrix to improve model performance. Experimental results on the\nFlicker30k and MS-COCO datasets demonstrate that UAMVSE outperforms\nstate-of-the-art models.",
        "translated": ""
    },
    {
        "title": "iHAS: Instance-wise Hierarchical Architecture Search for Deep Learning\n  Recommendation Models",
        "url": "http://arxiv.org/abs/2309.07967v1",
        "pub_date": "2023-09-14",
        "summary": "Current recommender systems employ large-sized embedding tables with uniform\ndimensions for all features, leading to overfitting, high computational cost,\nand suboptimal generalizing performance. Many techniques aim to solve this\nissue by feature selection or embedding dimension search. However, these\ntechniques typically select a fixed subset of features or embedding dimensions\nfor all instances and feed all instances into one recommender model without\nconsidering heterogeneity between items or users. This paper proposes a novel\ninstance-wise Hierarchical Architecture Search framework, iHAS, which automates\nneural architecture search at the instance level. Specifically, iHAS\nincorporates three stages: searching, clustering, and retraining. The searching\nstage identifies optimal instance-wise embedding dimensions across different\nfield features via carefully designed Bernoulli gates with stochastic selection\nand regularizers. After obtaining these dimensions, the clustering stage\ndivides samples into distinct groups via a deterministic selection approach of\nBernoulli gates. The retraining stage then constructs different recommender\nmodels, each one designed with optimal dimensions for the corresponding group.\nWe conduct extensive experiments to evaluate the proposed iHAS on two public\nbenchmark datasets from a real-world recommender system. The experimental\nresults demonstrate the effectiveness of iHAS and its outstanding\ntransferability to widely-used deep recommendation models.",
        "translated": ""
    },
    {
        "title": "Predictive Uncertainty-based Bias Mitigation in Ranking",
        "url": "http://arxiv.org/abs/2309.09833v1",
        "pub_date": "2023-09-18",
        "summary": "Societal biases that are contained in retrieved documents have received\nincreased interest. Such biases, which are often prevalent in the training data\nand learned by the model, can cause societal harms, by misrepresenting certain\ngroups, and by enforcing stereotypes. Mitigating such biases demands algorithms\nthat balance the trade-off between maximized utility for the user with fairness\nobjectives, which incentivize unbiased rankings. Prior work on bias mitigation\noften assumes that ranking scores, which correspond to the utility that a\ndocument holds for a user, can be accurately determined. In reality, there is\nalways a degree of uncertainty in the estimate of expected document utility.\nThis uncertainty can be approximated by viewing ranking models through a\nBayesian perspective, where the standard deterministic score becomes a\ndistribution.\n  In this work, we investigate whether uncertainty estimates can be used to\ndecrease the amount of bias in the ranked results, while minimizing loss in\nmeasured utility. We introduce a simple method that uses the uncertainty of the\nranking scores for an uncertainty-aware, post hoc approach to bias mitigation.\nWe compare our proposed method with existing baselines for bias mitigation with\nrespect to the utility-fairness trade-off, the controllability of methods, and\ncomputational costs. We show that an uncertainty-based approach can provide an\nintuitive and flexible trade-off that outperforms all baselines without\nadditional training requirements, allowing for the post hoc use of this\napproach on top of arbitrary retrieval models.",
        "translated": ""
    },
    {
        "title": "How Much Freedom Does An Effectiveness Metric Really Have?",
        "url": "http://arxiv.org/abs/2309.09477v1",
        "pub_date": "2023-09-18",
        "summary": "It is tempting to assume that because effectiveness metrics have free choice\nto assign scores to search engine result pages (SERPs) there must thus be a\nsimilar degree of freedom as to the relative order that SERP pairs can be put\ninto. In fact that second freedom is, to a considerable degree, illusory.\nThat's because if one SERP in a pair has been given a certain score by a\nmetric, fundamental ordering constraints in many cases then dictate that the\nscore for the second SERP must be either not less than, or not greater than,\nthe score assigned to the first SERP. We refer to these fixed relationships as\ninnate pairwise SERP orderings. Our first goal in this work is to describe and\ndefend those pairwise SERP relationship constraints, and tabulate their\nrelative occurrence via both exhaustive and empirical experimentation.\n  We then consider how to employ such innate pairwise relationships in IR\nexperiments, leading to a proposal for a new measurement paradigm.\nSpecifically, we argue that tables of results in which many different metrics\nare listed for champion versus challenger system comparisons should be avoided;\nand that instead a single metric be argued for in principled terms, with any\nrelationships identified by that metric then reinforced via an assessment of\nthe innate relationship as to whether other metrics - indeed, all other metrics\n- are likely to yield the same system-vs-system outcome.",
        "translated": ""
    },
    {
        "title": "Selecting which Dense Retriever to use for Zero-Shot Search",
        "url": "http://arxiv.org/abs/2309.09403v1",
        "pub_date": "2023-09-18",
        "summary": "We propose the new problem of choosing which dense retrieval model to use\nwhen searching on a new collection for which no labels are available, i.e. in a\nzero-shot setting. Many dense retrieval models are readily available. Each\nmodel however is characterized by very differing search effectiveness -- not\njust on the test portion of the datasets in which the dense representations\nhave been learned but, importantly, also across different datasets for which\ndata was not used to learn the dense representations. This is because dense\nretrievers typically require training on a large amount of labeled data to\nachieve satisfactory search effectiveness in a specific dataset or domain.\nMoreover, effectiveness gains obtained by dense retrievers on datasets for\nwhich they are able to observe labels during training, do not necessarily\ngeneralise to datasets that have not been observed during training. This is\nhowever a hard problem: through empirical experimentation we show that methods\ninspired by recent work in unsupervised performance evaluation with the\npresence of domain shift in the area of computer vision and machine learning\nare not effective for choosing highly performing dense retrievers in our setup.\nThe availability of reliable methods for the selection of dense retrieval\nmodels in zero-shot settings that do not require the collection of labels for\nevaluation would allow to streamline the widespread adoption of dense\nretrieval. This is therefore an important new problem we believe the\ninformation retrieval community should consider. Implementation of methods,\nalong with raw result files and analysis scripts are made publicly available at\nhttps://www.github.com/anonymized.",
        "translated": ""
    },
    {
        "title": "ChatGPT Hallucinates when Attributing Answers",
        "url": "http://arxiv.org/abs/2309.09401v1",
        "pub_date": "2023-09-17",
        "summary": "Can ChatGPT provide evidence to support its answers? Does the evidence it\nsuggests actually exist and does it really support its answer? We investigate\nthese questions using a collection of domain-specific knowledge-based\nquestions, specifically prompting ChatGPT to provide both an answer and\nsupporting evidence in the form of references to external sources. We also\ninvestigate how different prompts impact answers and evidence. We find that\nChatGPT provides correct or partially correct answers in about half of the\ncases (50.6% of the times), but its suggested references only exist 14% of the\ntimes. We further provide insights on the generated references that reveal\ncommon traits among the references that ChatGPT generates, and show how even if\na reference provided by the model does exist, this reference often does not\nsupport the claims ChatGPT attributes to it. Our findings are important because\n(1) they are the first systematic analysis of the references created by ChatGPT\nin its answers; (2) they suggest that the model may leverage good quality\ninformation in producing correct answers, but is unable to attribute real\nevidence to support its answers. Prompts, raw result files and manual analysis\nare made publicly available.",
        "translated": ""
    },
    {
        "title": "Towards Debiasing Frame Length Bias in Text-Video Retrieval via Causal\n  Intervention",
        "url": "http://arxiv.org/abs/2309.09311v1",
        "pub_date": "2023-09-17",
        "summary": "Many studies focus on improving pretraining or developing new backbones in\ntext-video retrieval. However, existing methods may suffer from the learning\nand inference bias issue, as recent research suggests in other\ntext-video-related tasks. For instance, spatial appearance features on action\nrecognition or temporal object co-occurrences on video scene graph generation\ncould induce spurious correlations. In this work, we present a unique and\nsystematic study of a temporal bias due to frame length discrepancy between\ntraining and test sets of trimmed video clips, which is the first such attempt\nfor a text-video retrieval task, to the best of our knowledge. We first\nhypothesise and verify the bias on how it would affect the model illustrated\nwith a baseline study. Then, we propose a causal debiasing approach and perform\nextensive experiments and ablation studies on the Epic-Kitchens-100, YouCook2,\nand MSR-VTT datasets. Our model overpasses the baseline and SOTA on nDCG, a\nsemantic-relevancy-focused evaluation metric which proves the bias is\nmitigated, as well as on the other conventional metrics.",
        "translated": ""
    },
    {
        "title": "Fairness for All: Investigating Harms to Within-Group Individuals in\n  Producer Fairness Re-ranking Optimization -- A Reproducibility Study",
        "url": "http://arxiv.org/abs/2309.09277v1",
        "pub_date": "2023-09-17",
        "summary": "Recommender systems are widely used to provide personalized recommendations\nto users. Recent research has shown that recommender systems may be subject to\ndifferent types of biases, such as popularity bias, leading to an uneven\ndistribution of recommendation exposure among producer groups. To mitigate\nthis, producer-centered fairness re-ranking (PFR) approaches have been proposed\nto ensure equitable recommendation utility across groups. However, these\napproaches overlook the harm they may cause to within-group individuals\nassociated with colder items, which are items with few or no interactions.\n  This study reproduces previous PFR approaches and shows that they\nsignificantly harm colder items, leading to a fairness gap for these items in\nboth advantaged and disadvantaged groups. Surprisingly, the unfair base\nrecommendation models were providing greater exposure opportunities to these\nindividual cold items, even though at the group level, they appeared to be\nunfair. To address this issue, the study proposes an amendment to the PFR\napproach that regulates the number of colder items recommended by the system.\nThis modification achieves a balance between accuracy and producer fairness\nwhile optimizing the selection of colder items within each group, thereby\npreventing or reducing harm to within-group individuals and augmenting the\nnovelty of all recommended items. The proposed method is able to register an\nincrease in sub-group fairness (SGF) from 0.3104 to 0.3782, 0.6156, and 0.9442\nwhile also improving group-level fairness (GF) (112% and 37% with respect to\nbase models and traditional PFR). Moreover, the proposed method achieves these\nimprovements with minimal or no reduction in accuracy (or even an increase\nsometimes). We evaluate the proposed method on various recommendation datasets\nand demonstrate promising results independent of the underlying model or\ndatasets.",
        "translated": ""
    },
    {
        "title": "Leveraging Large Language Models for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.09261v1",
        "pub_date": "2023-09-17",
        "summary": "Sequential recommendation problems have received increasing attention in\nresearch during the past few years, leading to the inception of a large variety\nof algorithmic approaches. In this work, we explore how large language models\n(LLMs), which are nowadays introducing disruptive effects in many AI-based\napplications, can be used to build or improve sequential recommendation\napproaches. Specifically, we devise and evaluate three approaches to leverage\nthe power of LLMs in different ways. Our results from experiments on two\ndatasets show that initializing the state-of-the-art sequential recommendation\nmodel BERT4Rec with embeddings obtained from an LLM improves NDCG by 15-20%\ncompared to the vanilla BERT4Rec model. Furthermore, we find that a simple\napproach that leverages LLM embeddings for producing recommendations, can\nprovide competitive performance by highlighting semantically related items. We\npublicly share the code and data of our experiments to ensure reproducibility.",
        "translated": ""
    },
    {
        "title": "SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription",
        "url": "http://arxiv.org/abs/2309.09085v1",
        "pub_date": "2023-09-16",
        "summary": "Guitar tablature is a form of music notation widely used among guitarists. It\ncaptures not only the musical content of a piece, but also its implementation\nand ornamentation on the instrument. Guitar Tablature Transcription (GTT) is an\nimportant task with broad applications in music education and entertainment.\nExisting datasets are limited in size and scope, causing state-of-the-art GTT\nmodels trained on such datasets to suffer from overfitting and to fail in\ngeneralization across datasets. To address this issue, we developed a\nmethodology for synthesizing SynthTab, a large-scale guitar tablature\ntranscription dataset using multiple commercial acoustic and electric guitar\nplugins. This dataset is built on tablatures from DadaGP, which offers a vast\ncollection and the degree of specificity we wish to transcribe. The proposed\nsynthesis pipeline produces audio which faithfully adheres to the original\nfingerings, styles, and techniques specified in the tablature with diverse\ntimbre. Experiments show that pre-training state-of-the-art GTT model on\nSynthTab improves transcription accuracy in same-dataset tests. More\nimportantly, it significantly mitigates overfitting problems of GTT models in\ncross-dataset evaluation.",
        "translated": ""
    },
    {
        "title": "Bridging Dense and Sparse Maximum Inner Product Search",
        "url": "http://arxiv.org/abs/2309.09013v1",
        "pub_date": "2023-09-16",
        "summary": "Maximum inner product search (MIPS) over dense and sparse vectors have\nprogressed independently in a bifurcated literature for decades; the latter is\nbetter known as top-$k$ retrieval in Information Retrieval. This duality exists\nbecause sparse and dense vectors serve different end goals. That is despite the\nfact that they are manifestations of the same mathematical problem. In this\nwork, we ask if algorithms for dense vectors could be applied effectively to\nsparse vectors, particularly those that violate the assumptions underlying\ntop-$k$ retrieval methods. We study IVF-based retrieval where vectors are\npartitioned into clusters and only a fraction of clusters are searched during\nretrieval. We conduct a comprehensive analysis of dimensionality reduction for\nsparse vectors, and examine standard and spherical KMeans for partitioning. Our\nexperiments demonstrate that IVF serves as an efficient solution for sparse\nMIPS. As byproducts, we identify two research opportunities and demonstrate\ntheir potential. First, we cast the IVF paradigm as a dynamic pruning technique\nand turn that insight into a novel organization of the inverted index for\napproximate MIPS for general sparse vectors. Second, we offer a unified regime\nfor MIPS over vectors that have dense and sparse subspaces, and show its\nrobustness to query distributions.",
        "translated": ""
    },
    {
        "title": "An Unified Search and Recommendation Foundation Model for Cold-Start\n  Scenario",
        "url": "http://arxiv.org/abs/2309.08939v1",
        "pub_date": "2023-09-16",
        "summary": "In modern commercial search engines and recommendation systems, data from\nmultiple domains is available to jointly train the multi-domain model.\nTraditional methods train multi-domain models in the multi-task setting, with\nshared parameters to learn the similarity of multiple tasks, and task-specific\nparameters to learn the divergence of features, labels, and sample\ndistributions of individual tasks. With the development of large language\nmodels, LLM can extract global domain-invariant text features that serve both\nsearch and recommendation tasks. We propose a novel framework called S\\&amp;R\nMulti-Domain Foundation, which uses LLM to extract domain invariant features,\nand Aspect Gating Fusion to merge the ID feature, domain invariant text\nfeatures and task-specific heterogeneous sparse features to obtain the\nrepresentations of query and item. Additionally, samples from multiple search\nand recommendation scenarios are trained jointly with Domain Adaptive\nMulti-Task module to obtain the multi-domain foundation model. We apply the\nS\\&amp;R Multi-Domain foundation model to cold start scenarios in the\npretrain-finetune manner, which achieves better performance than other SOTA\ntransfer learning methods. The S\\&amp;R Multi-Domain Foundation model has been\nsuccessfully deployed in Alipay Mobile Application's online services, such as\ncontent query recommendation and service card recommendation, etc.",
        "translated": ""
    },
    {
        "title": "Interactive Distillation of Large Single-Topic Corpora of Scientific\n  Papers",
        "url": "http://arxiv.org/abs/2309.10772v1",
        "pub_date": "2023-09-19",
        "summary": "Highly specific datasets of scientific literature are important for both\nresearch and education. However, it is difficult to build such datasets at\nscale. A common approach is to build these datasets reductively by applying\ntopic modeling on an established corpus and selecting specific topics. A more\nrobust but time-consuming approach is to build the dataset constructively in\nwhich a subject matter expert (SME) handpicks documents. This method does not\nscale and is prone to error as the dataset grows. Here we showcase a new tool,\nbased on machine learning, for constructively generating targeted datasets of\nscientific literature. Given a small initial \"core\" corpus of papers, we build\na citation network of documents. At each step of the citation network, we\ngenerate text embeddings and visualize the embeddings through dimensionality\nreduction. Papers are kept in the dataset if they are \"similar\" to the core or\nare otherwise pruned through human-in-the-loop selection. Additional insight\ninto the papers is gained through sub-topic modeling using SeNMFk. We\ndemonstrate our new tool for literature review by applying it to two different\nfields in machine learning.",
        "translated": ""
    },
    {
        "title": "MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation",
        "url": "http://arxiv.org/abs/2309.10738v1",
        "pub_date": "2023-09-19",
        "summary": "Pre-trained language models have achieved impressive results in various music\nunderstanding and generation tasks. However, existing pre-training methods for\nsymbolic melody generation struggle to capture multi-scale, multi-dimensional\nstructural information in note sequences, due to the domain knowledge\ndiscrepancy between text and music. Moreover, the lack of available large-scale\nsymbolic melody datasets limits the pre-training improvement. In this paper, we\npropose MelodyGLM, a multi-task pre-training framework for generating melodies\nwith long-term structure. We design the melodic n-gram and long span sampling\nstrategies to create local and global blank infilling tasks for modeling the\nlocal and global structures in melodies. Specifically, we incorporate pitch\nn-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram\nblank infilling tasks for modeling the multi-dimensional structures in\nmelodies. To this end, we have constructed a large-scale symbolic melody\ndataset, MelodyNet, containing more than 0.4 million melody pieces. MelodyNet\nis utilized for large-scale pre-training and domain-specific n-gram lexicon\nconstruction. Both subjective and objective evaluations demonstrate that\nMelodyGLM surpasses the standard and previous pre-training methods. In\nparticular, subjective evaluations show that, on the melody continuation task,\nMelodyGLM achieves average improvements of 0.82, 0.87, 0.78, and 0.94 in\nconsistency, rhythmicity, structure, and overall quality, respectively.\nNotably, MelodyGLM nearly matches the quality of human-composed melodies on the\nmelody inpainting task.",
        "translated": ""
    },
    {
        "title": "Large language models can accurately predict searcher preferences",
        "url": "http://arxiv.org/abs/2309.10621v1",
        "pub_date": "2023-09-19",
        "summary": "Relevance labels, which indicate whether a search result is valuable to a\nsearcher, are key to evaluating and optimising search systems. The best way to\ncapture the true preferences of users is to ask them for their careful feedback\non which results would be useful, but this approach does not scale to produce a\nlarge number of labels. Getting relevance labels at scale is usually done with\nthird-party labellers, who judge on behalf of the user, but there is a risk of\nlow-quality data if the labeller doesn't understand user needs. To improve\nquality, one standard approach is to study real users through interviews, user\nstudies and direct feedback, find areas where labels are systematically\ndisagreeing with users, then educate labellers about user needs through judging\nguidelines, training and monitoring. This paper introduces an alternate\napproach for improving label quality. It takes careful feedback from real\nusers, which by definition is the highest-quality first-party gold data that\ncan be derived, and develops an large language model prompt that agrees with\nthat data.\n  We present ideas and observations from deploying language models for\nlarge-scale relevance labelling at Bing, and illustrate with data from TREC. We\nhave found large language models can be effective, with accuracy as good as\nhuman labellers and similar capability to pick the hardest queries, best runs,\nand best groups. Systematic changes to the prompts make a difference in\naccuracy, but so too do simple paraphrases. To measure agreement with real\nsearchers needs high-quality ``gold'' labels, but with these we find that\nmodels produce better labels than third-party workers, for a fraction of the\ncost, and these labels let us train notably better rankers.",
        "translated": ""
    },
    {
        "title": "A Hierarchical Neural Framework for Classification and its Explanation\n  in Large Unstructured Legal Documents",
        "url": "http://arxiv.org/abs/2309.10563v1",
        "pub_date": "2023-09-19",
        "summary": "Automatic legal judgment prediction and its explanation suffer from the\nproblem of long case documents exceeding tens of thousands of words, in\ngeneral, and having a non-uniform structure. Predicting judgments from such\ndocuments and extracting their explanation becomes a challenging task, more so\non documents with no structural annotation. We define this problem as \"scarce\nannotated legal documents\" and explore their lack of structural information and\ntheir long lengths with a deep learning-based classification framework which we\ncall MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment\nprediction. Specifically, we divide a document into parts to extract their\nembeddings from the last four layers of a custom fine-tuned Large Language\nModel, and try to approximate their structure through unsupervised clustering.\nWhich we use in another set of transformer encoder layers to learn the\ninter-chunk representations. We explore the adaptability of LLMs with\nmulti-billion parameters (GPT-Neo, and GPT-J) to legal texts and their\nintra-domain(legal) transfer learning capacity. Alongside this, we compare\ntheir performance with MESc and the impact of combining embeddings from their\nlast layers. For such hierarchical models, we also propose an explanation\nextraction algorithm named ORSE; Occlusion sensitivity-based Relevant Sentence\nExtractor;",
        "translated": ""
    },
    {
        "title": "Proposal for an Organic Web, The missing link between the Web and the\n  Semantic Web, Part 1",
        "url": "http://arxiv.org/abs/2309.10531v1",
        "pub_date": "2023-09-19",
        "summary": "A huge amount of information is produced in digital form. The Semantic Web\nstems from the realisation that dealing efficiently with this production\nrequires getting better at interlinking digital informational resources\ntogether. Its focus is on linking data. Linking data isn't enough. We need to\nprovide infrastructural support for linking all sorts of informational\nresources including resources whose understanding and fine interlinking\nrequires domain-specific human expertise. At times when many problems scale to\nplanetary dimensions, it is essential to scale coordination of information\nprocessing and information production, without giving up on expertise and depth\nof analysis, nor forcing languages and formalisms onto thinkers,\ndecision-makers and innovators that are only suitable to some forms of\nintelligence. This article makes a proposal in this direction and in line with\nthe idea of interlinking championed by the Semantic Web.",
        "translated": ""
    },
    {
        "title": "A Digital Forensics Case Study of the DJI Mini 3 Pro and DJI RC",
        "url": "http://arxiv.org/abs/2309.10487v1",
        "pub_date": "2023-09-19",
        "summary": "The consumer drone market is rapidly expanding with new drone models\nfeaturing unique variations of hardware and software. The rapid development of\ndrone technology and variability in drone systems can make it difficult for\ndigital forensic investigators and tools to keep pace and effectively extract\nand analyse digital evidence from drones. Furthermore, the growing popularity\nof drones and their increased use in illegal and harmful activities, such as\nsmuggling, espionage, and even terrorism, has led to an increase in the number\nof drone forensic cases for authorities to manage. To assist forensic\ninvestigators, a static digital forensic case study was conducted on two drone\ndevices recently released by Da-Jiang Innovations (DJI): the Mini 3 Pro drone,\nand its remote controller, the DJI RC. The study discovered the presence of\nseveral digital artefacts on both devices, including recorded media, flight\nlogs, and other information that could help investigators trace the drone's\nusage and identify its operator. Additionally, this paper explored several\nmethods for extracting and visualising the drone's flight history, and\nhighlights some of the potential methods used to limit, obscure, or remove key\ntypes of digital evidence.",
        "translated": ""
    },
    {
        "title": "RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.10469v1",
        "pub_date": "2023-09-19",
        "summary": "Online recommender systems (RS) aim to match user needs with the vast amount\nof resources available on various platforms. A key challenge is to model user\npreferences accurately under the condition of data sparsity. To address this\nchallenge, some methods have leveraged external user behavior data from\nmultiple platforms to enrich user representation. However, all of these methods\nrequire a consistent user ID across platforms and ignore the information from\nsimilar users. In this study, we propose RUEL, a novel retrieval-based\nsequential recommender that can effectively incorporate external anonymous user\nbehavior data from Edge browser logs to enhance recommendation. We first\ncollect and preprocess a large volume of Edge browser logs over a one-year\nperiod and link them to target entities that correspond to candidate items in\nrecommendation datasets. We then design a contrastive learning framework with a\nmomentum encoder and a memory bank to retrieve the most relevant and diverse\nbrowsing sequences from the full browsing log based on the semantic similarity\nbetween user representations. After retrieval, we apply an item-level attentive\nselector to filter out noisy items and generate refined sequence embeddings for\nthe final predictor. RUEL is the first method that connects user browsing data\nwith typical recommendation datasets and can be generalized to various\nrecommendation scenarios and datasets. We conduct extensive experiments on four\nreal datasets for sequential recommendation tasks and demonstrate that RUEL\nsignificantly outperforms state-of-the-art baselines. We also conduct ablation\nstudies and qualitative analysis to validate the effectiveness of each\ncomponent of RUEL and provide additional insights into our method.",
        "translated": ""
    },
    {
        "title": "Reformulating Sequential Recommendation: Learning Dynamic User Interest\n  with Content-enriched Language Modeling",
        "url": "http://arxiv.org/abs/2309.10435v1",
        "pub_date": "2023-09-19",
        "summary": "Recommender systems are essential for online applications, and sequential\nrecommendation has enjoyed significant prevalence due to its expressive ability\nto capture dynamic user interests. However, previous sequential modeling\nmethods still have limitations in capturing contextual information. The primary\nreason for this issue is that language models often lack an understanding of\ndomain-specific knowledge and item-related textual content. To address this\nissue, we adopt a new sequential recommendation paradigm and propose LANCER,\nwhich leverages the semantic understanding capabilities of pre-trained language\nmodels to generate personalized recommendations. Our approach bridges the gap\nbetween language models and recommender systems, resulting in more human-like\nrecommendations. We demonstrate the effectiveness of our approach through\nexperiments on several benchmark datasets, showing promising results and\nproviding valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Deep Mutual Learning across Task Towers for Effective Multi-Task\n  Recommender Learning",
        "url": "http://arxiv.org/abs/2309.10357v1",
        "pub_date": "2023-09-19",
        "summary": "Recommender systems usually leverage multi-task learning methods to\nsimultaneously optimize several objectives because of the multi-faceted user\nbehavior data. The typical way of conducting multi-task learning is to\nestablish appropriate parameter sharing across multiple tasks at lower layers\nwhile reserving a separate task tower for each task at upper layers. Since the\ntask towers exert direct impact on the prediction results, we argue that the\narchitecture of standalone task towers is sub-optimal for promoting positive\nknowledge sharing. Accordingly, we propose the framework of Deep Mutual\nLearning across task towers, which is compatible with various backbone\nmulti-task networks. Extensive offline experiments and online AB tests are\nconducted to evaluate and verify the proposed approach's effectiveness.",
        "translated": ""
    },
    {
        "title": "Computational Approaches for App-to-App Retrieval and Design Consistency\n  Check",
        "url": "http://arxiv.org/abs/2309.10328v1",
        "pub_date": "2023-09-19",
        "summary": "Extracting semantic representations from mobile user interfaces (UI) and\nusing the representations for designers' decision-making processes have shown\nthe potential to be effective computational design support tools. Current\napproaches rely on machine learning models trained on small-sized mobile UI\ndatasets to extract semantic vectors and use screenshot-to-screenshot\ncomparison to retrieve similar-looking UIs given query screenshots. However,\nthe usability of these methods is limited because they are often not\nopen-sourced and have complex training pipelines for practitioners to follow,\nand are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval.\nTo this end, we (1) employ visual models trained with large web-scale images\nand test whether they could extract a UI representation in a zero-shot way and\noutperform existing specialized models, and (2) use mathematically founded\nmethods to enable app-to-app retrieval and design consistency analysis. Our\nexperiments show that our methods not only improve upon previous retrieval\nmodels but also enable multiple new applications.",
        "translated": ""
    },
    {
        "title": "Bravo MaRDI: A Wikibase Powered Knowledge Graph on Mathematics",
        "url": "http://arxiv.org/abs/2309.11484v1",
        "pub_date": "2023-09-20",
        "summary": "Mathematical world knowledge is a fundamental component of Wikidata. However,\nto date, no expertly curated knowledge graph has focused specifically on\ncontemporary mathematics. Addressing this gap, the Mathematical Research Data\nInitiative (MaRDI) has developed a comprehensive knowledge graph that links\nmultimodal research data in mathematics. This encompasses traditional research\ndata items like datasets, software, and publications and includes semantically\nadvanced objects such as mathematical formulas and hypotheses. This paper\ndetails the abilities of the MaRDI knowledge graph, which is based on Wikibase,\nleading up to its inaugural public release, codenamed Bravo, available on\nhttps://portal.mardi4nfdi.de.",
        "translated": ""
    },
    {
        "title": "Retrieving Supporting Evidence for Generative Question Answering",
        "url": "http://arxiv.org/abs/2309.11392v1",
        "pub_date": "2023-09-20",
        "summary": "Current large language models (LLMs) can exhibit near-human levels of\nperformance on many natural language-based tasks, including open-domain\nquestion answering. Unfortunately, at this time, they also convincingly\nhallucinate incorrect answers, so that responses to questions must be verified\nagainst external sources before they can be accepted at face value. In this\npaper, we report two simple experiments to automatically validate generated\nanswers against a corpus. We base our experiments on questions and passages\nfrom the MS MARCO (V1) test collection, and a retrieval pipeline consisting of\nsparse retrieval, dense retrieval and neural rerankers. In the first\nexperiment, we validate the generated answer in its entirety. After presenting\na question to an LLM and receiving a generated answer, we query the corpus with\nthe combination of the question + generated answer. We then present the LLM\nwith the combination of the question + generated answer + retrieved answer,\nprompting it to indicate if the generated answer can be supported by the\nretrieved answer. In the second experiment, we consider the generated answer at\na more granular level, prompting the LLM to extract a list of factual\nstatements from the answer and verifying each statement separately. We query\nthe corpus with each factual statement and then present the LLM with the\nstatement and the corresponding retrieved evidence. The LLM is prompted to\nindicate if the statement can be supported and make necessary edits using the\nretrieved material. With an accuracy of over 80%, we find that an LLM is\ncapable of verifying its generated answer when a corpus of supporting material\nis provided. However, manual assessment of a random sample of questions reveals\nthat incorrect generated answers are missed by this verification process. While\nthis verification process can reduce hallucinations, it can not entirely\neliminate them.",
        "translated": ""
    },
    {
        "title": "Long-tail Augmented Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2309.11177v1",
        "pub_date": "2023-09-20",
        "summary": "Graph Convolutional Networks (GCNs) has demonstrated promising results for\nrecommender systems, as they can effectively leverage high-order relationship.\nHowever, these methods usually encounter data sparsity issue in real-world\nscenarios. To address this issue, GCN-based recommendation methods employ\ncontrastive learning to introduce self-supervised signals. Despite their\neffectiveness, these methods lack consideration of the significant degree\ndisparity between head and tail nodes. This can lead to non-uniform\nrepresentation distribution, which is a crucial factor for the performance of\ncontrastive learning methods. To tackle the above issue, we propose a novel\nLong-tail Augmented Graph Contrastive Learning (LAGCL) method for\nrecommendation. Specifically, we introduce a learnable long-tail augmentation\napproach to enhance tail nodes by supplementing predicted neighbor information,\nand generate contrastive views based on the resulting augmented graph. To make\nthe data augmentation schema learnable, we design an auto drop module to\ngenerate pseudo-tail nodes from head nodes and a knowledge transfer module to\nreconstruct the head nodes from pseudo-tail nodes. Additionally, we employ\ngenerative adversarial networks to ensure that the distribution of the\ngenerated tail/head nodes matches that of the original tail/head nodes.\nExtensive experiments conducted on three benchmark datasets demonstrate the\nsignificant improvement in performance of our model over the state-of-the-arts.\nFurther analyses demonstrate the uniformity of learned representations and the\nsuperiority of LAGCL on long-tail performance. Code is publicly available at\nhttps://github.com/im0qianqian/LAGCL",
        "translated": ""
    },
    {
        "title": "Artificial Intelligence-Enabled Intelligent Assistant for Personalized\n  and Adaptive Learning in Higher Education",
        "url": "http://arxiv.org/abs/2309.10892v1",
        "pub_date": "2023-09-19",
        "summary": "This paper presents a novel framework, Artificial Intelligence-Enabled\nIntelligent Assistant (AIIA), for personalized and adaptive learning in higher\neducation. The AIIA system leverages advanced AI and Natural Language\nProcessing (NLP) techniques to create an interactive and engaging learning\nplatform. This platform is engineered to reduce cognitive load on learners by\nproviding easy access to information, facilitating knowledge assessment, and\ndelivering personalized learning support tailored to individual needs and\nlearning styles. The AIIA's capabilities include understanding and responding\nto student inquiries, generating quizzes and flashcards, and offering\npersonalized learning pathways. The research findings have the potential to\nsignificantly impact the design, implementation, and evaluation of AI-enabled\nVirtual Teaching Assistants (VTAs) in higher education, informing the\ndevelopment of innovative educational tools that can enhance student learning\noutcomes, engagement, and satisfaction. The paper presents the methodology,\nsystem architecture, intelligent services, and integration with Learning\nManagement Systems (LMSs) while discussing the challenges, limitations, and\nfuture directions for the development of AI-enabled intelligent assistants in\neducation.",
        "translated": ""
    },
    {
        "title": "Classifying Organizations for Food System Ontologies using Natural\n  Language Processing",
        "url": "http://arxiv.org/abs/2309.10880v1",
        "pub_date": "2023-09-19",
        "summary": "Our research explores the use of natural language processing (NLP) methods to\nautomatically classify entities for the purpose of knowledge graph population\nand integration with food system ontologies. We have created NLP models that\ncan automatically classify organizations with respect to categories associated\nwith environmental issues as well as Standard Industrial Classification (SIC)\ncodes, which are used by the U.S. government to characterize business\nactivities. As input, the NLP models are provided with text snippets retrieved\nby the Google search engine for each organization, which serves as a textual\ndescription of the organization that is used for learning. Our experimental\nresults show that NLP models can achieve reasonably good performance for these\ntwo classification tasks, and they rely on a general framework that could be\napplied to many other classification problems as well. We believe that NLP\nmodels represent a promising approach for automatically harvesting information\nto populate knowledge graphs and aligning the information with existing\nontologies through shared categories and concepts.",
        "translated": ""
    },
    {
        "title": "Improving VTE Identification through Adaptive NLP Model Selection and\n  Clinical Expert Rule-based Classifier from Radiology Reports",
        "url": "http://arxiv.org/abs/2309.12273v1",
        "pub_date": "2023-09-21",
        "summary": "Rapid and accurate identification of Venous thromboembolism (VTE), a severe\ncardiovascular condition including deep vein thrombosis (DVT) and pulmonary\nembolism (PE), is important for effective treatment. Leveraging Natural\nLanguage Processing (NLP) on radiology reports, automated methods have shown\npromising advancements in identifying VTE events from retrospective data\ncohorts or aiding clinical experts in identifying VTE events from radiology\nreports. However, effectively training Deep Learning (DL) and the NLP models is\nchallenging due to limited labeled medical text data, the complexity and\nheterogeneity of radiology reports, and data imbalance. This study proposes\nnovel method combinations of DL methods, along with data augmentation, adaptive\npre-trained NLP model selection, and a clinical expert NLP rule-based\nclassifier, to improve the accuracy of VTE identification in unstructured\n(free-text) radiology reports. Our experimental results demonstrate the model's\nefficacy, achieving an impressive 97\\% accuracy and 97\\% F1 score in predicting\nDVT, and an outstanding 98.3\\% accuracy and 98.4\\% F1 score in predicting PE.\nThese findings emphasize the model's robustness and its potential to\nsignificantly contribute to VTE research.",
        "translated": ""
    },
    {
        "title": "Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval",
        "url": "http://arxiv.org/abs/2309.12158v1",
        "pub_date": "2023-09-21",
        "summary": "A range of applications of multi-modal music information retrieval is centred\naround the problem of connecting large collections of sheet music (images) to\ncorresponding audio recordings, that is, identifying pairs of audio and score\nexcerpts that refer to the same musical content. One of the typical and most\nrecent approaches to this task employs cross-modal deep learning architectures\nto learn joint embedding spaces that link the two distinct modalities - audio\nand sheet music images. While there has been steady improvement on this front\nover the past years, a number of open problems still prevent large-scale\nemployment of this methodology. In this article we attempt to provide an\ninsightful examination of the current developments on audio-sheet music\nretrieval via deep learning methods. We first identify a set of main challenges\non the road towards robust and large-scale cross-modal music retrieval in real\nscenarios. We then highlight the steps we have taken so far to address some of\nthese challenges, documenting step-by-step improvement along several\ndimensions. We conclude by analysing the remaining challenges and present ideas\nfor solving these, in order to pave the way to a unified and robust methodology\nfor cross-modal music retrieval.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Contrastive Learning for Robust Audio-Sheet Music\n  Retrieval Systems",
        "url": "http://arxiv.org/abs/2309.12134v1",
        "pub_date": "2023-09-21",
        "summary": "Linking sheet music images to audio recordings remains a key problem for the\ndevelopment of efficient cross-modal music retrieval systems. One of the\nfundamental approaches toward this task is to learn a cross-modal embedding\nspace via deep neural networks that is able to connect short snippets of audio\nand sheet music. However, the scarcity of annotated data from real musical\ncontent affects the capability of such methods to generalize to real retrieval\nscenarios. In this work, we investigate whether we can mitigate this limitation\nwith self-supervised contrastive learning, by exposing a network to a large\namount of real music data as a pre-training step, by contrasting randomly\naugmented views of snippets of both modalities, namely audio and sheet images.\nThrough a number of experiments on synthetic and real piano data, we show that\npre-trained models are able to retrieve snippets with better precision in all\nscenarios and pre-training configurations. Encouraged by these results, we\nemploy the snippet embeddings in the higher-level task of cross-modal piece\nidentification and conduct more experiments on several retrieval\nconfigurations. In this task, we observe that the retrieval quality improves\nfrom 30% up to 100% when real music data is present. We then conclude by\narguing for the potential of self-supervised contrastive learning for\nalleviating the annotated data scarcity in multi-modal music retrieval models.",
        "translated": ""
    },
    {
        "title": "Passage Summarization with Recurrent Models for Audio-Sheet Music\n  Retrieval",
        "url": "http://arxiv.org/abs/2309.12111v1",
        "pub_date": "2023-09-21",
        "summary": "Many applications of cross-modal music retrieval are related to connecting\nsheet music images to audio recordings. A typical and recent approach to this\nis to learn, via deep neural networks, a joint embedding space that correlates\nshort fixed-size snippets of audio and sheet music by means of an appropriate\nsimilarity structure. However, two challenges that arise out of this strategy\nare the requirement of strongly aligned data to train the networks, and the\ninherent discrepancies of musical content between audio and sheet music\nsnippets caused by local and global tempo differences. In this paper, we\naddress these two shortcomings by designing a cross-modal recurrent network\nthat learns joint embeddings that can summarize longer passages of\ncorresponding audio and sheet music. The benefits of our method are that it\nonly requires weakly aligned audio-sheet music pairs, as well as that the\nrecurrent network handles the non-linearities caused by tempo variations\nbetween audio and sheet music. We conduct a number of experiments on synthetic\nand real piano data and scores, showing that our proposed recurrent method\nleads to more accurate retrieval in all possible configurations.",
        "translated": ""
    },
    {
        "title": "Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph\n  Pruning and Intent Graph for Effective Recommendations",
        "url": "http://arxiv.org/abs/2309.11741v1",
        "pub_date": "2023-09-21",
        "summary": "The recommendation of appropriate development pathways, also known as\necological civilization patterns for achieving Sustainable Development Goals\n(namely, sustainable development patterns), are of utmost importance for\npromoting ecological, economic, social, and resource sustainability in a\nspecific region. To achieve this, the recommendation process must carefully\nconsider the region's natural, environmental, resource, and economic\ncharacteristics. However, current recommendation algorithms in the field of\ncomputer science fall short in adequately addressing the spatial heterogeneity\nrelated to environment and sparsity of regional historical interaction data,\nwhich limits their effectiveness in recommending sustainable development\npatterns. To overcome these challenges, this paper proposes a method called\nUser Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the\nhigh-density linking capability of the pruned User Graph to address the issue\nof spatial heterogeneity neglect in recommendation algorithms. Secondly, we\nconstruct an Intent Graph by incorporating the intent network, which captures\nthe preferences for attributes including environmental elements of target\nregions. This approach effectively alleviates the problem of sparse historical\ninteraction data in the region. Through extensive experiments, we demonstrate\nthat UGPIG outperforms state-of-the-art recommendation algorithms like KGCN,\nKGAT, and KGIN in sustainable development pattern recommendations, with a\nmaximum improvement of 9.61% in Top-3 recommendation performance.",
        "translated": ""
    },
    {
        "title": "Candidate Set Sampling for Evaluating Top-N Recommendation",
        "url": "http://arxiv.org/abs/2309.11723v1",
        "pub_date": "2023-09-21",
        "summary": "The strategy for selecting candidate sets -- the set of items that the\nrecommendation system is expected to rank for each user -- is an important\ndecision in carrying out an offline top-$N$ recommender system evaluation. The\nset of candidates is composed of the union of the user's test items and an\narbitrary number of non-relevant items that we refer to as decoys. Previous\nstudies have aimed to understand the effect of different candidate set sizes\nand selection strategies on evaluation. In this paper, we extend this knowledge\nby studying the specific interaction of candidate set selection strategies with\npopularity bias, and use simulation to assess whether sampled candidate sets\nresult in metric estimates that are less biased with respect to the true metric\nvalues under complete data that is typically unavailable in ordinary\nexperiments.",
        "translated": ""
    },
    {
        "title": "SE-PEF: a Resource for Personalized Expert Finding",
        "url": "http://arxiv.org/abs/2309.11686v1",
        "pub_date": "2023-09-20",
        "summary": "The problem of personalization in Information Retrieval has been under study\nfor a long time. A well-known issue related to this task is the lack of\npublicly available datasets that can support a comparative evaluation of\npersonalized search systems. To contribute in this respect, this paper\nintroduces SE-PEF (StackExchange - Personalized Expert Finding), a resource\nuseful for designing and evaluating personalized models related to the task of\nExpert Finding (EF). The contributed dataset includes more than 250k queries\nand 565k answers from 3 306 experts, which are annotated with a rich set of\nfeatures modeling the social interactions among the users of a popular cQA\nplatform. The results of the preliminary experiments conducted show the\nappropriateness of SE-PEF to evaluate and to train effective EF models.",
        "translated": ""
    },
    {
        "title": "Popularity Degradation Bias in Local Music Recommendation",
        "url": "http://arxiv.org/abs/2309.11671v1",
        "pub_date": "2023-09-20",
        "summary": "In this paper, we study the effect of popularity degradation bias in the\ncontext of local music recommendations. Specifically, we examine how accurate\ntwo top-performing recommendation algorithms, Weight Relevance Matrix\nFactorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at\nrecommending artists as a function of artist popularity. We find that both\nalgorithms improve recommendation performance for more popular artists and, as\nsuch, exhibit popularity degradation bias. While both algorithms produce a\nsimilar level of performance for more popular artists, Mult-VAE shows better\nrelative performance for less popular artists. This suggests that this\nalgorithm should be preferred for local (long-tail) music artist\nrecommendation.",
        "translated": ""
    },
    {
        "title": "Leveraging Negative Signals with Self-Attention for Sequential Music\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.11623v1",
        "pub_date": "2023-09-20",
        "summary": "Music streaming services heavily rely on their recommendation engines to\ncontinuously provide content to their consumers. Sequential recommendation\nconsequently has seen considerable attention in current literature, where state\nof the art approaches focus on self-attentive models leveraging contextual\ninformation such as long and short-term user history and item features;\nhowever, most of these studies focus on long-form content domains (retail,\nmovie, etc.) rather than short-form, such as music. Additionally, many do not\nexplore incorporating negative session-level feedback during training. In this\nstudy, we investigate the use of transformer-based self-attentive architectures\nto learn implicit session-level information for sequential music\nrecommendation. We additionally propose a contrastive learning task to\nincorporate negative feedback (e.g skipped tracks) to promote positive hits and\npenalize negative hits. This task is formulated as a simple loss term that can\nbe incorporated into a variety of deep learning architectures for sequential\nrecommendation. Our experiments show that this results in consistent\nperformance gains over the baseline architectures ignoring negative user\nfeedback.",
        "translated": ""
    },
    {
        "title": "Diffusion Augmentation for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.12858v1",
        "pub_date": "2023-09-22",
        "summary": "Sequential recommendation (SRS) has become the technical foundation in many\napplications recently, which aims to recommend the next item based on the\nuser's historical interactions. However, sequential recommendation often faces\nthe problem of data sparsity, which widely exists in recommender systems.\nBesides, most users only interact with a few items, but existing SRS models\noften underperform these users. Such a problem, named the long-tail user\nproblem, is still to be resolved. Data augmentation is a distinct way to\nalleviate these two problems, but they often need fabricated training\nstrategies or are hindered by poor-quality generated interactions. To address\nthese problems, we propose a Diffusion Augmentation for Sequential\nRecommendation (DiffuASR) for a higher quality generation. The augmented\ndataset by DiffuASR can be used to train the sequential recommendation models\ndirectly, free from complex training procedures. To make the best of the\ngeneration ability of the diffusion model, we first propose a diffusion-based\npseudo sequence generation framework to fill the gap between image and sequence\ngeneration. Then, a sequential U-Net is designed to adapt the diffusion noise\nprediction model U-Net to the discrete sequence generation task. At last, we\ndevelop two guide strategies to assimilate the preference between generated and\norigin sequences. To validate the proposed DiffuASR, we conduct extensive\nexperiments on three real-world datasets with three sequential recommendation\nmodels. The experimental results illustrate the effectiveness of DiffuASR. As\nfar as we know, DiffuASR is one pioneer that introduce the diffusion model to\nthe recommendation.",
        "translated": ""
    },
    {
        "title": "Enhancing Graph Collaborative Filtering via Uniformly Co-Clustered\n  Intent Modeling",
        "url": "http://arxiv.org/abs/2309.12723v1",
        "pub_date": "2023-09-22",
        "summary": "Graph-based collaborative filtering has emerged as a powerful paradigm for\ndelivering personalized recommendations. Despite their demonstrated\neffectiveness, these methods often neglect the underlying intents of users,\nwhich constitute a pivotal facet of comprehensive user interests. Consequently,\na series of approaches have arisen to tackle this limitation by introducing\nindependent intent representations. However, these approaches fail to capture\nthe intricate relationships between intents of different users and the\ncompatibility between user intents and item properties.\n  To remedy the above issues, we propose a novel method, named uniformly\nco-clustered intent modeling. Specifically, we devise a uniformly contrastive\nintent modeling module to bring together the embeddings of users with similar\nintents and items with similar properties. This module aims to model the\nnuanced relations between intents of different users and properties of\ndifferent items, especially those unreachable to each other on the user-item\ngraph. To model the compatibility between user intents and item properties, we\ndesign the user-item co-clustering module, maximizing the mutual information of\nco-clusters of users and items. This approach is substantiated through\ntheoretical validation, establishing its efficacy in modeling compatibility to\nenhance the mutual information between user and item representations.\nComprehensive experiments on various real-world datasets verify the\neffectiveness of the proposed framework.",
        "translated": ""
    },
    {
        "title": "KuaiSim: A Comprehensive Simulator for Recommender Systems",
        "url": "http://arxiv.org/abs/2309.12645v1",
        "pub_date": "2023-09-22",
        "summary": "Reinforcement Learning (RL)-based recommender systems (RSs) have garnered\nconsiderable attention due to their ability to learn optimal recommendation\npolicies and maximize long-term user rewards. However, deploying RL models\ndirectly in online environments and generating authentic data through A/B tests\ncan pose challenges and require substantial resources. Simulators offer an\nalternative approach by providing training and evaluation environments for RS\nmodels, reducing reliance on real-world data. Existing simulators have shown\npromising results but also have limitations such as simplified user feedback,\nlacking consistency with real-world data, the challenge of simulator\nevaluation, and difficulties in migration and expansion across RSs. To address\nthese challenges, we propose KuaiSim, a comprehensive user environment that\nprovides user feedback with multi-behavior and cross-session responses. The\nresulting simulator can support three levels of recommendation problems: the\nrequest level list-wise recommendation task, the whole-session level sequential\nrecommendation task, and the cross-session level retention optimization task.\nFor each task, KuaiSim also provides evaluation protocols and baseline\nrecommendation algorithms that further serve as benchmarks for future research.\nWe also restructure existing competitive simulators on the KuaiRand Dataset and\ncompare them against KuaiSim to future assess their performance and behavioral\ndifferences. Furthermore, to showcase KuaiSim's flexibility in accommodating\ndifferent datasets, we demonstrate its versatility and robustness when\ndeploying it on the ML-1m dataset.",
        "translated": ""
    },
    {
        "title": "Modeling Spatiotemporal Periodicity and Collaborative Signal for\n  Local-Life Service Recommendation",
        "url": "http://arxiv.org/abs/2309.12565v1",
        "pub_date": "2023-09-22",
        "summary": "Online local-life service platforms provide services like nearby daily\nessentials and food delivery for hundreds of millions of users. Different from\nother types of recommender systems, local-life service recommendation has the\nfollowing characteristics: (1) spatiotemporal periodicity, which means a user's\npreferences for items vary from different locations at different times. (2)\nspatiotemporal collaborative signal, which indicates similar users have similar\npreferences at specific locations and times. However, most existing methods\neither focus on merely the spatiotemporal contexts in sequences, or model the\nuser-item interactions without spatiotemporal contexts in graphs. To address\nthis issue, we design a new method named SPCS in this paper. Specifically, we\npropose a novel spatiotemporal graph transformer (SGT) layer, which explicitly\nencodes relative spatiotemporal contexts, and aggregates the information from\nmulti-hop neighbors to unify spatiotemporal periodicity and collaborative\nsignal. With extensive experiments on both public and industrial datasets, this\npaper validates the state-of-the-art performance of SPCS.",
        "translated": ""
    },
    {
        "title": "Cluster Language Model for Improved E-Commerce Retrieval and Ranking:\n  Leveraging Query Similarity and Fine-Tuning for Personalized Results",
        "url": "http://arxiv.org/abs/2309.14323v1",
        "pub_date": "2023-09-25",
        "summary": "This paper proposes a novel method to improve the accuracy of product search\nin e-commerce by utilizing a cluster language model. The method aims to address\nthe limitations of the bi-encoder architecture while maintaining a minimal\nadditional training burden. The approach involves labeling top products for\neach query, generating semantically similar query clusters using the K-Means\nclustering algorithm, and fine-tuning a global language model into cluster\nlanguage models on individual clusters. The parameters of each cluster language\nmodel are fine-tuned to learn local manifolds in the feature space efficiently,\ncapturing the nuances of various query types within each cluster. The inference\nis performed by assigning a new query to its respective cluster and utilizing\nthe corresponding cluster language model for retrieval. The proposed method\nresults in more accurate and personalized retrieval results, offering a\nsuperior alternative to the popular bi-encoder based retrieval models in\nsemantic search.",
        "translated": ""
    },
    {
        "title": "Framework based on complex networks to model and mine patient pathways",
        "url": "http://arxiv.org/abs/2309.14208v1",
        "pub_date": "2023-09-25",
        "summary": "The automatic discovery of a model to represent the history of encounters of\na group of patients with the healthcare system -- the so-called ``pathway of\npatients'' -- is a new field of research that supports clinical and\norganisational decisions to improve the quality and efficiency of the treatment\nprovided. The pathways of patients with chronic conditions tend to vary\nsignificantly from one person to another, have repetitive tasks, and demand the\nanalysis of multiple perspectives (interventions, diagnoses, medical\nspecialities, among others) influencing the results. Therefore, modelling and\nmining those pathways is still a challenging task. In this work, we propose a\nframework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a\nnovel dissimilarity measurement to compare pathways taking the elapsed time\ninto account, and (iii) a mining method based on traditional centrality\nmeasures to discover the most relevant steps of the pathways. We evaluated the\nframework using the study cases of pregnancy and diabetes, which revealed its\nusefulness in finding clusters of similar pathways, representing them in an\neasy-to-interpret way, and highlighting the most significant patterns according\nto multiple perspectives.",
        "translated": ""
    },
    {
        "title": "Comprehensive Overview of Named Entity Recognition: Models,\n  Domain-Specific Applications and Challenges",
        "url": "http://arxiv.org/abs/2309.14084v1",
        "pub_date": "2023-09-25",
        "summary": "In the domain of Natural Language Processing (NLP), Named Entity Recognition\n(NER) stands out as a pivotal mechanism for extracting structured insights from\nunstructured text. This manuscript offers an exhaustive exploration into the\nevolving landscape of NER methodologies, blending foundational principles with\ncontemporary AI advancements. Beginning with the rudimentary concepts of NER,\nthe study spans a spectrum of techniques from traditional rule-based strategies\nto the contemporary marvels of transformer architectures, particularly\nhighlighting integrations such as BERT with LSTM and CNN. The narrative\naccentuates domain-specific NER models, tailored for intricate areas like\nfinance, legal, and healthcare, emphasizing their specialized adaptability.\nAdditionally, the research delves into cutting-edge paradigms including\nreinforcement learning, innovative constructs like E-NER, and the interplay of\nOptical Character Recognition (OCR) in augmenting NER capabilities. Grounding\nits insights in practical realms, the paper sheds light on the indispensable\nrole of NER in sectors like finance and biomedicine, addressing the unique\nchallenges they present. The conclusion outlines open challenges and avenues,\nmarking this work as a comprehensive guide for those delving into NER research\nand applications.",
        "translated": ""
    },
    {
        "title": "Diversify and Conquer: Bandits and Diversity for an Enhanced E-commerce\n  Homepage Experience",
        "url": "http://arxiv.org/abs/2309.14046v1",
        "pub_date": "2023-09-25",
        "summary": "In the realm of e-commerce, popular platforms utilize widgets to recommend\nadvertisements and products to their users. However, the prevalence of mobile\ndevice usage on these platforms introduces a unique challenge due to the\nlimited screen real estate available. Consequently, the positioning of relevant\nwidgets becomes pivotal in capturing and maintaining customer engagement. Given\nthe restricted screen size of mobile devices, widgets placed at the top of the\ninterface are more prominently displayed and thus attract greater user\nattention. Conversely, widgets positioned further down the page require users\nto scroll, resulting in reduced visibility and subsequent lower impression\nrates. Therefore it becomes imperative to place relevant widgets on top.\nHowever, selecting relevant widgets to display is a challenging task as the\nwidgets can be heterogeneous, widgets can be introduced or removed at any given\ntime from the platform. In this work, we model the vertical widget reordering\nas a contextual multi-arm bandit problem with delayed batch feedback. The\nobjective is to rank the vertical widgets in a personalized manner. We present\na two-stage ranking framework that combines contextual bandits with a diversity\nlayer to improve the overall ranking. We demonstrate its effectiveness through\noffline and online A/B results, conducted on proprietary data from Myntra, a\nmajor fashion e-commerce platform in India.",
        "translated": ""
    },
    {
        "title": "Multiple Relations Classification using Imbalanced Predictions\n  Adaptation",
        "url": "http://arxiv.org/abs/2309.13718v1",
        "pub_date": "2023-09-24",
        "summary": "The relation classification task assigns the proper semantic relation to a\npair of subject and object entities; the task plays a crucial role in various\ntext mining applications, such as knowledge graph construction and entities\ninteraction discovery in biomedical text. Current relation classification\nmodels employ additional procedures to identify multiple relations in a single\nsentence. Furthermore, they overlook the imbalanced predictions pattern. The\npattern arises from the presence of a few valid relations that need positive\nlabeling in a relatively large predefined relations set. We propose a multiple\nrelations classification model that tackles these issues through a customized\noutput architecture and by exploiting additional input features. Our findings\nsuggest that handling the imbalanced predictions leads to significant\nimprovements, even on a modest training design. The results demonstrate\nsuperiority performance on benchmark datasets commonly used in relation\nclassification. To the best of our knowledge, this work is the first that\nrecognizes the imbalanced predictions within the relation classification task.",
        "translated": ""
    },
    {
        "title": "Sparsity-regularized coded ptychography for robust and efficient\n  lensless microscopy on a chip",
        "url": "http://arxiv.org/abs/2309.13611v1",
        "pub_date": "2023-09-24",
        "summary": "In ptychographic imaging, the trade-off between the number of acquisitions\nand the resultant imaging quality presents a complex optimization problem.\nIncreasing the number of acquisitions typically yields reconstructions with\nhigher spatial resolution and finer details. Conversely, a reduction in\nmeasurement frequency often compromises the quality of the reconstructed\nimages, manifesting as increased noise and coarser details. To address this\nchallenge, we employ sparsity priors to reformulate the ptychographic\nreconstruction task as a total variation regularized optimization problem. We\nintroduce a new computational framework, termed the ptychographic proximal\ntotal-variation (PPTV) solver, designed to integrate into existing ptychography\nsettings without necessitating hardware modifications. Through comprehensive\nnumerical simulations, we validate that PPTV-driven coded ptychography is\ncapable of producing highly accurate reconstructions with a minimal set of\neight intensity measurements. Convergence analysis further substantiates the\nrobustness, stability, and computational feasibility of the proposed PPTV\nalgorithm. Experimental results obtained from optical setups unequivocally\ndemonstrate that the PPTV algorithm facilitates high-throughput,\nhigh-resolution imaging while significantly reducing the measurement burden.\nThese findings indicate that the PPTV algorithm has the potential to\nsubstantially mitigate the resource-intensive requirements traditionally\nassociated with high-quality ptychographic imaging, thereby offering a pathway\ntoward the development of more compact and efficient ptychographic microscopy\nsystems.",
        "translated": ""
    },
    {
        "title": "Related Rhythms: Recommendation System To Discover Music You May Like",
        "url": "http://arxiv.org/abs/2309.13544v1",
        "pub_date": "2023-09-24",
        "summary": "Machine Learning models are being utilized extensively to drive recommender\nsystems, which is a widely explored topic today. This is especially true of the\nmusic industry, where we are witnessing a surge in growth. Besides a large\nchunk of active users, these systems are fueled by massive amounts of data.\nThese large-scale systems yield applications that aim to provide a better user\nexperience and to keep customers actively engaged. In this paper, a distributed\nMachine Learning (ML) pipeline is delineated, which is capable of taking a\nsubset of songs as input and producing a new subset of songs identified as\nbeing similar to the inputted subset. The publicly accessible Million Songs\nDataset (MSD) enables researchers to develop and explore reasonably efficient\nsystems for audio track analysis and recommendations, without having to access\na commercialized music platform. The objective of the proposed application is\nto leverage an ML system trained to optimally recommend songs that a user might\nlike.",
        "translated": ""
    },
    {
        "title": "On the Sweet Spot of Contrastive Views for Knowledge-enhanced\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.13384v1",
        "pub_date": "2023-09-23",
        "summary": "In recommender systems, knowledge graph (KG) can offer critical information\nthat is lacking in the original user-item interaction graph (IG). Recent\nprocess has explored this direction and shows that contrastive learning is a\npromising way to integrate both. However, we observe that existing KG-enhanced\nrecommenders struggle in balancing between the two contrastive views of IG and\nKG, making them sometimes even less effective than simply applying contrastive\nlearning on IG without using KG. In this paper, we propose a new contrastive\nlearning framework for KG-enhanced recommendation. Specifically, to make full\nuse of the knowledge, we construct two separate contrastive views for KG and\nIG, and maximize their mutual information; to ease the contrastive learning on\nthe two views, we further fuse KG information into IG in a one-direction\nmanner.Extensive experimental results on three real-world datasets demonstrate\nthe effectiveness and efficiency of our method, compared to the\nstate-of-the-art. Our code is available through the anonymous\nlink:https://figshare.com/articles/conference_contribution/SimKGCL/22783382",
        "translated": ""
    },
    {
        "title": "Generative Retrieval with Semantic Tree-Structured Item Identifiers via\n  Contrastive Learning",
        "url": "http://arxiv.org/abs/2309.13375v1",
        "pub_date": "2023-09-23",
        "summary": "The retrieval phase is a vital component in recommendation systems, requiring\nthe model to be effective and efficient. Recently, generative retrieval has\nbecome an emerging paradigm for document retrieval, showing notable\nperformance. These methods enjoy merits like being end-to-end differentiable,\nsuggesting their viability in recommendation. However, these methods fall short\nin efficiency and effectiveness for large-scale recommendations. To obtain\nefficiency and effectiveness, this paper introduces a generative retrieval\nframework, namely SEATER, which learns SEmAntic Tree-structured item\nidentifiERs via contrastive learning. Specifically, we employ an\nencoder-decoder model to extract user interests from historical behaviors and\nretrieve candidates via tree-structured item identifiers. SEATER devises a\nbalanced k-ary tree structure of item identifiers, allocating semantic space to\neach token individually. This strategy maintains semantic consistency within\nthe same level, while distinct levels correlate to varying semantic\ngranularities. This structure also maintains consistent and fast inference\nspeed for all items. Considering the tree structure, SEATER learns identifier\ntokens' semantics, hierarchical relationships, and inter-token dependencies. To\nachieve this, we incorporate two contrastive learning tasks with the generation\ntask to optimize both the model and identifiers. The infoNCE loss aligns the\ntoken embeddings based on their hierarchical positions. The triplet loss ranks\nsimilar identifiers in desired orders. In this way, SEATER achieves both\nefficiency and effectiveness. Extensive experiments on three public datasets\nand an industrial dataset have demonstrated that SEATER outperforms\nstate-of-the-art models significantly.",
        "translated": ""
    },
    {
        "title": "Model-enhanced Vector Index",
        "url": "http://arxiv.org/abs/2309.13335v1",
        "pub_date": "2023-09-23",
        "summary": "Embedding-based retrieval methods construct vector indices to search for\ndocument representations that are most similar to the query representations.\nThey are widely used in document retrieval due to low latency and decent recall\nperformance. Recent research indicates that deep retrieval solutions offer\nbetter model quality, but are hindered by unacceptable serving latency and the\ninability to support document updates. In this paper, we aim to enhance the\nvector index with end-to-end deep generative models, leveraging the\ndifferentiable advantages of deep retrieval models while maintaining desirable\nserving efficiency. We propose Model-enhanced Vector Index (MEVI), a\ndifferentiable model-enhanced index empowered by a twin-tower representation\nmodel. MEVI leverages a Residual Quantization (RQ) codebook to bridge the\nsequence-to-sequence deep retrieval and embedding-based models. To\nsubstantially reduce the inference time, instead of decoding the unique\ndocument ids in long sequential steps, we first generate some semantic virtual\ncluster ids of candidate documents in a small number of steps, and then\nleverage the well-adapted embedding vectors to further perform a fine-grained\nsearch for the relevant documents in the candidate virtual clusters. We\nempirically show that our model achieves better performance on the commonly\nused academic benchmarks MSMARCO Passage and Natural Questions, with comparable\nserving latency to dense retrieval solutions.",
        "translated": ""
    },
    {
        "title": "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large\n  Language Models",
        "url": "http://arxiv.org/abs/2309.15088v1",
        "pub_date": "2023-09-26",
        "summary": "Researchers have successfully applied large language models (LLMs) such as\nChatGPT to reranking in an information retrieval context, but to date, such\nwork has mostly been built on proprietary models hidden behind opaque API\nendpoints. This approach yields experimental results that are not reproducible\nand non-deterministic, threatening the veracity of outcomes that build on such\nshaky foundations. To address this significant shortcoming, we present\nRankVicuna, the first fully open-source LLM capable of performing high-quality\nlistwise reranking in a zero-shot setting. Experimental results on the TREC\n2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness\ncomparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter\nmodel, although our effectiveness remains slightly behind reranking with GPT-4.\nWe hope our work provides the foundation for future research on reranking with\nmodern LLMs. All the code necessary to reproduce our results is available at\nhttps://github.com/castorini/rank_llm.",
        "translated": ""
    },
    {
        "title": "The Role of Document Embedding in Research Paper Recommender Systems: To\n  Breakdown or to Bolster Disciplinary Borders?",
        "url": "http://arxiv.org/abs/2309.14984v1",
        "pub_date": "2023-09-26",
        "summary": "In the extensive recommender systems literature, novelty and diversity have\nbeen identified as key properties of useful recommendations. However, these\nproperties have received limited attention in the specific sub-field of\nresearch paper recommender systems. In this work, we argue for the importance\nof offering novel and diverse research paper recommendations to scientists.\nThis approach aims to reduce siloed reading, break down filter bubbles, and\npromote interdisciplinary research. We propose a novel framework for evaluating\nthe novelty and diversity of research paper recommendations that leverages\nmethods from network analysis and natural language processing. Using this\nframework, we show that the choice of representational method within a larger\nresearch paper recommendation system can have a measurable impact on the nature\nof downstream recommendations, specifically on their novelty and diversity. We\nintroduce a novel paper embedding method, which we demonstrate offers more\ninnovative and diverse recommendations without sacrificing precision, compared\nto other state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Modeling Multi-aspect Preferences and Intents for Multi-behavioral\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.14938v1",
        "pub_date": "2023-09-26",
        "summary": "Multi-behavioral sequential recommendation has recently attracted increasing\nattention. However, existing methods suffer from two major limitations.\nFirstly, user preferences and intents can be described in fine-grained detail\nfrom multiple perspectives; yet, these methods fail to capture their\nmulti-aspect nature. Secondly, user behaviors may contain noises, and most\nexisting methods could not effectively deal with noises. In this paper, we\npresent an attentive recurrent model with multiple projections to capture\nMulti-Aspect preferences and INTents (MAINT in short). To extract multi-aspect\npreferences from target behaviors, we propose a multi-aspect projection\nmechanism for generating multiple preference representations from multiple\naspects. To extract multi-aspect intents from multi-typed behaviors, we propose\na behavior-enhanced LSTM and a multi-aspect refinement attention mechanism. The\nattention mechanism can filter out noises and generate multiple intent\nrepresentations from different aspects. To adaptively fuse user preferences and\nintents, we propose a multi-aspect gated fusion mechanism. Extensive\nexperiments conducted on real-world datasets have demonstrated the\neffectiveness of our model.",
        "translated": ""
    },
    {
        "title": "REFORM: Removing False Correlation in Multi-level Interaction for CTR\n  Prediction",
        "url": "http://arxiv.org/abs/2309.14891v1",
        "pub_date": "2023-09-26",
        "summary": "Click-through rate (CTR) prediction is a critical task in online advertising\nand recommendation systems, as accurate predictions are essential for user\ntargeting and personalized recommendations. Most recent cutting-edge methods\nprimarily focus on investigating complex implicit and explicit feature\ninteractions. However, these methods neglect the issue of false correlations\ncaused by confounding factors or selection bias. This problem is further\nmagnified by the complexity and redundancy of these interactions. We propose a\nCTR prediction framework that removes false correlation in multi-level feature\ninteraction, termed REFORM. The proposed REFORM framework exploits a wide range\nof multi-level high-order feature representations via a two-stream stacked\nrecurrent structure while eliminating false correlations. The framework has two\nkey components: I. The multi-level stacked recurrent (MSR) structure enables\nthe model to efficiently capture diverse nonlinear interactions from feature\nspaces of different levels, and the richer representations lead to enhanced CTR\nprediction accuracy. II. The false correlation elimination (FCE) module further\nleverages Laplacian kernel mapping and sample reweighting methods to eliminate\nfalse correlations concealed within the multi-level features, allowing the\nmodel to focus on the true causal effects. Extensive experiments based on four\nchallenging CTR datasets and our production dataset demonstrate that the\nproposed REFORM model achieves state-of-the-art performance. Codes, models and\nour dataset will be released at https://github.com/yansuoyuli/REFORM.",
        "translated": ""
    },
    {
        "title": "ALEX: Towards Effective Graph Transfer Learning with Noisy Labels",
        "url": "http://arxiv.org/abs/2309.14673v1",
        "pub_date": "2023-09-26",
        "summary": "Graph Neural Networks (GNNs) have garnered considerable interest due to their\nexceptional performance in a wide range of graph machine learning tasks.\nNevertheless, the majority of GNN-based approaches have been examined using\nwell-annotated benchmark datasets, leading to suboptimal performance in\nreal-world graph learning scenarios. To bridge this gap, the present paper\ninvestigates the problem of graph transfer learning in the presence of label\nnoise, which transfers knowledge from a noisy source graph to an unlabeled\ntarget graph. We introduce a novel technique termed Balance Alignment and\nInformation-aware Examination (ALEX) to address this challenge. ALEX first\nemploys singular value decomposition to generate different views with crucial\nstructural semantics, which help provide robust node representations using\ngraph contrastive learning. To mitigate both label shift and domain shift, we\nestimate a prior distribution to build subgraphs with balanced label\ndistributions. Building on this foundation, an adversarial domain discriminator\nis incorporated for the implicit domain alignment of complex multi-modal\ndistributions. Furthermore, we project node representations into a different\nspace, optimizing the mutual information between the projected features and\nlabels. Subsequently, the inconsistency of similarity structures is evaluated\nto identify noisy samples with potential overfitting. Comprehensive experiments\non various benchmark datasets substantiate the outstanding superiority of the\nproposed ALEX in different settings.",
        "translated": ""
    },
    {
        "title": "Tranformer-based classification of user queries for medical consultancy\n  with respect to expert specialisation",
        "url": "http://arxiv.org/abs/2309.14662v1",
        "pub_date": "2023-09-26",
        "summary": "The need for skilled medical support is growing in the era of digital\nhealthcare. This research presents an innovative strategy, utilising the RuBERT\nmodel, for categorising user inquiries in the field of medical consultation\nwith a focus on expert specialisation. By harnessing the capabilities of\ntransformers, we fine-tuned the pre-trained RuBERT model on a varied dataset,\nwhich facilitates precise correspondence between queries and particular medical\nspecialisms. Using a comprehensive dataset, we have demonstrated our approach's\nsuperior performance with an F1-score of over 92%, calculated through both\ncross-validation and the traditional split of test and train datasets. Our\napproach has shown excellent generalisation across medical domains such as\ncardiology, neurology and dermatology. This methodology provides practical\nbenefits by directing users to appropriate specialists for prompt and targeted\nmedical advice. It also enhances healthcare system efficiency, reduces\npractitioner burden, and improves patient care quality. In summary, our\nsuggested strategy facilitates the attainment of specific medical knowledge,\noffering prompt and precise advice within the digital healthcare field.",
        "translated": ""
    },
    {
        "title": "Algorithmic Collusion or Competition: the Role of Platforms' Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2309.14548v1",
        "pub_date": "2023-09-25",
        "summary": "Recent academic research has extensively examined algorithmic collusion\nresulting from the utilization of artificial intelligence (AI)-based dynamic\npricing algorithms. Nevertheless, e-commerce platforms employ recommendation\nalgorithms to allocate exposure to various products, and this important aspect\nhas been largely overlooked in previous studies on algorithmic collusion. Our\nstudy bridges this important gap in the literature and examines how\nrecommendation algorithms can determine the competitive or collusive dynamics\nof AI-based pricing algorithms. Specifically, two commonly deployed\nrecommendation algorithms are examined: (i) a recommender system that aims to\nmaximize the sellers' total profit (profit-based recommender system) and (ii) a\nrecommender system that aims to maximize the demand for products sold on the\nplatform (demand-based recommender system). We construct a repeated game\nframework that incorporates both pricing algorithms adopted by sellers and the\nplatform's recommender system. Subsequently, we conduct experiments to observe\nprice dynamics and ascertain the final equilibrium. Experimental results reveal\nthat a profit-based recommender system intensifies algorithmic collusion among\nsellers due to its congruence with sellers' profit-maximizing objectives.\nConversely, a demand-based recommender system fosters price competition among\nsellers and results in a lower price, owing to its misalignment with sellers'\ngoals. Extended analyses suggest the robustness of our findings in various\nmarket scenarios. Overall, we highlight the importance of platforms'\nrecommender systems in delineating the competitive structure of the digital\nmarketplace, providing important insights for market participants and\ncorresponding policymakers.",
        "translated": ""
    },
    {
        "title": "Temporal graph models fail to capture global temporal dynamics",
        "url": "http://arxiv.org/abs/2309.15730v1",
        "pub_date": "2023-09-27",
        "summary": "A recently released Temporal Graph Benchmark is analyzed in the context of\nDynamic Link Property Prediction. We outline our observations and propose a\ntrivial optimization-free baseline of \"recently popular nodes\" outperforming\nother methods on all medium and large-size datasets in the Temporal Graph\nBenchmark. We propose two measures based on Wasserstein distance which can\nquantify the strength of short-term and long-term global dynamics of datasets.\nBy analyzing our unexpectedly strong baseline, we show how standard negative\nsampling evaluation can be unsuitable for datasets with strong temporal\ndynamics. We also show how simple negative-sampling can lead to model\ndegeneration during training, resulting in impossible to rank, fully saturated\npredictions of temporal graph networks. We propose improved negative sampling\nschemes for both training and evaluation and prove their usefulness. We conduct\na comparison with a model trained non-contrastively without negative sampling.\nOur results provide a challenging baseline and indicate that temporal graph\nnetwork architectures need deep rethinking for usage in problems with\nsignificant global dynamics, such as social media, cryptocurrency markets or\ne-commerce. We open-source the code for baselines, measures and proposed\nnegative sampling schemes.",
        "translated": ""
    },
    {
        "title": "Cold &amp; Warm Net: Addressing Cold-Start Users in Recommender Systems",
        "url": "http://arxiv.org/abs/2309.15646v1",
        "pub_date": "2023-09-27",
        "summary": "Cold-start recommendation is one of the major challenges faced by recommender\nsystems (RS). Herein, we focus on the user cold-start problem. Recently,\nmethods utilizing side information or meta-learning have been used to model\ncold-start users. However, it is difficult to deploy these methods to\nindustrial RS. There has not been much research that pays attention to the user\ncold-start problem in the matching stage. In this paper, we propose Cold &amp; Warm\nNet based on expert models who are responsible for modeling cold-start and\nwarm-up users respectively. A gate network is applied to incorporate the\nresults from two experts. Furthermore, dynamic knowledge distillation acting as\na teacher selector is introduced to assist experts in better learning user\nrepresentation. With comprehensive mutual information, features highly relevant\nto user behavior are selected for the bias net which explicitly models user\nbehavior bias. Finally, we evaluate our Cold &amp; Warm Net on public datasets in\ncomparison to models commonly applied in the matching stage and it outperforms\nother models on all user types. The proposed model has also been deployed on an\nindustrial short video platform and achieves a significant increase in app\ndwell time and user retention rate.",
        "translated": ""
    },
    {
        "title": "Identifiability Matters: Revealing the Hidden Recoverable Condition in\n  Unbiased Learning to Rank",
        "url": "http://arxiv.org/abs/2309.15560v1",
        "pub_date": "2023-09-27",
        "summary": "The application of Unbiased Learning to Rank (ULTR) is widespread in modern\nsystems for training unbiased ranking models from biased click logs. The key is\nto explicitly model a generation process for user behavior and fit click data\nbased on examination hypothesis. Previous research found empirically that the\ntrue latent relevance can be recovered in most cases as long as the clicks are\nperfectly fitted. However, we demonstrate that this is not always achievable,\nresulting in a significant reduction in ranking performance. In this work, we\naim to answer if or when the true relevance can be recovered from click data,\nwhich is a foundation issue for ULTR field. We first define a ranking model as\nidentifiable if it can recover the true relevance up to a scaling\ntransformation, which is enough for pairwise ranking objective. Then we explore\nan equivalent condition for identifiability that can be novely expressed as a\ngraph connectivity test problem: if and only if a graph (namely identifiability\ngraph, or IG) constructed on the underlying structure of the dataset is\nconnected, we can guarantee that the relevance can be correctly recovered. When\nthe IG is not connected, there may be bad cases leading to poor ranking\nperformance. To address this issue, we propose two methods, namely node\nintervention and node merging, to modify the dataset and restore connectivity\nof the IG. Empirical results obtained on a simulation dataset and two LTR\nbenchmark datasets confirm the validity of our proposed theorems and show the\neffectiveness of our methods in mitigating data bias when the relevance model\nis unidentifiable.",
        "translated": ""
    },
    {
        "title": "Automatic Feature Fairness in Recommendation via Adversaries",
        "url": "http://arxiv.org/abs/2309.15418v1",
        "pub_date": "2023-09-27",
        "summary": "Fairness is a widely discussed topic in recommender systems, but its\npractical implementation faces challenges in defining sensitive features while\nmaintaining recommendation accuracy. We propose feature fairness as the\nfoundation to achieve equitable treatment across diverse groups defined by\nvarious feature combinations. This improves overall accuracy through balanced\nfeature generalizability. We introduce unbiased feature learning through\nadversarial training, using adversarial perturbation to enhance feature\nrepresentation. The adversaries improve model generalization for\nunder-represented features. We adapt adversaries automatically based on two\nforms of feature biases: frequency and combination variety of feature values.\nThis allows us to dynamically adjust perturbation strengths and adversarial\ntraining weights. Stronger perturbations are applied to feature values with\nfewer combination varieties to improve generalization, while higher weights for\nlow-frequency features address training imbalances. We leverage the Adaptive\nAdversarial perturbation based on the widely-applied Factorization Machine\n(AAFM) as our backbone model. In experiments, AAFM surpasses strong baselines\nin both fairness and accuracy measures. AAFM excels in providing item- and\nuser-fairness for single- and multi-feature tasks, showcasing their versatility\nand scalability. To maintain good accuracy, we find that adversarial\nperturbation must be well-managed: during training, perturbations should not\noverly persist and their strengths should decay.",
        "translated": ""
    },
    {
        "title": "Frequency and cardinality recovery from sketched data: a novel approach\n  bridging Bayesian and frequentist views",
        "url": "http://arxiv.org/abs/2309.15408v1",
        "pub_date": "2023-09-27",
        "summary": "We study how to recover the frequency of a symbol in a large discrete data\nset, using only a compressed representation, or sketch, of those data obtained\nvia random hashing. This is a classical problem in computer science, with\nvarious algorithms available, such as the count-min sketch. However, these\nalgorithms often assume that the data are fixed, leading to overly conservative\nand potentially inaccurate estimates when dealing with randomly sampled data.\nIn this paper, we consider the sketched data as a random sample from an unknown\ndistribution, and then we introduce novel estimators that improve upon existing\napproaches. Our method combines Bayesian nonparametric and classical\n(frequentist) perspectives, addressing their unique limitations to provide a\nprincipled and practical solution. Additionally, we extend our method to\naddress the related but distinct problem of cardinality recovery, which\nconsists of estimating the total number of distinct objects in the data set. We\nvalidate our method on synthetic and real data, comparing its performance to\nstate-of-the-art alternatives.",
        "translated": ""
    },
    {
        "title": "A Content-Driven Micro-Video Recommendation Dataset at Scale",
        "url": "http://arxiv.org/abs/2309.15379v1",
        "pub_date": "2023-09-27",
        "summary": "Micro-videos have recently gained immense popularity, sparking critical\nresearch in micro-video recommendation with significant implications for the\nentertainment, advertising, and e-commerce industries. However, the lack of\nlarge-scale public micro-video datasets poses a major challenge for developing\neffective recommender systems. To address this challenge, we introduce a very\nlarge micro-video recommendation dataset, named \"MicroLens\", consisting of one\nbillion user-item interaction behaviors, 34 million users, and one million\nmicro-videos. This dataset also contains various raw modality information about\nvideos, including titles, cover images, audio, and full-length videos.\nMicroLens serves as a benchmark for content-driven micro-video recommendation,\nenabling researchers to utilize various modalities of video information for\nrecommendation, rather than relying solely on item IDs or off-the-shelf video\nfeatures extracted from a pre-trained network. Our benchmarking of multiple\nrecommender models and video encoders on MicroLens has yielded valuable\ninsights into the performance of micro-video recommendation. We believe that\nthis dataset will not only benefit the recommender system community but also\npromote the development of the video understanding field. Our datasets and code\nare available at https://github.com/westlake-repl/MicroLens.",
        "translated": ""
    },
    {
        "title": "LD4MRec: Simplifying and Powering Diffusion Model for Multimedia\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.15363v1",
        "pub_date": "2023-09-27",
        "summary": "Multimedia recommendation aims to predict users' future behaviors based on\nhistorical behavioral data and item's multimodal information. However, noise\ninherent in behavioral data, arising from unintended user interactions with\nuninteresting items, detrimentally impacts recommendation performance.\nRecently, diffusion models have achieved high-quality information generation,\nin which the reverse process iteratively infers future information based on the\ncorrupted state. It meets the need of predictive tasks under noisy conditions,\nand inspires exploring their application to predicting user behaviors.\nNonetheless, several challenges must be addressed: 1) Classical diffusion\nmodels require excessive computation, which does not meet the efficiency\nrequirements of recommendation systems. 2) Existing reverse processes are\nmainly designed for continuous data, whereas behavioral information is discrete\nin nature. Therefore, an effective method is needed for the generation of\ndiscrete behavioral information.\n  To tackle the aforementioned issues, we propose a Light Diffusion model for\nMultimedia Recommendation. First, to reduce computational complexity, we\nsimplify the formula of the reverse process, enabling one-step inference\ninstead of multi-step inference. Second, to achieve effective behavioral\ninformation generation, we propose a novel Conditional neural Network. It maps\nthe discrete behavior data into a continuous latent space, and generates\nbehaviors with the guidance of collaborative signals and user multimodal\npreference. Additionally, considering that completely clean behavior data is\ninaccessible, we introduce a soft behavioral reconstruction constraint during\nmodel training, facilitating behavior prediction with noisy data. Empirical\nstudies conducted on three public datasets demonstrate the effectiveness of\nLD4MRec.",
        "translated": ""
    },
    {
        "title": "Decoding the Workplace &amp; EOR: An Employee Survey Analysis by Data\n  Science Techniques and Visualization",
        "url": "http://arxiv.org/abs/2309.16329v1",
        "pub_date": "2023-09-28",
        "summary": "This research study explores the new dynamics of employee-organi-zation\nrelationships (EOR) [6] using advanced data science methodologies and presents\nfindings through accessible visualizations. Leveraging a dataset pro-cured from\na comprehensive nationwide big employee survey, this study employs innovative\nstrategy for theoretical researcher by using our state-of-the-art\nvisual-ization. The results present insightful visualizations encapsulating\ndemographic analysis, workforce satisfaction, work environment scrutiny, and\nthe employee's view via word cloud interpretations and burnout predictions.\n  The study underscores the profound implications of data science across\nvarious management sectors, enhancing understanding of workplace dynamics and\npro-moting mutual growth and satisfaction. This multifaceted approach caters to\na diverse array of readers, from researchers in sociology and management to\nfirms seeking detailed understanding of their workforce's satisfaction,\nemphasizing on practicality and interpretability.\n  The research encourages proactive measures to improve workplace\nenviron-ments, boost employee satisfaction, and foster healthier, more\nproductive organ-izations. It serves as a resourceful tool for those committed\nto these objectives, manifesting the transformative potential of data science\nin driving insightful nar-ratives about workplace dynamics and\nemployee-organization relationships. In essence, this research unearths\nvaluable insights to aid management, HR profes-sionals, and companies",
        "translated": ""
    },
    {
        "title": "Multi-Granularity Click Confidence Learning via Self-Distillation in\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.16322v1",
        "pub_date": "2023-09-28",
        "summary": "Recommendation systems rely on historical clicks to learn user interests and\nprovide appropriate items. However, current studies tend to treat clicks\nequally, which may ignore the assorted intensities of user interests in\ndifferent clicks. In this paper, we aim to achieve multi-granularity Click\nconfidence Learning via Self-Distillation in recommendation (CLSD). Due to the\nlack of supervised signals in click confidence, we first apply self-supervised\nlearning to obtain click confidence scores via a global self-distillation\nmethod. After that, we define a local confidence function to adapt confidence\nscores at the user group level, since the confidence distributions can be\nvaried among user groups. With the combination of multi-granularity confidence\nlearning, we can distinguish the quality of clicks and model user interests\nmore accurately without involving extra data and model structures. The\nsignificant improvements over different backbones on industrial offline and\nonline experiments in a real-world recommender system prove the effectiveness\nof our model. Recently, CLSD has been deployed on a large-scale recommender\nsystem, affecting over 400 million users.",
        "translated": ""
    },
    {
        "title": "Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale\n  Localization",
        "url": "http://arxiv.org/abs/2309.16034v1",
        "pub_date": "2023-09-27",
        "summary": "Advancements in nanotechnology and material science are paving the way toward\nnanoscale devices that combine sensing, computing, data and energy storage, and\nwireless communication. In precision medicine, these nanodevices show promise\nfor disease diagnostics, treatment, and monitoring from within the patients'\nbloodstreams. Assigning the location of a sensed biological event with the\nevent itself, which is the main proposition of flow-guided in-body nanoscale\nlocalization, would be immensely beneficial from the perspective of precision\nmedicine. The nanoscale nature of the nanodevices and the challenging\nenvironment that the bloodstream represents, result in current flow-guided\nlocalization approaches being constrained in their communication and\nenergy-related capabilities. The communication and energy constraints of the\nnanodevices result in different features of raw data for flow-guided\nlocalization, in turn affecting its performance. An analytical modeling of the\neffects of imperfect communication and constrained energy causing intermittent\noperation of the nanodevices on the raw data produced by the nanodevices would\nbe beneficial. Hence, we propose an analytical model of raw data for\nflow-guided localization, where the raw data is modeled as a function of\ncommunication and energy-related capabilities of the nanodevice. We evaluate\nthe model by comparing its output with the one obtained through the utilization\nof a simulator for objective evaluation of flow-guided localization, featuring\ncomparably higher level of realism. Our results across a number of scenarios\nand heterogeneous performance metrics indicate high similarity between the\nmodel and simulator-generated raw datasets.",
        "translated": ""
    },
    {
        "title": "Toward Robust Recommendation via Real-time Vicinal Defense",
        "url": "http://arxiv.org/abs/2309.17278v1",
        "pub_date": "2023-09-29",
        "summary": "Recommender systems have been shown to be vulnerable to poisoning attacks,\nwhere malicious data is injected into the dataset to cause the recommender\nsystem to provide biased recommendations. To defend against such attacks,\nvarious robust learning methods have been proposed. However, most methods are\nmodel-specific or attack-specific, making them lack generality, while other\nmethods, such as adversarial training, are oriented towards evasion attacks and\nthus have a weak defense strength in poisoning attacks.\n  In this paper, we propose a general method, Real-time Vicinal Defense (RVD),\nwhich leverages neighboring training data to fine-tune the model before making\na recommendation for each user. RVD works in the inference phase to ensure the\nrobustness of the specific sample in real-time, so there is no need to change\nthe model structure and training process, making it more practical. Extensive\nexperimental results demonstrate that RVD effectively mitigates targeted\npoisoning attacks across various models without sacrificing accuracy. Moreover,\nthe defensive effect can be further amplified when our method is combined with\nother strategies.",
        "translated": ""
    },
    {
        "title": "SAppKG: Mobile App Recommendation Using Knowledge Graph and Side\n  Information-A Secure Framework",
        "url": "http://arxiv.org/abs/2309.17115v1",
        "pub_date": "2023-09-29",
        "summary": "Due to the rapid development of technology and the widespread usage of\nsmartphones, the number of mobile applications is exponentially growing.\nFinding a suitable collection of apps that aligns with users needs and\npreferences can be challenging. However, mobile app recommender systems have\nemerged as a helpful tool in simplifying this process. But there is a drawback\nto employing app recommender systems. These systems need access to user data,\nwhich is a serious security violation. While users seek accurate opinions, they\ndo not want to compromise their privacy in the process. We address this issue\nby developing SAppKG, an end-to-end user privacy-preserving knowledge graph\narchitecture for mobile app recommendation based on knowledge graph models such\nas SAppKG-S and SAppKG-D, that utilized the interaction data and side\ninformation of app attributes. We tested the proposed model on real-world data\nfrom the Google Play app store, using precision, recall, mean absolute\nprecision, and mean reciprocal rank. We found that the proposed model improved\nresults on all four metrics. We also compared the proposed model to baseline\nmodels and found that it outperformed them on all four metrics.",
        "translated": ""
    },
    {
        "title": "Aligning the Capabilities of Large Language Models with the Context of\n  Information Retrieval via Contrastive Feedback",
        "url": "http://arxiv.org/abs/2309.17078v1",
        "pub_date": "2023-09-29",
        "summary": "Information Retrieval (IR), the process of finding information to satisfy\nuser's information needs, plays an essential role in modern people's lives.\nRecently, large language models (LLMs) have demonstrated remarkable\ncapabilities across various tasks, some of which are important for IR.\nNonetheless, LLMs frequently confront the issue of generating responses that\nlack specificity. This has limited the overall effectiveness of LLMs for IR in\nmany cases. To address these issues, we present an unsupervised alignment\nframework called Reinforcement Learning from Contrastive Feedback (RLCF), which\nempowers LLMs to generate both high-quality and context-specific responses that\nsuit the needs of IR tasks. Specifically, we construct contrastive feedback by\ncomparing each document with its similar documents, and then propose a reward\nfunction named Batched-MRR to teach LLMs to generate responses that captures\nthe fine-grained information that distinguish documents from their similar\nones. To demonstrate the effectiveness of RLCF, we conducted experiments in two\ntypical applications of LLMs in IR, i.e., data augmentation and summarization.\nThe experimental results show that RLCF can effectively improve the performance\nof LLMs in IR context.",
        "translated": ""
    },
    {
        "title": "Beyond Co-occurrence: Multi-modal Session-based Recommendation",
        "url": "http://arxiv.org/abs/2309.17037v1",
        "pub_date": "2023-09-29",
        "summary": "Session-based recommendation is devoted to characterizing preferences of\nanonymous users based on short sessions. Existing methods mostly focus on\nmining limited item co-occurrence patterns exposed by item ID within sessions,\nwhile ignoring what attracts users to engage with certain items is rich\nmulti-modal information displayed on pages. Generally, the multi-modal\ninformation can be classified into two categories: descriptive information\n(e.g., item images and description text) and numerical information (e.g.,\nprice). In this paper, we aim to improve session-based recommendation by\nmodeling the above multi-modal information holistically. There are mainly three\nissues to reveal user intent from multi-modal information: (1) How to extract\nrelevant semantics from heterogeneous descriptive information with different\nnoise? (2) How to fuse these heterogeneous descriptive information to\ncomprehensively infer user interests? (3) How to handle probabilistic influence\nof numerical information on user behaviors? To solve above issues, we propose a\nnovel multi-modal session-based recommendation (MMSBR) that models both\ndescriptive and numerical information under a unified framework. Specifically,\na pseudo-modality contrastive learning is devised to enhance the representation\nlearning of descriptive information. Afterwards, a hierarchical pivot\ntransformer is presented to fuse heterogeneous descriptive information.\nMoreover, we represent numerical information with Gaussian distribution and\ndesign a Wasserstein self-attention to handle the probabilistic influence mode.\nExtensive experiments on three real-world datasets demonstrate the\neffectiveness of the proposed MMSBR. Further analysis also proves that our\nMMSBR can alleviate the cold-start problem in SBR effectively.",
        "translated": ""
    },
    {
        "title": "Hallucination Reduction in Long Input Text Summarization",
        "url": "http://arxiv.org/abs/2309.16781v1",
        "pub_date": "2023-09-28",
        "summary": "Hallucination in text summarization refers to the phenomenon where the model\ngenerates information that is not supported by the input source document.\nHallucination poses significant obstacles to the accuracy and reliability of\nthe generated summaries. In this paper, we aim to reduce hallucinated outputs\nor hallucinations in summaries of long-form text documents. We have used the\nPubMed dataset, which contains long scientific research documents and their\nabstracts. We have incorporated the techniques of data filtering and joint\nentity and summary generation (JAENS) in the fine-tuning of the Longformer\nEncoder-Decoder (LED) model to minimize hallucinations and thereby improve the\nquality of the generated summary. We have used the following metrics to measure\nfactual consistency at the entity level: precision-source, and F1-target. Our\nexperiments show that the fine-tuned LED model performs well in generating the\npaper abstract. Data filtering techniques based on some preprocessing steps\nreduce entity-level hallucinations in the generated summaries in terms of some\nof the factual consistency metrics.",
        "translated": ""
    },
    {
        "title": "CORec-Cri: How collaborative and social technologies can help to\n  contextualize crises?",
        "url": "http://arxiv.org/abs/2310.02143v1",
        "pub_date": "2023-10-03",
        "summary": "Crisis situations can present complex and multifaceted challenges, often\nrequiring the involvement of multiple organizations and stakeholders with\nvarying areas of expertise, responsibilities, and resources. Acquiring accurate\nand timely information about impacted areas is crucial to effectively respond\nto these crises. In this paper, we investigate how collaborative and social\ntechnologies help to contextualize crises, including identifying impacted areas\nand real-time needs. To this end, we define CORec-Cri (Contextulized\nOntology-based Recommender system for crisis management) based on existing\nwork. Our motivation for this approach is two-fold: first, effective\ncollaboration among stakeholders is essential for efficient and coordinated\ncrisis response; second, social computing facilitates interaction, information\nflow, and collaboration among stakeholders. We detail the key components of our\nsystem design, highlighting its potential to support decision-making, resource\nallocation, and communication among stakeholders. Finally, we provide examples\nof how our system can be applied to contextualize crises to improve crisis\nmanagement.",
        "translated": ""
    },
    {
        "title": "Online Multimedia Verification with Computational Tools and OSINT:\n  Russia-Ukraine Conflict Case Studies",
        "url": "http://arxiv.org/abs/2310.01978v1",
        "pub_date": "2023-10-03",
        "summary": "This paper investigates the use of computational tools and Open-Source\nIntelligence (OSINT) techniques for verifying online multimedia content, with a\nspecific focus on real-world cases from the Russia-Ukraine conflict. Over a\nnine-month period from April to December 2022, we examine verification\nworkflows, tools, and case studies published by \\faktiskbar. Our study\nshowcases the effectiveness of diverse resources, including AI tools,\ngeolocation tools, internet archives, and social media monitoring platforms, in\nenabling journalists and fact-checkers to efficiently process and corroborate\nevidence, ensuring the dissemination of accurate information. This research\nunderscores the vital role of computational tools and OSINT techniques in\npromoting evidence-based reporting and combatting misinformation. We also touch\non the current limitations of available tools and prospects for future\ndevelopments in multimedia verification.",
        "translated": ""
    },
    {
        "title": "DANI: Fast Diffusion Aware Network Inference with Preserving Topological\n  Structure Property",
        "url": "http://arxiv.org/abs/2310.01696v1",
        "pub_date": "2023-10-02",
        "summary": "The fast growth of social networks and their data access limitations in\nrecent years has led to increasing difficulty in obtaining the complete\ntopology of these networks. However, diffusion information over these networks\nis available, and many algorithms have been proposed to infer the underlying\nnetworks using this information. The previously proposed algorithms only focus\non inferring more links and ignore preserving the critical topological\ncharacteristics of the underlying social networks. In this paper, we propose a\nnovel method called DANI to infer the underlying network while preserving its\nstructural properties. It is based on the Markov transition matrix derived from\ntime series cascades, as well as the node-node similarity that can be observed\nin the cascade behavior from a structural point of view. In addition, the\npresented method has linear time complexity (increases linearly with the number\nof nodes, number of cascades, and square of the average length of cascades),\nand its distributed version in the MapReduce framework is also scalable. We\napplied the proposed approach to both real and synthetic networks. The\nexperimental results showed that DANI has higher accuracy and lower run time\nwhile maintaining structural properties, including modular structure, degree\ndistribution, connected components, density, and clustering coefficients, than\nwell-known network inference methods.",
        "translated": ""
    },
    {
        "title": "Towards Efficient and Effective Adaptation of Large Language Models for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2310.01612v1",
        "pub_date": "2023-10-02",
        "summary": "In recent years, with large language models (LLMs) achieving state-of-the-art\nperformance in context understanding, increasing efforts have been dedicated to\ndeveloping LLM-enhanced sequential recommendation (SR) methods. Considering\nthat most existing LLMs are not specifically optimized for recommendation\ntasks, adapting them for SR becomes a critical step in LLM-enhanced SR methods.\nThough numerous adaptation methods have been developed, it still remains a\nsignificant challenge to adapt LLMs for SR both efficiently and effectively. To\naddress this challenge, in this paper, we introduce a novel side sequential\nnetwork adaptation method, denoted as SSNA, for LLM enhanced SR. SSNA features\nthree key designs to allow both efficient and effective LLM adaptation. First,\nSSNA learns adapters separate from LLMs, while fixing all the pre-trained\nparameters within LLMs to allow efficient adaptation. In addition, SSNA adapts\nthe top-a layers of LLMs jointly, and integrates adapters sequentially for\nenhanced effectiveness (i.e., recommendation performance). We compare SSNA\nagainst five state-of-the-art baseline methods on five benchmark datasets using\nthree LLMs. The experimental results demonstrate that SSNA significantly\noutperforms all the baseline methods in terms of recommendation performance,\nand achieves substantial improvement over the best-performing baseline methods\nat both run-time and memory efficiency during training. Our analysis shows the\neffectiveness of integrating adapters in a sequential manner. Our parameter\nstudy demonstrates the effectiveness of jointly adapting the top-a layers of\nLLMs.",
        "translated": ""
    },
    {
        "title": "Causality-informed Rapid Post-hurricane Building Damage Detection in\n  Large Scale from InSAR Imagery",
        "url": "http://arxiv.org/abs/2310.01565v1",
        "pub_date": "2023-10-02",
        "summary": "Timely and accurate assessment of hurricane-induced building damage is\ncrucial for effective post-hurricane response and recovery efforts. Recently,\nremote sensing technologies provide large-scale optical or Interferometric\nSynthetic Aperture Radar (InSAR) imagery data immediately after a disastrous\nevent, which can be readily used to conduct rapid building damage assessment.\nCompared to optical satellite imageries, the Synthetic Aperture Radar can\npenetrate cloud cover and provide more complete spatial coverage of damaged\nzones in various weather conditions. However, these InSAR imageries often\ncontain highly noisy and mixed signals induced by co-occurring or co-located\nbuilding damage, flood, flood/wind-induced vegetation changes, as well as\nanthropogenic activities, making it challenging to extract accurate building\ndamage information. In this paper, we introduced an approach for rapid\npost-hurricane building damage detection from InSAR imagery. This approach\nencoded complex causal dependencies among wind, flood, building damage, and\nInSAR imagery using a holistic causal Bayesian network. Based on the causal\nBayesian network, we further jointly inferred the large-scale unobserved\nbuilding damage by fusing the information from InSAR imagery with prior\nphysical models of flood and wind, without the need for ground truth labels.\nFurthermore, we validated our estimation results in a real-world devastating\nhurricane -- the 2022 Hurricane Ian. We gathered and annotated building damage\nground truth data in Lee County, Florida, and compared the introduced method's\nestimation results with the ground truth and benchmarked it against\nstate-of-the-art models to assess the effectiveness of our proposed method.\nResults show that our method achieves rapid and accurate detection of building\ndamage, with significantly reduced processing time compared to traditional\nmanual inspection methods.",
        "translated": ""
    },
    {
        "title": "Replicating Relevance-Ranked Synonym Discovery in a New Language and\n  Domain",
        "url": "http://arxiv.org/abs/2310.01507v1",
        "pub_date": "2023-10-02",
        "summary": "Domain-specific synonyms occur in many specialized search tasks, such as when\nsearching medical documents, legal documents, and software engineering\nartifacts. We replicate prior work on ranking domain-specific synonyms in the\nconsumer health domain by applying the approach to a new language and domain:\nidentifying Swedish language synonyms in the building construction domain. We\nchose this setting because identifying synonyms in this domain is helpful for\ndownstream systems, where different users may query for documents (e.g.,\nengineering requirements) using different terminology. We consider two new\nfeatures inspired by the change in language and methodological advances since\nthe prior work's publication. An evaluation using data from the building\nconstruction domain supports the finding from the prior work that synonym\ndiscovery is best approached as a learning to rank task in which a human editor\nviews ranked synonym candidates in order to construct a domain-specific\nthesaurus. We additionally find that FastText embeddings alone provide a strong\nbaseline, though they do not perform as well as the strongest learning to rank\nmethod. Finally, we analyze the performance of individual features and the\ndifferences in the domains.",
        "translated": ""
    },
    {
        "title": "LEEC: A Legal Element Extraction Dataset with an Extensive\n  Domain-Specific Label System",
        "url": "http://arxiv.org/abs/2310.01271v1",
        "pub_date": "2023-10-02",
        "summary": "As a pivotal task in natural language processing, element extraction has\ngained significance in the legal domain. Extracting legal elements from\njudicial documents helps enhance interpretative and analytical capacities of\nlegal cases, and thereby facilitating a wide array of downstream applications\nin various domains of law. Yet existing element extraction datasets are limited\nby their restricted access to legal knowledge and insufficient coverage of\nlabels. To address this shortfall, we introduce a more comprehensive,\nlarge-scale criminal element extraction dataset, comprising 15,831 judicial\ndocuments and 159 labels. This dataset was constructed through two main steps:\nFirst, designing the label system by our team of legal experts based on prior\nlegal research which identified critical factors driving and processes\ngenerating sentencing outcomes in criminal cases; Second, employing the legal\nknowledge to annotate judicial documents according to the label system and\nannotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents\nthe most extensive and domain-specific legal element extraction dataset for the\nChinese legal system. Leveraging the annotated data, we employed various SOTA\nmodels that validates the applicability of LEEC for Document Event Extraction\n(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .",
        "translated": ""
    },
    {
        "title": "NewsRecLib: A PyTorch-Lightning Library for Neural News Recommendation",
        "url": "http://arxiv.org/abs/2310.01146v1",
        "pub_date": "2023-10-02",
        "summary": "NewsRecLib is an open-source library based on Pytorch-Lightning and Hydra\ndeveloped for training and evaluating neural news recommendation models. The\nforemost goals of NewsRecLib are to promote reproducible research and rigorous\nexperimental evaluation by (i) providing a unified and highly configurable\nframework for exhaustive experimental studies and (ii) enabling a thorough\nanalysis of the performance contribution of different model architecture\ncomponents and training regimes. NewsRecLib is highly modular, allows\nspecifying experiments in a single configuration file, and includes extensive\nlogging facilities. Moreover, NewsRecLib provides out-of-the-box\nimplementations of several prominent neural models, training methods, standard\nevaluation benchmarks, and evaluation metrics for news recommendation.",
        "translated": ""
    },
    {
        "title": "Dataset Condensation for Recommendation",
        "url": "http://arxiv.org/abs/2310.01038v1",
        "pub_date": "2023-10-02",
        "summary": "Training recommendation models on large datasets often requires significant\ntime and computational resources. Consequently, an emergent imperative has\narisen to construct informative, smaller-scale datasets for efficiently\ntraining. Dataset compression techniques explored in other domains show\npotential possibility to address this problem, via sampling a subset or\nsynthesizing a small dataset. However, applying existing approaches to condense\nrecommendation datasets is impractical due to following challenges: (i)\nsampling-based methods are inadequate in addressing the long-tailed\ndistribution problem; (ii) synthesizing-based methods are not applicable due to\ndiscreteness of interactions and large size of recommendation datasets; (iii)\nneither of them fail to address the specific issue in recommendation of false\nnegative items, where items with potential user interest are incorrectly\nsampled as negatives owing to insufficient exposure.\n  To bridge this gap, we investigate dataset condensation for recommendation,\nwhere discrete interactions are continualized with probabilistic\nre-parameterization. To avoid catastrophically expensive computations, we adopt\na one-step update strategy for inner model training and introducing policy\ngradient estimation for outer dataset synthesis. To mitigate amplification of\nlong-tailed problem, we compensate long-tailed users in the condensed dataset.\nFurthermore, we propose to utilize a proxy model to identify false negative\nitems. Theoretical analysis regarding the convergence property is provided.\nExtensive experiments on multiple datasets demonstrate the efficacy of our\nmethod. In particular, we reduce the dataset size by 75% while approximating\nover 98% of the original performance on Dianping and over 90% on other\ndatasets.",
        "translated": ""
    },
    {
        "title": "Organized Event Participant Prediction Enhanced by Social Media\n  Retweeting Data",
        "url": "http://arxiv.org/abs/2310.00896v1",
        "pub_date": "2023-10-02",
        "summary": "Nowadays, many platforms on the Web offer organized events, allowing users to\nbe organizers or participants. For such platforms, it is beneficial to predict\npotential event participants. Existing work on this problem tends to borrow\nrecommendation techniques. However, compared to e-commerce items and purchases,\nevents and participation are usually of a much smaller frequency, and the data\nmay be insufficient to learn an accurate model. In this paper, we propose to\nutilize social media retweeting activity data to enhance the learning of event\nparticipant prediction models. We create a joint knowledge graph to bridge the\nsocial media and the target domain, assuming that event descriptions and tweets\nare written in the same language. Furthermore, we propose a learning model that\nutilizes retweeting information for the target domain prediction more\neffectively. We conduct comprehensive experiments in two scenarios with\nreal-world data. In each scenario, we set up training data of different sizes,\nas well as warm and cold test cases. The evaluation results show that our\napproach consistently outperforms several baseline models, especially with the\nwarm test cases, and when target domain data is limited.",
        "translated": ""
    },
    {
        "title": "Retrieval meets Long Context Large Language Models",
        "url": "http://arxiv.org/abs/2310.03025v1",
        "pub_date": "2023-10-04",
        "summary": "Extending the context window of large language models (LLMs) is getting\npopular recently, while the solution of augmenting LLMs with retrieval has\nexisted for years. The natural questions are: i) Retrieval-augmentation versus\nlong context window, which one is better for downstream tasks? ii) Can both\nmethods be combined to get the best of both worlds? In this work, we answer\nthese questions by studying both solutions using two state-of-the-art\npretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps\nsurprisingly, we find that LLM with 4K context window using simple\nretrieval-augmentation at generation can achieve comparable performance to\nfinetuned LLM with 16K context window via positional interpolation on long\ncontext tasks, while taking much less computation. More importantly, we\ndemonstrate that retrieval can significantly improve the performance of LLMs\nregardless of their extended context window sizes. Our best model,\nretrieval-augmented LLaMA2-70B with 32K context window, outperforms\nGPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long\ncontext tasks including question answering and query-based summarization. It\nalso outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while\nbeing much faster at generation. Our study provides general insights on the\nchoice of retrieval-augmentation versus long context extension of LLM for\npractitioners.",
        "translated": ""
    },
    {
        "title": "Potential Factors Leading to Popularity Unfairness in Recommender\n  Systems: A User-Centered Analysis",
        "url": "http://arxiv.org/abs/2310.02961v1",
        "pub_date": "2023-10-04",
        "summary": "Popularity bias is a well-known issue in recommender systems where few\npopular items are over-represented in the input data, while majority of other\nless popular items are under-represented. This disparate representation often\nleads to bias in exposure given to the items in the recommendation results.\nExtensive research examined this bias from item perspective and attempted to\nmitigate it by enhancing the recommendation of less popular items. However, a\nrecent research has revealed the impact of this bias on users. Users with\ndifferent degree of tolerance toward popular items are not fairly served by the\nrecommendation system: users interested in less popular items receive more\npopular items in their recommendations, while users interested in popular items\nare recommended what they want. This is mainly due to the popularity bias that\npopular items are over-recommended. In this paper, we aim at investigating the\nfactors leading to this user-side unfairness of popularity bias in recommender\nsystems. In particular, we investigate two factors: 1) the relationship between\nthis unfairness and users' interest toward items' categories (e.g., movie\ngenres), 2) the relationship between this unfairness and the diversity of the\npopularity group in users' profile (the degree to which the user is interested\nin items with different degree of popularity). Experiments on a movie\nrecommendation dataset using multiple recommendation algorithms show that these\ntwo factors are significantly correlated with the degree of popularity\nunfairness in the recommendation results.",
        "translated": ""
    },
    {
        "title": "Auto-FP: An Experimental Study of Automated Feature Preprocessing for\n  Tabular Data",
        "url": "http://arxiv.org/abs/2310.02540v1",
        "pub_date": "2023-10-04",
        "summary": "Classical machine learning models, such as linear models and tree-based\nmodels, are widely used in industry. These models are sensitive to data\ndistribution, thus feature preprocessing, which transforms features from one\ndistribution to another, is a crucial step to ensure good model quality.\nManually constructing a feature preprocessing pipeline is challenging because\ndata scientists need to make difficult decisions about which preprocessors to\nselect and in which order to compose them. In this paper, we study how to\nautomate feature preprocessing (Auto-FP) for tabular data. Due to the large\nsearch space, a brute-force solution is prohibitively expensive. To address\nthis challenge, we interestingly observe that Auto-FP can be modelled as either\na hyperparameter optimization (HPO) or a neural architecture search (NAS)\nproblem. This observation enables us to extend a variety of HPO and NAS\nalgorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation\nand analysis of 15 algorithms on 45 public ML datasets. Overall,\nevolution-based algorithms show the leading average ranking. Surprisingly, the\nrandom search turns out to be a strong baseline. Many surrogate-model-based and\nbandit-based search algorithms, which achieve good performance for HPO and NAS,\ndo not outperform random search for Auto-FP. We analyze the reasons for our\nfindings and conduct a bottleneck analysis to identify the opportunities to\nimprove these algorithms. Furthermore, we explore how to extend Auto-FP to\nsupport parameter search and compare two ways to achieve this goal. In the end,\nwe evaluate Auto-FP in an AutoML context and discuss the limitations of popular\nAutoML tools. To the best of our knowledge, this is the first study on\nautomated feature preprocessing. We hope our work can inspire researchers to\ndevelop new algorithms tailored for Auto-FP.",
        "translated": ""
    },
    {
        "title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics\n  of Uncertainty and Prediction Error in Musical Improvisation",
        "url": "http://arxiv.org/abs/2310.02518v1",
        "pub_date": "2023-10-04",
        "summary": "Musical improvisation, much like spontaneous speech, reveals intricate facets\nof the improviser's state of mind and emotional character. However, the\nspecific musical components that reveal such individuality remain largely\nunexplored. Within the framework of brain's statistical learning and predictive\nprocessing, this study examined the temporal dynamics of uncertainty and\nsurprise (prediction error) in a piece of musical improvisation. This study\nemployed the HBSL model to analyze a corpus of 456 Jazz improvisations,\nspanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated\ndistinctive temporal patterns of surprise and uncertainty, especially in pitch\nand pitch-rhythm sequences, revealing era-specific features from the early 20th\nto the 21st centuries. Conversely, rhythm sequences exhibited a consistent\ndegree of uncertainty across eras. Further, the acoustic properties remain\nunchanged across different periods. These findings highlight the importance of\nhow temporal dynamics of surprise and uncertainty in improvisational music\nchange over periods, profoundly influencing the distinctive methodologies\nartists adopt for improvisation in each era. Further, it is suggested that the\ndevelopment of improvisational music can be attributed to the brain's adaptive\nstatistical learning mechanisms, which constantly refine internal models to\nmirror the cultural and emotional nuances of their respective epochs. This\nstudy unravels the evolutionary trajectory of improvisational music and\nhighlights the nuanced shifts artists employ to resonate with the cultural and\nemotional landscapes of their times.",
        "translated": ""
    },
    {
        "title": "Linear Recurrent Units for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2310.02367v1",
        "pub_date": "2023-10-03",
        "summary": "State-of-the-art sequential recommendation relies heavily on\nself-attention-based recommender models. Yet such models are computationally\nexpensive and often too slow for real-time recommendation. Furthermore, the\nself-attention operation is performed at a sequence-level, thereby making\nlow-cost incremental inference challenging. Inspired by recent advances in\nefficient language modeling, we propose linear recurrent units for sequential\nrecommendation (LRURec). Similar to recurrent neural networks, LRURec offers\nrapid inference and can achieve incremental inference on sequential inputs. By\ndecomposing the linear recurrence operation and designing recursive\nparallelization in our framework, LRURec provides the additional benefits of\nreduced model size and parallelizable training. Moreover, we optimize the\narchitecture of LRURec by implementing a series of modifications to address the\nlack of non-linearity and improve training dynamics. To validate the\neffectiveness of our proposed LRURec, we conduct extensive experiments on\nmultiple real-world datasets and compare its performance against\nstate-of-the-art sequential recommenders. Experimental results demonstrate the\neffectiveness of LRURec, which consistently outperforms baselines by a\nsignificant margin. Results also highlight the efficiency of LRURec with our\nparallelized training paradigm and fast inference on long sequences, showing\nits potential to further enhance user experience in sequential recommendation.",
        "translated": ""
    },
    {
        "title": "Beyond-Accuracy: A Review on Diversity, Serendipity and Fairness in\n  Recommender Systems Based on Graph Neural Networks",
        "url": "http://arxiv.org/abs/2310.02294v1",
        "pub_date": "2023-10-03",
        "summary": "By providing personalized suggestions to users, recommender systems have\nbecome essential to numerous online platforms. Collaborative filtering,\nparticularly graph-based approaches using Graph Neural Networks (GNNs), have\ndemonstrated great results in terms of recommendation accuracy. However,\naccuracy may not always be the most important criterion for evaluating\nrecommender systems' performance, since beyond-accuracy aspects such as\nrecommendation diversity, serendipity, and fairness can strongly influence user\nengagement and satisfaction. This review paper focuses on addressing these\ndimensions in GNN-based recommender systems, going beyond the conventional\naccuracy-centric perspective. We begin by reviewing recent developments in\napproaches that improve not only the accuracy-diversity trade-off but also\npromote serendipity and fairness in GNN-based recommender systems. We discuss\ndifferent stages of model development including data preprocessing, graph\nconstruction, embedding initialization, propagation layers, embedding fusion,\nscore computation, and training methodologies. Furthermore, we present a look\ninto the practical difficulties encountered in assuring diversity, serendipity,\nand fairness, while retaining high accuracy. Finally, we discuss potential\nfuture research directions for developing more robust GNN-based recommender\nsystems that go beyond the unidimensional perspective of focusing solely on\naccuracy. This review aims to provide researchers and practitioners with an\nin-depth understanding of the multifaceted issues that arise when designing\nGNN-based recommender systems, setting our work apart by offering a\ncomprehensive exploration of beyond-accuracy dimensions.",
        "translated": ""
    },
    {
        "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving\n  Pipelines",
        "url": "http://arxiv.org/abs/2310.03714v1",
        "pub_date": "2023-10-05",
        "summary": "The ML community is rapidly exploring techniques for prompting language\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\n\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward\na more systematic approach for developing and optimizing LM pipelines, we\nintroduce DSPy, a programming model that abstracts LM pipelines as text\ntransformation graphs, i.e. imperative computational graphs where LMs are\ninvoked through declarative modules. DSPy modules are parameterized, meaning\nthey can learn (by creating and collecting demonstrations) how to apply\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\nmetric. We conduct two case studies, showing that succinct DSPy programs can\nexpress and optimize sophisticated LM pipelines that reason about math word\nproblems, tackle multi-hop retrieval, answer complex questions, and control\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\nprompting (generally by over 25% and 65%, respectively) and pipelines with\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\nof that, DSPy programs compiled to open and relatively small LMs like\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\nhttps://github.com/stanfordnlp/dspy",
        "translated": ""
    },
    {
        "title": "FASER: Binary Code Similarity Search through the use of Intermediate\n  Representations",
        "url": "http://arxiv.org/abs/2310.03605v1",
        "pub_date": "2023-10-05",
        "summary": "Being able to identify functions of interest in cross-architecture software\nis useful whether you are analysing for malware, securing the software supply\nchain or conducting vulnerability research. Cross-Architecture Binary Code\nSimilarity Search has been explored in numerous studies and has used a wide\nrange of different data sources to achieve its goals. The data sources\ntypically used draw on common structures derived from binaries such as function\ncontrol flow graphs or binary level call graphs, the output of the disassembly\nprocess or the outputs of a dynamic analysis approach. One data source which\nhas received less attention is binary intermediate representations. Binary\nIntermediate representations possess two interesting properties: they are cross\narchitecture by their very nature and encode the semantics of a function\nexplicitly to support downstream usage. Within this paper we propose Function\nas a String Encoded Representation (FASER) which combines long document\ntransformers with the use of intermediate representations to create a model\ncapable of cross architecture function search without the need for manual\nfeature engineering, pre-training or a dynamic analysis step. We compare our\napproach against a series of baseline approaches for two tasks; A general\nfunction search task and a targeted vulnerability search task. Our approach\ndemonstrates strong performance across both tasks, performing better than all\nbaseline approaches.",
        "translated": ""
    },
    {
        "title": "TPDR: A Novel Two-Step Transformer-based Product and Class Description\n  Match and Retrieval Method",
        "url": "http://arxiv.org/abs/2310.03491v1",
        "pub_date": "2023-10-05",
        "summary": "There is a niche of companies responsible for intermediating the purchase of\nlarge batches of varied products for other companies, for which the main\nchallenge is to perform product description standardization, i.e., matching an\nitem described by a client with a product described in a catalog. The problem\nis complex since the client's product description may be: (1) potentially\nnoisy; (2) short and uninformative (e.g., missing information about model and\nsize); and (3) cross-language. In this paper, we formalize this problem as a\nranking task: given an initial client product specification (query), return the\nmost appropriate standardized descriptions (response). In this paper, we\npropose TPDR, a two-step Transformer-based Product and Class Description\nRetrieval method that is able to explore the semantic correspondence between IS\nand SD, by exploiting attention mechanisms and contrastive learning. First,\nTPDR employs the transformers as two encoders sharing the embedding vector\nspace: one for encoding the IS and another for the SD, in which corresponding\npairs (IS, SD) must be close in the vector space. Closeness is further enforced\nby a contrastive learning mechanism leveraging a specialized loss function.\nTPDR also exploits a (second) re-ranking step based on syntactic features that\nare very important for the exact matching (model, dimension) of certain\nproducts that may have been neglected by the transformers. To evaluate our\nproposal, we consider 11 datasets from a real company, covering different\napplication contexts. Our solution was able to retrieve the correct\nstandardized product before the 5th ranking position in 71% of the cases and\nits correct category in the first position in 80% of the situations. Moreover,\nthe effectiveness gains over purely syntactic or semantic baselines reach up to\n3.7 times, solving cases that none of the approaches in isolation can do by\nthemselves.",
        "translated": ""
    },
    {
        "title": "Personalized Transformer-based Ranking for e-Commerce at Yandex",
        "url": "http://arxiv.org/abs/2310.03481v1",
        "pub_date": "2023-10-05",
        "summary": "Personalizing the user experience with high-quality recommendations based on\nuser activities is vital for e-commerce platforms. This is particularly\nimportant in scenarios where the user's intent is not explicit, such as on the\nhomepage. Recently, personalized embedding-based systems have significantly\nimproved the quality of recommendations and search results in the e-commerce\ndomain. However, most of these works focus on enhancing the retrieval stage.\n  In this paper, we demonstrate that features produced by retrieval-focused\ndeep learning models are sub-optimal for ranking stage in e-commerce\nrecommendations. To address this issue, we propose a two-stage training process\nthat fine-tunes two-tower models to achieve optimal ranking performance. We\nprovide a detailed description of our transformer-based two-tower model\narchitecture, which is specifically designed for personalization in e-commerce.\n  Additionally, we introduce a novel technique for debiasing context in offline\nmodels and report significant improvements in ranking performance when using\nweb-search queries for e-commerce recommendations. Our model has been\nsuccessfully deployed at Yandex and has delivered strong performance in online\nA/B testing.",
        "translated": ""
    },
    {
        "title": "Amazon Books Rating prediction &amp; Recommendation Model",
        "url": "http://arxiv.org/abs/2310.03200v1",
        "pub_date": "2023-10-04",
        "summary": "This paper uses the dataset of Amazon to predict the books ratings listed on\nAmazon website. As part of this project, we predicted the ratings of the books,\nand also built a recommendation cluster. This recommendation cluster provides\nthe recommended books based on the column's values from dataset, for instance,\ncategory, description, author, price, reviews etc. This paper provides a flow\nof handling big data files, data engineering, building models and providing\npredictions. The models predict book ratings column using various PySpark\nMachine Learning APIs. Additionally, we used hyper-parameters and parameters\ntuning. Also, Cross Validation and TrainValidationSplit were used for\ngeneralization. Finally, we performed a comparison between Binary\nClassification and Multiclass Classification in their accuracies. We converted\nour label from multiclass to binary to see if we could find any difference\nbetween the two classifications. As a result, we found out that we get higher\naccuracy in binary classification than in multiclass classification.",
        "translated": ""
    },
    {
        "title": "Impedance Leakage Vulnerability and its Utilization in\n  Reverse-engineering Embedded Software",
        "url": "http://arxiv.org/abs/2310.03175v1",
        "pub_date": "2023-10-04",
        "summary": "Discovering new vulnerabilities and implementing security and privacy\nmeasures are important to protect systems and data against physical attacks.\nOne such vulnerability is impedance, an inherent property of a device that can\nbe exploited to leak information through an unintended side channel, thereby\nposing significant security and privacy risks. Unlike traditional\nvulnerabilities, impedance is often overlooked or narrowly explored, as it is\ntypically treated as a fixed value at a specific frequency in research and\ndesign endeavors. Moreover, impedance has never been explored as a source of\ninformation leakage. This paper demonstrates that the impedance of an embedded\ndevice is not constant and directly relates to the programs executed on the\ndevice. We define this phenomenon as impedance leakage and use this as a side\nchannel to extract software instructions from protected memory. Our experiment\non the ATmega328P microcontroller and the Artix 7 FPGA indicates that the\nimpedance side channel can detect software instructions with 96.1% and 92.6%\naccuracy, respectively. Furthermore, we explore the dual nature of the\nimpedance side channel, highlighting the potential for beneficial purposes and\nthe associated risk of intellectual property theft. Finally, potential\ncountermeasures that specifically address impedance leakage are discussed.",
        "translated": ""
    },
    {
        "title": "Policy-Gradient Training of Language Models for Ranking",
        "url": "http://arxiv.org/abs/2310.04407v1",
        "pub_date": "2023-10-06",
        "summary": "Text retrieval plays a crucial role in incorporating factual knowledge for\ndecision making into language processing pipelines, ranging from chat-based web\nsearch to question answering systems. Current state-of-the-art text retrieval\nmodels leverage pre-trained large language models (LLMs) to achieve competitive\nperformance, but training LLM-based retrievers via typical contrastive losses\nrequires intricate heuristics, including selecting hard negatives and using\nadditional supervision as learning signals. This reliance on heuristics stems\nfrom the fact that the contrastive loss itself is heuristic and does not\ndirectly optimize the downstream metrics of decision quality at the end of the\nprocessing pipeline. To address this issue, we introduce Neural PG-RANK, a\nnovel training algorithm that learns to rank by instantiating a LLM as a\nPlackett-Luce ranking policy. Neural PG-RANK provides a principled method for\nend-to-end training of retrieval models as part of larger decision systems via\npolicy gradient, with little reliance on complex heuristics, and it effectively\nunifies the training objective with downstream decision-making quality. We\nconduct extensive experiments on various text retrieval benchmarks. The results\ndemonstrate that when the training objective aligns with the evaluation setup,\nNeural PG-RANK yields remarkable in-domain performance improvement, with\nsubstantial out-of-domain generalization to some critical datasets employed in\ndownstream question answering tasks.",
        "translated": ""
    },
    {
        "title": "On the Embedding Collapse when Scaling up Recommendation Models",
        "url": "http://arxiv.org/abs/2310.04400v1",
        "pub_date": "2023-10-06",
        "summary": "Recent advances in deep foundation models have led to a promising trend of\ndeveloping large recommendation models to leverage vast amounts of available\ndata. However, we experiment to scale up existing recommendation models and\nobserve that the enlarged models do not improve satisfactorily. In this\ncontext, we investigate the embedding layers of enlarged models and identify a\nphenomenon of embedding collapse, which ultimately hinders scalability, wherein\nthe embedding matrix tends to reside in a low-dimensional subspace. Through\nempirical and theoretical analysis, we demonstrate that the feature interaction\nmodule specific to recommendation models has a two-sided effect. On the one\nhand, the interaction restricts embedding learning when interacting with\ncollapsed embeddings, exacerbating the collapse issue. On the other hand,\nfeature interaction is crucial in mitigating the fitting of spurious features,\nthereby improving scalability. Based on this analysis, we propose a simple yet\neffective multi-embedding design incorporating embedding-set-specific\ninteraction modules to capture diverse patterns and reduce collapse. Extensive\nexperiments demonstrate that this proposed design provides consistent\nscalability for various recommendation models.",
        "translated": ""
    },
    {
        "title": "Workload-aware and Learned Z-Indexes",
        "url": "http://arxiv.org/abs/2310.04268v1",
        "pub_date": "2023-10-06",
        "summary": "In this paper, a learned and workload-aware variant of a Z-index, which\njointly optimizes storage layout and search structures, as a viable solution\nfor the above challenges of spatial indexing. Specifically, we first formulate\na cost function to measure the performance of a Z-index on a dataset for a\nrange-query workload. Then, we optimize the Z-index structure by minimizing the\ncost function through adaptive partitioning and ordering for index\nconstruction. Moreover, we design a novel page-skipping mechanism to improve\nits query performance by reducing access to irrelevant data pages. Our\nextensive experiments show that our index improves range query time by 40% on\naverage over the baselines, while always performing better or comparably to\nstate-of-the-art spatial indexes. Additionally, our index maintains good point\nquery performance while providing favourable construction time and index size\ntradeoffs.",
        "translated": ""
    },
    {
        "title": "Lending Interaction Wings to Recommender Systems with Conversational\n  Agents",
        "url": "http://arxiv.org/abs/2310.04230v1",
        "pub_date": "2023-10-06",
        "summary": "Recommender systems trained on offline historical user behaviors are\nembracing conversational techniques to online query user preference. Unlike\nprior conversational recommendation approaches that systemically combine\nconversational and recommender parts through a reinforcement learning\nframework, we propose CORE, a new offline-training and online-checking paradigm\nthat bridges a COnversational agent and REcommender systems via a unified\nuncertainty minimization framework. It can benefit any recommendation platform\nin a plug-and-play style. Here, CORE treats a recommender system as an offline\nrelevance score estimator to produce an estimated relevance score for each\nitem; while a conversational agent is regarded as an online relevance score\nchecker to check these estimated scores in each session. We define uncertainty\nas the summation of unchecked relevance scores. In this regard, the\nconversational agent acts to minimize uncertainty via querying either\nattributes or items. Based on the uncertainty minimization framework, we derive\nthe expected certainty gain of querying each attribute and item, and develop a\nnovel online decision tree algorithm to decide what to query at each turn.\nExperimental results on 8 industrial datasets show that CORE could be\nseamlessly employed on 9 popular recommendation approaches. We further\ndemonstrate that our conversational agent could communicate as a human if\nempowered by a pre-trained large language model.",
        "translated": ""
    },
    {
        "title": "Keyword Augmented Retrieval: Novel framework for Information Retrieval\n  integrated with speech interface",
        "url": "http://arxiv.org/abs/2310.04205v1",
        "pub_date": "2023-10-06",
        "summary": "Retrieving answers in a quick and low cost manner without hallucinations from\na combination of structured and unstructured data using Language models is a\nmajor hurdle which prevents employment of Language models in knowledge\nretrieval automation. This becomes accentuated when one wants to integrate a\nspeech interface. Besides, for commercial search and chatbot applications,\ncomplete reliance on commercial large language models (LLMs) like GPT 3.5 etc.\ncan be very costly. In this work, authors have addressed this problem by first\ndeveloping a keyword based search framework which augments discovery of the\ncontext to be provided to the large language model. The keywords in turn are\ngenerated by LLM and cached for comparison with keywords generated by LLM\nagainst the query raised. This significantly reduces time and cost to find the\ncontext within documents. Once the context is set, LLM uses that to provide\nanswers based on a prompt tailored for Q&amp;A. This research work demonstrates\nthat use of keywords in context identification reduces the overall inference\ntime and cost of information retrieval. Given this reduction in inference time\nand cost with the keyword augmented retrieval framework, a speech based\ninterface for user input and response readout was integrated. This allowed a\nseamless interaction with the language model.",
        "translated": ""
    },
    {
        "title": "Searching COVID-19 clinical research using graphical abstracts",
        "url": "http://arxiv.org/abs/2310.04094v1",
        "pub_date": "2023-10-06",
        "summary": "Objective. Graphical abstracts are small graphs of concepts that visually\nsummarize the main findings of scientific articles. While graphical abstracts\nare customarily used in scientific publications to anticipate and summarize\ntheir main results, we propose them as a means for expressing graph searches\nover existing literature. Materials and methods. We consider the COVID-19 Open\nResearch Dataset (CORD-19), a corpus of more than one million abstracts; each\nof them is described as a graph of co-occurring ontological terms, selected\nfrom the Unified Medical Language System (UMLS) and the Ontology of Coronavirus\nInfectious Disease (CIDO). Graphical abstracts are also expressed as graphs of\nontological terms, possibly augmented by utility terms describing their\ninteractions (e.g., \"associated with\", \"increases\", \"induces\"). We build a\nco-occurrence network of concepts mentioned in the corpus; we then identify the\nbest matches of graphical abstracts on the network. We exploit graph database\ntechnology and shortest-path queries. Results. We build a large co-occurrence\nnetwork, consisting of 128,249 entities and 47,198,965 relationships. A\nwell-designed interface allows users to explore the network by formulating or\nadapting queries in the form of an abstract; it produces a bibliography of\npublications, globally ranked; each publication is further associated with the\nspecific parts of the abstract that it explains, thereby allowing the user to\nunderstand each aspect of the matching. Discussion and Conclusion. Our approach\nsupports the process of scientific hypothesis formulation and evidence search;\nit can be reapplied to any scientific domain, although our mastering of UMLS\nmakes it most suited to clinical domains.",
        "translated": ""
    },
    {
        "title": "AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term\n  User Engagement",
        "url": "http://arxiv.org/abs/2310.03984v1",
        "pub_date": "2023-10-06",
        "summary": "Growing attention has been paid to Reinforcement Learning (RL) algorithms\nwhen optimizing long-term user engagement in sequential recommendation tasks.\nOne challenge in large-scale online recommendation systems is the constant and\ncomplicated changes in users' behavior patterns, such as interaction rates and\nretention tendencies. When formulated as a Markov Decision Process (MDP), the\ndynamics and reward functions of the recommendation system are continuously\naffected by these changes. Existing RL algorithms for recommendation systems\nwill suffer from distribution shift and struggle to adapt in such an MDP. In\nthis paper, we introduce a novel paradigm called Adaptive Sequential\nRecommendation (AdaRec) to address this issue. AdaRec proposes a new\ndistance-based representation loss to extract latent information from users'\ninteraction trajectories. Such information reflects how RL policy fits to\ncurrent user behavior patterns, and helps the policy to identify subtle changes\nin the recommendation system. To make rapid adaptation to these changes, AdaRec\nencourages exploration with the idea of optimism under uncertainty. The\nexploration is further guarded by zero-order action optimization to ensure\nstable recommendation quality in complicated environments. We conduct extensive\nempirical analyses in both simulator-based and live sequential recommendation\ntasks, where AdaRec exhibits superior long-term performance compared to all\nbaseline algorithms.",
        "translated": ""
    },
    {
        "title": "An Efficient Content-based Time Series Retrieval System",
        "url": "http://arxiv.org/abs/2310.03919v1",
        "pub_date": "2023-10-05",
        "summary": "A Content-based Time Series Retrieval (CTSR) system is an information\nretrieval system for users to interact with time series emerged from multiple\ndomains, such as finance, healthcare, and manufacturing. For example, users\nseeking to learn more about the source of a time series can submit the time\nseries as a query to the CTSR system and retrieve a list of relevant time\nseries with associated metadata. By analyzing the retrieved metadata, users can\ngather more information about the source of the time series. Because the CTSR\nsystem is required to work with time series data from diverse domains, it needs\na high-capacity model to effectively measure the similarity between different\ntime series. On top of that, the model within the CTSR system has to compute\nthe similarity scores in an efficient manner as the users interact with the\nsystem in real-time. In this paper, we propose an effective and efficient CTSR\nmodel that outperforms alternative models, while still providing reasonable\ninference runtimes. To demonstrate the capability of the proposed method in\nsolving business problems, we compare it against alternative models using our\nin-house transaction data. Our findings reveal that the proposed model is the\nmost suitable solution compared to others for our transaction data problem.",
        "translated": ""
    },
    {
        "title": "Living Lab Evaluation for Life and Social Sciences Search Platforms --\n  LiLAS at CLEF 2021",
        "url": "http://arxiv.org/abs/2310.03859v1",
        "pub_date": "2023-10-05",
        "summary": "Meta-evaluation studies of system performances in controlled offline\nevaluation campaigns, like TREC and CLEF, show a need for innovation in\nevaluating IR-systems. The field of academic search is no exception to this.\nThis might be related to the fact that relevance in academic search is\nmultilayered and therefore the aspect of user-centric evaluation is becoming\nmore and more important. The Living Labs for Academic Search (LiLAS) lab aims\nto strengthen the concept of user-centric living labs for the domain of\nacademic search by allowing participants to evaluate their retrieval approaches\nin two real-world academic search systems from the life sciences and the social\nsciences. To this end, we provide participants with metadata on the systems'\ncontent as well as candidate lists with the task to rank the most relevant\ncandidate to the top. Using the STELLA-infrastructure, we allow participants to\neasily integrate their approaches into the real-world systems and provide the\npossibility to compare different approaches at the same time.",
        "translated": ""
    },
    {
        "title": "Accurate Cold-start Bundle Recommendation via Popularity-based\n  Coalescence and Curriculum Heating",
        "url": "http://arxiv.org/abs/2310.03813v1",
        "pub_date": "2023-10-05",
        "summary": "How can we accurately recommend cold-start bundles to users? The cold-start\nproblem in bundle recommendation is critical in practical scenarios since new\nbundles are continuously created for various marketing purposes. Despite its\nimportance, no previous studies have addressed cold-start bundle\nrecommendation. Moreover, existing methods for cold-start item recommendation\noverly rely on historical information, even for unpopular bundles, failing to\ntackle the primary challenge of the highly skewed distribution of bundle\ninteractions. In this work, we propose CoHeat (Popularity-based Coalescence and\nCurriculum Heating), an accurate approach for the cold-start bundle\nrecommendation. CoHeat tackles the highly skewed distribution of bundle\ninteractions by incorporating both historical and affiliation information based\non the bundle's popularity when estimating the user-bundle relationship.\nFurthermore, CoHeat effectively learns latent representations by exploiting\ncurriculum learning and contrastive learning. CoHeat demonstrates superior\nperformance in cold-start bundle recommendation, achieving up to 193% higher\nnDCG@20 compared to the best competitor.",
        "translated": ""
    },
    {
        "title": "Sequential Tag Recommendation",
        "url": "http://arxiv.org/abs/2310.05423v1",
        "pub_date": "2023-10-09",
        "summary": "With the development of Internet technology and the expansion of social\nnetworks, online platforms have become an important way for people to obtain\ninformation. The introduction of tags facilitates information categorization\nand retrieval. Meanwhile, the development of tag recommendation systems not\nonly enables users to input tags more efficiently, but also improves the\nquality of tags. However, current tag recommendation methods only consider the\ncontent of the current post and do not take into account the influence of user\npreferences. Since the main body of tag recommendation is the user, it is very\nnecessary to obtain the user's tagging habits. Therefore, this paper proposes a\ntag recommendation algorithm (MLP4STR) based on the dynamic preference of\nuser's behavioral sequence, which models the user's historical post information\nand historical tag information to obtain the user's dynamic interest changes. A\npure MLP structure across feature dimensions is used in sequence modeling to\nmodel the interaction between tag content and post content to fully extract the\nuser's interests. Finally tag recommendation is performed.",
        "translated": ""
    },
    {
        "title": "Augmented Embeddings for Custom Retrievals",
        "url": "http://arxiv.org/abs/2310.05380v1",
        "pub_date": "2023-10-09",
        "summary": "Information retrieval involves selecting artifacts from a corpus that are\nmost relevant to a given search query. The flavor of retrieval typically used\nin classical applications can be termed as homogeneous and relaxed, where\nqueries and corpus elements are both natural language (NL) utterances\n(homogeneous) and the goal is to pick most relevant elements from the corpus in\nthe Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed).\nRecently, retrieval is being used extensively in preparing prompts for large\nlanguage models (LLMs) to enable LLMs to perform targeted tasks. These new\napplications of retrieval are often heterogeneous and strict -- the queries and\nthe corpus contain different kinds of entities, such as NL and code, and there\nis a need for improving retrieval at Top-K for small values of K, such as K=1\nor 3 or 5. Current dense retrieval techniques based on pretrained embeddings\nprovide a general-purpose and powerful approach for retrieval, but they are\noblivious to task-specific notions of similarity of heterogeneous artifacts. We\nintroduce Adapted Dense Retrieval, a mechanism to transform embeddings to\nenable improved task-specific, heterogeneous and strict retrieval. Adapted\nDense Retrieval works by learning a low-rank residual adaptation of the\npretrained black-box embedding. We empirically validate our approach by showing\nimprovements over the state-of-the-art general-purpose embeddings-based\nbaseline.",
        "translated": ""
    },
    {
        "title": "A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and\n  Locations in the Healthcare Domain",
        "url": "http://arxiv.org/abs/2310.05258v1",
        "pub_date": "2023-10-08",
        "summary": "Efficiently finding doctors and locations is an important search problem for\npatients in the healthcare domain, for which traditional information retrieval\nmethods tend not to work optimally. In the last ten years, knowledge graphs\n(KGs) have emerged as a powerful way to combine the benefits of gleaning\ninsights from semi-structured data using semantic modeling, natural language\nprocessing techniques like information extraction, and robust querying using\nstructured query languages like SPARQL and Cypher. In this short paper, we\npresent a KG-based search engine architecture for robustly finding doctors and\nlocations in the healthcare domain. Early results demonstrate that our approach\ncan lead to significantly higher coverage for complex queries without degrading\nquality.",
        "translated": ""
    },
    {
        "title": "GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient\n  Partially Relevant Video Retrieval",
        "url": "http://arxiv.org/abs/2310.05195v1",
        "pub_date": "2023-10-08",
        "summary": "Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a \\textbf{G}aussian-\\textbf{M}ixture-\\textbf{M}odel based\nTrans\\textbf{former} which models clip representations implicitly. During frame\ninteractions, we incorporate Gaussian-Mixture-Model constraints to focus each\nframe on its adjacent frames instead of the whole video. Then generated\nrepresentations will contain multi-scale clip information, achieving implicit\nclip modeling. In addition, PRVR methods ignore semantic differences between\ntext queries relevant to the same video, leading to a sparse embedding space.\nWe propose a query diverse loss to distinguish these text queries, making the\nembedding space more intensive and contain more semantic information. Extensive\nexperiments on three large-scale video datasets (\\ie, TVR, ActivityNet\nCaptions, and Charades-STA) demonstrate the superiority and efficiency of\nGMMFormer.",
        "translated": ""
    },
    {
        "title": "From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for\n  Conversational Exploratory Search",
        "url": "http://arxiv.org/abs/2310.05150v1",
        "pub_date": "2023-10-08",
        "summary": "Exploratory search is an open-ended information retrieval process that aims\nat discovering knowledge about a topic or domain rather than searching for a\nspecific answer or piece of information. Conversational interfaces are\nparticularly suitable for supporting exploratory search, allowing users to\nrefine queries and examine search results through interactive dialogues. In\naddition to conversational search interfaces, knowledge graphs are also useful\nin supporting information exploration due to their rich semantic representation\nof data items. In this study, we demonstrate the synergistic effects of\ncombining knowledge graphs and conversational interfaces for exploratory\nsearch, bridging the gap between structured and unstructured information\nretrieval. To this end, we propose a knowledge-driven dialogue system for\nexploring news articles by asking natural language questions and using the\ngraph structure to navigate between related topics. Based on a user study with\n54 participants, we empirically evaluate the effectiveness of the graph-based\nexploratory search and discuss design implications for developing such systems.",
        "translated": ""
    },
    {
        "title": "CARLG: Leveraging Contextual Clues and Role Correlations for Improving\n  Document-level Event Argument Extraction",
        "url": "http://arxiv.org/abs/2310.05116v1",
        "pub_date": "2023-10-08",
        "summary": "Document-level event argument extraction (EAE) is a crucial but challenging\nsubtask in information extraction. Most existing approaches focus on the\ninteraction between arguments and event triggers, ignoring two critical points:\nthe information of contextual clues and the semantic correlations among\nargument roles. In this paper, we propose the CARLG model, which consists of\ntwo modules: Contextual Clues Aggregation (CCA) and Role-based Latent\nInformation Guidance (RLIG), effectively leveraging contextual clues and role\ncorrelations for improving document-level EAE. The CCA module adaptively\ncaptures and integrates contextual clues by utilizing context attention weights\nfrom a pre-trained encoder. The RLIG module captures semantic correlations\nthrough role-interactive encoding and provides valuable information guidance\nwith latent role representation. Notably, our CCA and RLIG modules are compact,\ntransplantable and efficient, which introduce no more than 1% new parameters\nand can be easily equipped on other span-base methods with significant\nperformance boost. Extensive experiments on the RAMS, WikiEvents, and MLEE\ndatasets demonstrate the superiority of the proposed CARLG model. It\noutperforms previous state-of-the-art approaches by 1.26 F1, 1.22 F1, and 1.98\nF1, respectively, while reducing the inference time by 31%. Furthermore, we\nprovide detailed experimental analyses based on the performance gains and\nillustrate the interpretability of our model.",
        "translated": ""
    },
    {
        "title": "A framework to generate sparsity-inducing regularizers for enhanced\n  low-rank matrix completion",
        "url": "http://arxiv.org/abs/2310.04954v1",
        "pub_date": "2023-10-08",
        "summary": "Applying half-quadratic optimization to loss functions can yield the\ncorresponding regularizers, while these regularizers are usually not\nsparsity-inducing regularizers (SIRs). To solve this problem, we devise a\nframework to generate an SIR with closed-form proximity operator. Besides, we\nspecify our framework using several commonly-used loss functions, and produce\nthe corresponding SIRs, which are then adopted as nonconvex rank surrogates for\nlow-rank matrix completion. Furthermore, algorithms based on the alternating\ndirection method of multipliers are developed. Extensive numerical results show\nthe effectiveness of our methods in terms of recovery performance and runtime.",
        "translated": ""
    },
    {
        "title": "Commercialized Generative AI: A Critical Study of the Feasibility and\n  Ethics of Generating Native Advertising Using Large Language Models in\n  Conversational Web Search",
        "url": "http://arxiv.org/abs/2310.04892v1",
        "pub_date": "2023-10-07",
        "summary": "How will generative AI pay for itself? Unless charging users for access,\nselling advertising is the only alternative. Especially in the multi-billion\ndollar web search market with ads as the main source of revenue, the\nintroduction of a subscription model seems unlikely. The recent disruption of\nsearch by generative large language models could thus ultimately be accompanied\nby generated ads. Our concern is that the commercialization of generative AI in\ngeneral and large language models in particular could lead to native\nadvertising in the form of quite subtle brand or product placements. In web\nsearch, the evolution of search engine results pages (SERPs) from traditional\nlists of ``ten blue links'' (lists SERPs) to generated text with web page\nreferences (text SERPs) may further blur the line between advertising-based and\norganic search results, making it difficult for users to distinguish between\nthe two, depending on how advertising is integrated and disclosed. To raise\nawareness of this potential development, we conduct a pilot study analyzing the\ncapabilities of current large language models to blend ads with organic search\nresults. Although the models still struggle to subtly frame ads in an unrelated\ncontext, their potential is evident when integrating ads into related topics\nwhich calls for further investigation.",
        "translated": ""
    },
    {
        "title": "Hybrid Recommendation System using Graph Neural Network and BERT\n  Embeddings",
        "url": "http://arxiv.org/abs/2310.04878v1",
        "pub_date": "2023-10-07",
        "summary": "Recommender systems have emerged as a crucial component of the modern web\necosystem. The effectiveness and accuracy of such systems are critical for\nproviding users with personalized recommendations that meet their specific\ninterests and needs. In this paper, we introduce a novel model that utilizes a\nGraph Neural Network (GNN) in conjunction with sentence transformer embeddings\nto predict anime recommendations for different users. Our model employs the\ntask of link prediction to create a recommendation system that considers both\nthe features of anime and user interactions with different anime. The\nhybridization of the GNN and transformer embeddings enables us to capture both\ninter-level and intra-level features of anime data.Our model not only\nrecommends anime to users but also predicts the rating a specific user would\ngive to an anime. We utilize the GraphSAGE network for model building and\nweighted root mean square error (RMSE) to evaluate the performance of the\nmodel. Our approach has the potential to significantly enhance the accuracy and\neffectiveness of anime recommendation systems and can be extended to other\ndomains that require personalized recommendations.",
        "translated": ""
    },
    {
        "title": "ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding",
        "url": "http://arxiv.org/abs/2310.04865v1",
        "pub_date": "2023-10-07",
        "summary": "Developing text mining approaches to mine aspects from customer reviews has\nbeen well-studied due to its importance in understanding customer needs and\nproduct attributes. In contrast, it remains unclear how to predict the future\nemerging aspects of a new product that currently has little review information.\nThis task, which we named product aspect forecasting, is critical for\nrecommending new products, but also challenging because of the missing reviews.\nHere, we propose ForeSeer, a novel textual mining and product embedding\napproach progressively trained on temporal product graphs for this novel\nproduct aspect forecasting task. ForeSeer transfers reviews from similar\nproducts on a large product graph and exploits these reviews to predict aspects\nthat might emerge in future reviews. A key novelty of our method is to jointly\nprovide review, product, and aspect embeddings that are both time-sensitive and\nless affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer\non a real-world product review system containing 11,536,382 reviews and 11,000\nproducts over 3 years. We observe that ForeSeer substantially outperformed\nexisting approaches with at least 49.1\\% AUPRC improvement under the real\nsetting where aspect associations are not given. ForeSeer further improves\nfuture link prediction on the product graph and the review aspect association\nprediction. Collectively, Foreseer offers a novel framework for review\nforecasting by effectively integrating review text, product network, and\ntemporal information, opening up new avenues for online shopping recommendation\nand e-commerce applications.",
        "translated": ""
    },
    {
        "title": "Efficient Retrieval of Images with Irregular Patterns using\n  Morphological Image Analysis: Applications to Industrial and Healthcare\n  datasets",
        "url": "http://arxiv.org/abs/2310.06566v1",
        "pub_date": "2023-10-10",
        "summary": "Image retrieval is the process of searching and retrieving images from a\ndatabase based on their visual content and features. Recently, much attention\nhas been directed towards the retrieval of irregular patterns within industrial\nor medical images by extracting features from the images, such as deep\nfeatures, colour-based features, shape-based features and local features. This\nhas applications across a spectrum of industries, including fault inspection,\ndisease diagnosis, and maintenance prediction. This paper proposes an image\nretrieval framework to search for images containing similar irregular patterns\nby extracting a set of morphological features (DefChars) from images; the\ndatasets employed in this paper contain wind turbine blade images with defects,\nchest computerised tomography scans with COVID-19 infection, heatsink images\nwith defects, and lake ice images. The proposed framework was evaluated with\ndifferent feature extraction methods (DefChars, resized raw image, local binary\npattern, and scale-invariant feature transforms) and distance metrics to\ndetermine the most efficient parameters in terms of retrieval performance\nacross datasets. The retrieval results show that the proposed framework using\nthe DefChars and the Manhattan distance metric achieves a mean average\nprecision of 80% and a low standard deviation of 0.09 across classes of\nirregular patterns, outperforming alternative feature-metric combinations\nacross all datasets. Furthermore, the low standard deviation between each class\nhighlights DefChars' capability for a reliable image retrieval task, even in\nthe presence of class imbalances or small-sized datasets.",
        "translated": ""
    },
    {
        "title": "A Multi-facet Paradigm to Bridge Large Language Model and Recommendation",
        "url": "http://arxiv.org/abs/2310.06491v1",
        "pub_date": "2023-10-10",
        "summary": "Large Language Models (LLMs) have garnered considerable attention in\nrecommender systems. To achieve LLM-based recommendation, item indexing and\ngeneration grounding are two essential steps, bridging between recommendation\nitems and natural language. Item indexing assigns a unique identifier to\nrepresent each item in natural language, and generation grounding grounds the\ngenerated token sequences to in-corpus items. However, previous works suffer\nfrom inherent limitations in the two steps. For item indexing, existing\nID-based identifiers (e.g., numeric IDs) and description-based identifiers\n(e.g., titles) often compromise semantic richness or uniqueness. Moreover,\ngeneration grounding might inadvertently produce out-of-corpus identifiers.\nWorse still, autoregressive generation heavily relies on the initial token's\nquality. To combat these issues, we propose a novel multi-facet paradigm,\nnamely TransRec, to bridge the LLMs to recommendation. Specifically, TransRec\nemploys multi-facet identifiers that incorporate ID, title, and attribute,\nachieving both distinctiveness and semantics. Additionally, we introduce a\nspecialized data structure for TransRec to guarantee the in-corpus identifier\ngeneration and adopt substring indexing to encourage LLMs to generate from any\nposition. We implement TransRec on two backbone LLMs, i.e., BART-large and\nLLaMA-7B. Empirical results on three real-world datasets under diverse settings\n(e.g., full training and few-shot training with warm- and cold-start testings)\nattest to the superiority of TransRec.",
        "translated": ""
    },
    {
        "title": "Topological RANSAC for instance verification and retrieval without\n  fine-tuning",
        "url": "http://arxiv.org/abs/2310.06486v1",
        "pub_date": "2023-10-10",
        "summary": "This paper presents an innovative approach to enhancing explainable image\nretrieval, particularly in situations where a fine-tuning set is unavailable.\nThe widely-used SPatial verification (SP) method, despite its efficacy, relies\non a spatial model and the hypothesis-testing strategy for instance\nrecognition, leading to inherent limitations, including the assumption of\nplanar structures and neglect of topological relations among features. To\naddress these shortcomings, we introduce a pioneering technique that replaces\nthe spatial model with a topological one within the RANSAC process. We propose\nbio-inspired saccade and fovea functions to verify the topological consistency\namong features, effectively circumventing the issues associated with SP's\nspatial model. Our experimental results demonstrate that our method\nsignificantly outperforms SP, achieving state-of-the-art performance in\nnon-fine-tuning retrieval. Furthermore, our approach can enhance performance\nwhen used in conjunction with fine-tuned features. Importantly, our method\nretains high explainability and is lightweight, offering a practical and\nadaptable solution for a variety of real-world applications.",
        "translated": ""
    },
    {
        "title": "Query-dominant User Interest Network for Large-Scale Search Ranking",
        "url": "http://arxiv.org/abs/2310.06444v1",
        "pub_date": "2023-10-10",
        "summary": "Historical behaviors have shown great effect and potential in various\nprediction tasks, including recommendation and information retrieval. The\noverall historical behaviors are various but noisy while search behaviors are\nalways sparse. Most existing approaches in personalized search ranking adopt\nthe sparse search behaviors to learn representation with bottleneck, which do\nnot sufficiently exploit the crucial long-term interest. In fact, there is no\ndoubt that user long-term interest is various but noisy for instant search, and\nhow to exploit it well still remains an open problem.\n  To tackle this problem, in this work, we propose a novel model named\nQuery-dominant user Interest Network (QIN), including two cascade units to\nfilter the raw user behaviors and reweigh the behavior subsequences.\nSpecifically, we propose a relevance search unit (RSU), which aims to search a\nsubsequence relevant to the query first and then search the sub-subsequences\nrelevant to the target item. These items are then fed into an attention unit\ncalled Fused Attention Unit (FAU). It should be able to calculate attention\nscores from the ID field and attribute field separately, and then adaptively\nfuse the item embedding and content embedding based on the user engagement of\npast period. Extensive experiments and ablation studies on real-world datasets\ndemonstrate the superiority of our model over state-of-the-art methods. The QIN\nnow has been successfully deployed on Kuaishou search, an online video search\nplatform, and obtained 7.6% improvement on CTR.",
        "translated": ""
    },
    {
        "title": "Harnessing Administrative Data Inventories to Create a Reliable\n  Transnational Reference Database for Crop Type Monitoring",
        "url": "http://arxiv.org/abs/2310.06393v1",
        "pub_date": "2023-10-10",
        "summary": "With leaps in machine learning techniques and their applicationon Earth\nobservation challenges has unlocked unprecedented performance across the\ndomain. While the further development of these methods was previously limited\nby the availability and volume of sensor data and computing resources, the lack\nof adequate reference data is now constituting new bottlenecks. Since creating\nsuch ground-truth information is an expensive and error-prone task, new ways\nmust be devised to source reliable, high-quality reference data on large\nscales. As an example, we showcase E URO C ROPS, a reference dataset for crop\ntype classification that aggregates and harmonizes administrative data surveyed\nin different countries with the goal of transnational interoperability.",
        "translated": ""
    },
    {
        "title": "P5: Plug-and-Play Persona Prompting for Personalized Response Selection",
        "url": "http://arxiv.org/abs/2310.06390v1",
        "pub_date": "2023-10-10",
        "summary": "The use of persona-grounded retrieval-based chatbots is crucial for\npersonalized conversations, but there are several challenges that need to be\naddressed. 1) In general, collecting persona-grounded corpus is very expensive.\n2) The chatbot system does not always respond in consideration of persona at\nreal applications. To address these challenges, we propose a plug-and-play\npersona prompting method. Our system can function as a standard open-domain\nchatbot if persona information is not available. We demonstrate that this\napproach performs well in the zero-shot setting, which reduces the dependence\non persona-ground training data. This makes it easier to expand the system to\nother languages without the need to build a persona-grounded corpus.\nAdditionally, our model can be fine-tuned for even better performance. In our\nexperiments, the zero-shot model improved the standard model by 7.71 and 1.04\npoints in the original persona and revised persona, respectively. The\nfine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39\npoints in the original persona and revised persona, respectively. To the best\nof our knowledge, this is the first attempt to solve the problem of\npersonalized response selection using prompt sequences. Our code is available\non github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.",
        "translated": ""
    },
    {
        "title": "MuseChat: A Conversational Music Recommendation System for Videos",
        "url": "http://arxiv.org/abs/2310.06282v1",
        "pub_date": "2023-10-10",
        "summary": "We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.",
        "translated": ""
    },
    {
        "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
        "url": "http://arxiv.org/abs/2310.07713v1",
        "pub_date": "2023-10-11",
        "summary": "Pretraining auto-regressive large language models (LLMs) with retrieval\ndemonstrates better perplexity and factual accuracy by leveraging external\ndatabases. However, the size of existing pretrained retrieval-augmented LLM is\nstill limited (e.g., Retro has 7.5B parameters), which limits the effectiveness\nof instruction tuning and zero-shot generalization. In this work, we introduce\nRetro 48B, the largest LLM pretrained with retrieval before instruction tuning.\nSpecifically, we continue to pretrain the 43B GPT model on additional 100\nbillion tokens using the Retro augmentation method by retrieving from 1.2\ntrillion tokens. The obtained foundation model, Retro 48B, largely outperforms\nthe original 43B GPT in terms of perplexity. After instruction tuning on Retro,\nInstructRetro demonstrates significant improvement over the instruction tuned\nGPT on zero-shot question answering (QA) tasks. Specifically, the average\nimprovement of InstructRetro is 7% over its GPT counterpart across 8 short-form\nQA tasks, and 10% over GPT across 4 challenging long-form QA tasks.\nSurprisingly, we find that one can ablate the encoder from InstructRetro\narchitecture and directly use its decoder backbone, while achieving comparable\nresults. We hypothesize that pretraining with retrieval makes its decoder good\nat incorporating context for QA. Our results highlights the promising direction\nto obtain a better GPT decoder for QA through continued pretraining with\nretrieval before instruction tuning.",
        "translated": ""
    },
    {
        "title": "Retrieve Anything To Augment Large Language Models",
        "url": "http://arxiv.org/abs/2310.07554v1",
        "pub_date": "2023-10-11",
        "summary": "Large language models (LLMs) face significant challenges stemming from the\ninherent limitations in knowledge, memory, alignment, and action. These\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\nfrom the external world, such as knowledge base, memory store, demonstration\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\nbridging the gap between LLMs and the external assistance. However,\nconventional methods encounter two pressing issues. On one hand, the\ngeneral-purpose retrievers are not properly optimized for the retrieval\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\nrequired versatility, hindering their performance across the diverse retrieval\naugmentation scenarios.\n  In this work, we present a novel approach, the LLM Embedder, which\ncomprehensively support the diverse needs of LLMs' retrieval augmentation with\none unified embedding model. Training such an unified model is non-trivial, as\nvarious retrieval tasks aim to capture distinct semantic relationships, often\nsubject to mutual interference. To address this challenge, we systematically\noptimize our training methodology. This includes reward formulation based on\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\nfine-tuning with explicit instructions, and the use of homogeneous in-batch\nnegative sampling. These optimization strategies contribute to the outstanding\nempirical performance of the LLM-Embedder. Notably, it yields remarkable\nenhancements in retrieval augmentation for LLMs, surpassing both\ngeneral-purpose and task-specific retrievers in various evaluation scenarios.\nThis project is made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.",
        "translated": ""
    },
    {
        "title": "GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized\n  Adaptive Testing",
        "url": "http://arxiv.org/abs/2310.07477v1",
        "pub_date": "2023-10-11",
        "summary": "Computerized Adaptive Testing(CAT) refers to an online system that adaptively\nselects the best-suited question for students with various abilities based on\ntheir historical response records. Most CAT methods only focus on the quality\nobjective of predicting the student ability accurately, but neglect concept\ndiversity or question exposure control, which are important considerations in\nensuring the performance and validity of CAT. Besides, the students' response\nrecords contain valuable relational information between questions and knowledge\nconcepts. The previous methods ignore this relational information, resulting in\nthe selection of sub-optimal test questions. To address these challenges, we\npropose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly,\nthree objectives, namely quality, diversity and novelty, are introduced into\nthe Scalarized Multi-Objective Reinforcement Learning framework of CAT, which\nrespectively correspond to improving the prediction accuracy, increasing the\nconcept diversity and reducing the question exposure. We use an Actor-Critic\nRecommender to select questions and optimize three objectives simultaneously by\nthe scalarization function. Secondly, we utilize the graph neural network to\nlearn relation-aware embeddings of questions and concepts. These embeddings are\nable to aggregate neighborhood information in the relation graphs between\nquestions and concepts. We conduct experiments on three real-world educational\ndatasets, and show that GMOCAT not only outperforms the state-of-the-art\nmethods in the ability prediction, but also achieve superior performance in\nimproving the concept diversity and alleviating the question exposure. Our code\nis available at https://github.com/justarter/GMOCAT.",
        "translated": ""
    },
    {
        "title": "Preliminary Results of a Scientometric Analysis of the German\n  Information Retrieval Community 2020-2023",
        "url": "http://arxiv.org/abs/2310.07346v1",
        "pub_date": "2023-10-11",
        "summary": "The German Information Retrieval community is located in two different\nsub-fields: Information and computer science. There are no current studies that\ninvestigate these communities on a scientometric level. Available studies only\nfocus on the information scientific part of the community. We generated a data\nset of 401 recent IR-related publications extracted from six core IR\nconferences from a mainly computer scientific background. We analyze this data\nset at the institutional and researcher level. The data set is publicly\nreleased, and we also demonstrate a mapping use case.",
        "translated": ""
    },
    {
        "title": "A Completely Locale-independent Session-based Recommender System by\n  Leveraging Trained Model",
        "url": "http://arxiv.org/abs/2310.07281v1",
        "pub_date": "2023-10-11",
        "summary": "In this paper, we propose a solution that won the 10th prize in the KDD Cup\n2023 Challenge Task 2 (Next Product Recommendation for Underrepresented\nLanguages/Locales). Our approach involves two steps: (i) Identify candidate\nitem sets based on co-visitation, and (ii) Re-ranking the items using LightGBM\nwith locale-independent features, including session-based features and product\nsimilarity. The experiment demonstrated that the locale-independent model\nperformed consistently well across different test locales, and performed even\nbetter when incorporating data from other locales into the training.",
        "translated": ""
    },
    {
        "title": "Validating Synthetic Usage Data in Living Lab Environments",
        "url": "http://arxiv.org/abs/2310.07142v1",
        "pub_date": "2023-10-11",
        "summary": "Evaluating retrieval performance without editorial relevance judgments is\nchallenging, but instead, user interactions can be used as relevance signals.\nLiving labs offer a way for small-scale platforms to validate information\nretrieval systems with real users. If enough user interaction data are\navailable, click models can be parameterized from historical sessions to\nevaluate systems before exposing users to experimental rankings. However,\ninteraction data are sparse in living labs, and little is studied about how\nclick models can be validated for reliable user simulations when click data are\navailable in moderate amounts.\n  This work introduces an evaluation approach for validating synthetic usage\ndata generated by click models in data-sparse human-in-the-loop environments\nlike living labs. We ground our methodology on the click model's estimates\nabout a system ranking compared to a reference ranking for which the relative\nperformance is known. Our experiments compare different click models and their\nreliability and robustness as more session log data becomes available. In our\nsetup, simple click models can reliably determine the relative system\nperformance with already 20 logged sessions for 50 queries. In contrast, more\ncomplex click models require more session data for reliable estimates, but they\nare a better choice in simulated interleaving experiments when enough session\ndata are available. While it is easier for click models to distinguish between\nmore diverse systems, it is harder to reproduce the system ranking based on the\nsame retrieval algorithm with different interpolation weights. Our setup is\nentirely open, and we share the code to reproduce the experiments.",
        "translated": ""
    },
    {
        "title": "AE-smnsMLC: Multi-Label Classification with Semantic Matching and\n  Negative Label Sampling for Product Attribute Value Extraction",
        "url": "http://arxiv.org/abs/2310.07137v1",
        "pub_date": "2023-10-11",
        "summary": "Product attribute value extraction plays an important role for many\nreal-world applications in e-Commerce such as product search and\nrecommendation. Previous methods treat it as a sequence labeling task that\nneeds more annotation for position of values in the product text. This limits\ntheir application to real-world scenario in which only attribute values are\nweakly-annotated for each product without their position. Moreover, these\nmethods only use product text (i.e., product title and description) and do not\nconsider the semantic connection between the multiple attribute values of a\ngiven product and its text, which can help attribute value extraction. In this\npaper, we reformulate this task as a multi-label classification task that can\nbe applied for real-world scenario in which only annotation of attribute values\nis available to train models (i.e., annotation of positional information of\nattribute values is not available). We propose a classification model with\nsemantic matching and negative label sampling for attribute value extraction.\nSemantic matching aims to capture semantic interactions between attribute\nvalues of a given product and its text. Negative label sampling aims to enhance\nthe model's ability of distinguishing similar values belonging to the same\nattribute. Experimental results on three subsets of a large real-world\ne-Commerce dataset demonstrate the effectiveness and superiority of our\nproposed model.",
        "translated": ""
    },
    {
        "title": "Answer Candidate Type Selection: Text-to-Text Language Model for Closed\n  Book Question Answering Meets Knowledge Graphs",
        "url": "http://arxiv.org/abs/2310.07008v1",
        "pub_date": "2023-10-10",
        "summary": "Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield\npromising results in the Knowledge Graph Question Answering (KGQA) task.\nHowever, the capacity of the models is limited and the quality decreases for\nquestions with less popular entities. In this paper, we present a novel\napproach which works on top of the pre-trained Text-to-Text QA system to\naddress this issue. Our simple yet effective method performs filtering and\nre-ranking of generated candidates based on their types derived from Wikidata\n\"instance_of\" property.",
        "translated": ""
    },
    {
        "title": "A Comparative Study of Transformer-based Neural Text Representation\n  Techniques on Bug Triaging",
        "url": "http://arxiv.org/abs/2310.06913v1",
        "pub_date": "2023-10-10",
        "summary": "Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n  Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.",
        "translated": ""
    },
    {
        "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval",
        "url": "http://arxiv.org/abs/2310.08319v1",
        "pub_date": "2023-10-12",
        "summary": "The effectiveness of multi-stage text retrieval has been solidly demonstrated\nsince before the era of pre-trained language models. However, most existing\nstudies utilize models that predate recent advances in large language models\n(LLMs). This study seeks to explore potential improvements that\nstate-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning\nthe latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise\nreranker (RankLLaMA) for both passage retrieval and document retrieval using\nthe MS MARCO datasets. Our findings demonstrate that the effectiveness of large\nlanguage models indeed surpasses that of smaller models. Additionally, since\nLLMs can inherently handle longer contexts, they can represent entire documents\nholistically, obviating the need for traditional segmenting and pooling\nstrategies. Furthermore, evaluations on BEIR demonstrate that our\nRepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model\ncheckpoints from this study are available on HuggingFace.",
        "translated": ""
    },
    {
        "title": "On Using GUI Interaction Data to Improve Text Retrieval-based Bug\n  Localization",
        "url": "http://arxiv.org/abs/2310.08083v1",
        "pub_date": "2023-10-12",
        "summary": "One of the most important tasks related to managing bug reports is localizing\nthe fault so that a fix can be applied. As such, prior work has aimed to\nautomate this task of bug localization by formulating it as an information\nretrieval problem, where potentially buggy files are retrieved and ranked\naccording to their textual similarity with a given bug report. However, there\nis often a notable semantic gap between the information contained in bug\nreports and identifiers or natural language contained within source code files.\nFor user-facing software, there is currently a key source of information that\ncould aid in bug localization, but has not been thoroughly investigated -\ninformation from the GUI.\n  We investigate the hypothesis that, for end user-facing applications,\nconnecting information in a bug report with information from the GUI, and using\nthis to aid in retrieving potentially buggy files, can improve upon existing\ntechniques for bug localization. To examine this phenomenon, we conduct a\ncomprehensive empirical study that augments four baseline techniques for bug\nlocalization with GUI interaction information from a reproduction scenario to\n(i) filter out potentially irrelevant files, (ii) boost potentially relevant\nfiles, and (iii) reformulate text-retrieval queries. To carry out our study, we\nsource the current largest dataset of fully-localized and reproducible real\nbugs for Android apps, with corresponding bug reports, consisting of 80 bug\nreports from 39 popular open-source apps. Our results illustrate that\naugmenting traditional techniques with GUI information leads to a marked\nincrease in effectiveness across multiple metrics, including a relative\nincrease in Hits@10 of 13-18%. Additionally, through further analysis, we find\nthat our studied augmentations largely complement existing techniques.",
        "translated": ""
    },
    {
        "title": "Rethinking Negative Pairs in Code Search",
        "url": "http://arxiv.org/abs/2310.08069v1",
        "pub_date": "2023-10-12",
        "summary": "Recently, contrastive learning has become a key component in fine-tuning code\nsearch models for software development efficiency and effectiveness. It pulls\ntogether positive code snippets while pushing negative samples away given\nsearch queries. Among contrastive learning, InfoNCE is the most widely used\nloss function due to its better performance. However, the following problems in\nnegative samples of InfoNCE may deteriorate its representation learning: 1) The\nexistence of false negative samples in large code corpora due to duplications.\n2). The failure to explicitly differentiate between the potential relevance of\nnegative samples. As an example, a bubble sorting algorithm example is less\n``negative'' than a file saving function for the quick sorting algorithm query.\nIn this paper, we tackle the above problems by proposing a simple yet effective\nSoft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss\nfunction, we apply three methods to estimate the weights of negative pairs and\nshow that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.\nTheoretically, we analyze the effects of Soft-InfoNCE on controlling the\ndistribution of learnt code representations and on deducing a more precise\nmutual information estimation. We furthermore discuss the superiority of\nproposed loss functions with other design alternatives. Extensive experiments\ndemonstrate the effectiveness of Soft-InfoNCE and weights estimation methods\nunder state-of-the-art code search models on a large-scale public dataset\nconsisting of six programming languages. Source code is available at\n\\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.",
        "translated": ""
    },
    {
        "title": "Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain\n  Models",
        "url": "http://arxiv.org/abs/2310.08039v1",
        "pub_date": "2023-10-12",
        "summary": "Industrial systems such as recommender systems and online advertising, have\nbeen widely equipped with multi-stage architectures, which are divided into\nseveral cascaded modules, including matching, pre-ranking, ranking and\nre-ranking. As a critical bridge between matching and ranking, existing\npre-ranking approaches mainly endure sample selection bias (SSB) problem owing\nto ignoring the entire-chain data dependence, resulting in sub-optimal\nperformances. In this paper, we rethink pre-ranking system from the perspective\nof the entire sample space, and propose Entire-chain Cross-domain Models (ECM),\nwhich leverage samples from the whole cascaded stages to effectively alleviate\nSSB problem. Besides, we design a fine-grained neural structure named ECMM to\nfurther improve the pre-ranking accuracy. Specifically, we propose a\ncross-domain multi-tower neural network to comprehensively predict for each\nstage result, and introduce the sub-networking routing strategy with $L0$\nregularization to reduce computational costs. Evaluations on real-world\nlarge-scale traffic logs demonstrate that our pre-ranking models outperform\nSOTA methods while time consumption is maintained within an acceptable level,\nwhich achieves better trade-off between efficiency and effectiveness.",
        "translated": ""
    },
    {
        "title": "Continual Learning via Manifold Expansion Replay",
        "url": "http://arxiv.org/abs/2310.08038v1",
        "pub_date": "2023-10-12",
        "summary": "In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.",
        "translated": ""
    },
    {
        "title": "Multi-View Variational Autoencoder for Missing Value Imputation in\n  Untargeted Metabolomics",
        "url": "http://arxiv.org/abs/2310.07990v1",
        "pub_date": "2023-10-12",
        "summary": "Background: Missing data is a common challenge in mass spectrometry-based\nmetabolomics, which can lead to biased and incomplete analyses. The integration\nof whole-genome sequencing (WGS) data with metabolomics data has emerged as a\npromising approach to enhance the accuracy of data imputation in metabolomics\nstudies. Method: In this study, we propose a novel method that leverages the\ninformation from WGS data and reference metabolites to impute unknown\nmetabolites. Our approach utilizes a multi-view variational autoencoder to\njointly model the burden score, polygenetic risk score (PGS), and linkage\ndisequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature\nextraction and missing metabolomics data imputation. By learning the latent\nrepresentations of both omics data, our method can effectively impute missing\nmetabolomics values based on genomic information. Results: We evaluate the\nperformance of our method on empirical metabolomics datasets with missing\nvalues and demonstrate its superiority compared to conventional imputation\ntechniques. Using 35 template metabolites derived burden scores, PGS and\nLD-pruned SNPs, the proposed methods achieved r2-scores &gt; 0.01 for 71.55% of\nmetabolites. Conclusion: The integration of WGS data in metabolomics imputation\nnot only improves data completeness but also enhances downstream analyses,\npaving the way for more comprehensive and accurate investigations of metabolic\npathways and disease associations. Our findings offer valuable insights into\nthe potential benefits of utilizing WGS data for metabolomics data imputation\nand underscore the importance of leveraging multi-modal data integration in\nprecision medicine research.",
        "translated": ""
    },
    {
        "title": "Refined Mechanism Design for Approximately Structured Priors via Active\n  Regression",
        "url": "http://arxiv.org/abs/2310.07874v1",
        "pub_date": "2023-10-11",
        "summary": "We consider the problem of a revenue-maximizing seller with a large number of\nitems $m$ for sale to $n$ strategic bidders, whose valuations are drawn\nindependently from high-dimensional, unknown prior distributions. It is\nwell-known that optimal and even approximately-optimal mechanisms for this\nsetting are notoriously difficult to characterize or compute, and, even when\nthey can be found, are often rife with various counter-intuitive properties. In\nthis paper, following a model introduced recently by Cai and\nDaskalakis~\\cite{cai2022recommender}, we consider the case that bidders' prior\ndistributions can be well-approximated by a topic model. We design an active\nlearning component, responsible for interacting with the bidders and outputting\nlow-dimensional approximations of their types, and a mechanism design\ncomponent, responsible for robustifying mechanisms for the low-dimensional\nmodel to work for the approximate types of the former component. On the active\nlearning front, we cast our problem in the framework of Randomized Linear\nAlgebra (RLA) for regression problems, allowing us to import several\nbreakthrough results from that line of research, and adapt them to our setting.\nOn the mechanism design front, we remove many restrictive assumptions of prior\nwork on the type of access needed to the underlying distributions and the\nassociated mechanisms. To the best of our knowledge, our work is the first to\nformulate connections between mechanism design, and RLA for active learning of\nregression problems, opening the door for further applications of randomized\nlinear algebra primitives to mechanism design.",
        "translated": ""
    },
    {
        "title": "Language Models As Semantic Indexers",
        "url": "http://arxiv.org/abs/2310.07815v1",
        "pub_date": "2023-10-11",
        "summary": "Semantic identifier (ID) is an important concept in information retrieval\nthat aims to preserve the semantics of objects such as documents and items\ninside their IDs. Previous studies typically adopt a two-stage pipeline to\nlearn semantic IDs by first procuring embeddings using off-the-shelf text\nencoders and then deriving IDs based on the embeddings. However, each step\nintroduces potential information loss and there is usually an inherent mismatch\nbetween the distribution of embeddings within the latent space produced by text\nencoders and the anticipated distribution required for semantic indexing.\nNevertheless, it is non-trivial to design a method that can learn the\ndocument's semantic representations and its hierarchical structure\nsimultaneously, given that semantic IDs are discrete and sequentially\nstructured, and the semantic supervision is deficient. In this paper, we\nintroduce LMINDEXER, a self-supervised framework to learn semantic IDs with a\ngenerative language model. We tackle the challenge of sequential discrete ID by\nintroducing a semantic indexer capable of generating neural sequential discrete\nrepresentations with progressive training and contrastive learning. In response\nto the semantic supervision deficiency, we propose to train the model with a\nself-supervised document reconstruction objective. The learned semantic indexer\ncan facilitate various downstream tasks, such as recommendation and retrieval.\nWe conduct experiments on three tasks including recommendation, product search,\nand document retrieval on five datasets from various domains, where LMINDEXER\noutperforms competitive baselines significantly and consistently.",
        "translated": ""
    },
    {
        "title": "Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble\n  Sampling",
        "url": "http://arxiv.org/abs/2310.07786v1",
        "pub_date": "2023-10-11",
        "summary": "Real-world applications of contextual bandits often exhibit non-stationarity\ndue to seasonality, serendipity, and evolving social trends. While a number of\nnon-stationary contextual bandit learning algorithms have been proposed in the\nliterature, they excessively explore due to a lack of prioritization for\ninformation of enduring value, or are designed in ways that do not scale in\nmodern applications with high-dimensional user-specific features and large\naction set, or both. In this paper, we introduce a novel non-stationary\ncontextual bandit algorithm that addresses these concerns. It combines a\nscalable, deep-neural-network-based architecture with a carefully designed\nexploration mechanism that strategically prioritizes collecting information\nwith the most lasting value in a non-stationary environment. Through empirical\nevaluations on two real-world recommendation datasets, which exhibit pronounced\nnon-stationarity, we demonstrate that our approach significantly outperforms\nthe state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "ClickPrompt: CTR Models are Strong Prompt Generators for Adapting\n  Language Models to CTR Prediction",
        "url": "http://arxiv.org/abs/2310.09234v1",
        "pub_date": "2023-10-13",
        "summary": "Click-through rate (CTR) prediction has become increasingly indispensable for\nvarious Internet applications. Traditional CTR models convert the multi-field\ncategorical data into ID features via one-hot encoding, and extract the\ncollaborative signals among features. Such a paradigm suffers from the problem\nof semantic information loss. Another line of research explores the potential\nof pretrained language models (PLMs) for CTR prediction by converting input\ndata into textual sentences through hard prompt templates. Although semantic\nsignals are preserved, they generally fail to capture the collaborative\ninformation (e.g., feature interactions, pure ID features), not to mention the\nunacceptable inference overhead brought by the huge model size. In this paper,\nwe aim to model both the semantic knowledge and collaborative knowledge for\naccurate CTR estimation, and meanwhile address the inference inefficiency\nissue. To benefit from both worlds and close their gaps, we propose a novel\nmodel-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models\nto generate interaction-aware soft prompts for PLMs. We design a\nprompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM\nhas to recover the masked tokens based on the language context, as well as the\nsoft prompts generated by CTR model. The collaborative and semantic knowledge\nfrom ID and textual features would be explicitly aligned and interacted via the\nprompt interface. Then, we can either tune the CTR model with PLM for superior\nperformance, or solely tune the CTR model without PLM for inference efficiency.\nExperiments on four real-world datasets validate the effectiveness of\nClickPrompt compared with existing baselines.",
        "translated": ""
    },
    {
        "title": "AgentCF: Collaborative Learning with Autonomous Language Agents for\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2310.09233v1",
        "pub_date": "2023-10-13",
        "summary": "Recently, there has been an emergence of employing LLM-powered agents as\nbelievable human proxies, based on their remarkable decision-making capability.\nHowever, existing studies mainly focus on simulating human dialogue. Human\nnon-verbal behaviors, such as item clicking in recommender systems, although\nimplicitly exhibiting user preferences and could enhance the modeling of users,\nhave not been deeply explored. The main reasons lie in the gap between language\nmodeling and behavior modeling, as well as the incomprehension of LLMs about\nuser-item relations.\n  To address this issue, we propose AgentCF for simulating user-item\ninteractions in recommender systems through agent-based collaborative\nfiltering. We creatively consider not only users but also items as agents, and\ndevelop a collaborative learning approach that optimizes both kinds of agents\ntogether. Specifically, at each time step, we first prompt the user and item\nagents to interact autonomously. Then, based on the disparities between the\nagents' decisions and real-world interaction records, user and item agents are\nprompted to reflect on and adjust the misleading simulations collaboratively,\nthereby modeling their two-sided relations. The optimized agents can also\npropagate their preferences to other agents in subsequent interactions,\nimplicitly capturing the collaborative filtering idea. Overall, the optimized\nagents exhibit diverse interaction behaviors within our framework, including\nuser-item, user-user, item-item, and collective interactions. The results show\nthat these agents can demonstrate personalized behaviors akin to those of\nreal-world individuals, sparking the development of next-generation user\nbehavior simulation.",
        "translated": ""
    },
    {
        "title": "EHI: End-to-end Learning of Hierarchical Index for Efficient Dense\n  Retrieval",
        "url": "http://arxiv.org/abs/2310.08891v1",
        "pub_date": "2023-10-13",
        "summary": "Dense embedding-based retrieval is now the industry standard for semantic\nsearch and ranking problems, like obtaining relevant web documents for a given\nquery. Such techniques use a two-stage process: (a) contrastive learning to\ntrain a dual encoder to embed both the query and documents and (b) approximate\nnearest neighbor search (ANNS) for finding similar documents for a given query.\nThese two stages are disjoint; the learned embeddings might be ill-suited for\nthe ANNS method and vice-versa, leading to suboptimal performance. In this\nwork, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns\nboth the embeddings and the ANNS structure to optimize retrieval performance.\nEHI uses a standard dual encoder model for embedding queries and documents\nwhile learning an inverted file index (IVF) style tree structure for efficient\nANNS. To ensure stable and efficient learning of discrete tree-based ANNS\nstructure, EHI introduces the notion of dense path embedding that captures the\nposition of a query/document in the tree. We demonstrate the effectiveness of\nEHI on several benchmarks, including de-facto industry standard MS MARCO (Dev\nset and TREC DL19) datasets. For example, with the same compute budget, EHI\noutperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and\nby 4.2% (nDCG@10) on TREC DL19 benchmarks.",
        "translated": ""
    },
    {
        "title": "Question Answering for Electronic Health Records: A Scoping Review of\n  datasets and models",
        "url": "http://arxiv.org/abs/2310.08759v1",
        "pub_date": "2023-10-12",
        "summary": "Question Answering (QA) systems on patient-related data can assist both\nclinicians and patients. They can, for example, assist clinicians in\ndecision-making and enable patients to have a better understanding of their\nmedical history. Significant amounts of patient data are stored in Electronic\nHealth Records (EHRs), making EHR QA an important research area. In EHR QA, the\nanswer is obtained from the medical record of the patient. Because of the\ndifferences in data format and modality, this differs greatly from other\nmedical QA tasks that employ medical websites or scientific papers to retrieve\nanswers, making it critical to research EHR question answering. This study\naimed to provide a methodological review of existing works on QA over EHRs. We\nsearched for articles from January 1st, 2005 to September 30th, 2023 in four\ndigital sources including Google Scholar, ACL Anthology, ACM Digital Library,\nand PubMed to collect relevant publications on EHR QA. 4111 papers were\nidentified for our study, and after screening based on our inclusion criteria,\nwe obtained a total of 47 papers for further study. Out of the 47 papers, 25\npapers were about EHR QA datasets, and 37 papers were about EHR QA models. It\nwas observed that QA on EHRs is relatively new and unexplored. Most of the\nworks are fairly recent. Also, it was observed that emrQA is by far the most\npopular EHR QA dataset, both in terms of citations and usage in other papers.\nFurthermore, we identified the different models used in EHR QA along with the\nevaluation metrics used for these models.",
        "translated": ""
    },
    {
        "title": "Individual Variation Affects Outbreak Magnitude and Predictability in an\n  Extended Multi-Pathogen SIR Model of Pigeons Vising Dairy Farms",
        "url": "http://arxiv.org/abs/2310.08613v1",
        "pub_date": "2023-10-12",
        "summary": "Zoonotic disease transmission between animals and humans is a growing risk\nand the agricultural context acts as a likely point of transition, with\nindividual heterogeneity acting as an important contributor. Thus,\nunderstanding the dynamics of disease spread in the wildlife-livestock\ninterface is crucial for mitigating these risks of transmission. Specifically,\nthe interactions between pigeons and in-door cows at dairy farms can lead to\nsignificant disease transmission and economic losses for farmers; putting\nlivestock, adjacent human populations, and other wildlife species at risk. In\nthis paper, we propose a novel spatio-temporal multi-pathogen model with\ncontinuous spatial movement. The model expands on the\nSusceptible-Exposed-Infected-Recovered-Dead (SEIRD) framework and accounts for\nboth within-species and cross-species transmission of pathogens, as well as the\nexploration-exploitation movement dynamics of pigeons, which play a critical\nrole in the spread of infection agents. In addition to model formulation, we\nalso implement it as an agent-based simulation approach and use empirical field\ndata to investigate different biologically realistic scenarios, evaluating the\neffect of various parameters on the epidemic spread. Namely, in agreement with\ntheoretical expectations, the model predicts that the heterogeneity of the\npigeons' movement dynamics can drastically affect both the magnitude and\nstability of outbreaks. In addition, joint infection by multiple pathogens can\nhave an interactive effect unobservable in single-pathogen SIR models,\nreflecting a non-intuitive inhibition of the outbreak. Our findings highlight\nthe impact of heterogeneity in host behavior on their pathogens and allow\nrealistic predictions of outbreak dynamics in the multi-pathogen\nwildlife-livestock interface with consequences to zoonotic diseases in various\nsystems.",
        "translated": ""
    },
    {
        "title": "Type-aware Decoding via Explicitly Aggregating Event Information for\n  Document-level Event Extraction",
        "url": "http://arxiv.org/abs/2310.10487v1",
        "pub_date": "2023-10-16",
        "summary": "Document-level event extraction (DEE) faces two main challenges:\narguments-scattering and multi-event. Although previous methods attempt to\naddress these challenges, they overlook the interference of event-unrelated\nsentences during event detection and neglect the mutual interference of\ndifferent event roles during argument extraction. Therefore, this paper\nproposes a novel Schema-based Explicitly Aggregating~(SEA) model to address\nthese limitations. SEA aggregates event information into event type and role\nrepresentations, enabling the decoding of event records based on specific\ntype-aware representations. By detecting each event based on its event type\nrepresentation, SEA mitigates the interference caused by event-unrelated\ninformation. Furthermore, SEA extracts arguments for each role based on its\nrole-aware representations, reducing mutual interference between different\nroles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEA\noutperforms the SOTA methods.",
        "translated": ""
    },
    {
        "title": "DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource\n  Event Extraction",
        "url": "http://arxiv.org/abs/2310.10481v1",
        "pub_date": "2023-10-16",
        "summary": "Most current Event Extraction (EE) methods focus on the high-resource\nscenario, which requires a large amount of annotated data and can hardly be\napplied to low-resource domains. To address EE more effectively with limited\nresources, we propose the Demonstration-enhanced Schema-guided Generation\n(DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we\npropose the demonstration-based learning paradigm for EE to fully use the\nannotated data, which transforms them into demonstrations to illustrate the\nextraction process and help the model learn effectively. Secondly, we formulate\nEE as a natural language generation task guided by schema-based prompts,\nthereby leveraging label semantics and promoting knowledge transfer in\nlow-resource scenarios. We conduct extensive experiments under in-domain and\ndomain adaptation low-resource settings on three datasets, and study the\nrobustness of DemoSG. The results show that DemoSG significantly outperforms\ncurrent methods in low-resource scenarios.",
        "translated": ""
    },
    {
        "title": "BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework\n  for Music-Dance Retrieval",
        "url": "http://arxiv.org/abs/2310.10300v1",
        "pub_date": "2023-10-16",
        "summary": "Dance and music are closely related forms of expression, with mutual\nretrieval between dance videos and music being a fundamental task in various\nfields like education, art, and sports. However, existing methods often suffer\nfrom unnatural generation effects or fail to fully explore the correlation\nbetween music and dance. To overcome these challenges, we propose BeatDance, a\nnovel beat-based model-agnostic contrastive learning framework. BeatDance\nincorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat\nBlender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval\nperformance by utilizing the alignment between music beats and dance movements.\nWe also introduce the Music-Dance (MD) dataset, a large-scale collection of\nover 10,000 music-dance video pairs for training and testing. Experimental\nresults on the MD dataset demonstrate the superiority of our method over\nexisting baselines, achieving state-of-the-art performance. The code and\ndataset will be made public available upon acceptance.",
        "translated": ""
    },
    {
        "title": "Rethinking Financial Service Promotion With Hybrid Recommender Systems\n  at PicPay",
        "url": "http://arxiv.org/abs/2310.10268v1",
        "pub_date": "2023-10-16",
        "summary": "The fintech PicPay offers a wide range of financial services to its 30\nmillion monthly active users, with more than 50 thousand items recommended in\nthe PicPay mobile app. In this scenario, promoting specific items that are\nstrategic to the company can be very challenging. In this work, we present a\nSwitching Hybrid Recommender System that combines two algorithms to effectively\npromote items without negatively impacting the user's experience. The results\nof our A/B tests show an uplift of up to 3.2\\% when compared to a default\nrecommendation strategy.",
        "translated": ""
    },
    {
        "title": "An Interpretable Deep-Learning Framework for Predicting Hospital\n  Readmissions From Electronic Health Records",
        "url": "http://arxiv.org/abs/2310.10187v1",
        "pub_date": "2023-10-16",
        "summary": "With the increasing availability of patients' data, modern medicine is\nshifting towards prospective healthcare. Electronic health records contain a\nvariety of information useful for clinical patient description and can be\nexploited for the construction of predictive models, given that similar medical\nhistories will likely lead to similar progressions. One example is unplanned\nhospital readmission prediction, an essential task for reducing hospital costs\nand improving patient health. Despite predictive models showing very good\nperformances especially with deep-learning models, they are often criticized\nfor the poor interpretability of their results, a fundamental characteristic in\nthe medical field, where incorrect predictions might have serious consequences\nfor the patient health. In this paper we propose a novel, interpretable\ndeep-learning framework for predicting unplanned hospital readmissions,\nsupported by NLP findings on word embeddings and by neural-network models\n(ConvLSTM) for better handling temporal data. We validate our system on the two\npredictive tasks of hospital readmission within 30 and 180 days, using\nreal-world data. In addition, we introduce and test a model-dependent technique\nto make the representation of results easily interpretable by the medical\nstaff. Our solution achieves better performances compared to traditional models\nbased on machine learning, while providing at the same time more interpretable\nresults.",
        "translated": ""
    },
    {
        "title": "DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy\n  Slot Filling Task",
        "url": "http://arxiv.org/abs/2310.10169v1",
        "pub_date": "2023-10-16",
        "summary": "Recently, prompt-based generative frameworks have shown impressive\ncapabilities in sequence labeling tasks. However, in practical dialogue\nscenarios, relying solely on simplistic templates and traditional corpora\npresents a challenge for these methods in generalizing to unknown input\nperturbations. To address this gap, we propose a multi-task demonstration based\ngenerative framework for noisy slot filling, named DemoNSF. Specifically, we\nintroduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask\n(RM), and hybrid discrimination (HD), to implicitly capture semantic structural\ninformation of input perturbations at different granularities. In the\ndownstream main task, we design a noisy demonstration construction strategy for\nthe generative framework, which explicitly incorporates task-specific\ninformation and perturbed distribution during training and inference.\nExperiments on two benchmarks demonstrate that DemoNSF outperforms all baseline\nmethods and achieves strong generalization. Further analysis provides empirical\nguidance for the practical application of generative frameworks. Our code is\nreleased at https://github.com/dongguanting/Demo-NSF.",
        "translated": ""
    },
    {
        "title": "DNA: Denoised Neighborhood Aggregation for Fine-grained Category\n  Discovery",
        "url": "http://arxiv.org/abs/2310.10151v1",
        "pub_date": "2023-10-16",
        "summary": "Discovering fine-grained categories from coarsely labeled data is a practical\nand challenging task, which can bridge the gap between the demand for\nfine-grained analysis and the high annotation cost. Previous works mainly focus\non instance-level discrimination to learn low-level features, but ignore\nsemantic similarities between data, which may prevent these models learning\ncompact cluster representations. In this paper, we propose Denoised\nNeighborhood Aggregation (DNA), a self-supervised framework that encodes\nsemantic structures of data into the embedding space. Specifically, we retrieve\nk-nearest neighbors of a query as its positive keys to capture semantic\nsimilarities between data and then aggregate information from the neighbors to\nlearn compact cluster representations, which can make fine-grained categories\nmore separatable. However, the retrieved neighbors can be noisy and contain\nmany false-positive keys, which can degrade the quality of learned embeddings.\nTo cope with this challenge, we propose three principles to filter out these\nfalse neighbors for better representation learning. Furthermore, we\ntheoretically justify that the learning objective of our framework is\nequivalent to a clustering loss, which can capture semantic similarities\nbetween data to form compact fine-grained clusters. Extensive experiments on\nthree benchmark datasets show that our method can retrieve more accurate\nneighbors (21.31% accuracy improvement) and outperform state-of-the-art models\nby a large margin (average 9.96% improvement on three metrics). Our code and\ndata are available at https://github.com/Lackel/DNA.",
        "translated": ""
    },
    {
        "title": "On Generative Agents in Recommendation",
        "url": "http://arxiv.org/abs/2310.10108v1",
        "pub_date": "2023-10-16",
        "summary": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie\nrecommendation simulator, leveraging LLM-empowered generative agents equipped\nwith user profile, memory, and actions modules specifically tailored for the\nrecommender system. In particular, these agents' profile modules are\ninitialized using the MovieLens dataset, capturing users' unique tastes and\nsocial traits; memory modules log both factual and emotional memories and are\nintegrated with an emotion-driven reflection mechanism; action modules support\na wide variety of behaviors, spanning both taste-driven and emotion-driven\nactions. Each agent interacts with personalized movie recommendations in a\npage-by-page manner, relying on a pre-implemented collaborative filtering-based\nrecommendation algorithm. We delve into both the capabilities and limitations\nof Agent4Rec, aiming to explore an essential research question: to what extent\ncan LLM-empowered generative agents faithfully simulate the behavior of real,\nautonomous humans in recommender systems? Extensive and multi-faceted\nevaluations of Agent4Rec highlight both the alignment and deviation between\nagents and user-personalized preferences. Beyond mere performance comparison,\nwe explore insightful experiments, such as emulating the filter bubble effect\nand discovering the underlying causal relationships in recommendation tasks.\nOur codes are available at https://github.com/LehengTHU/Agent4Rec.",
        "translated": ""
    },
    {
        "title": "Dual-Scale Interest Extraction Framework with Self-Supervision for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2310.10025v1",
        "pub_date": "2023-10-16",
        "summary": "In the sequential recommendation task, the recommender generally learns\nmultiple embeddings from a user's historical behaviors, to catch the diverse\ninterests of the user. Nevertheless, the existing approaches just extract each\ninterest independently for the corresponding sub-sequence while ignoring the\nglobal correlation of the entire interaction sequence, which may fail to\ncapture the user's inherent preference for the potential interests\ngeneralization and unavoidably make the recommended items homogeneous with the\nhistorical behaviors. In this paper, we propose a novel Dual-Scale Interest\nExtraction framework (DSIE) to precisely estimate the user's current interests.\nSpecifically, DSIE explicitly models the user's inherent preference with\ncontrastive learning by attending over his/her entire interaction sequence at\nthe global scale and catches the user's diverse interests in a fine granularity\nat the local scale. Moreover, we develop a novel interest aggregation module to\nintegrate the multi-interests according to the inherent preference to generate\nthe user's current interests for the next-item prediction. Experiments\nconducted on three real-world benchmark datasets demonstrate that DSIE\noutperforms the state-of-the-art models in terms of recommendation preciseness\nand novelty.",
        "translated": ""
    },
    {
        "title": "Farzi Data: Autoregressive Data Distillation",
        "url": "http://arxiv.org/abs/2310.09983v1",
        "pub_date": "2023-10-15",
        "summary": "We study data distillation for auto-regressive machine learning tasks, where\nthe input and output have a strict left-to-right causal structure. More\nspecifically, we propose Farzi, which summarizes an event sequence dataset into\na small number of synthetic sequences -- Farzi Data -- which are optimized to\nmaintain (if not improve) model performance compared to training on the full\ndataset. Under the hood, Farzi conducts memory-efficient data distillation by\n(i) deriving efficient reverse-mode differentiation of the Adam optimizer by\nleveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional\ndiscrete event-space into a latent-space which provably promotes implicit\nregularization. Empirically, for sequential recommendation and language\nmodeling tasks, we are able to achieve 98-120% of downstream full-data\nperformance when training state-of-the-art models on Farzi Data of size as\nlittle as 0.1% of the original dataset. Notably, being able to train better\nmodels with significantly less data sheds light on the design of future large\nauto-regressive models, and opens up new opportunities to further scale up\nmodel and data sizes.",
        "translated": ""
    },
    {
        "title": "On Coherence-based Predictors for Dense Query Performance Prediction",
        "url": "http://arxiv.org/abs/2310.11405v1",
        "pub_date": "2023-10-17",
        "summary": "Query Performance Prediction (QPP) estimates the effectiveness of a search\nengine's results in response to a query without relevance judgments.\nTraditionally, post-retrieval predictors have focused upon either the\ndistribution of the retrieval scores, or the coherence of the top-ranked\ndocuments using traditional bag-of-words index representations. More recently,\nBERT-based models using dense embedded document representations have been used\nto create new predictors, but mostly applied to predict the performance of\nrankings created by BM25. Instead, we aim to predict the effectiveness of\nrankings created by single-representation dense retrieval models (ANCE &amp;\nTCT-ColBERT). Therefore, we propose a number of variants of existing\nunsupervised coherence-based predictors that employ neural embedding\nrepresentations. In our experiments on the TREC Deep Learning Track datasets,\nwe demonstrate improved accuracy upon dense retrieval (up to 92% compared to\nsparse variants for TCT-ColBERT and 188% for ANCE). Going deeper, we select the\nmost representative and best performing predictors to study the importance of\ndifferences among predictors and query types on query performance. Using\nexisting distribution-based evaluation QPP measures and a particular type of\nlinear mixed models, we find that query types further significantly influence\nquery performance (and are up to 35% responsible for the unstable performance\nof QPP predictors), and that this sensitivity is unique to dense retrieval\nmodels. Our approach introduces a new setting for obtaining richer information\non query differences in dense QPP that can explain potential unstable\nperformance of existing predictors and outlines the unique characteristics of\ndifferent query types on dense retrieval models.",
        "translated": ""
    },
    {
        "title": "Graph Neural Networks for Recommendation: Reproducibility, Graph\n  Topology, and Node Representation",
        "url": "http://arxiv.org/abs/2310.11270v1",
        "pub_date": "2023-10-17",
        "summary": "Graph neural networks (GNNs) have gained prominence in recommendation systems\nin recent years. By representing the user-item matrix as a bipartite and\nundirected graph, GNNs have demonstrated their potential to capture short- and\nlong-distance user-item interactions, thereby learning more accurate preference\npatterns than traditional recommendation approaches. In contrast to previous\ntutorials on the same topic, this tutorial aims to present and examine three\nkey aspects that characterize GNNs for recommendation: (i) the reproducibility\nof state-of-the-art approaches, (ii) the potential impact of graph topological\ncharacteristics on the performance of these models, and (iii) strategies for\nlearning node representations when training features from scratch or utilizing\npre-trained embeddings as additional item information (e.g., multimodal\nfeatures). The goal is to provide three novel theoretical and practical\nperspectives on the field, currently subject to debate in graph learning but\nlong been overlooked in the context of recommendation systems.",
        "translated": ""
    },
    {
        "title": "MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.11088v1",
        "pub_date": "2023-10-17",
        "summary": "It is a long-standing challenge in modern recommender systems to effectively\nmake recommendations for new users, namely the cold-start problem. Cross-Domain\nRecommendation (CDR) has been proposed to address this challenge, but current\nways to represent users' interests across systems are still severely limited.\nWe introduce Personal Knowledge Graph (PKG) as a domain-invariant interest\nrepresentation, and propose a novel CDR paradigm named MeKB-Rec. We first link\nusers and entities in a knowledge base to construct a PKG of users' interests,\nnamed MeKB. Then we learn a semantic representation of MeKB for the\ncross-domain recommendation. To efficiently utilize limited training data in\nCDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into\nunderstanding users' interests. Beyond most existing systems, our approach\nbuilds a semantic mapping across domains which breaks the requirement for\nin-domain user behaviors, enabling zero-shot recommendations for new users in a\nlow-resource domain. We experiment MeKB-Rec on well-established public CDR\ndatasets, and demonstrate that the new formulation % is more powerful than\nprevious approaches, achieves a new state-of-the-art that significantly\nimproves HR@10 and NDCG@10 metrics over best previous approaches by 24\\%--91\\%,\nwith a 105\\% improvement for HR@10 of zero-shot users with no behavior in the\ntarget domain. We deploy MeKB-Rec in WeiXin recommendation scenarios and\nachieve significant gains in core online metrics. MeKB-Rec is now serving\nhundreds of millions of users in real-world products.",
        "translated": ""
    },
    {
        "title": "Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation",
        "url": "http://arxiv.org/abs/2310.11049v1",
        "pub_date": "2023-10-17",
        "summary": "This paper describes our submission to the SemEval-2023 for Task 6 on\nLegalEval: Understanding Legal Texts. Our submission concentrated on three\nsubtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment\nPrediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation\n(CJPE) for Task-C2. We conducted various experiments on these subtasks and\npresented the results in detail, including data statistics and methodology. It\nis worth noting that legal tasks, such as those tackled in this research, have\nbeen gaining importance due to the increasing need to automate legal analysis\nand support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$,\nand 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the\nleaderboard.",
        "translated": ""
    },
    {
        "title": "If the Sources Could Talk: Evaluating Large Language Models for Research\n  Assistance in History",
        "url": "http://arxiv.org/abs/2310.10808v1",
        "pub_date": "2023-10-16",
        "summary": "The recent advent of powerful Large-Language Models (LLM) provides a new\nconversational form of inquiry into historical memory (or, training data, in\nthis case). We show that by augmenting such LLMs with vector embeddings from\nhighly specialized academic sources, a conversational methodology can be made\naccessible to historians and other researchers in the Humanities. Concretely,\nwe evaluate and demonstrate how LLMs have the ability of assisting researchers\nwhile they examine a customized corpora of different types of documents,\nincluding, but not exclusive to: (1). primary sources, (2). secondary sources\nwritten by experts, and (3). the combination of these two. Compared to\nestablished search interfaces for digital catalogues, such as metadata and\nfull-text search, we evaluate the richer conversational style of LLMs on the\nperformance of two main types of tasks: (1). question-answering, and (2).\nextraction and organization of data. We demonstrate that LLMs semantic\nretrieval and reasoning abilities on problem-specific tasks can be applied to\nlarge textual archives that have not been part of the its training data.\nTherefore, LLMs can be augmented with sources relevant to specific research\nprojects, and can be queried privately by researchers.",
        "translated": ""
    },
    {
        "title": "Automated Attribute Extraction from Legal Proceedings",
        "url": "http://arxiv.org/abs/2310.12131v1",
        "pub_date": "2023-10-18",
        "summary": "The escalating number of pending cases is a growing concern world-wide.\nRecent advancements in digitization have opened up possibilities for leveraging\nartificial intelligence (AI) tools in the processing of legal documents.\nAdopting a structured representation for legal documents, as opposed to a mere\nbag-of-words flat text representation, can significantly enhance processing\ncapabilities. With the aim of achieving this objective, we put forward a set of\ndiverse attributes for criminal case proceedings. We use a state-of-the-art\nsequence labeling framework to automatically extract attributes from the legal\ndocuments. Moreover, we demonstrate the efficacy of the extracted attributes in\na downstream task, namely legal judgment prediction.",
        "translated": ""
    },
    {
        "title": "Unveiling the Siren's Song: Towards Reliable Fact-Conflicting\n  Hallucination Detection",
        "url": "http://arxiv.org/abs/2310.12086v1",
        "pub_date": "2023-10-18",
        "summary": "Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread\nattention owing to their myriad of practical applications, yet their adoption\nhas been constrained by issues of fact-conflicting hallucinations across web\nplatforms. The assessment of factuality in text, produced by LLMs, remains\ninadequately explored, extending not only to the judgment of vanilla facts but\nalso encompassing the evaluation of factual errors emerging in complex\ninferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a\nfact-conflicting hallucination detection benchmark meticulously designed for\nLLMs. Functioning as a pivotal tool in evaluating factuality within\n\"Query-Respons\" contexts, our benchmark assimilates a large-scale dataset,\nencapsulating a broad spectrum of factuality patterns, such as vanilla,\nmulti-hops, comparison, and set-operation patterns. A distinctive feature of\nour benchmark is its incorporation of fact-based chains of evidence, thereby\nfacilitating comprehensive and conducive factual reasoning throughout the\nassessment process. We evaluate multiple LLMs, demonstrating the effectiveness\nof the benchmark and current methods fall short of faithfully detecting factual\nerrors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective\nconsiderations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming\nto yield more credible detection through the amalgamation of predictive results\nand evidence. The benchmark dataset and source code will be made available in\nhttps://github.com/zjunlp/FactCHD.",
        "translated": ""
    },
    {
        "title": "Simulating Users in Interactive Web Table Retrieval",
        "url": "http://arxiv.org/abs/2310.11931v1",
        "pub_date": "2023-10-18",
        "summary": "Considering the multimodal signals of search items is beneficial for\nretrieval effectiveness. Especially in web table retrieval (WTR) experiments,\naccounting for multimodal properties of tables boosts effectiveness. However,\nit still remains an open question how the single modalities affect user\nexperience in particular. Previous work analyzed WTR performance in ad-hoc\nretrieval benchmarks, which neglects interactive search behavior and limits the\nconclusion about the implications for real-world user environments.\n  To this end, this work presents an in-depth evaluation of simulated\ninteractive WTR search sessions as a more cost-efficient and reproducible\nalternative to real user studies. As a first of its kind, we introduce\ninteractive query reformulation strategies based on Doc2Query, incorporating\ncognitive states of simulated user knowledge. Our evaluations include two\nperspectives on user effectiveness by considering different cost paradigms,\nnamely query-wise and time-oriented measures of effort. Our multi-perspective\nevaluation scheme reveals new insights about query strategies, the impact of\nmodalities, and different user types in simulated WTR search sessions.",
        "translated": ""
    },
    {
        "title": "CIR at the NTCIR-17 ULTRE-2 Task",
        "url": "http://arxiv.org/abs/2310.11852v1",
        "pub_date": "2023-10-18",
        "summary": "The Chinese academy of sciences Information Retrieval team (CIR) has\nparticipated in the NTCIR-17 ULTRE-2 task. This paper describes our approaches\nand reports our results on the ULTRE-2 task. We recognize the issue of false\nnegatives in the Baidu search data in this competition is very severe, much\nmore severe than position bias. Hence, we adopt the Dual Learning Algorithm\n(DLA) to address the position bias and use it as an auxiliary model to study\nhow to alleviate the false negative issue. We approach the problem from two\nperspectives: 1) correcting the labels for non-clicked items by a relevance\njudgment model trained from DLA, and learn a new ranker that is initialized\nfrom DLA; 2) including random documents as true negatives and documents that\nhave partial matching as hard negatives. Both methods can enhance the model\nperformance and our best method has achieved nDCG@10 of 0.5355, which is 2.66%\nbetter than the best score from the organizer.",
        "translated": ""
    },
    {
        "title": "DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing\n  in Multi-task Learning",
        "url": "http://arxiv.org/abs/2310.11777v1",
        "pub_date": "2023-10-18",
        "summary": "In recent years, DL has developed rapidly, and personalized services are\nexploring using DL algorithms to improve the performance of the recommendation\nsystem. For personalized services, a successful recommendation consists of two\nparts: attracting users to click the item and users being willing to consume\nthe item. If both tasks need to be predicted at the same time, traditional\nrecommendation systems generally train two independent models. This approach is\ncumbersome and does not effectively model the relationship between the two\nsubtasks of \"click-consumption\". Therefore, in order to improve the success\nrate of recommendation and reduce computational costs, researchers are trying\nto model multi-task learning.\n  At present, existing multi-task learning models generally adopt hard\nparameter sharing or soft parameter sharing architecture, but these two\narchitectures each have certain problems. Therefore, in this work, we propose a\nnovel recommendation model based on real recommendation scenarios, Deep Cross\nnetwork based on RNN for partial parameter sharing (DCRNN). The model has three\ninnovations: 1) It adopts the idea of cross network and uses RNN network to\ncross-process the features, thereby effectively improves the expressive ability\nof the model; 2) It innovatively proposes the structure of partial parameter\nsharing; 3) It can effectively capture the potential correlation between\ndifferent tasks to optimize the efficiency and methods for learning different\ntasks.",
        "translated": ""
    },
    {
        "title": "From Relevance to Utility: Evidence Retrieval with Feedback for Fact\n  Verification",
        "url": "http://arxiv.org/abs/2310.11675v1",
        "pub_date": "2023-10-18",
        "summary": "Retrieval-enhanced methods have become a primary approach in fact\nverification (FV); it requires reasoning over multiple retrieved pieces of\nevidence to verify the integrity of a claim. To retrieve evidence, existing\nwork often employs off-the-shelf retrieval models whose design is based on the\nprobability ranking principle. We argue that, rather than relevance, for FV we\nneed to focus on the utility that a claim verifier derives from the retrieved\nevidence. We introduce the feedback-based evidence retriever(FER) that\noptimizes the evidence retrieval process by incorporating feedback from the\nclaim verifier. As a feedback signal we use the divergence in utility between\nhow effectively the verifier utilizes the retrieved evidence and the\nground-truth evidence to produce the final claim label. Empirical studies\ndemonstrate the superiority of FER over prevailing baselines.",
        "translated": ""
    },
    {
        "title": "VKIE: The Application of Key Information Extraction on Video Text",
        "url": "http://arxiv.org/abs/2310.11650v1",
        "pub_date": "2023-10-18",
        "summary": "Extracting structured information from videos is critical for numerous\ndownstream applications in the industry. In this paper, we define a significant\ntask of extracting hierarchical key information from visual texts on videos. To\nfulfill this task, we decouples it into four subtasks and introduce two\nimplementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially\ncompletes the four subtasks in continuous stages, while UniVKIE is improved by\nunifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage\nmultimodal information from vision, text, and coordinates for feature\nrepresentation. Extensive experiments on one well-defined dataset demonstrate\nthat our solutions can achieve remarkable performance and efficient inference\nspeed. The code and dataset will be publicly available.",
        "translated": ""
    },
    {
        "title": "Open Information Extraction: A Review of Baseline Techniques,\n  Approaches, and Applications",
        "url": "http://arxiv.org/abs/2310.11644v1",
        "pub_date": "2023-10-18",
        "summary": "With the abundant amount of available online and offline text data, there\narises a crucial need to extract the relation between phrases and summarize the\nmain content of each document in a few words. For this purpose, there have been\nmany studies recently in Open Information Extraction (OIE). OIE improves upon\nrelation extraction techniques by analyzing relations across different domains\nand avoids requiring hand-labeling pre-specified relations in sentences. This\npaper surveys recent approaches of OIE and its applications on Knowledge Graph\n(KG), text summarization, and Question Answering (QA). Moreover, the paper\ndescribes OIE basis methods in relation extraction. It briefly discusses the\nmain approaches and the pros and cons of each method. Finally, it gives an\noverview about challenges, open issues, and future work opportunities for OIE,\nrelation extraction, and OIE applications.",
        "translated": ""
    },
    {
        "title": "Unified Browsing Models for Linear and Grid Layouts",
        "url": "http://arxiv.org/abs/2310.12524v1",
        "pub_date": "2023-10-19",
        "summary": "Many information access systems operationalize their results in terms of\nrankings, which are then displayed to users in various ranking layouts such as\nlinear lists or grids. User interaction with a retrieved item is highly\ndependent on the item's position in the layout, and users do not provide\nsimilar attention to every position in ranking (under any layout model). User\nattention is an important component in the evaluation process of ranking, due\nto its use in effectiveness metrics that estimate utility as well as fairness\nmetrics that evaluate ranking based on social and ethical concerns. These\nmetrics take user browsing behavior into account in their measurement\nstrategies to estimate the attention the user is likely to provide to each item\nin ranking. Research on understanding user browsing behavior has proposed\nseveral user browsing models, and further observed that user browsing behavior\ndiffers with different ranking layouts. However, the underlying concepts of\nthese browsing models are often similar, including varying components and\nparameter settings. We seek to leverage that similarity to represent multiple\nbrowsing models in a generalized, configurable framework which can be further\nextended to more complex ranking scenarios. In this paper, we describe a\nprobabilistic user browsing model for linear rankings, show how they can be\nconfigured to yield models commonly used in current evaluation practice, and\ngeneralize this model to also account for browsing behaviors in grid-based\nlayouts. This model provides configurable framework for estimating the\nattention that results from user browsing activity for a range of IR evaluation\nand measurement applications in multiple formats, and also identifies\nparameters that need to be estimated through user studies to provide realistic\nevaluation beyond ranked lists.",
        "translated": ""
    },
    {
        "title": "Auto Search Indexer for End-to-End Document Retrieval",
        "url": "http://arxiv.org/abs/2310.12455v1",
        "pub_date": "2023-10-19",
        "summary": "Generative retrieval, which is a new advanced paradigm for document\nretrieval, has recently attracted research interests, since it encodes all\ndocuments into the model and directly generates the retrieved documents.\nHowever, its power is still underutilized since it heavily relies on the\n\"preprocessed\" document identifiers (docids), thus limiting its retrieval\nperformance and ability to retrieve new documents. In this paper, we propose a\nnovel fully end-to-end retrieval paradigm. It can not only end-to-end learn the\nbest docids for existing and new documents automatically via a semantic\nindexing module, but also perform end-to-end document retrieval via an\nencoder-decoder-based generative model, namely Auto Search Indexer (ASI).\nBesides, we design a reparameterization mechanism to combine the above two\nmodules into a joint optimization framework. Extensive experimental results\ndemonstrate the superiority of our model over advanced baselines on both public\nand industrial datasets and also verify the ability to deal with new documents.",
        "translated": ""
    },
    {
        "title": "Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy\n  Searcher",
        "url": "http://arxiv.org/abs/2310.12443v1",
        "pub_date": "2023-10-19",
        "summary": "The advent of Large Language Models (LLMs) has shown the potential to improve\nrelevance and provide direct answers in web searches. However, challenges arise\nin validating the reliability of generated results and the credibility of\ncontributing sources, due to the limitations of traditional information\nretrieval algorithms and the LLM hallucination problem. Aiming to create a\n\"PageRank\" for the LLM era, we strive to transform LLM into a relevant,\nresponsible, and trustworthy searcher. We propose a novel generative retrieval\nframework leveraging the knowledge of LLMs to foster a direct link between\nqueries and online sources. This framework consists of three core modules:\nGenerator, Validator, and Optimizer, each focusing on generating trustworthy\nonline sources, verifying source reliability, and refining unreliable sources,\nrespectively. Extensive experiments and evaluations highlight our method's\nsuperior relevance, responsibility, and trustfulness against various SOTA\nmethods.",
        "translated": ""
    },
    {
        "title": "On Identifying Points of Semantic Shift Across Domains",
        "url": "http://arxiv.org/abs/2310.12369v1",
        "pub_date": "2023-10-18",
        "summary": "The semantics used for particular terms in an academic field organically\nevolve over time. Tracking this evolution through inspection of published\nliterature has either been from the perspective of Linguistic scholars or has\nconcentrated the focus of term evolution within a single domain of study. In\nthis paper, we performed a case study to identify semantic evolution across\ndifferent domains and identify examples of inter-domain semantic shifts. We\ninitially used keywords as the basis of our search and executed an iterative\nprocess of following citations to find the initial mention of the concepts in\nthe field. We found that a select set of keywords like ``semaphore'',\n``polymorphism'', and ``ontology'' were mentioned within Computer Science\nliterature and tracked the seminal study that borrowed those terms from\noriginal fields by citations. We marked these events as semantic evolution\npoints. Through this manual investigation method, we can identify term\nevolution across different academic fields. This study reports our initial\nfindings that will seed future automated and computational methods of\nincorporating concepts from additional academic fields.",
        "translated": ""
    },
    {
        "title": "Retrieve-Cluster-Summarize: An Alternative to End-to-End Training for\n  Query-specific Article Generation",
        "url": "http://arxiv.org/abs/2310.12361v1",
        "pub_date": "2023-10-18",
        "summary": "Query-specific article generation is the task of, given a search query,\ngenerate a single article that gives an overview of the topic. We envision such\narticles as an alternative to presenting a ranking of search results. While\ngenerative Large Language Models (LLMs) like chatGPT also address this task,\nthey are known to hallucinate new information, their models are secret, hard to\nanalyze and control. Some generative LLMs provide supporting references, yet\nthese are often unrelated to the generated content. As an alternative, we\npropose to study article generation systems that integrate document retrieval,\nquery-specific clustering, and summarization. By design, such models can\nprovide actual citations as provenance for their generated text. In particular,\nwe contribute an evaluation framework that allows to separately trains and\nevaluate each of these three components before combining them into one system.\nWe experimentally demonstrate that a system comprised of the best-performing\nindividual components also obtains the best F-1 overall system quality.",
        "translated": ""
    },
    {
        "title": "Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language",
        "url": "http://arxiv.org/abs/2310.13540v1",
        "pub_date": "2023-10-20",
        "summary": "With the thriving of pre-trained language model (PLM) widely verified in\nvarious of NLP tasks, pioneer efforts attempt to explore the possible\ncooperation of the general textual information in PLM with the personalized\nbehavioral information in user historical behavior sequences to enhance\nsequential recommendation (SR). However, despite the commonalities of input\nformat and task goal, there are huge gaps between the behavioral and textual\ninformation, which obstruct thoroughly modeling SR as language modeling via\nPLM. To bridge the gap, we propose a novel Unified pre-trained language model\nenhanced sequential recommendation (UPSR), aiming to build a unified\npre-trained recommendation model for multi-domain recommendation tasks. We\nformally design five key indicators, namely naturalness, domain consistency,\ninformativeness, noise &amp; ambiguity, and text length, to guide the text-&gt;item\nadaptation and behavior sequence-&gt;text sequence adaptation differently for\npre-training and fine-tuning stages, which are essential but under-explored by\nprevious works. In experiments, we conduct extensive evaluations on seven\ndatasets with both tuning and zero-shot settings and achieve the overall best\nperformance. Comprehensive model analyses also provide valuable insights for\nbehavior modeling via PLM, shedding light on large pre-trained recommendation\nmodels. The source codes will be released in the future.",
        "translated": ""
    },
    {
        "title": "Robust Training for Conversational Question Answering Models with\n  Reinforced Reformulation Generation",
        "url": "http://arxiv.org/abs/2310.13505v1",
        "pub_date": "2023-10-20",
        "summary": "Models for conversational question answering (ConvQA) over knowledge graphs\n(KGs) are usually trained and tested on benchmarks of gold QA pairs. This\nimplies that training is limited to surface forms seen in the respective\ndatasets, and evaluation is on a small set of held-out questions. Through our\nproposed framework REIGN, we take several steps to remedy this restricted\nlearning setup. First, we systematically generate reformulations of training\nquestions to increase robustness of models to surface form variations. This is\na particularly challenging problem, given the incomplete nature of such\nquestions. Second, we guide ConvQA models towards higher performance by feeding\nit only those reformulations that help improve their answering quality, using\ndeep reinforcement learning. Third, we demonstrate the viability of training\nmajor model components on one benchmark and applying them zero-shot to another.\nFinally, for a rigorous evaluation of robustness for trained models, we use and\nrelease large numbers of diverse reformulations generated by prompting GPT for\nbenchmark test sets (resulting in 20x increase in sizes). Our findings show\nthat ConvQA models with robust training via reformulations, significantly\noutperform those with standard training from gold QA pairs only.",
        "translated": ""
    },
    {
        "title": "Two-Stage Triplet Loss Training with Curriculum Augmentation for\n  Audio-Visual Retrieval",
        "url": "http://arxiv.org/abs/2310.13451v1",
        "pub_date": "2023-10-20",
        "summary": "The cross-modal retrieval model leverages the potential of triple loss\noptimization to learn robust embedding spaces. However, existing methods often\ntrain these models in a singular pass, overlooking the distinction between\nsemi-hard and hard triples in the optimization process. The oversight of not\ndistinguishing between semi-hard and hard triples leads to suboptimal model\nperformance. In this paper, we introduce a novel approach rooted in curriculum\nlearning to address this problem. We propose a two-stage training paradigm that\nguides the model's learning process from semi-hard to hard triplets. In the\nfirst stage, the model is trained with a set of semi-hard triplets, starting\nfrom a low-loss base. Subsequently, in the second stage, we augment the\nembeddings using an interpolation technique. This process identifies potential\nhard negatives, alleviating issues arising from high-loss functions due to a\nscarcity of hard triples. Our approach then applies hard triplet mining in the\naugmented embedding space to further optimize the model. Extensive experimental\nresults conducted on two audio-visual datasets show a significant improvement\nof approximately 9.8% in terms of average Mean Average Precision (MAP) over the\ncurrent state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal\nRetrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our\nproposed method.",
        "translated": ""
    },
    {
        "title": "Music Augmentation and Denoising For Peak-Based Audio Fingerprinting",
        "url": "http://arxiv.org/abs/2310.13388v1",
        "pub_date": "2023-10-20",
        "summary": "Audio fingerprinting is a well-established solution for song identification\nfrom short recording excerpts. Popular methods rely on the extraction of sparse\nrepresentations, generally spectral peaks, and have proven to be accurate,\nfast, and scalable to large collections. However, real-world applications of\naudio identification often happen in noisy environments, which can cause these\nsystems to fail. In this work, we tackle this problem by introducing and\nreleasing a new audio augmentation pipeline that adds noise to music snippets\nin a realistic way, by stochastically mimicking real-world scenarios. We then\npropose and release a deep learning model that removes noisy components from\nspectrograms in order to improve peak-based fingerprinting systems' accuracy.\nWe show that the addition of our model improves the identification performance\nof commonly used audio fingerprinting systems, even under noisy conditions.",
        "translated": ""
    },
    {
        "title": "Towards Multi-Subsession Conversational Recommendation",
        "url": "http://arxiv.org/abs/2310.13365v1",
        "pub_date": "2023-10-20",
        "summary": "Conversational recommendation systems (CRS) could acquire dynamic user\npreferences towards desired items through multi-round interactive dialogue.\nPrevious CRS mainly focuses on the single conversation (subsession) that user\nquits after a successful recommendation, neglecting the common scenario where\nuser has multiple conversations (multi-subsession) over a short period.\nTherefore, we propose a novel conversational recommendation scenario named\nMulti-Subsession Multi-round Conversational Recommendation (MSMCR), where user\nwould still resort to CRS after several subsessions and might preserve vague\ninterests, and system would proactively ask attributes to activate user\ninterests in the current subsession. To fill the gap in this new CRS scenario,\nwe devise a novel framework called Multi-Subsession Conversational Recommender\nwith Activation Attributes (MSCAA). Specifically, we first develop a\ncontext-aware recommendation module, comprehensively modeling user interests\nfrom historical interactions, previous subsessions, and feedback in the current\nsubsession. Furthermore, an attribute selection policy module is proposed to\nlearn a flexible strategy for asking appropriate attributes to elicit user\ninterests. Finally, we design a conversation policy module to manage the above\ntwo modules to decide actions between asking and recommending. Extensive\nexperiments on four datasets verify the effectiveness of our MSCAA framework\nfor the MSMCR setting.",
        "translated": ""
    },
    {
        "title": "Motif-Based Prompt Learning for Universal Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2310.13303v1",
        "pub_date": "2023-10-20",
        "summary": "Cross-Domain Recommendation (CDR) stands as a pivotal technology addressing\nissues of data sparsity and cold start by transferring general knowledge from\nthe source to the target domain. However, existing CDR models suffer\nlimitations in adaptability across various scenarios due to their inherent\ncomplexity. To tackle this challenge, recent advancements introduce universal\nCDR models that leverage shared embeddings to capture general knowledge across\ndomains and transfer it through \"Multi-task Learning\" or \"Pre-train, Fine-tune\"\nparadigms. However, these models often overlook the broader structural topology\nthat spans domains and fail to align training objectives, potentially leading\nto negative transfer. To address these issues, we propose a motif-based prompt\nlearning framework, MOP, which introduces motif-based shared embeddings to\nencapsulate generalized domain knowledge, catering to both intra-domain and\ninter-domain CDR tasks. Specifically, we devise three typical motifs:\nbutterfly, triangle, and random walk, and encode them through a Motif-based\nEncoder to obtain motif-based shared embeddings. Moreover, we train MOP under\nthe \"Pre-training \\&amp; Prompt Tuning\" paradigm. By unifying pre-training and\nrecommendation tasks as a common motif-based similarity learning task and\nintegrating adaptable prompt parameters to guide the model in downstream\nrecommendation tasks, MOP excels in transferring domain knowledge effectively.\nExperimental results on four distinct CDR tasks demonstrate the effectiveness\nof MOP than the state-of-the-art models.",
        "translated": ""
    },
    {
        "title": "VR PreM+: An Immersive Pre-learning Branching Visualization System for\n  Museum Tours",
        "url": "http://arxiv.org/abs/2310.13294v1",
        "pub_date": "2023-10-20",
        "summary": "We present VR PreM+, an innovative VR system designed to enhance web\nexploration beyond traditional computer screens. Unlike static 2D displays, VR\nPreM+ leverages 3D environments to create an immersive pre-learning experience.\nUsing keyword-based information retrieval allows users to manage and connect\nvarious content sources in a dynamic 3D space, improving communication and data\ncomparison. We conducted preliminary and user studies that demonstrated\nefficient information retrieval, increased user engagement, and a greater sense\nof presence. These findings yielded three design guidelines for future VR\ninformation systems: display, interaction, and user-centric design. VR PreM+\nbridges the gap between traditional web browsing and immersive VR, offering an\ninteractive and comprehensive approach to information acquisition. It holds\npromise for research, education, and beyond.",
        "translated": ""
    },
    {
        "title": "Unified Pretraining for Recommendation via Task Hypergraphs",
        "url": "http://arxiv.org/abs/2310.13286v1",
        "pub_date": "2023-10-20",
        "summary": "Although pretraining has garnered significant attention and popularity in\nrecent years, its application in graph-based recommender systems is relatively\nlimited. It is challenging to exploit prior knowledge by pretraining in widely\nused ID-dependent datasets. On one hand, user-item interaction history in one\ndataset can hardly be transferred to other datasets through pretraining, where\nIDs are different. On the other hand, pretraining and finetuning on the same\ndataset leads to a high risk of overfitting. In this paper, we propose a novel\nmultitask pretraining framework named Unified Pretraining for Recommendation\nvia Task Hypergraphs. For a unified learning pattern to handle diverse\nrequirements and nuances of various pretext tasks, we design task hypergraphs\nto generalize pretext tasks to hyperedge prediction. A novel transitional\nattention layer is devised to discriminatively learn the relevance between each\npretext task and recommendation. Experimental results on three benchmark\ndatasets verify the superiority of UPRTH. Additional detailed investigations\nare conducted to demonstrate the effectiveness of the proposed framework.",
        "translated": ""
    },
    {
        "title": "An Exploratory Study on Simulated Annealing for Feature Selection in\n  Learning-to-Rank",
        "url": "http://arxiv.org/abs/2310.13269v1",
        "pub_date": "2023-10-20",
        "summary": "Learning-to-rank is an applied domain of supervised machine learning. As\nfeature selection has been found to be effective for improving the accuracy of\nlearning models in general, it is intriguing to investigate this process for\nlearning-to-rank domain. In this study, we investigate the use of a popular\nmeta-heuristic approach called simulated annealing for this task. Under the\ngeneral framework of simulated annealing, we explore various neighborhood\nselection strategies and temperature cooling schemes. We further introduce a\nnew hyper-parameter called the progress parameter that can effectively be used\nto traverse the search space. Our algorithms are evaluated on five publicly\nbenchmark datasets of learning-to-rank. For a better validation, we also\ncompare the simulated annealing-based feature selection algorithm with another\neffective meta-heuristic algorithm, namely local beam search. Extensive\nexperimental results shows the efficacy of our proposed models.",
        "translated": ""
    },
    {
        "title": "A Data-Centric Multi-Objective Learning Framework for Responsible\n  Recommendation Systems",
        "url": "http://arxiv.org/abs/2310.13260v1",
        "pub_date": "2023-10-20",
        "summary": "Recommendation systems effectively guide users in locating their desired\ninformation within extensive content repositories. Generally, a recommendation\nmodel is optimized to enhance accuracy metrics from a user utility standpoint,\nsuch as click-through rate or matching relevance. However, a responsible\nindustrial recommendation system must address not only user utility\n(responsibility to users) but also other objectives, including increasing\nplatform revenue (responsibility to platforms), ensuring fairness\n(responsibility to content creators), and maintaining unbiasedness\n(responsibility to long-term healthy development). Multi-objective learning is\na potent approach for achieving responsible recommendation systems.\nNevertheless, current methods encounter two challenges: difficulty in scaling\nto heterogeneous objectives within a unified framework, and inadequate\ncontrollability over objective priority during optimization, leading to\nuncontrollable solutions.\n  In this paper, we present a data-centric optimization framework, MoRec, which\nunifies the learning of diverse objectives. MoRec is a tri-level framework: the\nouter level manages the balance between different objectives, utilizing a\nproportional-integral-derivative (PID)-based controller to ensure a preset\nregularization on the primary objective. The middle level transforms\nobjective-aware optimization into data sampling weights using sign gradients.\nThe inner level employs a standard optimizer to update model parameters with\nthe sampled data. Consequently, MoRec can flexibly support various objectives\nwhile maintaining the original model intact. Comprehensive experiments on two\npublic datasets and one industrial dataset showcase the effectiveness,\ncontrollability, flexibility, and Pareto efficiency of MoRec, making it highly\nsuitable for real-world implementation.",
        "translated": ""
    },
    {
        "title": "Budgeted Embedding Table For Recommender Systems",
        "url": "http://arxiv.org/abs/2310.14884v1",
        "pub_date": "2023-10-23",
        "summary": "At the heart of contemporary recommender systems (RSs) are latent factor\nmodels that provide quality recommendation experience to users. These models\nuse embedding vectors, which are typically of a uniform and fixed size, to\nrepresent users and items. As the number of users and items continues to grow,\nthis design becomes inefficient and hard to scale. Recent lightweight embedding\nmethods have enabled different users and items to have diverse embedding sizes,\nbut are commonly subject to two major drawbacks. Firstly, they limit the\nembedding size search to optimizing a heuristic balancing the recommendation\nquality and the memory complexity, where the trade-off coefficient needs to be\nmanually tuned for every memory budget requested. The implicitly enforced\nmemory complexity term can even fail to cap the parameter usage, making the\nresultant embedding table fail to meet the memory budget strictly. Secondly,\nmost solutions, especially reinforcement learning based ones derive and\noptimize the embedding size for each each user/item on an instance-by-instance\nbasis, which impedes the search efficiency. In this paper, we propose Budgeted\nEmbedding Table (BET), a novel method that generates table-level actions (i.e.,\nembedding sizes for all users and items) that is guaranteed to meet\npre-specified memory budgets. Furthermore, by leveraging a set-based action\nformulation and engaging set representation learning, we present an innovative\naction search strategy powered by an action fitness predictor that efficiently\nevaluates each table-level action. Experiments have shown state-of-the-art\nperformance on two real-world datasets when BET is paired with three popular\nrecommender models under different memory budgets.",
        "translated": ""
    },
    {
        "title": "DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye\n  Movement for Machine Reading",
        "url": "http://arxiv.org/abs/2310.14802v1",
        "pub_date": "2023-10-23",
        "summary": "The use of visually-rich documents (VRDs) in various fields has created a\ndemand for Document AI models that can read and comprehend documents like\nhumans, which requires the overcoming of technical, linguistic, and cognitive\nbarriers. Unfortunately, the lack of appropriate datasets has significantly\nhindered advancements in the field. To address this issue, we introduce\n\\textsc{DocTrack}, a VRD dataset really aligned with human eye-movement\ninformation using eye-tracking technology. This dataset can be used to\ninvestigate the challenges mentioned above. Additionally, we explore the impact\nof human reading order on document understanding tasks and examine what would\nhappen if a machine reads in the same order as a human. Our results suggest\nthat although Document AI models have made significant progress, they still\nhave a long way to go before they can read VRDs as accurately, continuously,\nand flexibly as humans do. These findings have potential implications for\nfuture research and development of Document AI models. The data is available at\n\\url{https://github.com/hint-lab/doctrack}.",
        "translated": ""
    },
    {
        "title": "Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue",
        "url": "http://arxiv.org/abs/2310.14626v1",
        "pub_date": "2023-10-23",
        "summary": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.",
        "translated": ""
    },
    {
        "title": "Large Search Model: Redefining Search Stack in the Era of LLMs",
        "url": "http://arxiv.org/abs/2310.14587v1",
        "pub_date": "2023-10-23",
        "summary": "Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.",
        "translated": ""
    },
    {
        "title": "CorefPrompt: Prompt-based Event Coreference Resolution by Measuring\n  Event Type and Argument Compatibilities",
        "url": "http://arxiv.org/abs/2310.14512v1",
        "pub_date": "2023-10-23",
        "summary": "Event coreference resolution (ECR) aims to group event mentions referring to\nthe same real-world event into clusters. Most previous studies adopt the\n\"encoding first, then scoring\" framework, making the coreference judgment rely\non event encoding. Furthermore, current methods struggle to leverage\nhuman-summarized ECR rules, e.g., coreferential events should have the same\nevent type, to guide the model. To address these two issues, we propose a\nprompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM\n(masked language model) task. This allows for simultaneous event modeling and\ncoreference discrimination within a single template, with a fully shared\ncontext. In addition, we introduce two auxiliary prompt tasks, event-type\ncompatibility and argument compatibility, to explicitly demonstrate the\nreasoning process of ECR, which helps the model make final predictions.\nExperimental results show that our method CorefPrompt performs well in a\nstate-of-the-art (SOTA) benchmark.",
        "translated": ""
    },
    {
        "title": "\"Why Should I Review This Paper?\" Unifying Semantic, Topic, and Citation\n  Factors for Paper-Reviewer Matching",
        "url": "http://arxiv.org/abs/2310.14483v1",
        "pub_date": "2023-10-23",
        "summary": "As many academic conferences are overwhelmed by a rapidly increasing number\nof paper submissions, automatically finding appropriate reviewers for each\nsubmission becomes a more urgent need than ever. Various factors have been\nconsidered by previous attempts on this task to measure the expertise relevance\nbetween a paper and a reviewer, including whether the paper is semantically\nclose to, shares topics with, and cites previous papers of the reviewer.\nHowever, the majority of previous studies take only one of these factors into\naccount, leading to an incomprehensive evaluation of paper-reviewer relevance.\nTo bridge this gap, in this paper, we propose a unified model for\npaper-reviewer matching that jointly captures semantic, topic, and citation\nfactors. In the unified model, a contextualized language model backbone is\nshared by all factors to learn common knowledge, while instruction tuning is\nintroduced to characterize the uniqueness of each factor by producing\nfactor-aware paper embeddings. Experiments on four datasets (one of which is\nnewly contributed by us) across different fields, including machine learning,\ncomputer vision, information retrieval, and data mining, consistently validate\nthe effectiveness of our proposed UniPR model in comparison with\nstate-of-the-art paper-reviewer matching methods and scientific pre-trained\nlanguage models.",
        "translated": ""
    },
    {
        "title": "PaRaDe: Passage Ranking using Demonstrations with Large Language Models",
        "url": "http://arxiv.org/abs/2310.14408v1",
        "pub_date": "2023-10-22",
        "summary": "Recent studies show that large language models (LLMs) can be instructed to\neffectively perform zero-shot passage re-ranking, in which the results of a\nfirst stage retrieval method, such as BM25, are rated and reordered to improve\nrelevance. In this work, we improve LLM-based re-ranking by algorithmically\nselecting few-shot demonstrations to include in the prompt. Our analysis\ninvestigates the conditions where demonstrations are most helpful, and shows\nthat adding even one demonstration is significantly beneficial. We propose a\nnovel demonstration selection strategy based on difficulty rather than the\ncommonly used semantic similarity. Furthermore, we find that demonstrations\nhelpful for ranking are also effective at question generation. We hope our work\nwill spur more principled research into question generation and passage\nranking.",
        "translated": ""
    },
    {
        "title": "Offline Metrics for Evaluating Explanation Goals in Recommender Systems",
        "url": "http://arxiv.org/abs/2310.14379v1",
        "pub_date": "2023-10-22",
        "summary": "Explanations are crucial for improving users' transparency, persuasiveness,\nengagement, and trust in Recommender Systems (RSs). However, evaluating the\neffectiveness of explanation algorithms regarding those goals remains\nchallenging due to existing offline metrics' limitations. This paper introduces\nnew metrics for the evaluation and validation of explanation algorithms based\non the items and properties used to form the sentence of an explanation.\nTowards validating the metrics, the results of three state-of-the-art post-hoc\nexplanation algorithms were evaluated for six RSs, comparing the offline\nmetrics results with those of an online user study. The findings show the\nproposed offline metrics can effectively measure the performance of explanation\nalgorithms and highlight a trade-off between the goals of transparency and\ntrust, which are related to popular properties, and the goals of engagement and\npersuasiveness, which are associated with the diversification of properties\ndisplayed to users. Furthermore, the study contributes to the development of\nmore robust evaluation methods for explanation algorithms in RSs.",
        "translated": ""
    },
    {
        "title": "Intent Contrastive Learning with Cross Subsequences for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.14318v1",
        "pub_date": "2023-10-22",
        "summary": "The user purchase behaviors are mainly influenced by their intentions (e.g.,\nbuying clothes for decoration, buying brushes for painting, etc.). Modeling a\nuser's latent intention can significantly improve the performance of\nrecommendations. Previous works model users' intentions by considering the\npredefined label in auxiliary information or introducing stochastic data\naugmentation to learn purposes in the latent space. However, the auxiliary\ninformation is sparse and not always available for recommender systems, and\nintroducing stochastic data augmentation may introduce noise and thus change\nthe intentions hidden in the sequence. Therefore, leveraging user intentions\nfor sequential recommendation (SR) can be challenging because they are\nfrequently varied and unobserved. In this paper, Intent contrastive learning\nwith Cross Subsequences for sequential Recommendation (ICSRec) is proposed to\nmodel users' latent intentions. Specifically, ICSRec first segments a user's\nsequential behaviors into multiple subsequences by using a dynamic sliding\noperation and takes these subsequences into the encoder to generate the\nrepresentations for the user's intentions. To tackle the problem of no explicit\nlabels for purposes, ICSRec assumes different subsequences with the same target\nitem may represent the same intention and proposes a coarse-grain intent\ncontrastive learning to push these subsequences closer. Then, fine-grain intent\ncontrastive learning is mentioned to capture the fine-grain intentions of\nsubsequences in sequential behaviors. Extensive experiments conducted on four\nreal-world datasets demonstrate the superior performance of the proposed ICSRec\nmodel compared with baseline methods.",
        "translated": ""
    },
    {
        "title": "One Model for All: Large Language Models are Domain-Agnostic\n  Recommendation Systems",
        "url": "http://arxiv.org/abs/2310.14304v1",
        "pub_date": "2023-10-22",
        "summary": "The purpose of sequential recommendation is to utilize the interaction\nhistory of a user and predict the next item that the user is most likely to\ninteract with. While data sparsity and cold start are two challenges that most\nrecommender systems are still facing, many efforts are devoted to utilizing\ndata from other domains, called cross-domain methods. However, general\ncross-domain methods explore the relationship between two domains by designing\ncomplex model architecture, making it difficult to scale to multiple domains\nand utilize more data. Moreover, existing recommendation systems use IDs to\nrepresent item, which carry less transferable signals in cross-domain\nscenarios, and user cross-domain behaviors are also sparse, making it\nchallenging to learn item relationship from different domains. These problems\nhinder the application of multi-domain methods to sequential recommendation.\nRecently, large language models (LLMs) exhibit outstanding performance in world\nknowledge learning from text corpora and general-purpose question answering.\nInspired by these successes, we propose a simple but effective framework for\ndomain-agnostic recommendation by exploiting the pre-trained LLMs (namely\nLLM-Rec). We mix the user's behavior across different domains, and then\nconcatenate the title information of these items into a sentence and model the\nuser's behaviors with a pre-trained language model. We expect that by mixing\nthe user's behaviors across different domains, we can exploit the common\nknowledge encoded in the pre-trained language model to alleviate the problems\nof data sparsity and cold start problems. Furthermore, we are curious about\nwhether the latest technical advances in nature language processing (NLP) can\ntransfer to the recommendation scenarios.",
        "translated": ""
    },
    {
        "title": "Representation Learning with Large Language Models for Recommendation",
        "url": "http://arxiv.org/abs/2310.15950v1",
        "pub_date": "2023-10-24",
        "summary": "Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.",
        "translated": ""
    },
    {
        "title": "Topology-aware Debiased Self-supervised Graph Learning for\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.15858v1",
        "pub_date": "2023-10-24",
        "summary": "In recommendation, graph-based Collaborative Filtering (CF) methods mitigate\nthe data sparsity by introducing Graph Contrastive Learning (GCL). However, the\nrandom negative sampling strategy in these GCL-based CF models neglects the\nsemantic structure of users (items), which not only introduces false negatives\n(negatives that are similar to anchor user (item)) but also ignores the\npotential positive samples. To tackle the above issues, we propose\nTopology-aware Debiased Self-supervised Graph Learning (TDSGL) for\nrecommendation, which constructs contrastive pairs according to the semantic\nsimilarity between users (items). Specifically, since the original user-item\ninteraction data commendably reflects the purchasing intent of users and\ncertain characteristics of items, we calculate the semantic similarity between\nusers (items) on interaction data. Then, given a user (item), we construct its\nnegative pairs by selecting users (items) which embed different semantic\nstructures to ensure the semantic difference between the given user (item) and\nits negatives. Moreover, for a user (item), we design a feature extraction\nmodule that converts other semantically similar users (items) into an auxiliary\npositive sample to acquire a more informative representation. Experimental\nresults show that the proposed model outperforms the state-of-the-art models\nsignificantly on three public datasets. Our model implementation codes are\navailable at https://github.com/malajikuai/TDSGL.",
        "translated": ""
    },
    {
        "title": "A statistical significance testing approach for measuring term\n  burstiness with applications to domain-specific terminology extraction",
        "url": "http://arxiv.org/abs/2310.15790v1",
        "pub_date": "2023-10-24",
        "summary": "Domain-specific terminology extraction is an important task in text analysis.\nA term in a corpus is said to be \"bursty\" when its occurrences are concentrated\nin few out of many documents. Being content rich, bursty terms are highly\nsuited for subject matter characterization, and serve as natural candidates for\nidentifying with technical terminology. Multiple measures of term burstiness\nhave been proposed in the literature. However, the statistical significance\ntesting paradigm has remained underexplored in text analysis, including in\nrelation to term burstiness. To test these waters, we propose as our main\ncontribution a multinomial language model-based exact test of statistical\nsignificance for term burstiness. Due to its prohibitive computational cost, we\nadvance a heuristic formula designed to serve as a proxy for test P-values. As\na complementary theoretical contribution, we derive a previously unreported\nrelationship connecting the inverse document frequency and inverse collection\nfrequency (two foundational quantities in text analysis) under the multinomial\nlanguage model. The relation is used in the evaluation of our heuristic. Using\nthe GENIA Term corpus benchmark, we compare our approach against established\nmethods, demonstrating our heuristic's potential in identifying domain-specific\ntechnical terms. We hope this demonstration of statistical significance testing\nin text analysis serves as a springboard for future research.",
        "translated": ""
    },
    {
        "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for\n  Inference Cost Reduction",
        "url": "http://arxiv.org/abs/2310.15556v1",
        "pub_date": "2023-10-24",
        "summary": "Since ChatGPT released its API for public use, the number of applications\nbuilt on top of commercial large language models (LLMs) increase exponentially.\nOne popular usage of such models is leveraging its in-context learning ability\nand generating responses given user queries leveraging knowledge obtained by\nretrieval augmentation. One problem of deploying commercial retrieval-augmented\nLLMs is the cost due to the additionally retrieved context that largely\nincreases the input token size of the LLMs. To mitigate this, we propose a\ntoken compression scheme that includes two methods: summarization compression\nand semantic compression. The first method applies a T5-based model that is\nfine-tuned by datasets generated using self-instruct containing samples with\nvarying lengths and reduce token size by doing summarization. The second method\nfurther compresses the token size by removing words with lower impact on the\nsemantic. In order to adequately evaluate the effectiveness of the proposed\nmethods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)\nfocusing on food recommendation for women around pregnancy period or infants.\nOur summarization compression can reduce 65% of the retrieval token size with\nfurther 0.3% improvement on the accuracy; semantic compression provides a more\nflexible way to trade-off the token size with performance, for which we can\nreduce the token size by 20% with only 1.6% of accuracy drop.",
        "translated": ""
    },
    {
        "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2310.15511v1",
        "pub_date": "2023-10-24",
        "summary": "We study the ability of state-of-the art models to answer constraint\nsatisfaction queries for information retrieval (e.g., 'a list of ice cream\nshops in San Diego'). In the past, such queries were considered to be tasks\nthat could only be solved via web-search or knowledge bases. More recently,\nlarge language models (LLMs) have demonstrated initial emergent abilities in\nthis task. However, many current retrieval benchmarks are either saturated or\ndo not measure constraint satisfaction. Motivated by rising concerns around\nfactual incorrectness and hallucinations of LLMs, we present KITAB, a new\ndataset for measuring constraint satisfaction abilities of language models.\nKITAB consists of book-related data across more than 600 authors and 13,000\nqueries, and also offers an associated dynamic data collection and constraint\nverification approach for acquiring similar test data for other authors. Our\nextended experiments on GPT4 and GPT3.5 characterize and decouple common\nfailure modes across dimensions such as information popularity, constraint\ntypes, and context availability. Results show that in the absence of context,\nmodels exhibit severe limitations as measured by irrelevant information,\nfactual errors, and incompleteness, many of which exacerbate as information\npopularity decreases. While context availability mitigates irrelevant\ninformation, it is not helpful for satisfying constraints, identifying\nfundamental barriers to constraint satisfaction. We open source our\ncontributions to foster further research on improving constraint satisfaction\nabilities of future models.",
        "translated": ""
    },
    {
        "title": "Robust Representation Learning for Unified Online Top-K Recommendation",
        "url": "http://arxiv.org/abs/2310.15492v1",
        "pub_date": "2023-10-24",
        "summary": "In large-scale industrial e-commerce, the efficiency of an online\nrecommendation system is crucial in delivering highly relevant item/content\nadvertising that caters to diverse business scenarios. However, most existing\nstudies focus solely on item advertising, neglecting the significance of\ncontent advertising. This oversight results in inconsistencies within the\nmulti-entity structure and unfair retrieval. Furthermore, the challenge of\nretrieving top-k advertisements from multi-entity advertisements across\ndifferent domains adds to the complexity. Recent research proves that\nuser-entity behaviors within different domains exhibit characteristics of\ndifferentiation and homogeneity. Therefore, the multi-domain matching models\ntypically rely on the hybrid-experts framework with domain-invariant and\ndomain-specific representations. Unfortunately, most approaches primarily focus\non optimizing the combination mode of different experts, failing to address the\ninherent difficulty in optimizing the expert modules themselves. The existence\nof redundant information across different domains introduces interference and\ncompetition among experts, while the distinct learning objectives of each\ndomain lead to varying optimization challenges among experts. To tackle these\nissues, we propose robust representation learning for the unified online top-k\nrecommendation. Our approach constructs unified modeling in entity space to\nensure data fairness. The robust representation learning employs domain\nadversarial learning and multi-view wasserstein distribution learning to learn\nrobust representations. Moreover, the proposed method balances conflicting\nobjectives through the homoscedastic uncertainty weights and orthogonality\nconstraints. Various experiments validate the effectiveness and rationality of\nour proposed method, which has been successfully deployed online to serve real\nbusiness scenarios.",
        "translated": ""
    },
    {
        "title": "Off-Policy Evaluation for Large Action Spaces via Policy Convolution",
        "url": "http://arxiv.org/abs/2310.15433v1",
        "pub_date": "2023-10-24",
        "summary": "Developing accurate off-policy estimators is crucial for both evaluating and\noptimizing for new policies. The main challenge in off-policy estimation is the\ndistribution shift between the logging policy that generates data and the\ntarget policy that we aim to evaluate. Typically, techniques for correcting\ndistribution shift involve some form of importance sampling. This approach\nresults in unbiased value estimation but often comes with the trade-off of high\nvariance, even in the simpler case of one-step contextual bandits. Furthermore,\nimportance sampling relies on the common support assumption, which becomes\nimpractical when the action space is large. To address these challenges, we\nintroduce the Policy Convolution (PC) family of estimators. These methods\nleverage latent structure within actions -- made available through action\nembeddings -- to strategically convolve the logging and target policies. This\nconvolution introduces a unique bias-variance trade-off, which can be\ncontrolled by adjusting the amount of convolution. Our experiments on synthetic\nand benchmark datasets demonstrate remarkable mean squared error (MSE)\nimprovements when using PC, especially when either the action space or policy\nmismatch becomes large, with gains of up to 5 - 6 orders of magnitude over\nexisting estimators.",
        "translated": ""
    },
    {
        "title": "Towards Hybrid-grained Feature Interaction Selection for Deep Sparse\n  Network",
        "url": "http://arxiv.org/abs/2310.15342v1",
        "pub_date": "2023-10-23",
        "summary": "Deep sparse networks are widely investigated as a neural network architecture\nfor prediction tasks with high-dimensional sparse features, with which feature\ninteraction selection is a critical component. While previous methods primarily\nfocus on how to search feature interaction in a coarse-grained space, less\nattention has been given to a finer granularity. In this work, we introduce a\nhybrid-grained feature interaction selection approach that targets both feature\nfield and feature value for deep sparse networks. To explore such expansive\nspace, we propose a decomposed space which is calculated on the fly. We then\ndevelop a selection algorithm called OptFeature, which efficiently selects the\nfeature interaction from both the feature field and the feature value\nsimultaneously. Results from experiments on three large real-world benchmark\ndatasets demonstrate that OptFeature performs well in terms of accuracy and\nefficiency. Additional studies support the feasibility of our method.",
        "translated": ""
    },
    {
        "title": "Triple Simplex Matrix Completion for Expense Forecasting",
        "url": "http://arxiv.org/abs/2310.15275v1",
        "pub_date": "2023-10-23",
        "summary": "Forecasting project expenses is a crucial step for businesses to avoid budget\noverruns and project failures. Traditionally, this has been done by financial\nanalysts or data science techniques such as time-series analysis. However,\nthese approaches can be uncertain and produce results that differ from the\nplanned budget, especially at the start of a project with limited data points.\nThis paper proposes a constrained non-negative matrix completion model that\npredicts expenses by learning the likelihood of the project correlating with\ncertain expense patterns in the latent space. The model is constrained on three\nprobability simplexes, two of which are on the factor matrices and the third on\nthe missing entries. Additionally, the predicted expense values are guaranteed\nto meet the budget constraint without the need of post-processing. An inexact\nalternating optimization algorithm is developed to solve the associated\noptimization problem and is proven to converge to a stationary point. Results\nfrom two real datasets demonstrate the effectiveness of the proposed method in\ncomparison to state-of-the-art algorithms.",
        "translated": ""
    },
    {
        "title": "Improving Conversational Recommendation Systems via Bias Analysis and\n  Language-Model-Enhanced Data Augmentation",
        "url": "http://arxiv.org/abs/2310.16738v1",
        "pub_date": "2023-10-25",
        "summary": "Conversational Recommendation System (CRS) is a rapidly growing research area\nthat has gained significant attention alongside advancements in language\nmodelling techniques. However, the current state of conversational\nrecommendation faces numerous challenges due to its relative novelty and\nlimited existing contributions. In this study, we delve into benchmark datasets\nfor developing CRS models and address potential biases arising from the\nfeedback loop inherent in multi-turn interactions, including selection bias and\nmultiple popularity bias variants. Drawing inspiration from the success of\ngenerative data via using language models and data augmentation techniques, we\npresent two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model\nperformance while mitigating biases. Through extensive experiments on ReDial\nand TG-ReDial benchmark datasets, we show a consistent improvement of CRS\ntechniques with our data augmentation approaches and offer additional insights\non addressing multiple newly formulated biases.",
        "translated": ""
    },
    {
        "title": "Exploring Large Language Models for Code Explanation",
        "url": "http://arxiv.org/abs/2310.16673v1",
        "pub_date": "2023-10-25",
        "summary": "Automating code documentation through explanatory text can prove highly\nbeneficial in code understanding. Large Language Models (LLMs) have made\nremarkable strides in Natural Language Processing, especially within software\nengineering tasks such as code generation and code summarization. This study\nspecifically delves into the task of generating natural-language summaries for\ncode snippets, using various LLMs. The findings indicate that Code LLMs\noutperform their generic counterparts, and zero-shot methods yield superior\nresults when dealing with datasets with dissimilar distributions between\ntraining and testing sets.",
        "translated": ""
    },
    {
        "title": "Distributionally Robust Unsupervised Dense Retrieval Training on Web\n  Graphs",
        "url": "http://arxiv.org/abs/2310.16605v1",
        "pub_date": "2023-10-25",
        "summary": "This paper introduces Web-DRO, an unsupervised dense retrieval model, which\nclusters documents based on web structures and reweights the groups during\ncontrastive training. Specifically, we first leverage web graph links and\ncontrastively train an embedding model for clustering anchor-document pairs.\nThen we use Group Distributional Robust Optimization to reweight different\nclusters of anchor-document pairs, which guides the model to assign more\nweights to the group with higher contrastive loss and pay more attention to the\nworst case during training. Our experiments on MS MARCO and BEIR show that our\nmodel, Web-DRO, significantly improves the retrieval effectiveness in\nunsupervised scenarios. A comparison of clustering techniques shows that\ntraining on the web graph combining URL information reaches optimal performance\non clustering. Further analysis confirms that group weights are stable and\nvalid, indicating consistent model preferences as well as effective\nup-weighting of valuable groups and down-weighting of uninformative ones. The\ncode of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "translated": ""
    },
    {
        "title": "Model-enhanced Contrastive Reinforcement Learning for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.16566v1",
        "pub_date": "2023-10-25",
        "summary": "Reinforcement learning (RL) has been widely applied in recommendation systems\ndue to its potential in optimizing the long-term engagement of users. From the\nperspective of RL, recommendation can be formulated as a Markov decision\nprocess (MDP), where recommendation system (agent) can interact with users\n(environment) and acquire feedback (reward signals).However, it is impractical\nto conduct online interactions with the concern on user experience and\nimplementation complexity, and we can only train RL recommenders with offline\ndatasets containing limited reward signals and state transitions. Therefore,\nthe data sparsity issue of reward signals and state transitions is very severe,\nwhile it has long been overlooked by existing RL recommenders.Worse still, RL\nmethods learn through the trial-and-error mode, but negative feedback cannot be\nobtained in implicit feedback recommendation tasks, which aggravates the\noverestimation problem of offline RL recommender. To address these challenges,\nwe propose a novel RL recommender named model-enhanced contrastive\nreinforcement learning (MCRL). On the one hand, we learn a value function to\nestimate the long-term engagement of users, together with a conservative value\nlearning mechanism to alleviate the overestimation problem.On the other hand,\nwe construct some positive and negative state-action pairs to model the reward\nfunction and state transition function with contrastive learning to exploit the\ninternal structure information of MDP. Experiments demonstrate that the\nproposed method significantly outperforms existing offline RL and\nself-supervised RL methods with different representative backbone networks on\ntwo real-world datasets.",
        "translated": ""
    },
    {
        "title": "Faithful Path Language Modelling for Explainable Recommendation over\n  Knowledge Graph",
        "url": "http://arxiv.org/abs/2310.16452v1",
        "pub_date": "2023-10-25",
        "summary": "Path reasoning methods over knowledge graphs have gained popularity for their\npotential to improve transparency in recommender systems. However, the\nresulting models still rely on pre-trained knowledge graph embeddings, fail to\nfully exploit the interdependence between entities and relations in the KG for\nrecommendation, and may generate inaccurate explanations. In this paper, we\nintroduce PEARLM, a novel approach that efficiently captures user behaviour and\nproduct-side knowledge through language modelling. With our approach, knowledge\ngraph embeddings are directly learned from paths over the KG by the language\nmodel, which also unifies entities and relations in the same optimisation\nspace. Constraints on the sequence decoding additionally guarantee path\nfaithfulness with respect to the KG. Experiments on two datasets show the\neffectiveness of our approach compared to state-of-the-art baselines. Source\ncode and datasets: AVAILABLE AFTER GETTING ACCEPTED.",
        "translated": ""
    },
    {
        "title": "Multiple Key-value Strategy in Recommendation Systems Incorporating\n  Large Language Model",
        "url": "http://arxiv.org/abs/2310.16409v1",
        "pub_date": "2023-10-25",
        "summary": "Recommendation system (RS) plays significant roles in matching users\ninformation needs for Internet applications, and it usually utilizes the\nvanilla neural network as the backbone to handle embedding details. Recently,\nthe large language model (LLM) has exhibited emergent abilities and achieved\ngreat breakthroughs both in the CV and NLP communities. Thus, it is logical to\nincorporate RS with LLM better, which has become an emerging research\ndirection. Although some existing works have made their contributions to this\nissue, they mainly consider the single key situation (e.g. historical\ninteractions), especially in sequential recommendation. The situation of\nmultiple key-value data is simply neglected. This significant scenario is\nmainstream in real practical applications, where the information of users (e.g.\nage, occupation, etc) and items (e.g. title, category, etc) has more than one\nkey. Therefore, we aim to implement sequential recommendations based on\nmultiple key-value data by incorporating RS with LLM. In particular, we\ninstruct tuning a prevalent open-source LLM (Llama 7B) in order to inject\ndomain knowledge of RS into the pre-trained LLM. Since we adopt multiple\nkey-value strategies, LLM is hard to learn well among these keys. Thus the\ngeneral and innovative shuffle and mask strategies, as an innovative manner of\ndata argument, are designed. To demonstrate the effectiveness of our approach,\nextensive experiments are conducted on the popular and suitable dataset\nMovieLens which contains multiple keys-value. The experimental results\ndemonstrate that our approach can nicely and effectively complete this\nchallenging issue.",
        "translated": ""
    },
    {
        "title": "URL-BERT: Training Webpage Representations via Social Media Engagements",
        "url": "http://arxiv.org/abs/2310.16303v1",
        "pub_date": "2023-10-25",
        "summary": "Understanding and representing webpages is crucial to online social networks\nwhere users may share and engage with URLs. Common language model (LM) encoders\nsuch as BERT can be used to understand and represent the textual content of\nwebpages. However, these representations may not model thematic information of\nweb domains and URLs or accurately capture their appeal to social media users.\nIn this work, we introduce a new pre-training objective that can be used to\nadapt LMs to understand URLs and webpages. Our proposed framework consists of\ntwo steps: (1) scalable graph embeddings to learn shallow representations of\nURLs based on user engagement on social media and (2) a contrastive objective\nthat aligns LM representations with the aforementioned graph-based\nrepresentation. We apply our framework to the multilingual version of BERT to\nobtain the model URL-BERT. We experimentally demonstrate that our continued\npre-training approach improves webpage understanding on a variety of tasks and\nTwitter internal and external benchmarks.",
        "translated": ""
    },
    {
        "title": "Context-aware feature attribution through argumentation",
        "url": "http://arxiv.org/abs/2310.16157v1",
        "pub_date": "2023-10-24",
        "summary": "Feature attribution is a fundamental task in both machine learning and data\nanalysis, which involves determining the contribution of individual features or\nvariables to a model's output. This process helps identify the most important\nfeatures for predicting an outcome. The history of feature attribution methods\ncan be traced back to General Additive Models (GAMs), which extend linear\nregression models by incorporating non-linear relationships between dependent\nand independent variables. In recent years, gradient-based methods and\nsurrogate models have been applied to unravel complex Artificial Intelligence\n(AI) systems, but these methods have limitations. GAMs tend to achieve lower\naccuracy, gradient-based methods can be difficult to interpret, and surrogate\nmodels often suffer from stability and fidelity issues. Furthermore, most\nexisting methods do not consider users' contexts, which can significantly\ninfluence their preferences. To address these limitations and advance the\ncurrent state-of-the-art, we define a novel feature attribution framework\ncalled Context-Aware Feature Attribution Through Argumentation (CA-FATA). Our\nframework harnesses the power of argumentation by treating each feature as an\nargument that can either support, attack or neutralize a prediction.\nAdditionally, CA-FATA formulates feature attribution as an argumentation\nprocedure, and each computation has explicit semantics, which makes it\ninherently interpretable. CA-FATA also easily integrates side information, such\nas users' contexts, resulting in more accurate predictions.",
        "translated": ""
    },
    {
        "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model\n  System for Answering Medical Questions using Scientific Literature",
        "url": "http://arxiv.org/abs/2310.16146v1",
        "pub_date": "2023-10-24",
        "summary": "The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.",
        "translated": ""
    },
    {
        "title": "Context-aware explainable recommendations over knowledge graphs",
        "url": "http://arxiv.org/abs/2310.16141v1",
        "pub_date": "2023-10-24",
        "summary": "Knowledge graphs contain rich semantic relationships related to items and\nincorporating such semantic relationships into recommender systems helps to\nexplore the latent connections of items, thus improving the accuracy of\nprediction and enhancing the explainability of recommendations. However, such\nexplainability is not adapted to users' contexts, which can significantly\ninfluence their preferences. In this work, we propose CA-KGCN (Context-Aware\nKnowledge Graph Convolutional Network), an end-to-end framework that can model\nusers' preferences adapted to their contexts and can incorporate rich semantic\nrelationships in the knowledge graph related to items. This framework captures\nusers' attention to different factors: contexts and features of items. More\nspecifically, the framework can model users' preferences adapted to their\ncontexts and provide explanations adapted to the given context. Experiments on\nthree real-world datasets show the effectiveness of our framework: modeling\nusers' preferences adapted to their contexts and explaining the recommendations\ngenerated.",
        "translated": ""
    },
    {
        "title": "LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset",
        "url": "http://arxiv.org/abs/2310.17609v1",
        "pub_date": "2023-10-26",
        "summary": "As an important component of intelligent legal systems, legal case retrieval\nplays a critical role in ensuring judicial justice and fairness. However, the\ndevelopment of legal case retrieval technologies in the Chinese legal system is\nrestricted by three problems in existing datasets: limited data size, narrow\ndefinitions of legal relevance, and naive candidate pooling strategies used in\ndata sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale\nLegal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192\ncandidates extracted from 4.3 million criminal case documents. To the best of\nour knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval\ndatasets, providing extensive coverage of criminal charges. Additionally, we\nenrich the existing relevance criteria by considering three key aspects:\ncharacterization, penalty, procedure. This comprehensive criteria enriches the\ndataset and may provides a more holistic perspective. Furthermore, we propose a\ntwo-level candidate set pooling strategy that effectively identify potential\ncandidates for each query case. It's important to note that all cases in the\ndataset have been annotated by multiple legal experts specializing in criminal\nlaw. Their expertise ensures the accuracy and reliability of the annotations.\nWe evaluate several state-of-the-art retrieval models at LeCaRDv2,\ndemonstrating that there is still significant room for improvement in legal\ncase retrieval. The details of LeCaRDv2 can be found at the anonymous website\nhttps://github.com/anonymous1113243/LeCaRDv2.",
        "translated": ""
    },
    {
        "title": "LightLM: A Lightweight Deep and Narrow Language Model for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.17488v1",
        "pub_date": "2023-10-26",
        "summary": "This paper presents LightLM, a lightweight Transformer-based language model\nfor generative recommendation. While Transformer-based generative modeling has\ngained importance in various AI sub-fields such as NLP and vision, generative\nrecommendation is still in its infancy due to its unique demand on personalized\ngenerative modeling. Existing works on generative recommendation often use\nNLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are\nheavy-weight and are not specifically designed for recommendation tasks.\nLightLM tackles the issue by introducing a light-weight deep and narrow\nTransformer architecture, which is specifically tailored for direct generation\nof recommendation items. This structure is especially apt for straightforward\ngenerative recommendation and stems from the observation that language model\ndoes not have to be too wide for this task, as the input predominantly consists\nof short tokens that are well-suited for the model's capacity. We also show\nthat our devised user and item ID indexing methods, i.e., Spectral\nCollaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables\nthe deep and narrow Transformer architecture to outperform large-scale language\nmodels for recommendation. Besides, to address the hallucination problem of\ngenerating items as output, we propose the constrained generation process for\ngenerative recommenders. Experiments on real-world datasets show that LightLM\noutperforms various competitive baselines in terms of both recommendation\naccuracy and efficiency. The code can be found at\nhttps://github.com/dongyuanjushi/LightLM.",
        "translated": ""
    },
    {
        "title": "FMMRec: Fairness-aware Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2310.17373v1",
        "pub_date": "2023-10-26",
        "summary": "Recently, multimodal recommendations have gained increasing attention for\neffectively addressing the data sparsity problem by incorporating\nmodality-based representations. Although multimodal recommendations excel in\naccuracy, the introduction of different modalities (e.g., images, text, and\naudio) may expose more users' sensitive information (e.g., gender and age) to\nrecommender systems, resulting in potentially more serious unfairness issues.\nDespite many efforts on fairness, existing fairness-aware methods are either\nincompatible with multimodal scenarios, or lead to suboptimal fairness\nperformance due to neglecting sensitive information of multimodal content. To\nachieve counterfactual fairness in multimodal recommendations, we propose a\nnovel fairness-aware multimodal recommendation approach (dubbed as FMMRec) to\ndisentangle the sensitive and non-sensitive information from modal\nrepresentations and leverage the disentangled modal representations to guide\nfairer representation learning. Specifically, we first disentangle biased and\nfiltered modal representations by maximizing and minimizing their sensitive\nattribute prediction ability respectively. With the disentangled modal\nrepresentations, we mine the modality-based unfair and fair (corresponding to\nbiased and filtered) user-user structures for enhancing explicit user\nrepresentation with the biased and filtered neighbors from the corresponding\nstructures, followed by adversarially filtering out sensitive information.\nExperiments on two real-world public datasets demonstrate the superiority of\nour FMMRec relative to the state-of-the-art baselines. Our source code is\navailable at https://anonymous.4open.science/r/FMMRec.",
        "translated": ""
    },
    {
        "title": "Exploring the Potential of Generative AI for the World Wide Web",
        "url": "http://arxiv.org/abs/2310.17370v1",
        "pub_date": "2023-10-26",
        "summary": "Generative Artificial Intelligence (AI) is a cutting-edge technology capable\nof producing text, images, and various media content leveraging generative\nmodels and user prompts. Between 2022 and 2023, generative AI surged in\npopularity with a plethora of applications spanning from AI-powered movies to\nchatbots. In this paper, we delve into the potential of generative AI within\nthe realm of the World Wide Web, specifically focusing on image generation. Web\ndevelopers already harness generative AI to help crafting text and images,\nwhile Web browsers might use it in the future to locally generate images for\ntasks like repairing broken webpages, conserving bandwidth, and enhancing\nprivacy. To explore this research area, we have developed WebDiffusion, a tool\nthat allows to simulate a Web powered by stable diffusion, a popular\ntext-to-image model, from both a client and server perspective. WebDiffusion\nfurther supports crowdsourcing of user opinions, which we use to evaluate the\nquality and accuracy of 409 AI-generated images sourced from 60 webpages. Our\nfindings suggest that generative AI is already capable of producing pertinent\nand high-quality Web images, even without requiring Web designers to manually\ninput prompts, just by leveraging contextual information available within the\nwebpages. However, we acknowledge that direct in-browser image generation\nremains a challenge, as only highly powerful GPUs, such as the A40 and A100,\ncan (partially) compete with classic image downloads. Nevertheless, this\napproach could be valuable for a subset of the images, for example when fixing\nbroken webpages or handling highly private content.",
        "translated": ""
    },
    {
        "title": "On Surgical Fine-tuning for Language Encoders",
        "url": "http://arxiv.org/abs/2310.17041v1",
        "pub_date": "2023-10-25",
        "summary": "Fine-tuning all the layers of a pre-trained neural language encoder (either\nusing all the parameters or using parameter-efficient methods) is often the\nde-facto way of adapting it to a new task. We show evidence that for different\ndownstream language tasks, fine-tuning only a subset of layers is sufficient to\nobtain performance that is close to and often better than fine-tuning all the\nlayers in the language encoder. We propose an efficient metric based on the\ndiagonal of the Fisher information matrix (FIM score), to select the candidate\nlayers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE\ntasks and across distinct language encoders, that this metric can effectively\nselect layers leading to a strong downstream performance. Our work highlights\nthat task-specific information corresponding to a given downstream task is\noften localized within a few layers, and tuning only those is sufficient for\nstrong performance. Additionally, we demonstrate the robustness of the FIM\nscore to rank layers in a manner that remains constant during the optimization\nprocess.",
        "translated": ""
    },
    {
        "title": "The Word2vec Graph Model for Author Attribution and Genre Detection in\n  Literary Analysis",
        "url": "http://arxiv.org/abs/2310.16972v1",
        "pub_date": "2023-10-25",
        "summary": "Analyzing the writing styles of authors and articles is a key to supporting\nvarious literary analyses such as author attribution and genre detection. Over\nthe years, rich sets of features that include stylometry, bag-of-words, n-grams\nhave been widely used to perform such analysis. However, the effectiveness of\nthese features largely depends on the linguistic aspects of a particular\nlanguage and datasets specific characteristics. Consequently, techniques based\non these feature sets cannot give desired results across domains. In this\npaper, we propose a novel Word2vec graph based modeling of a document that can\nrightly capture both context and style of the document. By using these Word2vec\ngraph based features, we perform classification to perform author attribution\nand genre detection tasks. Our detailed experimental study with a comprehensive\nset of literary writings shows the effectiveness of this method over\ntraditional feature based approaches. Our code and data are publicly available\nat https://cutt.ly/svLjSgk",
        "translated": ""
    },
    {
        "title": "Text2Bundle: Towards Personalized Query-based Bundle Generation",
        "url": "http://arxiv.org/abs/2310.18004v1",
        "pub_date": "2023-10-27",
        "summary": "Bundle generation aims to provide a bundle of items for the user, and has\nbeen widely studied and applied on online service platforms. Existing bundle\ngeneration methods mainly utilized user's preference from historical\ninteractions in common recommendation paradigm, and ignored the potential\ntextual query which is user's current explicit intention. There can be a\nscenario in which a user proactively queries a bundle with some natural\nlanguage description, the system should be able to generate a bundle that\nexactly matches the user's intention through the user's query and preferences.\nIn this work, we define this user-friendly scenario as Query-based Bundle\nGeneration task and propose a novel framework Text2Bundle that leverages both\nthe user's short-term interests from the query and the user's long-term\npreferences from the historical interactions. Our framework consists of three\nmodules: (1) a query interest extractor that mines the user's fine-grained\ninterests from the query; (2) a unified state encoder that learns the current\nbundle context state and the user's preferences based on historical interaction\nand current query; and (3) a bundle generator that generates personalized and\ncomplementary bundles using a reinforcement learning with specifically designed\nrewards. We conduct extensive experiments on three real-world datasets and\ndemonstrate the effectiveness of our framework compared with several\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Chain-of-Choice Hierarchical Policy Learning for Conversational\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.17922v1",
        "pub_date": "2023-10-27",
        "summary": "Conversational Recommender Systems (CRS) illuminate user preferences via\nmulti-round interactive dialogues, ultimately navigating towards precise and\nsatisfactory recommendations. However, contemporary CRS are limited to\ninquiring binary or multi-choice questions based on a single attribute type\n(e.g., color) per round, which causes excessive rounds of interaction and\ndiminishes the user's experience. To address this, we propose a more realistic\nand efficient conversational recommendation problem setting, called\nMulti-Type-Attribute Multi-round Conversational Recommendation (MTAMCR), which\nenables CRS to inquire about multi-choice questions covering multiple types of\nattributes in each round, thereby improving interactive efficiency. Moreover,\nby formulating MTAMCR as a hierarchical reinforcement learning task, we propose\na Chain-of-Choice Hierarchical Policy Learning (CoCHPL) framework to enhance\nboth the questioning efficiency and recommendation effectiveness in MTAMCR.\nSpecifically, a long-term policy over options (i.e., ask or recommend)\ndetermines the action type, while two short-term intra-option policies\nsequentially generate the chain of attributes or items through multi-step\nreasoning and selection, optimizing the diversity and interdependence of\nquestioning attributes. Finally, extensive experiments on four benchmarks\ndemonstrate the superior performance of CoCHPL over prevailing state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "Ranking with Slot Constraints",
        "url": "http://arxiv.org/abs/2310.17870v1",
        "pub_date": "2023-10-27",
        "summary": "We introduce the problem of ranking with slot constraints, which can be used\nto model a wide range of application problems -- from college admission with\nlimited slots for different majors, to composing a stratified cohort of\neligible participants in a medical trial. We show that the conventional\nProbability Ranking Principle (PRP) can be highly sub-optimal for\nslot-constrained ranking problems, and we devise a new ranking algorithm,\ncalled MatchRank. The goal of MatchRank is to produce rankings that maximize\nthe number of filled slots if candidates are evaluated by a human decision\nmaker in the order of the ranking. In this way, MatchRank generalizes the PRP,\nand it subsumes the PRP as a special case when there are no slot constraints.\nOur theoretical analysis shows that MatchRank has a strong approximation\nguarantee without any independence assumptions between slots or candidates.\nFurthermore, we show how MatchRank can be implemented efficiently. Beyond the\ntheoretical guarantees, empirical evaluations show that MatchRank can provide\nsubstantial improvements over a range of synthetic and real-world tasks.",
        "translated": ""
    },
    {
        "title": "GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value\n  in Similar Item Recommendation",
        "url": "http://arxiv.org/abs/2310.17732v1",
        "pub_date": "2023-10-26",
        "summary": "Similar item recommendation is a critical task in the e-Commerce industry,\nwhich helps customers explore similar and relevant alternatives based on their\ninterested products. Despite the traditional machine learning models, Graph\nNeural Networks (GNNs), by design, can understand complex relations like\nsimilarity between products. However, in contrast to their wide usage in\nretrieval tasks and their focus on optimizing the relevance, the current GNN\narchitectures are not tailored toward maximizing revenue-related objectives\nsuch as Gross Merchandise Value (GMV), which is one of the major business\nmetrics for e-Commerce companies. In addition, defining accurate edge relations\nin GNNs is non-trivial in large-scale e-Commerce systems, due to the\nheterogeneity nature of the item-item relationships. This work aims to address\nthese issues by designing a new GNN architecture called GNN-GMVO (Graph Neural\nNetwork - Gross Merchandise Value Optimizer). This model directly optimizes GMV\nwhile considering the complex relations between items. In addition, we propose\na customized edge construction method to tailor the model toward similar item\nrecommendation task and alleviate the noisy and complex item-item relations. In\nour comprehensive experiments on three real-world datasets, we show higher\nprediction performance and expected GMV for top ranked items recommended by our\nmodel when compared with selected state-of-the-art benchmark models.",
        "translated": ""
    },
    {
        "title": "Poisoning Retrieval Corpora by Injecting Adversarial Passages",
        "url": "http://arxiv.org/abs/2310.19156v1",
        "pub_date": "2023-10-29",
        "summary": "Dense retrievers have achieved state-of-the-art performance in various\ninformation retrieval tasks, but to what extent can they be safely deployed in\nreal-world applications? In this work, we propose a novel attack for dense\nretrieval systems in which a malicious user generates a small number of\nadversarial passages by perturbing discrete tokens to maximize similarity with\na provided set of training queries. When these adversarial passages are\ninserted into a large retrieval corpus, we show that this attack is highly\neffective in fooling these systems to retrieve them for queries that were not\nseen by the attacker. More surprisingly, these adversarial passages can\ndirectly generalize to out-of-domain queries and corpora with a high success\nattack rate -- for instance, we find that 50 generated passages optimized on\nNatural Questions can mislead &gt;94% of questions posed in financial documents or\nonline forums. We also benchmark and compare a range of state-of-the-art dense\nretrievers, both unsupervised and supervised. Although different systems\nexhibit varying levels of vulnerability, we show they can all be successfully\nattacked by injecting up to 500 passages, a small fraction compared to a\nretrieval corpus of millions of passages.",
        "translated": ""
    },
    {
        "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query\n  Expansion",
        "url": "http://arxiv.org/abs/2310.19056v1",
        "pub_date": "2023-10-29",
        "summary": "Query expansion is a commonly-used technique in many search systems to better\nrepresent users' information needs with additional query terms. Existing\nstudies for this task usually propose to expand a query with retrieved or\ngenerated contextual documents. However, both types of methods have clear\nlimitations. For retrieval-based methods, the documents retrieved with the\noriginal query might not be accurate enough to reveal the search intent,\nespecially when the query is brief or ambiguous. For generation-based methods,\nexisting models can hardly be trained or aligned on a particular corpus, due to\nthe lack of corpus-specific labeled data. In this paper, we propose a novel\nLarge Language Model (LLM) based mutual verification framework for query\nexpansion, which alleviates the aforementioned limitations. Specifically, we\nfirst design a query-query-document generation pipeline, which can effectively\nleverage the contextual knowledge encoded in LLMs to generate sub-queries and\ncorresponding documents from multiple perspectives. Next, we employ a mutual\nverification method for both generated and retrieved contextual documents,\nwhere 1) retrieved documents are filtered with the external contextual\nknowledge in generated documents, and 2) generated documents are filtered with\nthe corpus-specific knowledge in retrieved documents. Overall, the proposed\nmethod allows retrieved and generated documents to complement each other to\nfinalize a better query expansion. We conduct extensive experiments on three\ninformation retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.\nThe results demonstrate that our method outperforms other baselines\nsignificantly.",
        "translated": ""
    },
    {
        "title": "A Multimodal Ecological Civilization Pattern Recommendation Method Based\n  on Large Language Models and Knowledge Graph",
        "url": "http://arxiv.org/abs/2310.18951v1",
        "pub_date": "2023-10-29",
        "summary": "The Ecological Civilization Pattern Recommendation System (ECPRS) aims to\nrecommend suitable ecological civilization patterns for target regions,\npromoting sustainable development and reducing regional disparities. However,\nthe current representative recommendation methods are not suitable for\nrecommending ecological civilization patterns in a geographical context. There\nare two reasons for this. Firstly, regions have spatial heterogeneity, and the\n(ECPRS)needs to consider factors like climate, topography, vegetation, etc., to\nrecommend civilization patterns adapted to specific ecological environments,\nensuring the feasibility and practicality of the recommendations. Secondly, the\nabstract features of the ecological civilization patterns in the real world\nhave not been fully utilized., resulting in poor richness in their embedding\nrepresentations and consequently, lower performance of the recommendation\nsystem. Considering these limitations, we propose the ECPR-MML method.\nInitially, based on the novel method UGPIG, we construct a knowledge graph to\nextract regional representations incorporating spatial heterogeneity features.\nFollowing that, inspired by the significant progress made by Large Language\nModels (LLMs) in the field of Natural Language Processing (NLP), we employ\nLarge LLMs to generate multimodal features for ecological civilization patterns\nin the form of text and images. We extract and integrate these multimodal\nfeatures to obtain semantically rich representations of ecological\ncivilization. Through extensive experiments, we validate the performance of our\nECPR-MML model. Our results show that F1@5 is 2.11% higher compared to\nstate-of-the-art models, 2.02% higher than NGCF, and 1.16% higher than UGPIG.\nFurthermore, multimodal data can indeed enhance recommendation performance.\nHowever, the data generated by LLM is not as effective as real data to a\ncertain extent.",
        "translated": ""
    },
    {
        "title": "The diminishing state of shared reality on US television news",
        "url": "http://arxiv.org/abs/2310.18863v1",
        "pub_date": "2023-10-29",
        "summary": "The potential for a large, diverse population to coexist peacefully is\nthought to depend on the existence of a ``shared reality:'' a public sphere in\nwhich participants are exposed to similar facts about similar topics. A\ngeneration ago, broadcast television news was widely considered to serve this\nfunction; however, since the rise of cable news in the 1990s, critics and\nscholars have worried that the corresponding fragmentation and segregation of\naudiences along partisan lines has caused this shared reality to be lost. Here\nwe examine this concern using a unique combination of data sets tracking the\nproduction (since 2012) and consumption (since 2016) of television news content\non the three largest cable and broadcast networks respectively. With regard to\nproduction, we find strong evidence for the ``loss of shared reality\nhypothesis:'' while broadcast continues to cover similar topics with similar\nlanguage, cable news networks have become increasingly distinct, both from\nbroadcast news and each other, diverging both in terms of content and language.\nWith regard to consumption, we find more mixed evidence: while broadcast news\nhas indeed declined in popularity, it remains the dominant source of news for\nroughly 50\\% more Americans than does cable; moreover, its decline, while\nsomewhat attributable to cable, appears driven more by a shift away from news\nconsumption altogether than a growth in cable consumption. We conclude that\nshared reality on US television news is indeed diminishing, but is more robust\nthan previously thought and is declining for somewhat different reasons.",
        "translated": ""
    },
    {
        "title": "Leveraging Multimodal Features and Item-level User Feedback for Bundle\n  Construction",
        "url": "http://arxiv.org/abs/2310.18770v1",
        "pub_date": "2023-10-28",
        "summary": "Automatic bundle construction is a crucial prerequisite step in various\nbundle-aware online services. Previous approaches are mostly designed to model\nthe bundling strategy of existing bundles. However, it is hard to acquire\nlarge-scale well-curated bundle dataset, especially for those platforms that\nhave not offered bundle services before. Even for platforms with mature bundle\nservices, there are still many items that are included in few or even zero\nbundles, which give rise to sparsity and cold-start challenges in the bundle\nconstruction models. To tackle these issues, we target at leveraging multimodal\nfeatures, item-level user feedback signals, and the bundle composition\ninformation, to achieve a comprehensive formulation of bundle construction.\nNevertheless, such formulation poses two new technical challenges: 1) how to\nlearn effective representations by optimally unifying multiple features, and 2)\nhow to address the problems of modality missing, noise, and sparsity problems\ninduced by the incomplete query bundles. In this work, to address these\ntechnical challenges, we propose a Contrastive Learning-enhanced Hierarchical\nEncoder method (CLHE). Specifically, we use self-attention modules to combine\nthe multimodal and multi-item features, and then leverage both item- and\nbundle-level contrastive learning to enhance the representation learning, thus\nto counter the modality missing, noise, and sparsity problems. Extensive\nexperiments on four datasets in two application domains demonstrate that our\nmethod outperforms a list of SOTA methods. The code and dataset are available\nat https://github.com/Xiaohao-Liu/CLHE.",
        "translated": ""
    },
    {
        "title": "Empowering Collaborative Filtering with Principled Adversarial\n  Contrastive Loss",
        "url": "http://arxiv.org/abs/2310.18700v1",
        "pub_date": "2023-10-28",
        "summary": "Contrastive Learning (CL) has achieved impressive performance in\nself-supervised learning tasks, showing superior generalization ability.\nInspired by the success, adopting CL into collaborative filtering (CF) is\nprevailing in semi-supervised top-K recommendations. The basic idea is to\nroutinely conduct heuristic-based data augmentation and apply contrastive\nlosses (e.g., InfoNCE) on the augmented views. Yet, some CF-tailored challenges\nmake this adoption suboptimal, such as the issue of out-of-distribution, the\nrisk of false negatives, and the nature of top-K evaluation. They necessitate\nthe CL-based CF scheme to focus more on mining hard negatives and\ndistinguishing false negatives from the vast unlabeled user-item interactions,\nfor informative contrast signals. Worse still, there is limited understanding\nof contrastive loss in CF methods, especially w.r.t. its generalization\nability. To bridge the gap, we delve into the reasons underpinning the success\nof contrastive loss in CF, and propose a principled Adversarial InfoNCE loss\n(AdvInfoNCE), which is a variant of InfoNCE, specially tailored for CF methods.\nAdvInfoNCE adaptively explores and assigns hardness to each negative instance\nin an adversarial fashion and further utilizes a fine-grained hardness-aware\nranking criterion to empower the recommender's generalization ability. Training\nCF models with AdvInfoNCE, we validate the effectiveness of AdvInfoNCE on both\nsynthetic and real-world benchmark datasets, thus showing its generalization\nability to mitigate out-of-distribution problems. Given the theoretical\nguarantees and empirical superiority of AdvInfoNCE over most contrastive loss\nfunctions, we advocate its adoption as a standard loss in recommender systems,\nparticularly for the out-of-distribution tasks. Codes are available at\nhttps://github.com/LehengTHU/AdvInfoNCE.",
        "translated": ""
    },
    {
        "title": "Dense Retrieval as Indirect Supervision for Large-space Decision Making",
        "url": "http://arxiv.org/abs/2310.18619v1",
        "pub_date": "2023-10-28",
        "summary": "Many discriminative natural language understanding (NLU) tasks have large\nlabel spaces. Learning such a process of large-space decision making is\nparticularly challenging due to the lack of training instances per label and\nthe difficulty of selection among many fine-grained labels. Inspired by dense\nretrieval methods for passage finding in open-domain QA, we propose a\nreformulation of large-space discriminative NLU tasks as a learning-to-retrieve\ntask, leading to a novel solution named Dense Decision Retrieval (DDR ).\nInstead of predicting fine-grained decisions as logits, DDR adopts a\ndual-encoder architecture that learns to predict by retrieving from a decision\nthesaurus. This approach not only leverages rich indirect supervision signals\nfrom easy-to-consume learning resources for dense retrieval, it also leads to\nenhanced prediction generalizability with a semantically meaningful\nrepresentation of the large decision space. When evaluated on tasks with\ndecision spaces ranging from hundreds to hundred-thousand scales, DDR\noutperforms strong baselines greatly by 27.54% in P@1 on two extreme\nmulti-label classification tasks, 1.17% in F1 score ultra-fine entity typing,\nand 1.26% in accuracy on three few-shot intent classification tasks on average.\nCode and resources are available at https://github.com/luka-group/DDR",
        "translated": ""
    },
    {
        "title": "Embedding in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2310.18608v1",
        "pub_date": "2023-10-28",
        "summary": "Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that coverts the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors and can enhance\nthe recommendation performance. Applying embedding techniques captures complex\nentity relationships and has spurred substantial research. In this survey, we\nprovide an overview of the recent literature on embedding techniques in\nrecommender systems. This survey covers embedding methods like collaborative\nfiltering, self-supervised learning, and graph-based techniques. Collaborative\nfiltering generates embeddings capturing user-item preferences, excelling in\nsparse data. Self-supervised methods leverage contrastive or generative\nlearning for various tasks. Graph-based techniques like node2vec exploit\ncomplex relationships in network-rich environments. Addressing the scalability\nchallenges inherent to embedding methods, our survey delves into innovative\ndirections within the field of recommendation systems. These directions aim to\nenhance performance and reduce computational complexity, paving the way for\nimproved recommender systems. Among these innovative approaches, we will\nintroduce Auto Machine Learning (AutoML), hash techniques, and quantization\ntechniques in this survey. We discuss various architectures and techniques and\nhighlight the challenges and future directions in these aspects. This survey\naims to provide a comprehensive overview of the state-of-the-art in this\nrapidly evolving field and serve as a useful resource for researchers and\npractitioners working in the area of recommender systems.",
        "translated": ""
    },
    {
        "title": "GATSY: Graph Attention Network for Music Artist Similarity",
        "url": "http://arxiv.org/abs/2311.00635v1",
        "pub_date": "2023-11-01",
        "summary": "The artist similarity quest has become a crucial subject in social and\nscientific contexts. Modern research solutions facilitate music discovery\naccording to user tastes. However, defining similarity among artists may\ninvolve several aspects, even related to a subjective perspective, and it often\naffects a recommendation. This paper presents GATSY, a recommendation system\nbuilt upon graph attention networks and driven by a clusterized embedding of\nartists. The proposed framework takes advantage of a graph topology of the\ninput data to achieve outstanding performance results without relying heavily\non hand-crafted features. This flexibility allows us to introduce fictitious\nartists in a music dataset, create bridges to previously unrelated artists, and\nget recommendations conditioned by possibly heterogeneous sources. Experimental\nresults prove the effectiveness of the proposed method with respect to\nstate-of-the-art solutions.",
        "translated": ""
    },
    {
        "title": "A Collaborative Filtering-Based Two Stage Model with Item Dependency for\n  Course Recommendation",
        "url": "http://arxiv.org/abs/2311.00612v1",
        "pub_date": "2023-11-01",
        "summary": "Recommender systems have been studied for decades with numerous promising\nmodels been proposed. Among them, Collaborative Filtering (CF) models are\narguably the most successful one due to its high accuracy in recommendation and\nelimination of privacy-concerned personal meta-data from training. This paper\nextends the usage of CF-based model to the task of course recommendation. We\npoint out several challenges in applying the existing CF-models to build a\ncourse recommendation engine, including the lack of rating and meta-data, the\nimbalance of course registration distribution, and the demand of course\ndependency modeling. We then propose several ideas to address these challenges.\nEventually, we combine a two-stage CF model regularized by course dependency\nwith a graph-based recommender based on course-transition network, to achieve\nAUC as high as 0.97 with a real-world dataset.",
        "translated": ""
    },
    {
        "title": "Bayes-enhanced Multi-view Attention Networks for Robust POI\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.00491v1",
        "pub_date": "2023-11-01",
        "summary": "POI recommendation is practically important to facilitate various\nLocation-Based Social Network services, and has attracted rising research\nattention recently. Existing works generally assume the available POI check-ins\nreported by users are the ground-truth depiction of user behaviors. However, in\nreal application scenarios, the check-in data can be rather unreliable due to\nboth subjective and objective causes including positioning error and user\nprivacy concerns, leading to significant negative impacts on the performance of\nthe POI recommendation. To this end, we investigate a novel problem of robust\nPOI recommendation by considering the uncertainty factors of the user\ncheck-ins, and proposes a Bayes-enhanced Multi-view Attention Network.\nSpecifically, we construct personal POI transition graph, the semantic-based\nPOI graph and distance-based POI graph to comprehensively model the\ndependencies among the POIs. As the personal POI transition graph is usually\nsparse and sensitive to noise, we design a Bayes-enhanced spatial dependency\nlearning module for data augmentation from the local view. A Bayesian posterior\nguided graph augmentation approach is adopted to generate a new graph with\ncollaborative signals to increase the data diversity. Then both the original\nand the augmented graphs are used for POI representation learning to counteract\nthe data uncertainty issue. Next, the POI representations of the three view\ngraphs are input into the proposed multi-view attention-based user preference\nlearning module. By incorporating the semantic and distance correlations of\nPOIs, the user preference can be effectively refined and finally robust\nrecommendation results are achieved. The results of extensive experiments show\nthat BayMAN significantly outperforms the state-of-the-art methods in POI\nrecommendation when the available check-ins are incomplete and noisy.",
        "translated": ""
    },
    {
        "title": "LLMRec: Large Language Models with Graph Augmentation for Recommendation",
        "url": "http://arxiv.org/abs/2311.00423v1",
        "pub_date": "2023-11-01",
        "summary": "The problem of data sparsity has long been a challenge in recommendation\nsystems, and previous studies have attempted to address this issue by\nincorporating side information. However, this approach often introduces side\neffects such as noise, availability issues, and low data quality, which in turn\nhinder the accurate modeling of user preferences and adversely impact\nrecommendation performance. In light of the recent advancements in large\nlanguage models (LLMs), which possess extensive knowledge bases and strong\nreasoning capabilities, we propose a novel framework called LLMRec that\nenhances recommender systems by employing three simple yet effective LLM-based\ngraph augmentation strategies. Our approach leverages the rich content\navailable within online platforms (e.g., Netflix, MovieLens) to augment the\ninteraction graph in three ways: (i) reinforcing user-item interaction egde,\n(ii) enhancing the understanding of item node attributes, and (iii) conducting\nuser node profiling, intuitively from the natural language perspective. By\nemploying these strategies, we address the challenges posed by sparse implicit\nfeedback and low-quality side information in recommenders. Besides, to ensure\nthe quality of the augmentation, we develop a denoised data robustification\nmechanism that includes techniques of noisy implicit feedback pruning and\nMAE-based feature enhancement that help refine the augmented data and improve\nits reliability. Furthermore, we provide theoretical analysis to support the\neffectiveness of LLMRec and clarify the benefits of our method in facilitating\nmodel optimization. Experimental results on benchmark datasets demonstrate the\nsuperiority of our LLM-based augmentation approach over state-of-the-art\ntechniques. To ensure reproducibility, we have made our code and augmented data\npublicly available at: https://github.com/HKUDS/LLMRec.git",
        "translated": ""
    },
    {
        "title": "Towards Automatic Sampling of User Behaviors for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2311.00388v1",
        "pub_date": "2023-11-01",
        "summary": "Sequential recommender systems (SRS) have gained widespread popularity in\nrecommendation due to their ability to effectively capture dynamic user\npreferences. One default setting in the current SRS is to uniformly consider\neach historical behavior as a positive interaction. Actually, this setting has\nthe potential to yield sub-optimal performance, as each item makes a distinct\ncontribution to the user's interest. For example, purchased items should be\ngiven more importance than clicked ones. Hence, we propose a general automatic\nsampling framework, named AutoSAM, to non-uniformly treat historical behaviors.\nSpecifically, AutoSAM augments the standard sequential recommendation\narchitecture with an additional sampler layer to adaptively learn the skew\ndistribution of the raw input, and then sample informative sub-sets to build\nmore generalizable SRS. To overcome the challenges of non-differentiable\nsampling actions and also introduce multiple decision factors for sampling, we\nfurther introduce a novel reinforcement learning based method to guide the\ntraining of the sampler. We theoretically design multi-objective sampling\nrewards including Future Prediction and Sequence Perplexity, and then optimize\nthe whole framework in an end-to-end manner by combining the policy gradient.\nWe conduct extensive experiments on benchmark recommender models and four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nthe proposed approach. We will make our code publicly available after the\nacceptance.",
        "translated": ""
    },
    {
        "title": "Caseformer: Pre-training for Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2311.00333v1",
        "pub_date": "2023-11-01",
        "summary": "Legal case retrieval aims to help legal workers find relevant cases related\nto their cases at hand, which is important for the guarantee of fairness and\njustice in legal judgments. While recent advances in neural retrieval methods\nhave significantly improved the performance of open-domain retrieval tasks\n(e.g., Web search), their advantages have not been observed in legal case\nretrieval due to their thirst for annotated data. As annotating large-scale\ntraining data in legal domains is prohibitive due to the need for domain\nexpertise, traditional search techniques based on lexical matching such as\nTF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval\nsystems. While previous studies have designed several pre-training methods for\nIR models in open-domain tasks, these methods are usually suboptimal in legal\ncase retrieval because they cannot understand and capture the key knowledge and\ndata structures in the legal corpus. To this end, we propose a novel\npre-training framework named Caseformer that enables the pre-trained models to\nlearn legal knowledge and domain-specific relevance information in legal case\nretrieval without any human-labeled data. Through three unsupervised learning\ntasks, Caseformer is able to capture the special language, document structure,\nand relevance patterns of legal case documents, making it a strong backbone for\ndownstream legal case retrieval tasks. Experimental results show that our model\nhas achieved state-of-the-art performance in both zero-shot and full-data\nfine-tuning settings. Also, experiments on both Chinese and English legal\ndatasets demonstrate that the effectiveness of Caseformer is\nlanguage-independent in legal case retrieval.",
        "translated": ""
    },
    {
        "title": "Federated Topic Model and Model Pruning Based on Variational Autoencoder",
        "url": "http://arxiv.org/abs/2311.00314v1",
        "pub_date": "2023-11-01",
        "summary": "Topic modeling has emerged as a valuable tool for discovering patterns and\ntopics within large collections of documents. However, when cross-analysis\ninvolves multiple parties, data privacy becomes a critical concern. Federated\ntopic modeling has been developed to address this issue, allowing multiple\nparties to jointly train models while protecting pri-vacy. However, there are\ncommunication and performance challenges in the federated sce-nario. In order\nto solve the above problems, this paper proposes a method to establish a\nfederated topic model while ensuring the privacy of each node, and use neural\nnetwork model pruning to accelerate the model, where the client periodically\nsends the model neu-ron cumulative gradients and model weights to the server,\nand the server prunes the model. To address different requirements, two\ndifferent methods are proposed to determine the model pruning rate. The first\nmethod involves slow pruning throughout the entire model training process,\nwhich has limited acceleration effect on the model training process, but can\nensure that the pruned model achieves higher accuracy. This can significantly\nreduce the model inference time during the inference process. The second\nstrategy is to quickly reach the target pruning rate in the early stage of\nmodel training in order to accelerate the model training speed, and then\ncontinue to train the model with a smaller model size after reaching the target\npruning rate. This approach may lose more useful information but can complete\nthe model training faster. Experimental results show that the federated topic\nmodel pruning based on the variational autoencoder proposed in this paper can\ngreatly accelerate the model training speed while ensuring the model's\nperformance.",
        "translated": ""
    },
    {
        "title": "DistDNAS: Search Efficient Feature Interactions within 2 Hours",
        "url": "http://arxiv.org/abs/2311.00231v1",
        "pub_date": "2023-11-01",
        "summary": "Search efficiency and serving efficiency are two major axes in building\nfeature interactions and expediting the model development process in\nrecommender systems. On large-scale benchmarks, searching for the optimal\nfeature interaction design requires extensive cost due to the sequential\nworkflow on the large volume of data. In addition, fusing interactions of\nvarious sources, orders, and mathematical operations introduces potential\nconflicts and additional redundancy toward recommender models, leading to\nsub-optimal trade-offs in performance and serving cost. In this paper, we\npresent DistDNAS as a neat solution to brew swift and efficient feature\ninteraction design. DistDNAS proposes a supernet to incorporate interaction\nmodules of varying orders and types as a search space. To optimize search\nefficiency, DistDNAS distributes the search and aggregates the choice of\noptimal interaction modules on varying data dates, achieving over 25x speed-up\nand reducing search cost from 2 days to 2 hours. To optimize serving\nefficiency, DistDNAS introduces a differentiable cost-aware loss to penalize\nthe selection of redundant interaction modules, enhancing the efficiency of\ndiscovered feature interactions in serving. We extensively evaluate the best\nmodels crafted by DistDNAS on a 1TB Criteo Terabyte dataset. Experimental\nevaluations demonstrate 0.001 AUC improvement and 60% FLOPs saving over current\nstate-of-the-art CTR models.",
        "translated": ""
    },
    {
        "title": "Farthest Greedy Path Sampling for Two-shot Recommender Search",
        "url": "http://arxiv.org/abs/2310.20705v1",
        "pub_date": "2023-10-31",
        "summary": "Weight-sharing Neural Architecture Search (WS-NAS) provides an efficient\nmechanism for developing end-to-end deep recommender models. However, in\ncomplex search spaces, distinguishing between superior and inferior\narchitectures (or paths) is challenging. This challenge is compounded by the\nlimited coverage of the supernet and the co-adaptation of subnet weights, which\nrestricts the exploration and exploitation capabilities inherent to\nweight-sharing mechanisms. To address these challenges, we introduce Farthest\nGreedy Path Sampling (FGPS), a new path sampling strategy that balances path\nquality and diversity. FGPS enhances path diversity to facilitate more\ncomprehensive supernet exploration, while emphasizing path quality to ensure\nthe effective identification and utilization of promising architectures. By\nincorporating FGPS into a Two-shot NAS (TS-NAS) framework, we derive\nhigh-performance architectures. Evaluations on three Click-Through Rate (CTR)\nprediction benchmarks demonstrate that our approach consistently achieves\nsuperior results, outperforming both manually designed and most NAS-based\nmodels.",
        "translated": ""
    },
    {
        "title": "Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding",
        "url": "http://arxiv.org/abs/2310.20588v1",
        "pub_date": "2023-10-31",
        "summary": "In the era of the Internet of Things (IoT), the retrieval of relevant medical\ninformation has become essential for efficient clinical decision-making. This\npaper introduces MedFusionRank, a novel approach to zero-shot medical\ninformation retrieval (MIR) that combines the strengths of pre-trained language\nmodels and statistical methods while addressing their limitations. The proposed\napproach leverages a pre-trained BERT-style model to extract compact yet\ninformative keywords. These keywords are then enriched with domain knowledge by\nlinking them to conceptual entities within a medical knowledge graph.\nExperimental evaluations on medical datasets demonstrate MedFusion Rank's\nsuperior performance over existing methods, with promising results with a\nvariety of evaluation metrics. MedFusionRank demonstrates efficacy in\nretrieving relevant information, even from short or single-term queries.",
        "translated": ""
    },
    {
        "title": "Collaborative Large Language Model for Recommender Systems",
        "url": "http://arxiv.org/abs/2311.01343v1",
        "pub_date": "2023-11-02",
        "summary": "Recently, there is a growing interest in developing next-generation\nrecommender systems (RSs) based on pretrained large language models (LLMs),\nfully utilizing their encoded knowledge and reasoning ability. However, the\nsemantic gap between natural language and recommendation tasks is still not\nwell addressed, leading to multiple issues such as spuriously-correlated\nuser/item descriptors, ineffective language modeling on user/item contents, and\ninefficient recommendations via auto-regression, etc. In this paper, we propose\nCLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and\nID paradigm of RS, aiming to address the above challenges simultaneously. We\nfirst extend the vocabulary of pretrained LLMs with user/item ID tokens to\nfaithfully model the user/item collaborative and content semantics.\nAccordingly, in the pretraining stage, a novel soft+hard prompting strategy is\nproposed to effectively learn user/item collaborative/content token embeddings\nvia language modeling on RS-specific corpora established from user-item\ninteractions and user/item features, where each document is split into a prompt\nconsisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and\na main text consisting of homogeneous item tokens or vocab tokens that\nfacilitates stable and effective language modeling. In addition, a novel mutual\nregularization strategy is introduced to encourage the CLLM4Rec to capture\nrecommendation-oriented information from user/item contents. Finally, we\npropose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where\nan item prediction head with multinomial likelihood is added to the pretrained\nCLLM4Rec backbone to predict hold-out items based on the soft+hard prompts\nestablished from masked user-item interaction history, where recommendations of\nmultiple items can be generated efficiently.",
        "translated": ""
    },
    {
        "title": "Recommendations by Concise User Profiles from Review Text",
        "url": "http://arxiv.org/abs/2311.01314v1",
        "pub_date": "2023-11-02",
        "summary": "Recommender systems are most successful for popular items and users with\nample interactions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of supporting users who have very sparse interactions but\npost informative review texts. Our experimental studies address two book\ncommunities with these characteristics. We design a framework with\nTransformer-based representation learning, covering user-item interactions,\nitem content, and user-provided reviews. To overcome interaction sparseness, we\ndevise techniques for selecting the most informative cues to construct concise\nuser profiles. Comprehensive experiments, with datasets from Amazon and\nGoodreads, show that judicious selection of text snippets achieves the best\nperformance, even in comparison to ChatGPT-generated user profiles.",
        "translated": ""
    },
    {
        "title": "VM-Rec: A Variational Mapping Approach for Cold-start User\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.01304v1",
        "pub_date": "2023-11-02",
        "summary": "The cold-start problem is a common challenge for most recommender systems.\nWith extremely limited interactions of cold-start users, conventional\nrecommender models often struggle to generate embeddings with sufficient\nexpressivity. Moreover, the absence of auxiliary content information of users\nexacerbates the presence of challenges, rendering most cold-start methods\ndifficult to apply. To address this issue, our motivation is based on the\nobservation that if a model can generate expressive embeddings for existing\nusers with relatively more interactions, who were also initially cold-start\nusers, then we can establish a mapping from few initial interactions to\nexpressive embeddings, simulating the process of generating embeddings for\ncold-start users. Based on this motivation, we propose a Variational Mapping\napproach for cold-start user Recommendation (VM-Rec). Firstly, we generate a\npersonalized mapping function for cold-start users based on their initial\ninteractions, and parameters of the function are generated from a variational\ndistribution. For the sake of interpretability and computational efficiency, we\nmodel the personalized mapping function as a sparse linear model, where each\nparameter indicates the association to a specific existing user. Consequently,\nwe use this mapping function to map the embeddings of existing users to an\nembedding of the cold-start user in the same space. The resulting embedding has\nsimilar expressivity to that of existing users and can be directly integrated\ninto a pre-trained recommender model to predict click through rates or ranking\nscores. We evaluate our method based on three widely used recommender models as\npre-trained base recommender models, outperforming four popular cold-start\nmethods on two datasets under the same base model.",
        "translated": ""
    },
    {
        "title": "Efficient Neural Ranking using Forward Indexes and Lightweight Encoders",
        "url": "http://arxiv.org/abs/2311.01263v1",
        "pub_date": "2023-11-02",
        "summary": "Dual-encoder-based dense retrieval models have become the standard in IR.\nThey employ large Transformer-based language models, which are notoriously\ninefficient in terms of resources and latency. We propose Fast-Forward indexes\n-- vector forward indexes which exploit the semantic matching capabilities of\ndual-encoder models for efficient and effective re-ranking. Our framework\nenables re-ranking at very high retrieval depths and combines the merits of\nboth lexical and semantic matching via score interpolation. Furthermore, in\norder to mitigate the limitations of dual-encoders, we tackle two main\nchallenges: Firstly, we improve computational efficiency by either\npre-computing representations, avoiding unnecessary computations altogether, or\nreducing the complexity of encoders. This allows us to considerably improve\nranking efficiency and latency. Secondly, we optimize the memory footprint and\nmaintenance cost of indexes; we propose two complementary techniques to reduce\nthe index size and show that, by dynamically dropping irrelevant document\ntokens, the index maintenance efficiency can be improved substantially. We\nperform evaluation to show the effectiveness and efficiency of Fast-Forward\nindexes -- our method has low latency and achieves competitive results without\nthe need for hardware acceleration, such as GPUs.",
        "translated": ""
    },
    {
        "title": "Navigating Complex Search Tasks with AI Copilots",
        "url": "http://arxiv.org/abs/2311.01235v1",
        "pub_date": "2023-11-02",
        "summary": "As many of us in the information retrieval (IR) research community know and\nappreciate, search is far from being a solved problem. Millions of people\nstruggle with tasks on search engines every day. Often, their struggles relate\nto the intrinsic complexity of their task and the failure of search systems to\nfully understand the task and serve relevant results. The task motivates the\nsearch, creating the gap/problematic situation that searchers attempt to\nbridge/resolve and drives search behavior as they work through different task\nfacets. Complex search tasks require more than support for rudimentary fact\nfinding or re-finding. Research on methods to support complex tasks includes\nwork on generating query and website suggestions, personalizing and\ncontextualizing search, and developing new search experiences, including those\nthat span time and space. The recent emergence of generative artificial\nintelligence (AI) and the arrival of assistive agents, or copilots, based on\nthis technology, has the potential to offer further assistance to searchers,\nespecially those engaged in complex tasks. There are profound implications from\nthese advances for the design of intelligent systems and for the future of\nsearch itself. This article, based on a keynote by the author at the 2023 ACM\nSIGIR Conference, explores these issues and charts a course toward new horizons\nin information access guided by AI copilots.",
        "translated": ""
    },
    {
        "title": "Bi-Preference Learning Heterogeneous Hypergraph Networks for\n  Session-based Recommendation",
        "url": "http://arxiv.org/abs/2311.01125v1",
        "pub_date": "2023-11-02",
        "summary": "Session-based recommendation intends to predict next purchased items based on\nanonymous behavior sequences. Numerous economic studies have revealed that item\nprice is a key factor influencing user purchase decisions. Unfortunately,\nexisting methods for session-based recommendation only aim at capturing user\ninterest preference, while ignoring user price preference. Actually, there are\nprimarily two challenges preventing us from accessing price preference.\nFirstly, the price preference is highly associated to various item features\n(i.e., category and brand), which asks us to mine price preference from\nheterogeneous information. Secondly, price preference and interest preference\nare interdependent and collectively determine user choice, necessitating that\nwe jointly consider both price and interest preference for intent modeling. To\nhandle above challenges, we propose a novel approach Bi-Preference Learning\nHeterogeneous Hypergraph Networks (BiPNet) for session-based recommendation.\nSpecifically, the customized heterogeneous hypergraph networks with a\ntriple-level convolution are devised to capture user price and interest\npreference from heterogeneous features of items. Besides, we develop a\nBi-Preference Learning schema to explore mutual relations between price and\ninterest preference and collectively learn these two preferences under the\nmulti-task learning architecture. Extensive experiments on multiple public\ndatasets confirm the superiority of BiPNet over competitive baselines.\nAdditional research also supports the notion that the price is crucial for the\ntask.",
        "translated": ""
    },
    {
        "title": "Collaboration and Transition: Distilling Item Transitions into\n  Multi-Query Self-Attention for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2311.01056v1",
        "pub_date": "2023-11-02",
        "summary": "Modern recommender systems employ various sequential modules such as\nself-attention to learn dynamic user interests. However, these methods are less\neffective in capturing collaborative and transitional signals within user\ninteraction sequences. First, the self-attention architecture uses the\nembedding of a single item as the attention query, which is inherently\nchallenging to capture collaborative signals. Second, these methods typically\nfollow an auto-regressive framework, which is unable to learn global item\ntransition patterns. To overcome these limitations, we propose a new method\ncalled Multi-Query Self-Attention with Transition-Aware Embedding Distillation\n(MQSA-TED). First, we propose an $L$-query self-attention module that employs\nflexible window sizes for attention queries to capture collaborative signals.\nIn addition, we introduce a multi-query self-attention method that balances the\nbias-variance trade-off in modeling user preferences by combining long and\nshort-query self-attentions. Second, we develop a transition-aware embedding\ndistillation module that distills global item-to-item transition patterns into\nitem embeddings, which enables the model to memorize and leverage transitional\nsignals and serves as a calibrator for collaborative signals. Experimental\nresults on four real-world datasets show the superiority of our proposed method\nover state-of-the-art sequential recommendation methods.",
        "translated": ""
    },
    {
        "title": "Evaluation Measures of Individual Item Fairness for Recommender Systems:\n  A Critical Study",
        "url": "http://arxiv.org/abs/2311.01013v1",
        "pub_date": "2023-11-02",
        "summary": "Fairness is an emerging and challenging topic in recommender systems. In\nrecent years, various ways of evaluating and therefore improving fairness have\nemerged. In this study, we examine existing evaluation measures of fairness in\nrecommender systems. Specifically, we focus solely on exposure-based fairness\nmeasures of individual items that aim to quantify the disparity in how\nindividual items are recommended to users, separate from item relevance to\nusers. We gather all such measures and we critically analyse their theoretical\nproperties. We identify a series of limitations in each of them, which\ncollectively may render the affected measures hard or impossible to interpret,\nto compute, or to use for comparing recommendations. We resolve these\nlimitations by redefining or correcting the affected measures, or we argue why\ncertain limitations cannot be resolved. We further perform a comprehensive\nempirical analysis of both the original and our corrected versions of these\nfairness measures, using real-world and synthetic datasets. Our analysis\nprovides novel insights into the relationship between measures based on\ndifferent fairness concepts, and different levels of measure sensitivity and\nstrictness. We conclude with practical suggestions of which fairness measures\nshould be used and when. Our code is publicly available. To our knowledge, this\nis the first critical comparison of individual item fairness measures in\nrecommender systems.",
        "translated": ""
    },
    {
        "title": "Research Team Identification Based on Representation Learning of\n  Academic Heterogeneous Information Network",
        "url": "http://arxiv.org/abs/2311.00922v1",
        "pub_date": "2023-11-02",
        "summary": "Academic networks in the real world can usually be described by heterogeneous\ninformation networks composed of multi-type nodes and relationships. Some\nexisting research on representation learning for homogeneous information\nnetworks lacks the ability to explore heterogeneous information networks in\nheterogeneous information networks. It cannot be applied to heterogeneous\ninformation networks. Aiming at the practical needs of effectively identifying\nand discovering scientific research teams from the academic heterogeneous\ninformation network composed of massive and complex scientific and\ntechnological big data, this paper proposes a scientific research team\nidentification method based on representation learning of academic\nheterogeneous information networks. The attention mechanism at node level and\nmeta-path level learns low-dimensional, dense and real-valued vector\nrepresentations on the basis of retaining the rich topological information of\nnodes in the network and the semantic information based on meta-paths, and\nrealizes effective identification and discovery of scientific research teams\nand important team members in academic heterogeneous information networks based\non maximizing node influence. Experimental results show that our proposed\nmethod outperforms the comparative methods.",
        "translated": ""
    },
    {
        "title": "Enhancing search engine precision and user experience through\n  sentiment-based polysemy resolution",
        "url": "http://arxiv.org/abs/2311.01895v1",
        "pub_date": "2023-11-03",
        "summary": "With the proliferation of digital content and the need for efficient\ninformation retrieval, this study's insights can be applied to various domains,\nincluding news services, e-commerce, and digital marketing, to provide users\nwith more meaningful and tailored experiences. The study addresses the common\nproblem of polysemy in search engines, where the same keyword may have multiple\nmeanings. It proposes a solution to this issue by embedding a smart search\nfunction into the search engine, which can differentiate between different\nmeanings based on sentiment. The study leverages sentiment analysis, a powerful\nnatural language processing (NLP) technique, to classify and categorize news\narticles based on their emotional tone. This can provide more insightful and\nnuanced search results. The article reports an impressive accuracy rate of 85%\nfor the proposed smart search function, which outperforms conventional search\nengines. This indicates the effectiveness of the sentiment-based approach. The\nresearch explores multiple sentiment analysis models, including Sentistrength\nand Valence Aware Dictionary for Sentiment Reasoning (VADER), to determine the\nbest-performing approach. The findings can be applied to enhance search\nengines, making them more capable of understanding the context and intent\nbehind users 'queries. This can lead to better search results that are more\naligned with what users are looking for. The proposed smart search function can\nimprove the user experience by reducing the need to sift through irrelevant\nsearch results. This is particularly important in an age where information\noverload is common.",
        "translated": ""
    },
    {
        "title": "Multi-EuP: The Multilingual European Parliament Dataset for Analysis of\n  Bias in Information Retrieval",
        "url": "http://arxiv.org/abs/2311.01870v1",
        "pub_date": "2023-11-03",
        "summary": "We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K\nmulti-lingual documents collected from the European Parliament, spanning 24\nlanguages. This dataset is designed to investigate fairness in a multilingual\ninformation retrieval (IR) context to analyze both language and demographic\nbias in a ranking context. It boasts an authentic multilingual corpus,\nfeaturing topics translated into all 24 languages, as well as cross-lingual\nrelevance judgments. Furthermore, it offers rich demographic information\nassociated with its documents, facilitating the study of demographic bias. We\nreport the effectiveness of Multi-EuP for benchmarking both monolingual and\nmultilingual IR. We also conduct a preliminary experiment on language bias\ncaused by the choice of tokenization strategy.",
        "translated": ""
    },
    {
        "title": "SortNet: Learning To Rank By a Neural-Based Sorting Algorithm",
        "url": "http://arxiv.org/abs/2311.01864v1",
        "pub_date": "2023-11-03",
        "summary": "The problem of relevance ranking consists of sorting a set of objects with\nrespect to a given criterion. Since users may prefer different relevance\ncriteria, the ranking algorithms should be adaptable to the user needs. Two\nmain approaches exist in literature for the task of learning to rank: 1) a\nscore function, learned by examples, which evaluates the properties of each\nobject yielding an absolute relevance value that can be used to order the\nobjects or 2) a pairwise approach, where a \"preference function\" is learned\nusing pairs of objects to define which one has to be ranked first. In this\npaper, we present SortNet, an adaptive ranking algorithm which orders objects\nusing a neural network as a comparator. The neural network training set\nprovides examples of the desired ordering between pairs of items and it is\nconstructed by an iterative procedure which, at each iteration, adds the most\ninformative training examples. Moreover, the comparator adopts a connectionist\narchitecture that is particularly suited for implementing a preference\nfunction. We also prove that such an architecture has the universal\napproximation property and can implement a wide class of functions. Finally,\nthe proposed algorithm is evaluated on the LETOR dataset showing promising\nperformances in comparison with other state of the art algorithms.",
        "translated": ""
    },
    {
        "title": "Universal Multi-modal Multi-domain Pre-trained Recommendation",
        "url": "http://arxiv.org/abs/2311.01831v1",
        "pub_date": "2023-11-03",
        "summary": "There is a rapidly-growing research interest in modeling user preferences via\npre-training multi-domain interactions for recommender systems. However,\nExisting pre-trained multi-domain recommendations mostly select the item texts\nto be bridges across domains, and simply explore the user behaviors in target\ndomains. Hence, they ignore other informative multi-modal item contents (e.g.,\nvisual information), and also lack of thorough consideration of user behaviors\nfrom all interactive domains. To address these issues, in this paper, we\npropose to pre-train universal multi-modal item content presentation for\nmulti-domain recommendation, called UniM^2Rec, which could smoothly learn the\nmulti-modal item content presentations and the multi-modal user preferences\nfrom all domains. With the pre-trained multi-domain recommendation model,\nUniM^2Rec could be efficiently and effectively transferred to new target\ndomains in practice. Extensive experiments conducted on five real-world\ndatasets in target domains demonstrate the superiority of the proposed method\nover existing competitive methods, especially for the real-world recommendation\nscenarios that usually struggle with seriously missing or noisy item contents.",
        "translated": ""
    },
    {
        "title": "Unbiased Offline Evaluation for Learning to Rank with Business Rules",
        "url": "http://arxiv.org/abs/2311.01828v1",
        "pub_date": "2023-11-03",
        "summary": "For industrial learning-to-rank (LTR) systems, it is common that the output\nof a ranking model is modified, either as a results of post-processing logic\nthat enforces business requirements, or as a result of unforeseen design flaws\nor bugs present in real-world production systems. This poses a challenge for\ndeploying off-policy learning and evaluation methods, as these often rely on\nthe assumption that rankings implied by the model's scores coincide with\ndisplayed items to the users. Further requirements for reliable offline\nevaluation are proper randomization and correct estimation of the propensities\nof displaying each item in any given position of the ranking, which are also\nimpacted by the aforementioned post-processing. We investigate empirically how\nthese scenarios impair off-policy evaluation for learning-to-rank models. We\nthen propose a novel correction method based on the Birkhoff-von-Neumann\ndecomposition that is robust to this type of post-processing. We obtain more\naccurate off-policy estimates in offline experiments, overcoming the problem of\npost-processed rankings. To the best of our knowledge this is the first study\non the impact of real-world business rules on offline evaluation of LTR models.",
        "translated": ""
    },
    {
        "title": "Epidemic Decision-making System Based Federated Reinforcement Learning",
        "url": "http://arxiv.org/abs/2311.01749v1",
        "pub_date": "2023-11-03",
        "summary": "Epidemic decision-making can effectively help the government to\ncomprehensively consider public security and economic development to respond to\npublic health and safety emergencies. Epidemic decision-making can effectively\nhelp the government to comprehensively consider public security and economic\ndevelopment to respond to public health and safety emergencies. Some studies\nhave shown that intensive learning can effectively help the government to make\nepidemic decision, thus achieving the balance between health security and\neconomic development. Some studies have shown that intensive learning can\neffectively help the government to make epidemic decision, thus achieving the\nbalance between health security and economic development. However, epidemic\ndata often has the characteristics of limited samples and high privacy.\nHowever, epidemic data often has the characteristics of limited samples and\nhigh privacy. This model can combine the epidemic situation data of various\nprovinces for cooperative training to use as an enhanced learning model for\nepidemic situation decision, while protecting the privacy of data. The\nexperiment shows that the enhanced federated learning can obtain more optimized\nperformance and return than the enhanced learning, and the enhanced federated\nlearning can also accelerate the training convergence speed of the training\nmodel. accelerate the training convergence speed of the client. At the same\ntime, through the experimental comparison, A2C is the most suitable\nreinforcement learning model for the epidemic situation decision-making.\nlearning model for the epidemic situation decision-making scenario, followed by\nthe PPO model, and the performance of DDPG is unsatisfactory.",
        "translated": ""
    },
    {
        "title": "Plot Retrieval as an Assessment of Abstract Semantic Association",
        "url": "http://arxiv.org/abs/2311.01666v1",
        "pub_date": "2023-11-03",
        "summary": "Retrieving relevant plots from the book for a query is a critical task, which\ncan improve the reading experience and efficiency of readers. Readers usually\nonly give an abstract and vague description as the query based on their own\nunderstanding, summaries, or speculations of the plot, which requires the\nretrieval model to have a strong ability to estimate the abstract semantic\nassociations between the query and candidate plots. However, existing\ninformation retrieval (IR) datasets cannot reflect this ability well. In this\npaper, we propose Plot Retrieval, a labeled dataset to train and evaluate the\nperformance of IR models on the novel task Plot Retrieval. Text pairs in Plot\nRetrieval have less word overlap and more abstract semantic association, which\ncan reflect the ability of the IR models to estimate the abstract semantic\nassociation, rather than just traditional lexical or semantic matching.\nExtensive experiments across various lexical retrieval, sparse retrieval, dense\nretrieval, and cross-encoder methods compared with human studies on Plot\nRetrieval show current IR models still struggle in capturing abstract semantic\nassociation between texts. Plot Retrieval can be the benchmark for further\nresearch on the semantic association modeling ability of IR models.",
        "translated": ""
    },
    {
        "title": "Instruction Distillation Makes Large Language Models Efficient Zero-shot\n  Rankers",
        "url": "http://arxiv.org/abs/2311.01555v1",
        "pub_date": "2023-11-02",
        "summary": "Recent studies have demonstrated the great potential of Large Language Models\n(LLMs) serving as zero-shot relevance rankers. The typical approach involves\nmaking comparisons between pairs or lists of documents. Although effective,\nthese listwise and pairwise methods are not efficient and also heavily rely on\nintricate prompt engineering. To tackle this problem, we introduce a novel\ninstruction distillation method. The key idea is to distill the pairwise\nranking ability of open-sourced LLMs to a simpler but more efficient pointwise\nranking. Specifically, given the same LLM, we first rank documents using the\neffective pairwise approach with complex instructions, and then distill the\nteacher predictions to the pointwise approach with simpler instructions.\nEvaluation results on the BEIR, TREC, and ReDial datasets demonstrate that\ninstruction distillation can improve efficiency by 10 to 100x and also enhance\nthe ranking performance of LLMs. Furthermore, our approach surpasses the\nperformance of existing supervised methods like monoT5 and is on par with the\nstate-of-the-art zero-shot methods. The code to reproduce our results is\navailable at www.github.com/sunnweiwei/RankGPT.",
        "translated": ""
    }
]