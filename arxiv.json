[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": ""
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": ""
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": ""
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": ""
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": ""
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": ""
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": ""
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": ""
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": ""
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": ""
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": ""
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": ""
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": ""
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": ""
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": ""
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": ""
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": ""
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": ""
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": ""
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Breaking the Curse of Quality Saturation with User-Centric Ranking",
        "url": "http://arxiv.org/abs/2305.15333v1",
        "pub_date": "2023-05-24",
        "summary": "A key puzzle in search, ads, and recommendation is that the ranking model can\nonly utilize a small portion of the vastly available user interaction data. As\na result, increasing data volume, model size, or computation FLOPs will quickly\nsuffer from diminishing returns. We examined this problem and found that one of\nthe root causes may lie in the so-called ``item-centric'' formulation, which\nhas an unbounded vocabulary and thus uncontrolled model complexity. To mitigate\nquality saturation, we introduce an alternative formulation named\n``user-centric ranking'', which is based on a transposed view of the dyadic\nuser-item interaction data. We show that this formulation has a promising\nscaling property, enabling us to train better-converged models on substantially\nlarger data sets.",
        "translated": ""
    },
    {
        "title": "Neural Summarization of Electronic Health Records",
        "url": "http://arxiv.org/abs/2305.15222v1",
        "pub_date": "2023-05-24",
        "summary": "Hospital discharge documentation is among the most essential, yet\ntime-consuming documents written by medical practitioners. The objective of\nthis study was to automatically generate hospital discharge summaries using\nneural network summarization models. We studied various data preparation and\nneural network training techniques that generate discharge summaries. Using\nnursing notes and discharge summaries from the MIMIC-III dataset, we studied\nthe viability of the automatic generation of various sections of a discharge\nsummary using four state-of-the-art neural network summarization models (BART,\nT5, Longformer and FLAN-T5). Our experiments indicated that training\nenvironments including nursing notes as the source, and discrete sections of\nthe discharge summary as the target output (e.g. \"History of Present Illness\")\nimprove language model efficiency and text quality. According to our findings,\nthe fine-tuned BART model improved its ROUGE F1 score by 43.6% against its\nstandard off-the-shelf version. We also found that fine-tuning the baseline\nBART model with other setups caused different degrees of improvement (up to 80%\nrelative improvement). We also observed that a fine-tuned T5 generally achieves\nhigher ROUGE F1 scores than other fine-tuned models and a fine-tuned FLAN-T5\nachieves the highest ROUGE score overall, i.e., 45.6. For majority of the\nfine-tuned language models, summarizing discharge summary report sections\nseparately outperformed the summarization the entire report quantitatively. On\nthe other hand, fine-tuning language models that were previously instruction\nfine-tuned showed better performance in summarizing entire reports. This study\nconcludes that a focused dataset designed for the automatic generation of\ndischarge summaries by a language model can produce coherent Discharge Summary\nsections.",
        "translated": ""
    },
    {
        "title": "Collaborative Recommendation Model Based on Multi-modal Multi-view\n  Attention Network: Movie and literature cases",
        "url": "http://arxiv.org/abs/2305.15159v1",
        "pub_date": "2023-05-24",
        "summary": "The existing collaborative recommendation models that use multi-modal\ninformation emphasize the representation of users' preferences but easily\nignore the representation of users' dislikes. Nevertheless, modelling users'\ndislikes facilitates comprehensively characterizing user profiles. Thus, the\nrepresentation of users' dislikes should be integrated into the user modelling\nwhen we construct a collaborative recommendation model. In this paper, we\npropose a novel Collaborative Recommendation Model based on Multi-modal\nmulti-view Attention Network (CRMMAN), in which the users are represented from\nboth preference and dislike views. Specifically, the users' historical\ninteractions are divided into positive and negative interactions, used to model\nthe user's preference and dislike views, respectively. Furthermore, the\nsemantic and structural information extracted from the scene is employed to\nenrich the item representation. We validate CRMMAN by designing contrast\nexperiments based on two benchmark MovieLens-1M and Book-Crossing datasets.\nMovielens-1m has about a million ratings, and Book-Crossing has about 300,000\nratings. Compared with the state-of-the-art knowledge-graph-based and\nmulti-modal recommendation methods, the AUC, NDCG@5 and NDCG@10 are improved by\n2.08%, 2.20% and 2.26% on average of two datasets. We also conduct controlled\nexperiments to explore the effects of multi-modal information and multi-view\nmechanism. The experimental results show that both of them enhance the model's\nperformance.",
        "translated": ""
    },
    {
        "title": "Bert4CMR: Cross-Market Recommendation with Bidirectional Encoder\n  Representations from Transformer",
        "url": "http://arxiv.org/abs/2305.15145v1",
        "pub_date": "2023-05-24",
        "summary": "Real-world multinational e-commerce companies, such as Amazon and eBay, serve\nin multiple countries and regions. Obviously, these markets have similar goods\nbut different users. Some markets are data-scarce, while others are data-rich.\nIn recent years, cross-market recommendation (CMR) has been proposed to enhance\ndata-scarce markets by leveraging auxiliary information from data-rich markets.\nPrevious works fine-tune the pre-trained model on the local market after\nfreezing part of the parameters or introducing inter-market similarity into the\nlocal market to improve the performance of CMR. However, they generally do not\nconsider eliminating the mutual interference between markets. Therefore, the\nexisting methods are neither unable to learn unbiased general knowledge nor\nefficient transfer reusable information across markets. In this paper, we\npropose a novel attention-based model called Bert4CMR to simultaneously improve\nall markets' recommendation performance. Specifically, we employ the attention\nmechanism to capture user interests by modelling user behavioural sequences. We\npre-train the proposed model on global data to learn the general knowledge of\nitems. Then we fine-tune specific target markets to perform local\nrecommendations. We propose market embedding to model the bias of each market\nand reduce the mutual inference between the parallel markets. Extensive\nexperiments conducted on seven markets show that our model is state-of-the-art.\nOur model outperforms the suboptimal model by 4.82%, 4.73%, 7.66% and 6.49% on\naverage of seven datasets in terms of four metrics, respectively. We conduct\nablation experiments to analyse the effectiveness of the proposed components.\nExperimental results indicate that our model is able to learn general knowledge\nthrough global data and shield the mutual interference between markets.",
        "translated": ""
    },
    {
        "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning\n  Strategies",
        "url": "http://arxiv.org/abs/2305.15115v1",
        "pub_date": "2023-05-24",
        "summary": "Recently, a new paradigm called Differentiable Search Index (DSI) has been\nproposed for document retrieval, wherein a sequence-to-sequence model is\nlearned to directly map queries to relevant document identifiers. The key idea\nbehind DSI is to fully parameterize traditional ``index-retrieve'' pipelines\nwithin a single neural model, by encoding all documents in the corpus into the\nmodel parameters. In essence, DSI needs to resolve two major questions: (1) how\nto assign an identifier to each document, and (2) how to learn the associations\nbetween a document and its identifier. In this work, we propose a\nSemantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the\narea of Cognitive Psychology. Our approach advances original DSI in two ways:\n(1) For the document identifier, we take inspiration from Elaboration\nStrategies in human learning. Specifically, we assign each document an\nElaborative Description based on the query generation technique, which is more\nmeaningful than a string of integers in the original DSI; and (2) For the\nassociations between a document and its identifier, we take inspiration from\nRehearsal Strategies in human learning. Specifically, we select fine-grained\nsemantic features from a document as Rehearsal Contents to improve document\nmemorization. Both the offline and online experiments show improved retrieval\nperformance over prevailing baselines.",
        "translated": ""
    },
    {
        "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
        "url": "http://arxiv.org/abs/2305.15053v1",
        "pub_date": "2023-05-24",
        "summary": "When re-finding items, users who forget or are uncertain about identifying\ndetails often rely on creative strategies for expressing their information\nneeds -- complex queries that describe content elements (e.g., book characters\nor events), information beyond the document text (e.g., descriptions of book\ncovers), or personal context (e.g., when they read a book). This retrieval\nsetting, called tip of the tongue (TOT), is especially challenging for models\nheavily reliant on lexical and semantic overlap between query and document\ntext. In this work, we introduce a simple yet effective framework for handling\nsuch complex queries by decomposing the query into individual clues, routing\nthose as sub-queries to specialized retrievers, and ensembling the results.\nThis approach allows us to take advantage of off-the-shelf retrievers (e.g.,\nCLIP for retrieving images of book covers) or incorporate retriever-specific\nlogic (e.g., date constraints). We show that our framework incorportating query\ndecompositions into retrievers can improve gold book recall up to 7% relative\nagain for Recall@5 on a new collection of 14,441 real-world query-book pairs\nfrom an online community for resolving TOT inquiries.",
        "translated": ""
    },
    {
        "title": "Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation",
        "url": "http://arxiv.org/abs/2305.15048v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of\neffect-size-based meta-analysis for multi-task evaluation in NLP and IR. We\nobserved that our communities often face the challenge of aggregating results\nover incomparable metrics and scenarios, which makes conclusions and take-away\nmessages less reliable. With Ranger, we aim to address this issue by providing\na task-agnostic toolkit that combines the effect of a treatment on multiple\ntasks into one statistical evaluation, allowing for comparison of metrics and\ncomputation of an overall summary effect. Our toolkit produces\npublication-ready forest plots that enable clear communication of evaluation\nresults over multiple tasks. Our goal with the ready-to-use Ranger toolkit is\nto promote robust, effect-size-based evaluation and improve evaluation\nstandards in the community. We provide two case studies for common IR and NLP\nsettings to highlight Ranger's benefits.",
        "translated": ""
    },
    {
        "title": "Exploring Adapter-based Transfer Learning for Recommender Systems:\n  Empirical Studies and Practical Insights",
        "url": "http://arxiv.org/abs/2305.15036v1",
        "pub_date": "2023-05-24",
        "summary": "Adapters, a plug-in neural network module with some tunable parameters, have\nemerged as a parameter-efficient transfer learning technique for adapting\npre-trained models to downstream tasks, especially for natural language\nprocessing (NLP) and computer vision (CV) fields. Meanwhile, learning\nrecommendation models directly from raw item modality features -- e.g., texts\nof NLP and images of CV -- can enable effective and transferable recommender\nsystems (called TransRec). In view of this, a natural question arises: can\nadapter-based learning techniques achieve parameter-efficient TransRec with\ngood performance?\n  To this end, we perform empirical studies to address several key\nsub-questions. First, we ask whether the adapter-based TransRec performs\ncomparably to TransRec based on standard full-parameter fine-tuning? does it\nhold for recommendation with different item modalities, e.g., textual RS and\nvisual RS. If yes, we benchmark these existing adapters, which have been shown\nto be effective in NLP and CV tasks, in the item recommendation settings.\nThird, we carefully study several key factors for the adapter-based TransRec in\nterms of where and how to insert these adapters? Finally, we look at the\neffects of adapter-based TransRec by either scaling up its source training data\nor scaling down its target training data. Our paper provides key insights and\npractical guidance on unified &amp; transferable recommendation -- a less studied\nrecommendation scenario. We promise to release all code &amp; datasets for future\nresearch.",
        "translated": ""
    },
    {
        "title": "How Graph Convolutions Amplify Popularity Bias for Recommendation?",
        "url": "http://arxiv.org/abs/2305.14886v1",
        "pub_date": "2023-05-24",
        "summary": "Graph convolutional networks (GCNs) have become prevalent in recommender\nsystem (RS) due to their superiority in modeling collaborative patterns.\nAlthough improving the overall accuracy, GCNs unfortunately amplify popularity\nbias -- tail items are less likely to be recommended. This effect prevents the\nGCN-based RS from making precise and fair recommendations, decreasing the\neffectiveness of recommender systems in the long run.\n  In this paper, we investigate how graph convolutions amplify the popularity\nbias in RS. Through theoretical analyses, we identify two fundamental factors:\n(1) with graph convolution (\\textit{i.e.,} neighborhood aggregation), popular\nitems exert larger influence than tail items on neighbor users, making the\nusers move towards popular items in the representation space; (2) after\nmultiple times of graph convolution, popular items would affect more high-order\nneighbors and become more influential. The two points make popular items get\ncloser to almost users and thus being recommended more frequently. To rectify\nthis, we propose to estimate the amplified effect of popular nodes on each\nnode's representation, and intervene the effect after each graph convolution.\nSpecifically, we adopt clustering to discover highly-influential nodes and\nestimate the amplification effect of each node, then remove the effect from the\nnode embeddings at each graph convolution layer. Our method is simple and\ngeneric -- it can be used in the inference stage to correct existing models\nrather than training a new model from scratch, and can be applied to various\nGCN models. We demonstrate our method on two representative GCN backbones\nLightGCN and UltraGCN, verifying its ability in improving the recommendations\nof tail items without sacrificing the performance of popular items. Codes are\nopen-sourced \\footnote{https://github.com/MEICRS/DAP}.",
        "translated": ""
    },
    {
        "title": "Machine Reading Comprehension using Case-based Reasoning",
        "url": "http://arxiv.org/abs/2305.14815v1",
        "pub_date": "2023-05-24",
        "summary": "We present an accurate and interpretable method for answer extraction in\nmachine reading comprehension that is reminiscent of case-based reasoning (CBR)\nfrom classical AI. Our method (CBR-MRC) builds on the hypothesis that\ncontextualized answers to similar questions share semantic similarities with\neach other. Given a target question, CBR-MRC retrieves a set of similar\nquestions from a memory of observed cases and predicts an answer by selecting\nthe span in the target context that is most similar to the contextualized\nrepresentations of answers in the retrieved cases. The semi-parametric nature\nof our approach allows CBR-MRC to attribute a prediction to the specific set of\ncases used during inference, making it a desirable choice for building reliable\nand debuggable QA systems. We show that CBR-MRC achieves high test accuracy\ncomparable with large reader models, outperforming baselines by 11.5 and 8.4 EM\non NaturalQuestions and NewsQA, respectively. Further, we also demonstrate the\nability of CBR-MRC in identifying not just the correct answer tokens but also\nthe span with the most relevant supporting evidence. Lastly, we observe that\ncontexts for certain question types show higher lexical diversity than others\nand find CBR-MRC to be robust to these variations while performance using\nfully-parametric methods drops.",
        "translated": ""
    },
    {
        "title": "Candidate Set Re-ranking for Composed Image Retrieval with Dual\n  Multi-modal Encoder",
        "url": "http://arxiv.org/abs/2305.16304v1",
        "pub_date": "2023-05-25",
        "summary": "Composed image retrieval aims to find an image that best matches a given\nmulti-modal user query consisting of a reference image and text pair. Existing\nmethods commonly pre-compute image embeddings over the entire corpus and\ncompare these to a reference image embedding modified by the query text at test\ntime. Such a pipeline is very efficient at test time since fast vector\ndistances can be used to evaluate candidates, but modifying the reference image\nembedding guided only by a short textual description can be difficult,\nespecially independent of potential candidates. An alternative approach is to\nallow interactions between the query and every possible candidate, i.e.,\nreference-text-candidate triplets, and pick the best from the entire set.\nThough this approach is more discriminative, for large-scale datasets the\ncomputational cost is prohibitive since pre-computation of candidate embeddings\nis no longer possible. We propose to combine the merits of both schemes using a\ntwo-stage model. Our first stage adopts the conventional vector distancing\nmetric and performs a fast pruning among candidates. Meanwhile, our second\nstage employs a dual-encoder architecture, which effectively attends to the\ninput triplet of reference-text-candidate and re-ranks the candidates. Both\nstages utilize a vision-and-language pre-trained network, which has proven\nbeneficial for various downstream tasks. Our method consistently outperforms\nstate-of-the-art approaches on standard benchmarks for the task.",
        "translated": ""
    },
    {
        "title": "A Survey on Asking Clarification Questions Datasets in Conversational\n  Systems",
        "url": "http://arxiv.org/abs/2305.15933v1",
        "pub_date": "2023-05-25",
        "summary": "The ability to understand a user's underlying needs is critical for\nconversational systems, especially with limited input from users in a\nconversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to\nreveal users' true intent from their queries or utterances arise as an\nessential task. However, it is noticeable that a key limitation of the existing\nACQs studies is their incomparability, from inconsistent use of data, distinct\nexperimental setups and evaluation strategies. Therefore, in this paper, to\nassist the development of ACQs techniques, we comprehensively analyse the\ncurrent ACQs research status, which offers a detailed comparison of publicly\navailable datasets, and discusses the applied evaluation metrics, joined with\nbenchmarks for multiple ACQs-related tasks. In particular, given a thorough\nanalysis of the ACQs task, we discuss a number of corresponding research\ndirections for the investigation of ACQs as well as the development of\nconversational systems.",
        "translated": ""
    },
    {
        "title": "Enhancing the Ranking Context of Dense Retrieval Methods through\n  Reciprocal Nearest Neighbors",
        "url": "http://arxiv.org/abs/2305.15720v1",
        "pub_date": "2023-05-25",
        "summary": "Sparse annotation poses persistent challenges to training dense retrieval\nmodels, such as the problem of false negatives, i.e. unlabeled relevant\ndocuments that are spuriously used as negatives in contrastive learning,\ndistorting the training signal. To alleviate this problem, we introduce\nevidence-based label smoothing, a computationally efficient method that\nprevents penalizing the model for assigning high relevance to false negatives.\nTo compute the target relevance distribution over candidate documents within\nthe ranking context of a given query, candidates most similar to the ground\ntruth are assigned a non-zero relevance probability based on the degree of\ntheir similarity to the ground-truth document(s). As a relevance estimate we\nleverage an improved similarity metric based on reciprocal nearest neighbors,\nwhich can also be used independently to rerank candidates in post-processing.\nThrough extensive experiments on two large-scale ad hoc text retrieval datasets\nwe demonstrate that both methods can improve the ranking effectiveness of dense\nretrieval models.",
        "translated": ""
    },
    {
        "title": "BookGPT: A General Framework for Book Recommendation Empowered by Large\n  Language Model",
        "url": "http://arxiv.org/abs/2305.15673v1",
        "pub_date": "2023-05-25",
        "summary": "With the continuous development and change exhibited by large language model\n(LLM) technology, represented by generative pretrained transformers (GPTs),\nmany classic scenarios in various fields have re-emerged with new\nopportunities. This paper takes ChatGPT as the modeling object, incorporates\nLLM technology into the typical book resource understanding and recommendation\nscenario for the first time, and puts it into practice. By building a\nChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT,\nthis paper attempts to apply ChatGPT to recommendation modeling for three\ntypical tasks, book rating recommendation, user rating recommendation, and book\nsummary recommendation, and explores the feasibility of LLM technology in book\nrecommendation scenarios. At the same time, based on different evaluation\nschemes for book recommendation tasks and the existing classic recommendation\nmodels, this paper discusses the advantages and disadvantages of the BookGPT in\nbook recommendation scenarios and analyzes the opportunities and improvement\ndirections for subsequent LLMs in these scenarios.",
        "translated": ""
    },
    {
        "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
        "url": "http://arxiv.org/abs/2305.15645v1",
        "pub_date": "2023-05-25",
        "summary": "In conversational search, the user's real search intent for the current turn\nis dependent on the previous conversation history. It is challenging to\ndetermine a good search query from the whole conversation context. To avoid the\nexpensive re-training of the query encoder, most existing methods try to learn\na rewriting model to de-contextualize the current query by mimicking the manual\nquery rewriting. However, manually rewritten queries are not always the best\nsearch queries. Training a rewriting model on them would limit the model's\nability to produce good search queries. Another useful hint is the potential\nanswer to the question. In this paper, we propose ConvGQR, a new framework to\nreformulate conversational queries based on generative pre-trained language\nmodels (PLMs), one for query rewriting and another for generating potential\nanswers. By combining both, ConvGQR can produce better search queries. In\naddition, to relate query reformulation to retrieval performance, we propose a\nknowledge infusion mechanism to optimize both query reformulation and\nretrieval. Extensive experiments on four conversational search datasets\ndemonstrate the effectiveness of ConvGQR.",
        "translated": ""
    },
    {
        "title": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language\n  Models",
        "url": "http://arxiv.org/abs/2305.15597v1",
        "pub_date": "2023-05-24",
        "summary": "The mission of open knowledge graph (KG) completion is to draw new findings\nfrom known facts. Existing works that augment KG completion require either (1)\nfactual triples to enlarge the graph reasoning space or (2) manually designed\nprompts to extract knowledge from a pre-trained language model (PLM),\nexhibiting limited performance and requiring expensive efforts from experts. To\nthis end, we propose TAGREAL that automatically generates quality query prompts\nand retrieves support information from large text corpora to probe knowledge\nfrom PLM for KG completion. The results show that TAGREAL achieves\nstate-of-the-art performance on two benchmark datasets. We find that TAGREAL\nhas superb performance even with limited training data, outperforming existing\nembedding-based, graph-based, and PLM-based methods.",
        "translated": ""
    },
    {
        "title": "Representation Online Matters: Practical End-to-End Diversification in\n  Search and Recommender Systems",
        "url": "http://arxiv.org/abs/2305.15534v1",
        "pub_date": "2023-05-24",
        "summary": "As the use of online platforms continues to grow across all demographics,\nusers often express a desire to feel represented in the content. To improve\nrepresentation in search results and recommendations, we introduce end-to-end\ndiversification, ensuring that diverse content flows throughout the various\nstages of these systems, from retrieval to ranking. We develop, experiment, and\ndeploy scalable diversification mechanisms in multiple production surfaces on\nthe Pinterest platform, including Search, Related Products, and New User\nHomefeed, to improve the representation of different skin tones in beauty and\nfashion content. Diversification in production systems includes three\ncomponents: identifying requests that will trigger diversification, ensuring\ndiverse content is retrieved from the large content corpus during the retrieval\nstage, and finally, balancing the diversity-utility trade-off in a\nself-adjusting manner in the ranking stage. Our approaches, which evolved from\nusing Strong-OR logical operator to bucketized retrieval at the retrieval stage\nand from greedy re-rankers to multi-objective optimization using determinantal\npoint processes for the ranking stage, balances diversity and utility while\nenabling fast iterations and scalable expansion to diversification over\nmultiple dimensions. Our experiments indicate that these approaches\nsignificantly improve diversity metrics, with a neutral to a positive impact on\nutility metrics and improved user satisfaction, both qualitatively and\nquantitatively, in production.",
        "translated": ""
    },
    {
        "title": "Large Language Models for User Interest Journeys",
        "url": "http://arxiv.org/abs/2305.15498v1",
        "pub_date": "2023-05-24",
        "summary": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
        "translated": ""
    },
    {
        "title": "Adversarial Attacks on Online Learning to Rank with Click Feedback",
        "url": "http://arxiv.org/abs/2305.17071v1",
        "pub_date": "2023-05-26",
        "summary": "Online learning to rank (OLTR) is a sequential decision-making problem where\na learning agent selects an ordered list of items and receives feedback through\nuser clicks. Although potential attacks against OLTR algorithms may cause\nserious losses in real-world applications, little is known about adversarial\nattacks on OLTR. This paper studies attack strategies against multiple variants\nof OLTR. Our first result provides an attack strategy against the UCB algorithm\non classical stochastic bandits with binary feedback, which solves the key\nissues caused by bounded and discrete feedback that previous works can not\nhandle. Building on this result, we design attack algorithms against UCB-based\nOLTR algorithms in position-based and cascade models. Finally, we propose a\ngeneral attack strategy against any algorithm under the general click model.\nEach attack algorithm manipulates the learning agent into choosing the target\nattack item $T-o(T)$ times, incurring a cumulative cost of $o(T)$. Experiments\non synthetic and real data further validate the effectiveness of our proposed\nattack algorithms.",
        "translated": ""
    },
    {
        "title": "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System",
        "url": "http://arxiv.org/abs/2305.17034v1",
        "pub_date": "2023-05-26",
        "summary": "Significant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type.",
        "translated": ""
    },
    {
        "title": "Is googling risky? A study on risk perception and experiences of adverse\n  consequences in web search",
        "url": "http://arxiv.org/abs/2305.16990v1",
        "pub_date": "2023-05-26",
        "summary": "Search engines, such as Google, have a considerable impact on society.\nTherefore, undesirable consequences, such as retrieving incorrect search\nresults, pose a risk to users. Although previous research has reported the\nadverse outcomes of web search, little is known about how search engine users\nevaluate those outcomes. In this study, we show which aspects of web search are\nperceived as risky using a sample (N = 3,884) representative of the German\nInternet population. We found that many participants are often concerned with\nadverse consequences immediately appearing on the search engine result page.\nMoreover, participants' experiences with adverse consequences are directly\nrelated to their risk perception. Our results demonstrate that people perceive\nrisks related to web search. In addition to our study, there is a need for more\nindependent research on the possible detrimental outcomes of web search to\nmonitor and mitigate risks. Apart from risks for individuals, search engines\nwith a massive number of users have an extraordinary impact on society;\ntherefore, the acceptable risks of web search should be discussed.",
        "translated": ""
    },
    {
        "title": "Efficient Decoding of Compositional Structure in Holistic\n  Representations",
        "url": "http://arxiv.org/abs/2305.16873v1",
        "pub_date": "2023-05-26",
        "summary": "We investigate the task of retrieving information from compositional\ndistributed representations formed by Hyperdimensional Computing/Vector\nSymbolic Architectures and present novel techniques which achieve new\ninformation rate bounds. First, we provide an overview of the decoding\ntechniques that can be used to approach the retrieval task. The techniques are\ncategorized into four groups. We then evaluate the considered techniques in\nseveral settings that involve, e.g., inclusion of external noise and storage\nelements with reduced precision. In particular, we find that the decoding\ntechniques from the sparse coding and compressed sensing literature (rarely\nused for Hyperdimensional Computing/Vector Symbolic Architectures) are also\nwell-suited for decoding information from the compositional distributed\nrepresentations. Combining these decoding techniques with interference\ncancellation ideas from communications improves previously reported bounds\n(Hersche et al., 2021) of the information rate of the distributed\nrepresentations from 1.20 to 1.40 bits per dimension for smaller codebooks and\nfrom 0.60 to 1.26 bits per dimension for larger codebooks.",
        "translated": ""
    },
    {
        "title": "Automating the Analysis of Institutional Design in International\n  Agreements",
        "url": "http://arxiv.org/abs/2305.16750v1",
        "pub_date": "2023-05-26",
        "summary": "This paper explores the automatic knowledge extraction of formal\ninstitutional design - norms, rules, and actors - from international\nagreements. The focus was to analyze the relationship between the visibility\nand centrality of actors in the formal institutional design in regulating\ncritical aspects of cultural heritage relations. The developed tool utilizes\ntechniques such as collecting legal documents, annotating them with\nInstitutional Grammar, and using graph analysis to explore the formal\ninstitutional design. The system was tested against the 2003 UNESCO Convention\nfor the Safeguarding of the Intangible Cultural Heritage.",
        "translated": ""
    },
    {
        "title": "The Search for Stability: Learning Dynamics of Strategic Publishers with\n  Initial Documents",
        "url": "http://arxiv.org/abs/2305.16695v1",
        "pub_date": "2023-05-26",
        "summary": "We study a game-theoretic model of information retrieval, in which strategic\npublishers aim to maximize their chances of being ranked first by the search\nengine, while maintaining the integrity of their original documents. We show\nthat the commonly used PRP ranking scheme results in an unstable environment\nwhere games often fail to reach pure Nash equilibrium. We propose the Relative\nRanking Principle (RRP) as an alternative ranking principle, and introduce two\nranking functions that are instances of the RRP. We provide both theoretical\nand empirical evidence that these methods lead to a stable search ecosystem, by\nproviding positive results on the learning dynamics convergence. We also define\nthe publishers' and users' welfare, and demonstrate a possible publisher-user\ntrade-off, which highlights the complexity of determining which ranking\nfunction should be selected by the search engine designer.",
        "translated": ""
    },
    {
        "title": "Multiview Identifiers Enhanced Generative Retrieval",
        "url": "http://arxiv.org/abs/2305.16675v1",
        "pub_date": "2023-05-26",
        "summary": "Instead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
        "translated": ""
    },
    {
        "title": "FARA: Future-aware Ranking Algorithm for Fairness Optimization",
        "url": "http://arxiv.org/abs/2305.16637v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking systems are the key components of modern Information Retrieval (IR)\napplications, such as search engines and recommender systems. Besides the\nranking relevance to users, the exposure fairness to item providers has also\nbeen considered an important factor in ranking optimization. Many fair ranking\nalgorithms have been proposed to jointly optimize both ranking relevance and\nfairness. However, we find that most existing fair ranking methods adopt greedy\nalgorithms that only optimize rankings for the next immediate session or\nrequest. As shown in this paper, such a myopic paradigm could limit the upper\nbound of ranking optimization and lead to suboptimal performance in the long\nterm. To this end, we propose FARA, a novel Future-Aware Ranking Algorithm for\nranking relevance and fairness optimization. Instead of greedily optimizing\nrankings for the next immediate session, FARA plans ahead by jointly optimizing\nmultiple ranklists together and saving them for future sessions. Particularly,\nFARA first uses the Taylor expansion to investigate how future ranklists will\ninfluence the overall fairness of the system. Then, based on the analysis of\nthe Taylor expansion, FARA adopts a two-phase optimization algorithm where we\nfirst solve an optimal future exposure planning problem and then construct the\noptimal ranklists according to the optimal future exposure planning.\nTheoretically, we show that FARA is optimal for ranking relevance and fairness\njoint optimization. Empirically, our extensive experiments on three\nsemi-synthesized datasets show that FARA is efficient, effective, and can\ndeliver significantly better ranking performance compared to state-of-the-art\nfair ranking methods.",
        "translated": ""
    },
    {
        "title": "DataFinder: Scientific Dataset Recommendation from Natural Language\n  Descriptions",
        "url": "http://arxiv.org/abs/2305.16636v1",
        "pub_date": "2023-05-26",
        "summary": "Modern machine learning relies on datasets to develop and validate research\nideas. Given the growth of publicly available data, finding the right dataset\nto use is increasingly difficult. Any research question imposes explicit and\nimplicit constraints on how well a given dataset will enable researchers to\nanswer this question, such as dataset size, modality, and domain. We introduce\na new task of recommending relevant datasets given a short natural language\ndescription of a research idea, to help people find relevant datasets for their\nneeds. Dataset recommendation poses unique challenges as an information\nretrieval problem; datasets are hard to directly index for search and there are\nno corpora readily available for this task. To operationalize this task, we\nbuild the DataFinder Dataset which consists of a larger\nautomatically-constructed training set (17.5K queries) and a smaller\nexpert-annotated evaluation set (392 queries). Using this data, we compare\nvarious information retrieval algorithms on our test set and present the\nfirst-ever published system for text-based dataset recommendation using machine\nlearning techniques. This system, trained on the DataFinder Dataset, finds more\nrelevant search results than existing third-party dataset search engines. To\nencourage progress on dataset recommendation, we release our dataset and models\nto the public.",
        "translated": ""
    },
    {
        "title": "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach",
        "url": "http://arxiv.org/abs/2305.16606v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
        "translated": ""
    },
    {
        "title": "Large Language Models are not Fair Evaluators",
        "url": "http://arxiv.org/abs/2305.17926v1",
        "pub_date": "2023-05-29",
        "summary": "We uncover a systematic bias in the evaluation paradigm of adopting large\nlanguage models~(LLMs), e.g., GPT-4, as a referee to score the quality of\nresponses generated by candidate models. We find that the quality ranking of\ncandidate responses can be easily hacked by simply altering their order of\nappearance in the context. This manipulation allows us to skew the evaluation\nresult, making one model appear considerably superior to the other, e.g.,\nvicuna could beat ChatGPT on 66 over 80 tested queries. To address this issue,\nwe propose two simple yet effective calibration strategies: 1) Multiple\nEvidence Calibration, which requires the evaluator model to generate multiple\ndetailed pieces of evidence before assigning ratings; 2) Balanced Position\nCalibration, which aggregates results across various orders to determine the\nfinal score. Extensive experiments demonstrate that our approach successfully\nmitigates evaluation bias, resulting in closer alignment with human judgments.\nTo facilitate future research on more robust large language model comparison,\nwe integrate the techniques in the paper into an easy-to-use toolkit\n\\emph{FairEval}, along with the human\nannotations.\\footnote{\\url{https://github.com/i-Eval/FairEval}}",
        "translated": ""
    },
    {
        "title": "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation",
        "url": "http://arxiv.org/abs/2305.17866v1",
        "pub_date": "2023-05-29",
        "summary": "Traditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "HyperFormer: Learning Expressive Sparse Feature Representations via\n  Hypergraph Transformer",
        "url": "http://arxiv.org/abs/2305.17386v1",
        "pub_date": "2023-05-27",
        "summary": "Learning expressive representations for high-dimensional yet sparse features\nhas been a longstanding problem in information retrieval. Though recent deep\nlearning methods can partially solve the problem, they often fail to handle the\nnumerous sparse features, particularly those tail feature values with\ninfrequent occurrences in the training data. Worse still, existing methods\ncannot explicitly leverage the correlations among different instances to help\nfurther improve the representation learning on sparse features since such\nrelational prior knowledge is not provided. To address these challenges, in\nthis paper, we tackle the problem of representation learning on feature-sparse\ndata from a graph learning perspective. Specifically, we propose to model the\nsparse features of different instances using hypergraphs where each node\nrepresents a data instance and each hyperedge denotes a distinct feature value.\nBy passing messages on the constructed hypergraphs based on our Hypergraph\nTransformer (HyperFormer), the learned feature representations capture not only\nthe correlations among different instances but also the correlations among\nfeatures. Our experiments demonstrate that the proposed approach can\neffectively improve feature representation learning on sparse features.",
        "translated": ""
    },
    {
        "title": "Counterfactual Evaluation of Peer-Review Assignment Policies",
        "url": "http://arxiv.org/abs/2305.17339v1",
        "pub_date": "2023-05-27",
        "summary": "Peer review assignment algorithms aim to match research papers to suitable\nexpert reviewers, working to maximize the quality of the resulting reviews. A\nkey challenge in designing effective assignment policies is evaluating how\nchanges to the assignment algorithm map to changes in review quality. In this\nwork, we leverage recently proposed policies that introduce randomness in\npeer-review assignment--in order to mitigate fraud--as a valuable opportunity\nto evaluate counterfactual assignment policies. Specifically, we exploit how\nsuch randomized assignments provide a positive probability of observing the\nreviews of many assignment policies of interest. To address challenges in\napplying standard off-policy evaluation methods, such as violations of\npositivity, we introduce novel methods for partial identification based on\nmonotonicity and Lipschitz smoothness assumptions for the mapping between\nreviewer-paper covariates and outcomes. We apply our methods to peer-review\ndata from two computer science venues: the TPDP'21 workshop (95 papers and 35\nreviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We\nconsider estimates of (i) the effect on review quality when changing weights in\nthe assignment algorithm, e.g., weighting reviewers' bids vs. textual\nsimilarity (between the review's past papers and the submission), and (ii) the\n\"cost of randomization\", capturing the difference in expected quality between\nthe perturbed and unperturbed optimal match. We find that placing higher weight\non text similarity results in higher review quality and that introducing\nrandomization in the reviewer-paper assignment only marginally reduces the\nreview quality. Our methods for partial identification may be of independent\ninterest, while our off-policy approach can likely find use evaluating a broad\nclass of algorithmic matching systems.",
        "translated": ""
    },
    {
        "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and\n  Document Deduplication",
        "url": "http://arxiv.org/abs/2305.17310v1",
        "pub_date": "2023-05-27",
        "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To\nremove duplicate results in a Web search, for example, a common approach looks\nat the Jaccard index between all pairs of pages. In social network analysis, a\nmuch-celebrated metric is the Adamic-Adar index, widely used to compare node\nneighborhood sets in the important problem of predicting links. However, with\nthe increasing amount of data to be processed, calculating the exact similarity\nbetween all pairs can be intractable. The challenge of working at this scale\nhas motivated research into efficient estimators for set similarity metrics.\nThe two most popular estimators, MinHash and SimHash, are indeed used in\napplications such as document deduplication and recommender systems where large\nvolumes of data need to be processed. Given the importance of these tasks, the\ndemand for advancing estimators is evident. We propose DotHash, an unbiased\nestimator for the intersection size of two sets. DotHash can be used to\nestimate the Jaccard index and, to the best of our knowledge, is the first\nmethod that can also estimate the Adamic-Adar index and a family of related\nmetrics. We formally define this family of metrics, provide theoretical bounds\non the probability of estimate errors, and analyze its empirical performance.\nOur experimental results indicate that DotHash is more accurate than the other\nestimators in link prediction and detecting duplicate documents with the same\ncomplexity and similar comparison time.",
        "translated": ""
    },
    {
        "title": "Table Detection for Visually Rich Document Images",
        "url": "http://arxiv.org/abs/2305.19181v1",
        "pub_date": "2023-05-30",
        "summary": "Table Detection (TD) is a fundamental task towards visually rich document\nunderstanding. Current studies usually formulate the TD problem as an object\ndetection problem, then leverage Intersection over Union (IoU) based metrics to\nevaluate the model performance and IoU-based loss functions to optimize the\nmodel. TD applications usually require the prediction results to cover all the\ntable contents and avoid information loss. However, IoU and IoU-based loss\nfunctions cannot directly reflect the degree of information loss for the\nprediction results. Therefore, we propose to decouple IoU into a ground truth\ncoverage term and a prediction coverage term, in which the former can be used\nto measure the information loss of the prediction results.\n  Besides, tables in the documents are usually large, sparsely distributed, and\nhave no overlaps because they are designed to summarize essential information\nto make it easy to read and interpret for human readers. Therefore, in this\nstudy, we use SparseR-CNN as the base model, and further improve the model by\nusing Gaussian Noise Augmented Image Size region proposals and many-to-one\nlabel assignments.\n  To demonstrate the effectiveness of proposed method and compare with\nstate-of-the-art methods fairly, we conduct experiments and use IoU-based\nevaluation metrics to evaluate the model performance. The experimental results\nshow that the proposed method can consistently outperform state-of-the-art\nmethods under different IoU-based metric on a variety of datasets. We conduct\nfurther experiments to show the superiority of the proposed decoupled IoU for\nthe TD applications by replacing the IoU-based loss functions and evaluation\nmetrics with proposed decoupled IoU counterparts. The experimental results show\nthat our proposed decoupled IoU loss can encourage the model to alleviate\ninformation loss.",
        "translated": ""
    },
    {
        "title": "Event-Centric Query Expansion in Web Search",
        "url": "http://arxiv.org/abs/2305.19019v1",
        "pub_date": "2023-05-30",
        "summary": "In search engines, query expansion (QE) is a crucial technique to improve\nsearch experience. Previous studies often rely on long-term search log mining,\nwhich leads to slow updates and is sub-optimal for time-sensitive news\nsearches. In this work, we present Event-Centric Query Expansion (EQE), a novel\nQE system that addresses these issues by mining the best expansion from a\nsignificant amount of potential events rapidly and accurately. This system\nconsists of four stages, i.e., event collection, event reformulation, semantic\nretrieval and online ranking. Specifically, we first collect and filter news\nheadlines from websites. Then we propose a generation model that incorporates\ncontrastive learning and prompt-tuning techniques to reformulate these\nheadlines to concise candidates. Additionally, we fine-tune a dual-tower\nsemantic model to function as an encoder for event retrieval and explore a\ntwo-stage contrastive training approach to enhance the accuracy of event\nretrieval. Finally, we rank the retrieved events and select the optimal one as\nQE, which is then used to improve the retrieval of event-related documents.\nThrough offline analysis and online A/B testing, we observe that the EQE system\nsignificantly improves many metrics compared to the baseline. The system has\nbeen deployed in Tencent QQ Browser Search and served hundreds of millions of\nusers. The dataset and baseline codes are available at\nhttps://open-event-hub.github.io/eqe .",
        "translated": ""
    },
    {
        "title": "A Recipe for Efficient SBIR Models: Combining Relative Triplet Loss with\n  Batch Normalization and Knowledge Distillation",
        "url": "http://arxiv.org/abs/2305.18988v1",
        "pub_date": "2023-05-30",
        "summary": "Sketch-Based Image Retrieval (SBIR) is a crucial task in multimedia\nretrieval, where the goal is to retrieve a set of images that match a given\nsketch query. Researchers have already proposed several well-performing\nsolutions for this task, but most focus on enhancing embedding through\ndifferent approaches such as triplet loss, quadruplet loss, adding data\naugmentation, and using edge extraction. In this work, we tackle the problem\nfrom various angles. We start by examining the training data quality and show\nsome of its limitations. Then, we introduce a Relative Triplet Loss (RTL), an\nadapted triplet loss to overcome those limitations through loss weighting based\non anchors similarity. Through a series of experiments, we demonstrate that\nreplacing a triplet loss with RTL outperforms previous state-of-the-art without\nthe need for any data augmentation. In addition, we demonstrate why batch\nnormalization is more suited for SBIR embeddings than l2-normalization and show\nthat it improves significantly the performance of our models. We further\ninvestigate the capacity of models required for the photo and sketch domains\nand demonstrate that the photo encoder requires a higher capacity than the\nsketch encoder, which validates the hypothesis formulated in [34]. Then, we\npropose a straightforward approach to train small models, such as ShuffleNetv2\n[22] efficiently with a marginal loss of accuracy through knowledge\ndistillation. The same approach used with larger models enabled us to\noutperform previous state-of-the-art results and achieve a recall of 62.38% at\nk = 1 on The Sketchy Database [30].",
        "translated": ""
    },
    {
        "title": "The Information Retrieval Experiment Platform",
        "url": "http://arxiv.org/abs/2305.18932v1",
        "pub_date": "2023-05-30",
        "summary": "We integrate ir_datasets, ir_measures, and PyTerrier with TIRA in the\nInformation Retrieval Experiment Platform (TIREx) to promote more standardized,\nreproducible, scalable, and even blinded retrieval experiments. Standardization\nis achieved when a retrieval approach implements PyTerrier's interfaces and the\ninput and output of an experiment are compatible with ir_datasets and\nir_measures. However, none of this is a must for reproducibility and\nscalability, as TIRA can run any dockerized software locally or remotely in a\ncloud-native execution environment. Version control and caching ensure\nefficient (re)execution. TIRA allows for blind evaluation when an experiment\nruns on a remote server or cloud not under the control of the experimenter. The\ntest data and ground truth are then hidden from public access, and the\nretrieval software has to process them in a sandbox that prevents data leaks.\n  We currently host an instance of TIREx with 15 corpora (1.9 billion\ndocuments) on which 32 shared retrieval tasks are based. Using Docker images of\n50 standard retrieval approaches, we automatically evaluated all approaches on\nall tasks (50 $\\cdot$ 32 = 1,600~runs) in less than a week on a midsize cluster\n(1,620 CPU cores and 24 GPUs). This instance of TIREx is open for submissions\nand will be integrated with the IR Anthology, as well as released open source.",
        "translated": ""
    },
    {
        "title": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light\n  Graph Convolution for Effective Multi-Criteria Recommendation",
        "url": "http://arxiv.org/abs/2305.18885v1",
        "pub_date": "2023-05-30",
        "summary": "The multi-criteria (MC) recommender system, which leverages MC rating\ninformation in a wide range of e-commerce areas, is ubiquitous nowadays.\nSurprisingly, although graph neural networks (GNNs) have been widely applied to\ndevelop various recommender systems due to GNN's high expressive capability in\nlearning graph representations, it has been still unexplored how to design MC\nrecommender systems with GNNs. In light of this, we make the first attempt\ntowards designing a GNN-aided MC recommender system. Specifically, rather than\nstraightforwardly adopting existing GNN-based recommendation methods, we devise\na novel criteria preference-aware light graph convolution CPA-LGC method, which\nis capable of precisely capturing the criteria preference of users as well as\nthe collaborative signal in complex high-order connectivities. To this end, we\nfirst construct an MC expansion graph that transforms user--item MC ratings\ninto an expanded bipartite graph to potentially learn from the collaborative\nsignal in MC ratings. Next, to strengthen the capability of criteria preference\nawareness, CPA-LGC incorporates newly characterized embeddings, including\nuser-specific criteria-preference embeddings and item-specific criterion\nembeddings, into our graph convolution model. Through comprehensive evaluations\nusing four real-world datasets, we demonstrate (a) the superiority over\nbenchmark MC recommendation methods and benchmark recommendation methods using\nGNNs with tremendous gains, (b) the effectiveness of core components in\nCPA-LGC, and (c) the computational efficiency.",
        "translated": ""
    },
    {
        "title": "Robust Reinforcement Learning Objectives for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.18820v1",
        "pub_date": "2023-05-30",
        "summary": "Attention-based sequential recommendation methods have demonstrated promising\nresults by accurately capturing users' dynamic interests from historical\ninteractions. In addition to generating superior user representations, recent\nstudies have begun integrating reinforcement learning (RL) into these models.\nFraming sequential recommendation as an RL problem with reward signals, unlocks\ndeveloping recommender systems (RS) that consider a vital aspect-incorporating\ndirect user feedback in the form of rewards to deliver a more personalized\nexperience. Nonetheless, employing RL algorithms presents challenges, including\noff-policy training, expansive combinatorial action spaces, and the scarcity of\ndatasets with sufficient reward signals. Contemporary approaches have attempted\nto combine RL and sequential modeling, incorporating contrastive-based\nobjectives and negative sampling strategies for training the RL component. In\nthis study, we further emphasize the efficacy of contrastive-based objectives\npaired with augmentation to address datasets with extended horizons.\nAdditionally, we recognize the potential instability issues that may arise\nduring the application of negative sampling. These challenges primarily stem\nfrom the data imbalance prevalent in real-world datasets, which is a common\nissue in offline RL contexts. While our established baselines attempt to\nmitigate this through various techniques, instability remains an issue.\nTherefore, we introduce an enhanced methodology aimed at providing a more\neffective solution to these challenges.",
        "translated": ""
    },
    {
        "title": "Who Would be Interested in Services? An Entity Graph Learning System for\n  User Targeting",
        "url": "http://arxiv.org/abs/2305.18780v1",
        "pub_date": "2023-05-30",
        "summary": "With the growing popularity of various mobile devices, user targeting has\nreceived a growing amount of attention, which aims at effectively and\nefficiently locating target users that are interested in specific services.\nMost pioneering works for user targeting tasks commonly perform\nsimilarity-based expansion with a few active users as seeds, suffering from the\nfollowing major issues: the unavailability of seed users for newcoming services\nand the unfriendliness of black-box procedures towards marketers. In this\npaper, we design an Entity Graph Learning (EGL) system to provide explainable\nuser targeting ability meanwhile applicable to addressing the cold-start issue.\nEGL System follows the hybrid online-offline architecture to satisfy the\nrequirements of scalability and timeliness. Specifically, in the offline stage,\nthe system focuses on the heavyweight entity graph construction and user entity\npreference learning, in which we propose a Three-stage Relation Mining\nProcedure (TRMP), breaking loose from the expensive seed users. At the online\nstage, the system offers the ability of user targeting in real-time based on\nthe entity graph from the offline stage. Since the user targeting process is\nbased on graph reasoning, the whole process is transparent and\noperation-friendly to marketers. Finally, extensive offline experiments and\nonline A/B testing demonstrate the superior performance of the proposed EGL\nSystem.",
        "translated": ""
    },
    {
        "title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts",
        "url": "http://arxiv.org/abs/2305.18727v1",
        "pub_date": "2023-05-30",
        "summary": "With a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
        "translated": ""
    },
    {
        "title": "Known by the Company it Keeps: Proximity-Based Indexing for Physical\n  Content in Archival Repositories",
        "url": "http://arxiv.org/abs/2305.18683v1",
        "pub_date": "2023-05-30",
        "summary": "Despite the plethora of born-digital content, vast troves of important\ncontent remain accessible only on physical media such as paper or microfilm.\nThe traditional approach to indexing undigitized content is using manually\ncreated metadata that describes content at some level of aggregation (e.g.,\nfolder, box, or collection). Searchers led in this way to some subset of the\ncontent often must then manually examine substantial quantities of physical\nmedia to find what they are looking for. This paper proposes a complementary\napproach, in which selective digitization of a small portion of the content is\nused as a basis for proximity-based indexing as a way of bringing the user\ncloser to the specific content for which they are looking. Experiments with 35\nboxes of partially digitized US State Department records indicate that\nbox-level indexes built in this way can provide a useful basis for search.",
        "translated": ""
    },
    {
        "title": "Improving Generalization for Multimodal Fake News Detection",
        "url": "http://arxiv.org/abs/2305.18599v1",
        "pub_date": "2023-05-29",
        "summary": "The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for fake news\ndetection. However, state-of-the-art approaches are usually trained on datasets\nof smaller size or with a limited set of specific topics. As a consequence,\nthese models lack generalization capabilities and are not applicable to\nreal-world data. In this paper, we propose three models that adopt and\nfine-tune state-of-the-art multimodal transformers for multimodal fake news\ndetection. We conduct an in-depth analysis by manipulating the input data aimed\nto explore models performance in realistic use cases on social media. Our study\nacross multiple models demonstrates that these systems suffer significant\nperformance drops against manipulated data. To reduce the bias and improve\nmodel generalization, we suggest training data augmentation to conduct more\nmeaningful experiments for fake news detection on social media. The proposed\ndata augmentation techniques enable models to generalize better and yield\nimproved state-of-the-art results.",
        "translated": ""
    },
    {
        "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on\n  Structured Data",
        "url": "http://arxiv.org/abs/2305.19912v1",
        "pub_date": "2023-05-31",
        "summary": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which\nencodes user queries and structured data in one universal embedding space for\nretrieving structured data. SANTA proposes two pretraining methods to make\nlanguage models structure-aware and learn effective representations for\nstructured data: 1) Structured Data Alignment, which utilizes the natural\nalignment relations between structured data and unstructured data for\nstructure-aware pretraining. It contrastively trains language models to\nrepresent multi-modal text data and teaches models to distinguish matched\nstructured data for unstructured texts. 2) Masked Entity Prediction, which\ndesigns an entity-oriented mask strategy and asks language models to fill in\nthe masked entities. Our experiments show that SANTA achieves state-of-the-art\non code search and product search and conducts convincing results in the\nzero-shot setting. SANTA learns tailored representations for multi-modal text\ndata by aligning structured and unstructured data pairs and capturing\nstructural semantics by masking and predicting entities in the structured data.\nAll codes are available at https://github.com/OpenMatch/OpenMatch.",
        "translated": ""
    },
    {
        "title": "Web scraping: a promising tool for geographic data acquisition",
        "url": "http://arxiv.org/abs/2305.19893v1",
        "pub_date": "2023-05-31",
        "summary": "With much of our lives taking place online, researchers are increasingly\nturning to information from the World Wide Web to gain insights into geographic\npatterns and processes. Web scraping as an online data acquisition technique\nallows us to gather intelligence especially on social and economic actions for\nwhich the Web serves as a platform. Specific opportunities relate to\nnear-real-time access to object-level geolocated data, which can be captured in\na cost-effective way. The studied geographic phenomena include, but are not\nlimited to, the rental market and associated processes such as gentrification,\nentrepreneurial ecosystems, or spatial planning processes. Since the\ninformation retrieved from the Web is not made available for that purpose, Web\nscraping faces several unique challenges, several of which relate to location.\nEthical and legal issues mainly relate to intellectual property rights,\ninformed consent and (geo-) privacy, and website integrity and contract. These\nissues also effect the practice of open science. In addition, there are\ntechnical and statistical challenges that relate to dependability and\nincompleteness, data inconsistencies and bias, as well as the limited\nhistorical coverage. Geospatial analyses furthermore usually require the\nautomated extraction and subsequent resolution of toponyms or addresses\n(geoparsing, geocoding). A study on apartment rent in Leipzig, Germany is used\nto illustrate the use of Web scraping and its challenges. We conclude that\ngeographic researchers should embrace Web scraping as a powerful and affordable\ndigital fieldwork tool while paying special attention to its legal, ethical,\nand methodological challenges.",
        "translated": ""
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.19860v1",
        "pub_date": "2023-05-31",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in the field of\nNatural Language Processing (NLP) and have recently gained significant\nattention in the domain of Recommendation Systems (RS). These models, trained\non massive amounts of data using self-supervised learning, have demonstrated\nremarkable success in learning universal representations and have the potential\nto enhance various aspects of recommendation systems by some effective transfer\ntechniques such as fine-tuning and prompt tuning, and so on. The crucial aspect\nof harnessing the power of language models in enhancing recommendation quality\nis the utilization of their high-quality representations of textual features\nand their extensive coverage of external knowledge to establish correlations\nbetween items and users. To provide a comprehensive understanding of the\nexisting LLM-based recommendation systems, this survey presents a taxonomy that\ncategorizes these models into two major paradigms, respectively Discriminative\nLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation\n(GLLM4Rec), with the latter being systematically sorted out for the first time.\nFurthermore, we systematically review and analyze existing LLM-based\nrecommendation systems within each paradigm, providing insights into their\nmethodologies, techniques, and performance. Additionally, we identify key\nchallenges and several valuable findings to provide researchers and\npractitioners with inspiration.",
        "translated": ""
    },
    {
        "title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish\n  Language",
        "url": "http://arxiv.org/abs/2305.19840v1",
        "pub_date": "2023-05-31",
        "summary": "The BEIR dataset is a large, heterogeneous benchmark for Information\nRetrieval (IR) in zero-shot settings, garnering considerable attention within\nthe research community. However, BEIR and analogous datasets are predominantly\nrestricted to the English language. Our objective is to establish extensive\nlarge-scale resources for IR in the Polish language, thereby advancing the\nresearch in this NLP area. In this work, inspired by mMARCO and Mr.~TyDi\ndatasets, we translated all accessible open IR datasets into Polish, and we\nintroduced the BEIR-PL benchmark -- a new benchmark which comprises 13\ndatasets, facilitating further development, training and evaluation of modern\nPolish language models for IR tasks. We executed an evaluation and comparison\nof numerous IR models on the newly introduced BEIR-PL benchmark. Furthermore,\nwe publish pre-trained open IR models for Polish language,d marking a\npioneering development in this field. Additionally, the evaluation revealed\nthat BM25 achieved significantly lower scores for Polish than for English,\nwhich can be attributed to high inflection and intricate morphological\nstructure of the Polish language. Finally, we trained various re-ranking models\nto enhance the BM25 retrieval, and we compared their performance to identify\ntheir unique characteristic features. To ensure accurate model comparisons, it\nis necessary to scrutinise individual results rather than to average across the\nentire benchmark. Thus, we thoroughly analysed the outcomes of IR models in\nrelation to each individual data subset encompassed by the BEIR benchmark. The\nbenchmark data is available at URL {\\bf https://huggingface.co/clarin-knext}.",
        "translated": ""
    },
    {
        "title": "Medication Recommendation via Domain Knowledge Informed Deep Learning",
        "url": "http://arxiv.org/abs/2305.19604v1",
        "pub_date": "2023-05-31",
        "summary": "Medication recommendation is a fundamental yet crucial branch of healthcare,\nwhich provides opportunities to support clinical physicians with more accurate\nmedication prescriptions for patients with complex health conditions. Learning\nfrom electronic health records (EHR) to recommend medications is the most\ncommon way in previous studies. However, most of them neglect incorporating\ndomain knowledge according to the clinical manifestations in the EHR of the\npatient. To address these issues, we propose a novel \\textbf{D}omain\n\\textbf{K}nowledge \\textbf{I}nformed \\textbf{Net}work (DKINet) to integrate\ndomain knowledge with observable clinical manifestations of the patient, which\nis the first dynamic domain knowledge informed framework toward medication\nrecommendation. In particular, we first design a knowledge-driven encoder to\ncapture the domain information and then develop a data-driven encoder to\nintegrate domain knowledge into the observable EHR. To endow the model with the\ncapability of temporal decision, we design an explicit medication encoder for\nlearning the longitudinal dependence of the patient. Extensive experiments on\nthree publicly available datasets verify the superiority of our method. The\ncode will be public upon acceptance.",
        "translated": ""
    },
    {
        "title": "Towards Semi-supervised Universal Graph Classification",
        "url": "http://arxiv.org/abs/2305.19598v1",
        "pub_date": "2023-05-31",
        "summary": "Graph neural networks have pushed state-of-the-arts in graph classifications\nrecently. Typically, these methods are studied within the context of supervised\nend-to-end training, which necessities copious task-specific labels. However,\nin real-world circumstances, labeled data could be limited, and there could be\na massive corpus of unlabeled data, even from unknown classes as a\ncomplementary. Towards this end, we study the problem of semi-supervised\nuniversal graph classification, which not only identifies graph samples which\ndo not belong to known classes, but also classifies the remaining samples into\ntheir respective classes. This problem is challenging due to a severe lack of\nlabels and potential class shifts. In this paper, we propose a novel graph\nneural network framework named UGNN, which makes the best of unlabeled data\nfrom the subgraph perspective. To tackle class shifts, we estimate the\ncertainty of unlabeled graphs using multiple subgraphs, which facilities the\ndiscovery of unlabeled data from unknown categories. Moreover, we construct\nsemantic prototypes in the embedding space for both known and unknown\ncategories and utilize posterior prototype assignments inferred from the\nSinkhorn-Knopp algorithm to learn from abundant unlabeled graphs across\ndifferent subgraph views. Extensive experiments on six datasets verify the\neffectiveness of UGNN in different settings.",
        "translated": ""
    },
    {
        "title": "Multi-Epoch Learning for Deep Click-Through Rate Prediction Models",
        "url": "http://arxiv.org/abs/2305.19531v1",
        "pub_date": "2023-05-31",
        "summary": "The one-epoch overfitting phenomenon has been widely observed in industrial\nClick-Through Rate (CTR) applications, where the model performance experiences\na significant degradation at the beginning of the second epoch. Recent advances\ntry to understand the underlying factors behind this phenomenon through\nextensive experiments. However, it is still unknown whether a multi-epoch\ntraining paradigm could achieve better results, as the best performance is\nusually achieved by one-epoch training. In this paper, we hypothesize that the\nemergence of this phenomenon may be attributed to the susceptibility of the\nembedding layer to overfitting, which can stem from the high-dimensional\nsparsity of data. To maintain feature sparsity while simultaneously avoiding\noverfitting of embeddings, we propose a novel Multi-Epoch learning with Data\nAugmentation (MEDA), which can be directly applied to most deep CTR models.\nMEDA achieves data augmentation by reinitializing the embedding layer in each\nepoch, thereby avoiding embedding overfitting and simultaneously improving\nconvergence. To our best knowledge, MEDA is the first multi-epoch training\nparadigm designed for deep CTR prediction models. We conduct extensive\nexperiments on several public datasets, and the effectiveness of our proposed\nMEDA is fully verified. Notably, the results show that MEDA can significantly\noutperform the conventional one-epoch training. Besides, MEDA has exhibited\nsignificant benefits in a real-world scene on Kuaishou.",
        "translated": ""
    },
    {
        "title": "AdANNS: A Framework for Adaptive Semantic Search",
        "url": "http://arxiv.org/abs/2305.19435v1",
        "pub_date": "2023-05-30",
        "summary": "Web-scale search systems learn an encoder to embed a given query which is\nthen hooked into an approximate nearest neighbor search (ANNS) pipeline to\nretrieve similar data points. To accurately capture tail queries and data\npoints, learned representations typically are rigid, high-dimensional vectors\nthat are generally used as-is in the entire ANNS pipeline and can lead to\ncomputationally expensive retrieval. In this paper, we argue that instead of\nrigid representations, different stages of ANNS can leverage adaptive\nrepresentations of varying capacities to achieve significantly better\naccuracy-compute trade-offs, i.e., stages of ANNS that can get away with more\napproximate computation should use a lower-capacity representation of the same\ndata point. To this end, we introduce AdANNS, a novel ANNS design framework\nthat explicitly leverages the flexibility of Matryoshka Representations. We\ndemonstrate state-of-the-art accuracy-compute trade-offs using novel\nAdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF)\nand quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is\nup to 1.5% more accurate than the rigid representations-based IVF at the same\ncompute budget; and matches accuracy while being up to 90x faster in wall-clock\ntime. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the\n64-byte OPQ baseline constructed using rigid representations -- same accuracy\nat half the cost! We further show that the gains from AdANNS translate to\nmodern-day composite ANNS indices that combine search structures and\nquantization. Finally, we demonstrate that AdANNS can enable inference-time\nadaptivity for compute-aware search on ANNS indices built non-adaptively on\nmatryoshka representations. Code is open-sourced at\nhttps://github.com/RAIVNLab/AdANNS.",
        "translated": ""
    },
    {
        "title": "DuoSearch: A Novel Search Engine for Bulgarian Historical Documents",
        "url": "http://arxiv.org/abs/2305.19392v1",
        "pub_date": "2023-05-30",
        "summary": "Search in collections of digitised historical documents is hindered by a\ntwo-prong problem, orthographic variety and optical character recognition (OCR)\nmistakes. We present a new search engine for historical documents, DuoSearch,\nwhich uses ElasticSearch and machine learning methods based on deep neural\nnetworks to offer a solution to this problem. It was tested on a collection of\nhistorical newspapers in Bulgarian from the mid-19th to the mid-20th century.\nThe system provides an interactive and intuitive interface for the end-users\nallowing them to enter search terms in modern Bulgarian and search across\nhistorical spellings. This is the first solution facilitating the use of\ndigitised historical documents in Bulgarian.",
        "translated": ""
    },
    {
        "title": "AMR4NLI: Interpretable and robust NLI measures from semantic graphs",
        "url": "http://arxiv.org/abs/2306.00936v1",
        "pub_date": "2023-06-01",
        "summary": "The task of natural language inference (NLI) asks whether a given premise\n(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human\nratings of entailment, but the meaning relationships driving these ratings are\nnot formalized. Can the underlying sentence pair relationships be made more\nexplicit in an interpretable yet robust fashion? We compare semantic structures\nto represent premise and hypothesis, including sets of contextualized\nembeddings and semantic graphs (Abstract Meaning Representations), and measure\nwhether the hypothesis is a semantic substructure of the premise, utilizing\ninterpretable metrics. Our evaluation on three English benchmarks finds value\nin both contextualized embeddings and semantic graphs; moreover, they provide\ncomplementary signals, and can be leveraged together in a hybrid model.",
        "translated": ""
    },
    {
        "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach\n  for Low-Resource Complex NER",
        "url": "http://arxiv.org/abs/2306.00928v1",
        "pub_date": "2023-06-01",
        "summary": "Complex Named Entity Recognition (NER) is the task of detecting\nlinguistically complex named entities in low-context text. In this paper, we\npresent ACLM Attention-map aware keyword selection for Conditional Language\nModel fine-tuning), a novel data augmentation approach based on conditional\ngeneration to address the data scarcity problem in low-resource complex NER.\nACLM alleviates the context-entity mismatch issue, a problem existing NER data\naugmentation techniques suffer from and often generates incoherent\naugmentations by placing complex named entities in the wrong context. ACLM\nbuilds on BART and is optimized on a novel text reconstruction or denoising\ntask - we use selective masking (aided by attention maps) to retain the named\nentities and certain keywords in the input sentence that provide contextually\nrelevant additional knowledge or hints about the named entities. Compared with\nother data augmentation strategies, ACLM can generate more diverse and coherent\naugmentations preserving the true word sense of complex entities in the\nsentence. We demonstrate the effectiveness of ACLM both qualitatively and\nquantitatively on monolingual, cross-lingual, and multilingual complex NER\nacross various low-resource settings. ACLM outperforms all our neural baselines\nby a significant margin (1%-36%). In addition, we demonstrate the application\nof ACLM to other domains that suffer from data scarcity (e.g., biomedical). In\npractice, ACLM generates more effective and factual augmentations for these\ndomains than prior methods. Code: https://github.com/Sreyan88/ACLM",
        "translated": ""
    },
    {
        "title": "SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in\n  Graph Neural Networks",
        "url": "http://arxiv.org/abs/2306.00899v1",
        "pub_date": "2023-06-01",
        "summary": "Graph Neural Networks (GNNs) have demonstrated promising outcomes across\nvarious tasks, including node classification and link prediction. Despite their\nremarkable success in various high-impact applications, we have identified\nthree common pitfalls in message passing for link prediction. Particularly, in\nprevalent GNN frameworks (e.g., DGL and PyTorch-Geometric), the target edges\n(i.e., the edges being predicted) consistently exist as message passing edges\nin the graph during training. Consequently, this results in overfitting and\ndistribution shift, both of which adversely impact the generalizability to test\nthe target edges. Additionally, during test time, the failure to exclude the\ntest target edges leads to implicit test leakage caused by neighborhood\naggregation. In this paper, we analyze these three pitfalls and investigate the\nimpact of including or excluding target edges on the performance of nodes with\nvarying degrees during training and test phases. Our theoretical and empirical\nanalysis demonstrates that low-degree nodes are more susceptible to these\npitfalls. These pitfalls can have detrimental consequences when GNNs are\nimplemented in production systems. To systematically address these pitfalls, we\npropose SpotTarget, an effective and efficient GNN training framework. During\ntraining, SpotTarget leverages our insight regarding low-degree nodes and\nexcludes train target edges connected to at least one low-degree node. During\ntest time, it emulates real-world scenarios of GNN usage in production and\nexcludes all test target edges. Our experiments conducted on diverse real-world\ndatasets, demonstrate that SpotTarget significantly enhances GNNs, achieving up\nto a 15x increase in accuracy in sparse graphs. Furthermore, SpotTarget\nconsistently and dramatically improves the performance for low-degree nodes in\ndense graphs.",
        "translated": ""
    },
    {
        "title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection",
        "url": "http://arxiv.org/abs/2306.00765v1",
        "pub_date": "2023-06-01",
        "summary": "Stance Detection is concerned with identifying the attitudes expressed by an\nauthor towards a target of interest. This task spans a variety of domains\nranging from social media opinion identification to detecting the stance for a\nlegal claim. However, the framing of the task varies within these domains, in\nterms of the data collection protocol, the label dictionary and the number of\navailable annotations. Furthermore, these stance annotations are significantly\nimbalanced on a per-topic and inter-topic basis. These make multi-domain stance\ndetection a challenging task, requiring standardization and domain adaptation.\nTo overcome this challenge, we propose $\\textbf{T}$opic $\\textbf{E}$fficient\n$\\textbf{St}$anc$\\textbf{E}$ $\\textbf{D}$etection (TESTED), consisting of a\ntopic-guided diversity sampling technique and a contrastive objective that is\nused for fine-tuning a stance classifier. We evaluate the method on an existing\nbenchmark of $16$ datasets with in-domain, i.e. all topics seen and\nout-of-domain, i.e. unseen topics, experiments. The results show that our\nmethod outperforms the state-of-the-art with an average of $3.5$ F1 points\nincrease in-domain, and is more generalizable with an averaged increase of\n$10.2$ F1 on out-of-domain evaluation while using $\\leq10\\%$ of the training\ndata. We show that our sampling technique mitigates both inter- and per-topic\nclass imbalances. Finally, our analysis demonstrates that the contrastive\nlearning objective allows the model a more pronounced segmentation of samples\nwith varying labels.",
        "translated": ""
    },
    {
        "title": "End-to-End Document Classification and Key Information Extraction using\n  Assignment Optimization",
        "url": "http://arxiv.org/abs/2306.00750v1",
        "pub_date": "2023-06-01",
        "summary": "We propose end-to-end document classification and key information extraction\n(KIE) for automating document processing in forms. Through accurate document\nclassification we harness known information from templates to enhance KIE from\nforms. We use text and layout encoding with a cosine similarity measure to\nclassify visually-similar documents. We then demonstrate a novel application of\nmixed integer programming by using assignment optimization to extract key\ninformation from documents. Our approach is validated on an in-house dataset of\nnoisy scanned forms. The best performing document classification approach\nachieved 0.97 f1 score. A mean f1 score of 0.94 for the KIE task suggests there\nis significant potential in applying optimization techniques. Abation results\nshow that the method relies on document preprocessing techniques to mitigate\nType II errors and achieve optimal performance.",
        "translated": ""
    },
    {
        "title": "Class Anchor Margin Loss for Content-Based Image Retrieval",
        "url": "http://arxiv.org/abs/2306.00630v1",
        "pub_date": "2023-06-01",
        "summary": "The performance of neural networks in content-based image retrieval (CBIR) is\nhighly influenced by the chosen loss (objective) function. The majority of\nobjective functions for neural models can be divided into metric learning and\nstatistical learning. Metric learning approaches require a pair mining strategy\nthat often lacks efficiency, while statistical learning approaches are not\ngenerating highly compact features due to their indirect feature optimization.\nTo this end, we propose a novel repeller-attractor loss that falls in the\nmetric learning paradigm, yet directly optimizes for the L2 metric without the\nneed of generating pairs. Our loss is formed of three components. One leading\nobjective ensures that the learned features are attracted to each designated\nlearnable class anchor. The second loss component regulates the anchors and\nforces them to be separable by a margin, while the third objective ensures that\nthe anchors do not collapse to zero. Furthermore, we develop a more efficient\ntwo-stage retrieval system by harnessing the learned class anchors during the\nfirst stage of the retrieval process, eliminating the need of comparing the\nquery with every image in the database. We establish a set of four datasets\n(CIFAR-100, Food-101, SVHN, and Tiny ImageNet) and evaluate the proposed\nobjective in the context of few-shot and full-set training on the CBIR task, by\nusing both convolutional and transformer architectures. Compared to existing\nobjective functions, our empirical evidence shows that the proposed objective\nis generating superior and more consistent results.",
        "translated": ""
    },
    {
        "title": "End-to-end Knowledge Retrieval with Multi-modal Queries",
        "url": "http://arxiv.org/abs/2306.00424v1",
        "pub_date": "2023-06-01",
        "summary": "We investigate knowledge retrieval with multi-modal queries, i.e. queries\ncontaining information split across image and text inputs, a challenging task\nthat differs from previous work on cross-modal retrieval. We curate a new\ndataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a\nsystem to retrieve knowledge from a large corpus by integrating contents from\nboth text and image queries. We introduce a retriever model ``ReViz'' that can\ndirectly process input text and images to retrieve relevant knowledge in an\nend-to-end fashion without being dependent on intermediate modules such as\nobject detectors or caption generators. We introduce a new pretraining task\nthat is effective for learning knowledge retrieval with multimodal queries and\nalso improves performance on downstream tasks. We demonstrate superior\nperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot\nsettings as well as further improvements when finetuned on these datasets.",
        "translated": ""
    },
    {
        "title": "A Survey on Fairness-aware Recommender Systems",
        "url": "http://arxiv.org/abs/2306.00403v1",
        "pub_date": "2023-06-01",
        "summary": "As information filtering services, recommender systems have extremely\nenriched our daily life by providing personalized suggestions and facilitating\npeople in decision-making, which makes them vital and indispensable to human\nsociety in the information era. However, as people become more dependent on\nthem, recent studies show that recommender systems potentially own\nunintentional impacts on society and individuals because of their unfairness\n(e.g., gender discrimination in job recommendations). To develop trustworthy\nservices, it is crucial to devise fairness-aware recommender systems that can\nmitigate these bias issues. In this survey, we summarise existing methodologies\nand practices of fairness in recommender systems. Firstly, we present concepts\nof fairness in different recommendation scenarios, comprehensively categorize\ncurrent advances, and introduce typical methods to promote fairness in\ndifferent stages of recommender systems. Next, after introducing datasets and\nevaluation metrics applied to assess the fairness of recommender systems, we\nwill delve into the significant influence that fairness-aware recommender\nsystems exert on real-world industrial applications. Subsequently, we highlight\nthe connection between fairness and other principles of trustworthy recommender\nsystems, aiming to consider trustworthiness principles holistically while\nadvocating for fairness. Finally, we summarize this review, spotlighting\npromising opportunities in comprehending concepts, frameworks, the balance\nbetween accuracy and fairness, and the ties with trustworthiness, with the\nultimate goal of fostering the development of fairness-aware recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "Explicit Feature Interaction-aware Uplift Network for Online Marketing",
        "url": "http://arxiv.org/abs/2306.00315v1",
        "pub_date": "2023-06-01",
        "summary": "As a key component in online marketing, uplift modeling aims to accurately\ncapture the degree to which different treatments motivate different users, such\nas coupons or discounts, also known as the estimation of individual treatment\neffect (ITE). In an actual business scenario, the options for treatment may be\nnumerous and complex, and there may be correlations between different\ntreatments. In addition, each marketing instance may also have rich user and\ncontextual features. However, existing methods still fall short in both fully\nexploiting treatment information and mining features that are sensitive to a\nparticular treatment. In this paper, we propose an explicit feature\ninteraction-aware uplift network (EFIN) to address these two problems. Our EFIN\nincludes four customized modules: 1) a feature encoding module encodes not only\nthe user and contextual features, but also the treatment features; 2) a\nself-interaction module aims to accurately model the user's natural response\nwith all but the treatment features; 3) a treatment-aware interaction module\naccurately models the degree to which a particular treatment motivates a user\nthrough interactions between the treatment features and other features, i.e.,\nITE; and 4) an intervention constraint module is used to balance the ITE\ndistribution of users between the control and treatment groups so that the\nmodel would still achieve a accurate uplift ranking on data collected from a\nnon-random intervention marketing scenario. We conduct extensive experiments on\ntwo public datasets and one product dataset to verify the effectiveness of our\nEFIN. In addition, our EFIN has been deployed in a credit card bill payment\nscenario of a large online financial platform with a significant improvement.",
        "translated": ""
    },
    {
        "title": "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest",
        "url": "http://arxiv.org/abs/2306.00248v1",
        "pub_date": "2023-05-31",
        "summary": "Sequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
        "translated": ""
    },
    {
        "title": "Fresh Content Needs More Attention: Multi-funnel Fresh Content\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.01720v1",
        "pub_date": "2023-06-02",
        "summary": "Recommendation system serves as a conduit connecting users to an incredibly\nlarge, diverse and ever growing collection of contents. In practice, missing\ninformation on fresh (and tail) contents needs to be filled in order for them\nto be exposed and discovered by their audience. We here share our success\nstories in building a dedicated fresh content recommendation stack on a large\ncommercial platform. To nominate fresh contents, we built a multi-funnel\nnomination system that combines (i) a two-tower model with strong\ngeneralization power for coverage, and (ii) a sequence model with near\nreal-time update on user feedback for relevance. The multi-funnel setup\neffectively balances between coverage and relevance. An in-depth study uncovers\nthe relationship between user activity level and their proximity toward fresh\ncontents, which further motivates a contextual multi-funnel setup. Nominated\nfresh candidates are then scored and ranked by systems considering prediction\nuncertainty to further bootstrap content with less exposure. We evaluate the\nbenefits of the dedicated fresh content recommendation stack, and the\nmulti-funnel nomination system in particular, through user corpus co-diverted\nlive experiments. We conduct multiple rounds of live experiments on a\ncommercial platform serving billion of users demonstrating efficacy of our\nproposed methods.",
        "translated": ""
    },
    {
        "title": "Pretrained Language Model based Web Search Ranking: From Relevance to\n  Satisfaction",
        "url": "http://arxiv.org/abs/2306.01599v1",
        "pub_date": "2023-06-02",
        "summary": "Search engine plays a crucial role in satisfying users' diverse information\nneeds. Recently, Pretrained Language Models (PLMs) based text ranking models\nhave achieved huge success in web search. However, many state-of-the-art text\nranking approaches only focus on core relevance while ignoring other dimensions\nthat contribute to user satisfaction, e.g., document quality, recency,\nauthority, etc. In this work, we focus on ranking user satisfaction rather than\nrelevance in web search, and propose a PLM-based framework, namely SAT-Ranker,\nwhich comprehensively models different dimensions of user satisfaction in a\nunified manner. In particular, we leverage the capacities of PLMs on both\ntextual and numerical inputs, and apply a multi-field input that modularizes\neach dimension of user satisfaction as an input field. Overall, SAT-Ranker is\nan effective, extensible, and data-centric framework that has huge potential\nfor industrial applications. On rigorous offline and online experiments,\nSAT-Ranker obtains remarkable gains on various evaluation sets targeting\ndifferent dimensions of user satisfaction. It is now fully deployed online to\nimprove the usability of our search engine.",
        "translated": ""
    },
    {
        "title": "Influence Maximization with Fairness at Scale (Extended Version)",
        "url": "http://arxiv.org/abs/2306.01587v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we revisit the problem of influence maximization with\nfairness, which aims to select k influential nodes to maximise the spread of\ninformation in a network, while ensuring that selected sensitive user\nattributes are fairly affected, i.e., are proportionally similar between the\noriginal network and the affected users. Recent studies on this problem focused\nonly on extremely small networks, hence the challenge remains on how to achieve\na scalable solution, applicable to networks with millions or billions of nodes.\nWe propose an approach that is based on learning node representations for fair\nspread from diffusion cascades, instead of the social connectivity s.t. we can\ndeal with very large graphs. We propose two data-driven approaches: (a)\nfairness-based participant sampling (FPS), and (b) fairness as context (FAC).\nSpread related user features, such as the probability of diffusing information\nto others, are derived from the historical information cascades, using a deep\nneural network. The extracted features are then used in selecting influencers\nthat maximize the influence spread, while being also fair with respect to the\nchosen sensitive attributes. In FPS, fairness and cascade length information\nare considered independently in the decision-making process, while FAC\nconsiders these information facets jointly and considers correlations between\nthem. The proposed algorithms are generic and represent the first policy-driven\nsolutions that can be applied to arbitrary sets of sensitive attributes at\nscale. We evaluate the performance of our solutions on a real-world public\ndataset (Sina Weibo) and on a hybrid real-synthethic dataset (Digg), which\nexhibit all the facets that we exploit, namely diffusion network, diffusion\ntraces, and user profiles. These experiments show that our methods outperform\nthe state-the-art solutions in terms of spread, fairness, and scalability.",
        "translated": ""
    },
    {
        "title": "Système de recommandations basé sur les contraintes pour les\n  simulations de gestion de crise",
        "url": "http://arxiv.org/abs/2306.01504v1",
        "pub_date": "2023-06-02",
        "summary": "In the context of the evacuation of populations, some citizens/volunteers may\nwant and be able to participate in the evacuation of populations in difficulty\nby coming to lend a hand to emergency/evacuation vehicles with their own\nvehicles. One way of framing these impulses of solidarity would be to be able\nto list in real-time the citizens/volunteers available with their vehicles\n(land, sea, air, etc.), to be able to geolocate them according to the risk\nareas to be evacuated, and adding them to the evacuation/rescue vehicles.\nBecause it is difficult to propose an effective real-time operational system on\nthe field in a real crisis situation, in this work, we propose to add a module\nfor recommending driver/vehicle pairs (with their specificities) to a system of\ncrisis management simulation. To do that, we chose to model and develop an\nontology-supported constraint-based recommender system for crisis management\nsimulations.",
        "translated": ""
    },
    {
        "title": "Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking\n  Intent in Recommender Systems",
        "url": "http://arxiv.org/abs/2306.01476v1",
        "pub_date": "2023-06-02",
        "summary": "Recommending novel content, which expands user horizons by introducing them\nto new interests, has been shown to improve users' long-term experience on\nrecommendation platforms \\cite{chen2021values}. Users however are not\nconstantly looking to explore novel content. It is therefore crucial to\nunderstand their novelty-seeking intent and adjust the recommendation policy\naccordingly. Most existing literature models a user's propensity to choose\nnovel content or to prefer a more diverse set of recommendations at individual\ninteractions. Hierarchical structure, on the other hand, exists in a user's\nnovelty-seeking intent, which is manifested as a static and intrinsic user\npreference for seeking novelty along with a dynamic session-based propensity.\nTo this end, we propose a novel hierarchical reinforcement learning-based\nmethod to model the hierarchical user novelty-seeking intent, and to adapt the\nrecommendation policy accordingly based on the extracted user novelty-seeking\npropensity. We further incorporate diversity and novelty-related measurement in\nthe reward function of the hierarchical RL (HRL) agent to encourage user\nexploration \\cite{chen2021values}. We demonstrate the benefits of explicitly\nmodeling hierarchical user novelty-seeking intent in recommendations through\nextensive experiments on simulated and real-world datasets. In particular, we\ndemonstrate that the effectiveness of our proposed hierarchical RL-based method\nlies in its ability to capture such hierarchically-structured intent. As a\nresult, the proposed HRL model achieves superior performance on several public\ndatasets, compared with state-of-art baselines.",
        "translated": ""
    },
    {
        "title": "Prompt Tuning Large Language Models on Personalized Aspect Extraction\n  for Recommendations",
        "url": "http://arxiv.org/abs/2306.01475v1",
        "pub_date": "2023-06-02",
        "summary": "Existing aspect extraction methods mostly rely on explicit or ground truth\naspect information, or using data mining or machine learning approaches to\nextract aspects from implicit user feedback such as user reviews. It however\nremains under-explored how the extracted aspects can help generate more\nmeaningful recommendations to the users. Meanwhile, existing research on\naspect-based recommendations often relies on separate aspect extraction models\nor assumes the aspects are given, without accounting for the fact the optimal\nset of aspects could be dependent on the recommendation task at hand.\n  In this work, we propose to combine aspect extraction together with\naspect-based recommendations in an end-to-end manner, achieving the two goals\ntogether in a single framework. For the aspect extraction component, we\nleverage the recent advances in large language models and design a new prompt\nlearning mechanism to generate aspects for the end recommendation task. For the\naspect-based recommendation component, the extracted aspects are concatenated\nwith the usual user and item features used by the recommendation model. The\nrecommendation task mediates the learning of the user embeddings and item\nembeddings, which are used as soft prompts to generate aspects. Therefore, the\nextracted aspects are personalized and contextualized by the recommendation\ntask. We showcase the effectiveness of our proposed method through extensive\nexperiments on three industrial datasets, where our proposed framework\nsignificantly outperforms state-of-the-art baselines in both the personalized\naspect extraction and aspect-based recommendation tasks. In particular, we\ndemonstrate that it is necessary and beneficial to combine the learning of\naspect extraction and aspect-based recommendation together. We also conduct\nextensive ablation studies to understand the contribution of each design\ncomponent in our framework.",
        "translated": ""
    },
    {
        "title": "An OPC UA-based industrial Big Data architecture",
        "url": "http://arxiv.org/abs/2306.01418v1",
        "pub_date": "2023-06-02",
        "summary": "Industry 4.0 factories are complex and data-driven. Data is yielded from many\nsources, including sensors, PLCs, and other devices, but also from IT, like ERP\nor CRM systems. We ask how to collect and process this data in a way, such that\nit includes metadata and can be used for industrial analytics or to derive\nintelligent support systems. This paper describes a new, query model based\napproach, which uses a big data architecture to capture data from various\nsources using OPC UA as a foundation. It buffers and preprocesses the\ninformation for the purpose of harmonizing and providing a holistic state space\nof a factory, as well as mappings to the current state of a production site.\nThat information can be made available to multiple processing sinks, decoupled\nfrom the data sources, which enables them to work with the information without\ninterfering with devices of the production, disturbing the network devices they\nare working in, or influencing the production process negatively. Metadata and\nconnected semantic information is kept throughout the process, allowing to feed\nalgorithms with meaningful data, so that it can be accessed in its entirety to\nperform time series analysis, machine learning or similar evaluations as well\nas replaying the data from the buffer for repeatable simulations.",
        "translated": ""
    },
    {
        "title": "DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG\n  2000 Compressed Documents",
        "url": "http://arxiv.org/abs/2306.01359v1",
        "pub_date": "2023-06-02",
        "summary": "For any digital application with document images such as retrieval, the\nclassification of document images becomes an essential stage. Conventionally\nfor the purpose, the full versions of the documents, that is the uncompressed\ndocument images make the input dataset, which poses a threat due to the big\nvolume required to accommodate the full versions of the documents. Therefore,\nit would be novel, if the same classification task could be accomplished\ndirectly (with some partial decompression) with the compressed representation\nof documents in order to make the whole process computationally more efficient.\nIn this research work, a novel deep learning model, DWT CompCNN is proposed for\nclassification of documents that are compressed using High Throughput JPEG 2000\n(HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional\nlayers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each\nincreasing layer to improve learning from the wavelet coefficients extracted\nfrom the compressed images. Experiments are performed on two benchmark\ndatasets- Tobacco-3482 and RVL-CDIP, which demonstrate that the proposed model\nis time and space efficient, and also achieves a better classification accuracy\nin compressed domain.",
        "translated": ""
    },
    {
        "title": "Reducing Popularity Bias in Recommender Systems through AUC-Optimal\n  Negative Sampling",
        "url": "http://arxiv.org/abs/2306.01348v1",
        "pub_date": "2023-06-02",
        "summary": "Popularity bias is a persistent issue associated with recommendation systems,\nposing challenges to both fairness and efficiency. Existing literature widely\nacknowledges that reducing popularity bias often requires sacrificing\nrecommendation accuracy. In this paper, we challenge this commonly held belief.\nOur analysis under general bias-variance decomposition framework shows that\nreducing bias can actually lead to improved model performance under certain\nconditions. To achieve this win-win situation, we propose to intervene in model\ntraining through negative sampling thereby modifying model predictions.\nSpecifically, we provide an optimal negative sampling rule that maximizes\npartial AUC to preserve the accuracy of any given model, while correcting\nsample information and prior information to reduce popularity bias in a\nflexible and principled way. Our experimental results on real-world datasets\ndemonstrate the superiority of our approach in improving recommendation\nperformance and reducing popularity bias.",
        "translated": ""
    },
    {
        "title": "LyricSIM: A novel Dataset and Benchmark for Similarity Detection in\n  Spanish Song LyricS",
        "url": "http://arxiv.org/abs/2306.01325v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we present a new dataset and benchmark tailored to the task of\nsemantic similarity in song lyrics. Our dataset, originally consisting of 2775\npairs of Spanish songs, was annotated in a collective annotation experiment by\n63 native annotators. After collecting and refining the data to ensure a high\ndegree of consensus and data integrity, we obtained 676 high-quality annotated\npairs that were used to evaluate the performance of various state-of-the-art\nmonolingual and multilingual language models. Consequently, we established\nbaseline results that we hope will be useful to the community in all future\nacademic and industrial applications conducted in this context.",
        "translated": ""
    },
    {
        "title": "Learning Similarity among Users for Personalized Session-Based\n  Recommendation from hierarchical structure of User-Session-Item",
        "url": "http://arxiv.org/abs/2306.03040v1",
        "pub_date": "2023-06-05",
        "summary": "The task of the session-based recommendation is to predict the next\ninteraction of the user based on the anonymized user's behavior pattern. And\npersonalized version of this system is a promising research field due to its\navailability to deal with user information. However, there's a problem that the\nuser's preferences and historical sessions were not considered in the typical\nsession-based recommendation since it concentrates only on user-item\ninteraction. In addition, the existing personalized session-based\nrecommendation model has a limited capability in that it only considers the\npreference of the current user without considering those of similar users. It\nmeans there can be the loss of information included within the hierarchical\ndata structure of the user-session-item. To tackle with this problem, we\npropose USP-SBR(abbr. of User Similarity Powered - Session Based Recommender).\nTo model global historical sessions of users, we propose UserGraph that has two\ntypes of nodes - ItemNode and UserNode. We then connect the nodes with three\ntypes of edges. The first type of edges connects ItemNode as chronological\norder, and the second connects ItemNode to UserNode, and the last connects\nUserNode to ItemNode. With these user embeddings, we propose additional\ncontrastive loss, that makes users with similar intention be close to each\nother in the vector space. we apply graph neural network on these UserGraph and\nupdate nodes. Experimental results on two real-world datasets demonstrate that\nour method outperforms some state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Gen-IR @ SIGIR 2023: The First Workshop on Generative Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2306.02887v1",
        "pub_date": "2023-06-05",
        "summary": "Generative information retrieval (IR) has experienced substantial growth\nacross multiple research communities (e.g., information retrieval, computer\nvision, natural language processing, and machine learning), and has been highly\nvisible in the popular press. Theoretical, empirical, and actual user-facing\nproducts have been released that retrieve documents (via generation) or\ndirectly generate answers given an input request. We would like to investigate\nwhether end-to-end generative models are just another trend or, as some claim,\na paradigm change for IR. This necessitates new metrics, theoretical grounding,\nevaluation methods, task definitions, models, user interfaces, etc. The goal of\nthis workshop (https://coda.io/@sigir/gen-ir) is to focus on previously\nexplored Generative IR techniques like document retrieval and direct Grounded\nAnswer Generation, while also offering a venue for the discussion and\nexploration of how Generative IR can be applied to new domains like\nrecommendation systems, summarization, etc. The format of the workshop is\ninteractive, including roundtable and keynote sessions and tends to avoid the\none-sided dialogue of a mini-conference.",
        "translated": ""
    },
    {
        "title": "Benchmarking Middle-Trained Language Models for Neural Search",
        "url": "http://arxiv.org/abs/2306.02867v1",
        "pub_date": "2023-06-05",
        "summary": "Middle training methods aim to bridge the gap between the Masked Language\nModel (MLM) pre-training and the final finetuning for retrieval. Recent models\nsuch as CoCondenser, RetroMAE, and LexMAE argue that the MLM task is not\nsufficient enough to pre-train a transformer network for retrieval and hence\npropose various tasks to do so. Intrigued by those novel methods, we noticed\nthat all these models used different finetuning protocols, making it hard to\nassess the benefits of middle training. We propose in this paper a benchmark of\nCoCondenser, RetroMAE, and LexMAE, under the same finetuning conditions. We\ncompare both dense and sparse approaches under various finetuning protocols and\nmiddle training on different collections (MS MARCO, Wikipedia or Tripclick). We\nuse additional middle training baselines, such as a standard MLM finetuning on\nthe retrieval collection, optionally augmented by a CLS predicting the passage\nterm frequency. For the sparse approach, our study reveals that there is almost\nno statistical difference between those methods: the more effective the\nfinetuning procedure is, the less difference there is between those models. For\nthe dense approach, RetroMAE using MS MARCO as middle-training collection shows\nexcellent results in almost all the settings. Finally, we show that middle\ntraining on the retrieval collection, thus adapting the language model to it,\nis a critical factor. Overall, a better experimental setup should be adopted to\nevaluate middle training methods. Code available at\nhttps://github.com/naver/splade/tree/benchmarch-SIGIR23",
        "translated": ""
    },
    {
        "title": "CTRL: Connect Tabular and Language Model for CTR Prediction",
        "url": "http://arxiv.org/abs/2306.02841v1",
        "pub_date": "2023-06-05",
        "summary": "Traditional click-through rate (CTR) prediction models convert the tabular\ndata into one-hot vectors and leverage the collaborative relations among\nfeatures for inferring user's preference over items. This modeling paradigm\ndiscards the essential semantic information. Though some recent works like P5\nand M6-Rec have explored the potential of using Pre-trained Language Models\n(PLMs) to extract semantic signals for CTR prediction, they are computationally\nexpensive and suffer from low efficiency. Besides, the beneficial collaborative\nrelations are not considered, hindering the recommendation performance. To\nsolve these problems, in this paper, we propose a novel framework\n\\textbf{CTRL}, which is industrial friendly and model-agnostic with high\ntraining and inference efficiency. Specifically, the original tabular data is\nfirst converted into textual data. Both tabular data and converted textual data\nare regarded as two different modalities and are separately fed into the\ncollaborative CTR model and pre-trained language model. A cross-modal knowledge\nalignment procedure is performed to fine-grained align and integrate the\ncollaborative and semantic signals, and the lightweight collaborative model can\nbe deployed online for efficient serving after fine-tuned with supervised\nsignals. Experimental results on three public datasets show that CTRL\noutperforms the SOTA CTR models significantly. Moreover, we further verify its\neffectiveness on a large-scale industrial recommender system.",
        "translated": ""
    },
    {
        "title": "Path-Specific Counterfactual Fairness for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02615v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender systems (RSs) have become an indispensable part of online\nplatforms. With the growing concerns of algorithmic fairness, RSs are not only\nexpected to deliver high-quality personalized content, but are also demanded\nnot to discriminate against users based on their demographic information.\nHowever, existing RSs could capture undesirable correlations between sensitive\nfeatures and observed user behaviors, leading to biased recommendations. Most\nfair RSs tackle this problem by completely blocking the influences of sensitive\nfeatures on recommendations. But since sensitive features may also affect user\ninterests in a fair manner (e.g., race on culture-based preferences),\nindiscriminately eliminating all the influences of sensitive features\ninevitably degenerate the recommendations quality and necessary diversities. To\naddress this challenge, we propose a path-specific fair RS (PSF-RS) for\nrecommendations. Specifically, we summarize all fair and unfair correlations\nbetween sensitive features and observed ratings into two latent proxy\nmediators, where the concept of path-specific bias (PS-Bias) is defined based\non path-specific counterfactual inference. Inspired by Pearl's minimal change\nprinciple, we address the PS-Bias by minimally transforming the biased factual\nworld into a hypothetically fair world, where a fair RS model can be learned\naccordingly by solving a constrained optimization problem. For the technical\npart, we propose a feasible implementation of PSF-RS, i.e., PSF-VAE, with\nweakly-supervised variational inference, which robustly infers the latent\nmediators such that unfairness can be mitigated while necessary recommendation\ndiversities can be maximally preserved simultaneously. Experiments conducted on\nsemi-simulated and real-world datasets demonstrate the effectiveness of PSF-RS.",
        "translated": ""
    },
    {
        "title": "Learning to Relate to Previous Turns in Conversational Search",
        "url": "http://arxiv.org/abs/2306.02553v1",
        "pub_date": "2023-06-05",
        "summary": "Conversational search allows a user to interact with a search system in\nmultiple turns. A query is strongly dependent on the conversation context. An\neffective way to improve retrieval effectiveness is to expand the current query\nwith historical queries. However, not all the previous queries are related to,\nand useful for expanding the current query. In this paper, we propose a new\nmethod to select relevant historical queries that are useful for the current\nquery. To cope with the lack of labeled training data, we use a pseudo-labeling\napproach to annotate useful historical queries based on their impact on the\nretrieval results. The pseudo-labeled data are used to train a selection model.\nWe further propose a multi-task learning framework to jointly train the\nselector and the retriever during fine-tuning, allowing us to mitigate the\npossible inconsistency between the pseudo labels and the changed retriever.\nExtensive experiments on four conversational search datasets demonstrate the\neffectiveness and broad applicability of our method compared with several\nstrong baselines.",
        "translated": ""
    },
    {
        "title": "RecAgent: A Novel Simulation Paradigm for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02552v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender system has deeply revolutionized people's daily life and\nproduction, bringing a large amount of business value. In the recommendation\ndomain, simulation and real data-based studies are two typical research\nparadigms, with each having different advantages. Previously, real data-based\nstudies occupy more important positions, since accurately simulating the user\npreference is quite difficult. Recently, large language models (LLM) have shown\ngreat potential to achieve human-like intelligence, which provides new\nopportunities to overcome the shortcomings of simulation-based studies and thus\nhighlight their advantages, such as much more application scenarios and cheaper\ndata acquisition strategies. To shed lights on this direction, in this paper,\nwe introduce an LLM-based recommender simulator called RecAgent. Our simulator\nis composed of two modules: (1) the user module and (2) the recommender module.\nThe user module can browse the recommendation website, communicate with other\nusers and broadcast messages on the social media. The recommender module is\ndesigned to provide search or recommendation lists to the users, and one can\ndesign different models to implement the recommender. All the users take\nactions based on LLMs, and can freely evolve like in the real world. We present\nseveral case studies to demonstrate that the users in our simulator can indeed\nbehave in a reasonable manner as expected. Our project has been released at\nhttps://github.com/RUC-GSAI/YuLan-Rec.",
        "translated": ""
    },
    {
        "title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions",
        "url": "http://arxiv.org/abs/2306.02549v1",
        "pub_date": "2023-06-05",
        "summary": "This paper investigates the use of artificial intelligence chatbots for\npatient-specific question answering (QA) from clinical notes using several\nlarge language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google\nBard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and\ncoherence of the answers generated by each model using a 5-point Likert scale\non a set of patient-specific questions.",
        "translated": ""
    },
    {
        "title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models\n  with Same Tower Negatives",
        "url": "http://arxiv.org/abs/2306.02516v1",
        "pub_date": "2023-06-05",
        "summary": "Dual encoders have been used for retrieval tasks and representation learning\nwith good results. A standard way to train dual encoders is using a contrastive\nloss with in-batch negatives. In this work, we propose an improved contrastive\nlearning objective by adding queries or documents from the same encoder towers\nto the negatives, for which we name it as \"contrastive loss with SAMe TOwer\nNEgatives\" (SamToNe). By evaluating on question answering retrieval benchmarks\nfrom MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval\nbenchmarks (BEIR), we demonstrate that SamToNe can effectively improve the\nretrieval quality for both symmetric and asymmetric dual encoders. By directly\nprobing the embedding spaces of the two encoding towers via the t-SNE algorithm\n(van der Maaten and Hinton, 2008), we observe that SamToNe ensures the\nalignment between the embedding spaces from the two encoder towers. Based on\nthe analysis of the embedding distance distributions of the top-$1$ retrieved\nresults, we further explain the efficacy of the method from the perspective of\nregularisation.",
        "translated": ""
    },
    {
        "title": "I^3 Retriever: Incorporating Implicit Interaction in Pre-trained\n  Language Models for Passage Retrieval",
        "url": "http://arxiv.org/abs/2306.02371v1",
        "pub_date": "2023-06-04",
        "summary": "Passage retrieval is a fundamental task in many information systems, such as\nweb search and question answering, where both efficiency and effectiveness are\ncritical concerns. In recent years, neural retrievers based on pre-trained\nlanguage models (PLM), such as dual-encoders, have achieved huge success. Yet,\nstudies have found that the performance of dual-encoders are often limited due\nto the neglecting of the interaction information between queries and candidate\npassages. Therefore, various interaction paradigms have been proposed to\nimprove the performance of vanilla dual-encoders. Particularly, recent\nstate-of-the-art methods often introduce late-interaction during the model\ninference process. However, such late-interaction based methods usually bring\nextensive computation and storage cost on large corpus. Despite their\neffectiveness, the concern of efficiency and space footprint is still an\nimportant factor that limits the application of interaction-based neural\nretrieval models. To tackle this issue, we incorporate implicit interaction\ninto dual-encoders, and propose I^3 retriever. In particular, our implicit\ninteraction paradigm leverages generated pseudo-queries to simulate\nquery-passage interaction, which jointly optimizes with query and passage\nencoders in an end-to-end manner. It can be fully pre-computed and cached, and\nits inference process only involves simple dot product operation of the query\nvector and passage vector, which makes it as efficient as the vanilla dual\nencoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep\nLearning Datasets, demonstrating the I^3 retriever's superiority in terms of\nboth effectiveness and efficiency. Moreover, the proposed implicit interaction\nis compatible with special pre-training and knowledge distillation for passage\nretrieval, which brings a new state-of-the-art performance.",
        "translated": ""
    }
]