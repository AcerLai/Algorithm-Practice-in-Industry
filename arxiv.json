[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": ""
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": ""
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": ""
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": ""
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": ""
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": ""
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": ""
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": ""
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": ""
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": ""
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": ""
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": ""
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": ""
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": ""
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": ""
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": ""
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": ""
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": ""
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": ""
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Breaking the Curse of Quality Saturation with User-Centric Ranking",
        "url": "http://arxiv.org/abs/2305.15333v1",
        "pub_date": "2023-05-24",
        "summary": "A key puzzle in search, ads, and recommendation is that the ranking model can\nonly utilize a small portion of the vastly available user interaction data. As\na result, increasing data volume, model size, or computation FLOPs will quickly\nsuffer from diminishing returns. We examined this problem and found that one of\nthe root causes may lie in the so-called ``item-centric'' formulation, which\nhas an unbounded vocabulary and thus uncontrolled model complexity. To mitigate\nquality saturation, we introduce an alternative formulation named\n``user-centric ranking'', which is based on a transposed view of the dyadic\nuser-item interaction data. We show that this formulation has a promising\nscaling property, enabling us to train better-converged models on substantially\nlarger data sets.",
        "translated": ""
    },
    {
        "title": "Neural Summarization of Electronic Health Records",
        "url": "http://arxiv.org/abs/2305.15222v1",
        "pub_date": "2023-05-24",
        "summary": "Hospital discharge documentation is among the most essential, yet\ntime-consuming documents written by medical practitioners. The objective of\nthis study was to automatically generate hospital discharge summaries using\nneural network summarization models. We studied various data preparation and\nneural network training techniques that generate discharge summaries. Using\nnursing notes and discharge summaries from the MIMIC-III dataset, we studied\nthe viability of the automatic generation of various sections of a discharge\nsummary using four state-of-the-art neural network summarization models (BART,\nT5, Longformer and FLAN-T5). Our experiments indicated that training\nenvironments including nursing notes as the source, and discrete sections of\nthe discharge summary as the target output (e.g. \"History of Present Illness\")\nimprove language model efficiency and text quality. According to our findings,\nthe fine-tuned BART model improved its ROUGE F1 score by 43.6% against its\nstandard off-the-shelf version. We also found that fine-tuning the baseline\nBART model with other setups caused different degrees of improvement (up to 80%\nrelative improvement). We also observed that a fine-tuned T5 generally achieves\nhigher ROUGE F1 scores than other fine-tuned models and a fine-tuned FLAN-T5\nachieves the highest ROUGE score overall, i.e., 45.6. For majority of the\nfine-tuned language models, summarizing discharge summary report sections\nseparately outperformed the summarization the entire report quantitatively. On\nthe other hand, fine-tuning language models that were previously instruction\nfine-tuned showed better performance in summarizing entire reports. This study\nconcludes that a focused dataset designed for the automatic generation of\ndischarge summaries by a language model can produce coherent Discharge Summary\nsections.",
        "translated": ""
    },
    {
        "title": "Collaborative Recommendation Model Based on Multi-modal Multi-view\n  Attention Network: Movie and literature cases",
        "url": "http://arxiv.org/abs/2305.15159v1",
        "pub_date": "2023-05-24",
        "summary": "The existing collaborative recommendation models that use multi-modal\ninformation emphasize the representation of users' preferences but easily\nignore the representation of users' dislikes. Nevertheless, modelling users'\ndislikes facilitates comprehensively characterizing user profiles. Thus, the\nrepresentation of users' dislikes should be integrated into the user modelling\nwhen we construct a collaborative recommendation model. In this paper, we\npropose a novel Collaborative Recommendation Model based on Multi-modal\nmulti-view Attention Network (CRMMAN), in which the users are represented from\nboth preference and dislike views. Specifically, the users' historical\ninteractions are divided into positive and negative interactions, used to model\nthe user's preference and dislike views, respectively. Furthermore, the\nsemantic and structural information extracted from the scene is employed to\nenrich the item representation. We validate CRMMAN by designing contrast\nexperiments based on two benchmark MovieLens-1M and Book-Crossing datasets.\nMovielens-1m has about a million ratings, and Book-Crossing has about 300,000\nratings. Compared with the state-of-the-art knowledge-graph-based and\nmulti-modal recommendation methods, the AUC, NDCG@5 and NDCG@10 are improved by\n2.08%, 2.20% and 2.26% on average of two datasets. We also conduct controlled\nexperiments to explore the effects of multi-modal information and multi-view\nmechanism. The experimental results show that both of them enhance the model's\nperformance.",
        "translated": ""
    },
    {
        "title": "Bert4CMR: Cross-Market Recommendation with Bidirectional Encoder\n  Representations from Transformer",
        "url": "http://arxiv.org/abs/2305.15145v1",
        "pub_date": "2023-05-24",
        "summary": "Real-world multinational e-commerce companies, such as Amazon and eBay, serve\nin multiple countries and regions. Obviously, these markets have similar goods\nbut different users. Some markets are data-scarce, while others are data-rich.\nIn recent years, cross-market recommendation (CMR) has been proposed to enhance\ndata-scarce markets by leveraging auxiliary information from data-rich markets.\nPrevious works fine-tune the pre-trained model on the local market after\nfreezing part of the parameters or introducing inter-market similarity into the\nlocal market to improve the performance of CMR. However, they generally do not\nconsider eliminating the mutual interference between markets. Therefore, the\nexisting methods are neither unable to learn unbiased general knowledge nor\nefficient transfer reusable information across markets. In this paper, we\npropose a novel attention-based model called Bert4CMR to simultaneously improve\nall markets' recommendation performance. Specifically, we employ the attention\nmechanism to capture user interests by modelling user behavioural sequences. We\npre-train the proposed model on global data to learn the general knowledge of\nitems. Then we fine-tune specific target markets to perform local\nrecommendations. We propose market embedding to model the bias of each market\nand reduce the mutual inference between the parallel markets. Extensive\nexperiments conducted on seven markets show that our model is state-of-the-art.\nOur model outperforms the suboptimal model by 4.82%, 4.73%, 7.66% and 6.49% on\naverage of seven datasets in terms of four metrics, respectively. We conduct\nablation experiments to analyse the effectiveness of the proposed components.\nExperimental results indicate that our model is able to learn general knowledge\nthrough global data and shield the mutual interference between markets.",
        "translated": ""
    },
    {
        "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning\n  Strategies",
        "url": "http://arxiv.org/abs/2305.15115v1",
        "pub_date": "2023-05-24",
        "summary": "Recently, a new paradigm called Differentiable Search Index (DSI) has been\nproposed for document retrieval, wherein a sequence-to-sequence model is\nlearned to directly map queries to relevant document identifiers. The key idea\nbehind DSI is to fully parameterize traditional ``index-retrieve'' pipelines\nwithin a single neural model, by encoding all documents in the corpus into the\nmodel parameters. In essence, DSI needs to resolve two major questions: (1) how\nto assign an identifier to each document, and (2) how to learn the associations\nbetween a document and its identifier. In this work, we propose a\nSemantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the\narea of Cognitive Psychology. Our approach advances original DSI in two ways:\n(1) For the document identifier, we take inspiration from Elaboration\nStrategies in human learning. Specifically, we assign each document an\nElaborative Description based on the query generation technique, which is more\nmeaningful than a string of integers in the original DSI; and (2) For the\nassociations between a document and its identifier, we take inspiration from\nRehearsal Strategies in human learning. Specifically, we select fine-grained\nsemantic features from a document as Rehearsal Contents to improve document\nmemorization. Both the offline and online experiments show improved retrieval\nperformance over prevailing baselines.",
        "translated": ""
    },
    {
        "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
        "url": "http://arxiv.org/abs/2305.15053v1",
        "pub_date": "2023-05-24",
        "summary": "When re-finding items, users who forget or are uncertain about identifying\ndetails often rely on creative strategies for expressing their information\nneeds -- complex queries that describe content elements (e.g., book characters\nor events), information beyond the document text (e.g., descriptions of book\ncovers), or personal context (e.g., when they read a book). This retrieval\nsetting, called tip of the tongue (TOT), is especially challenging for models\nheavily reliant on lexical and semantic overlap between query and document\ntext. In this work, we introduce a simple yet effective framework for handling\nsuch complex queries by decomposing the query into individual clues, routing\nthose as sub-queries to specialized retrievers, and ensembling the results.\nThis approach allows us to take advantage of off-the-shelf retrievers (e.g.,\nCLIP for retrieving images of book covers) or incorporate retriever-specific\nlogic (e.g., date constraints). We show that our framework incorportating query\ndecompositions into retrievers can improve gold book recall up to 7% relative\nagain for Recall@5 on a new collection of 14,441 real-world query-book pairs\nfrom an online community for resolving TOT inquiries.",
        "translated": ""
    },
    {
        "title": "Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation",
        "url": "http://arxiv.org/abs/2305.15048v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of\neffect-size-based meta-analysis for multi-task evaluation in NLP and IR. We\nobserved that our communities often face the challenge of aggregating results\nover incomparable metrics and scenarios, which makes conclusions and take-away\nmessages less reliable. With Ranger, we aim to address this issue by providing\na task-agnostic toolkit that combines the effect of a treatment on multiple\ntasks into one statistical evaluation, allowing for comparison of metrics and\ncomputation of an overall summary effect. Our toolkit produces\npublication-ready forest plots that enable clear communication of evaluation\nresults over multiple tasks. Our goal with the ready-to-use Ranger toolkit is\nto promote robust, effect-size-based evaluation and improve evaluation\nstandards in the community. We provide two case studies for common IR and NLP\nsettings to highlight Ranger's benefits.",
        "translated": ""
    },
    {
        "title": "Exploring Adapter-based Transfer Learning for Recommender Systems:\n  Empirical Studies and Practical Insights",
        "url": "http://arxiv.org/abs/2305.15036v1",
        "pub_date": "2023-05-24",
        "summary": "Adapters, a plug-in neural network module with some tunable parameters, have\nemerged as a parameter-efficient transfer learning technique for adapting\npre-trained models to downstream tasks, especially for natural language\nprocessing (NLP) and computer vision (CV) fields. Meanwhile, learning\nrecommendation models directly from raw item modality features -- e.g., texts\nof NLP and images of CV -- can enable effective and transferable recommender\nsystems (called TransRec). In view of this, a natural question arises: can\nadapter-based learning techniques achieve parameter-efficient TransRec with\ngood performance?\n  To this end, we perform empirical studies to address several key\nsub-questions. First, we ask whether the adapter-based TransRec performs\ncomparably to TransRec based on standard full-parameter fine-tuning? does it\nhold for recommendation with different item modalities, e.g., textual RS and\nvisual RS. If yes, we benchmark these existing adapters, which have been shown\nto be effective in NLP and CV tasks, in the item recommendation settings.\nThird, we carefully study several key factors for the adapter-based TransRec in\nterms of where and how to insert these adapters? Finally, we look at the\neffects of adapter-based TransRec by either scaling up its source training data\nor scaling down its target training data. Our paper provides key insights and\npractical guidance on unified &amp; transferable recommendation -- a less studied\nrecommendation scenario. We promise to release all code &amp; datasets for future\nresearch.",
        "translated": ""
    },
    {
        "title": "How Graph Convolutions Amplify Popularity Bias for Recommendation?",
        "url": "http://arxiv.org/abs/2305.14886v1",
        "pub_date": "2023-05-24",
        "summary": "Graph convolutional networks (GCNs) have become prevalent in recommender\nsystem (RS) due to their superiority in modeling collaborative patterns.\nAlthough improving the overall accuracy, GCNs unfortunately amplify popularity\nbias -- tail items are less likely to be recommended. This effect prevents the\nGCN-based RS from making precise and fair recommendations, decreasing the\neffectiveness of recommender systems in the long run.\n  In this paper, we investigate how graph convolutions amplify the popularity\nbias in RS. Through theoretical analyses, we identify two fundamental factors:\n(1) with graph convolution (\\textit{i.e.,} neighborhood aggregation), popular\nitems exert larger influence than tail items on neighbor users, making the\nusers move towards popular items in the representation space; (2) after\nmultiple times of graph convolution, popular items would affect more high-order\nneighbors and become more influential. The two points make popular items get\ncloser to almost users and thus being recommended more frequently. To rectify\nthis, we propose to estimate the amplified effect of popular nodes on each\nnode's representation, and intervene the effect after each graph convolution.\nSpecifically, we adopt clustering to discover highly-influential nodes and\nestimate the amplification effect of each node, then remove the effect from the\nnode embeddings at each graph convolution layer. Our method is simple and\ngeneric -- it can be used in the inference stage to correct existing models\nrather than training a new model from scratch, and can be applied to various\nGCN models. We demonstrate our method on two representative GCN backbones\nLightGCN and UltraGCN, verifying its ability in improving the recommendations\nof tail items without sacrificing the performance of popular items. Codes are\nopen-sourced \\footnote{https://github.com/MEICRS/DAP}.",
        "translated": ""
    },
    {
        "title": "Machine Reading Comprehension using Case-based Reasoning",
        "url": "http://arxiv.org/abs/2305.14815v1",
        "pub_date": "2023-05-24",
        "summary": "We present an accurate and interpretable method for answer extraction in\nmachine reading comprehension that is reminiscent of case-based reasoning (CBR)\nfrom classical AI. Our method (CBR-MRC) builds on the hypothesis that\ncontextualized answers to similar questions share semantic similarities with\neach other. Given a target question, CBR-MRC retrieves a set of similar\nquestions from a memory of observed cases and predicts an answer by selecting\nthe span in the target context that is most similar to the contextualized\nrepresentations of answers in the retrieved cases. The semi-parametric nature\nof our approach allows CBR-MRC to attribute a prediction to the specific set of\ncases used during inference, making it a desirable choice for building reliable\nand debuggable QA systems. We show that CBR-MRC achieves high test accuracy\ncomparable with large reader models, outperforming baselines by 11.5 and 8.4 EM\non NaturalQuestions and NewsQA, respectively. Further, we also demonstrate the\nability of CBR-MRC in identifying not just the correct answer tokens but also\nthe span with the most relevant supporting evidence. Lastly, we observe that\ncontexts for certain question types show higher lexical diversity than others\nand find CBR-MRC to be robust to these variations while performance using\nfully-parametric methods drops.",
        "translated": ""
    },
    {
        "title": "Candidate Set Re-ranking for Composed Image Retrieval with Dual\n  Multi-modal Encoder",
        "url": "http://arxiv.org/abs/2305.16304v1",
        "pub_date": "2023-05-25",
        "summary": "Composed image retrieval aims to find an image that best matches a given\nmulti-modal user query consisting of a reference image and text pair. Existing\nmethods commonly pre-compute image embeddings over the entire corpus and\ncompare these to a reference image embedding modified by the query text at test\ntime. Such a pipeline is very efficient at test time since fast vector\ndistances can be used to evaluate candidates, but modifying the reference image\nembedding guided only by a short textual description can be difficult,\nespecially independent of potential candidates. An alternative approach is to\nallow interactions between the query and every possible candidate, i.e.,\nreference-text-candidate triplets, and pick the best from the entire set.\nThough this approach is more discriminative, for large-scale datasets the\ncomputational cost is prohibitive since pre-computation of candidate embeddings\nis no longer possible. We propose to combine the merits of both schemes using a\ntwo-stage model. Our first stage adopts the conventional vector distancing\nmetric and performs a fast pruning among candidates. Meanwhile, our second\nstage employs a dual-encoder architecture, which effectively attends to the\ninput triplet of reference-text-candidate and re-ranks the candidates. Both\nstages utilize a vision-and-language pre-trained network, which has proven\nbeneficial for various downstream tasks. Our method consistently outperforms\nstate-of-the-art approaches on standard benchmarks for the task.",
        "translated": ""
    },
    {
        "title": "A Survey on Asking Clarification Questions Datasets in Conversational\n  Systems",
        "url": "http://arxiv.org/abs/2305.15933v1",
        "pub_date": "2023-05-25",
        "summary": "The ability to understand a user's underlying needs is critical for\nconversational systems, especially with limited input from users in a\nconversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to\nreveal users' true intent from their queries or utterances arise as an\nessential task. However, it is noticeable that a key limitation of the existing\nACQs studies is their incomparability, from inconsistent use of data, distinct\nexperimental setups and evaluation strategies. Therefore, in this paper, to\nassist the development of ACQs techniques, we comprehensively analyse the\ncurrent ACQs research status, which offers a detailed comparison of publicly\navailable datasets, and discusses the applied evaluation metrics, joined with\nbenchmarks for multiple ACQs-related tasks. In particular, given a thorough\nanalysis of the ACQs task, we discuss a number of corresponding research\ndirections for the investigation of ACQs as well as the development of\nconversational systems.",
        "translated": ""
    },
    {
        "title": "Enhancing the Ranking Context of Dense Retrieval Methods through\n  Reciprocal Nearest Neighbors",
        "url": "http://arxiv.org/abs/2305.15720v1",
        "pub_date": "2023-05-25",
        "summary": "Sparse annotation poses persistent challenges to training dense retrieval\nmodels, such as the problem of false negatives, i.e. unlabeled relevant\ndocuments that are spuriously used as negatives in contrastive learning,\ndistorting the training signal. To alleviate this problem, we introduce\nevidence-based label smoothing, a computationally efficient method that\nprevents penalizing the model for assigning high relevance to false negatives.\nTo compute the target relevance distribution over candidate documents within\nthe ranking context of a given query, candidates most similar to the ground\ntruth are assigned a non-zero relevance probability based on the degree of\ntheir similarity to the ground-truth document(s). As a relevance estimate we\nleverage an improved similarity metric based on reciprocal nearest neighbors,\nwhich can also be used independently to rerank candidates in post-processing.\nThrough extensive experiments on two large-scale ad hoc text retrieval datasets\nwe demonstrate that both methods can improve the ranking effectiveness of dense\nretrieval models.",
        "translated": ""
    },
    {
        "title": "BookGPT: A General Framework for Book Recommendation Empowered by Large\n  Language Model",
        "url": "http://arxiv.org/abs/2305.15673v1",
        "pub_date": "2023-05-25",
        "summary": "With the continuous development and change exhibited by large language model\n(LLM) technology, represented by generative pretrained transformers (GPTs),\nmany classic scenarios in various fields have re-emerged with new\nopportunities. This paper takes ChatGPT as the modeling object, incorporates\nLLM technology into the typical book resource understanding and recommendation\nscenario for the first time, and puts it into practice. By building a\nChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT,\nthis paper attempts to apply ChatGPT to recommendation modeling for three\ntypical tasks, book rating recommendation, user rating recommendation, and book\nsummary recommendation, and explores the feasibility of LLM technology in book\nrecommendation scenarios. At the same time, based on different evaluation\nschemes for book recommendation tasks and the existing classic recommendation\nmodels, this paper discusses the advantages and disadvantages of the BookGPT in\nbook recommendation scenarios and analyzes the opportunities and improvement\ndirections for subsequent LLMs in these scenarios.",
        "translated": ""
    },
    {
        "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
        "url": "http://arxiv.org/abs/2305.15645v1",
        "pub_date": "2023-05-25",
        "summary": "In conversational search, the user's real search intent for the current turn\nis dependent on the previous conversation history. It is challenging to\ndetermine a good search query from the whole conversation context. To avoid the\nexpensive re-training of the query encoder, most existing methods try to learn\na rewriting model to de-contextualize the current query by mimicking the manual\nquery rewriting. However, manually rewritten queries are not always the best\nsearch queries. Training a rewriting model on them would limit the model's\nability to produce good search queries. Another useful hint is the potential\nanswer to the question. In this paper, we propose ConvGQR, a new framework to\nreformulate conversational queries based on generative pre-trained language\nmodels (PLMs), one for query rewriting and another for generating potential\nanswers. By combining both, ConvGQR can produce better search queries. In\naddition, to relate query reformulation to retrieval performance, we propose a\nknowledge infusion mechanism to optimize both query reformulation and\nretrieval. Extensive experiments on four conversational search datasets\ndemonstrate the effectiveness of ConvGQR.",
        "translated": ""
    },
    {
        "title": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language\n  Models",
        "url": "http://arxiv.org/abs/2305.15597v1",
        "pub_date": "2023-05-24",
        "summary": "The mission of open knowledge graph (KG) completion is to draw new findings\nfrom known facts. Existing works that augment KG completion require either (1)\nfactual triples to enlarge the graph reasoning space or (2) manually designed\nprompts to extract knowledge from a pre-trained language model (PLM),\nexhibiting limited performance and requiring expensive efforts from experts. To\nthis end, we propose TAGREAL that automatically generates quality query prompts\nand retrieves support information from large text corpora to probe knowledge\nfrom PLM for KG completion. The results show that TAGREAL achieves\nstate-of-the-art performance on two benchmark datasets. We find that TAGREAL\nhas superb performance even with limited training data, outperforming existing\nembedding-based, graph-based, and PLM-based methods.",
        "translated": ""
    },
    {
        "title": "Representation Online Matters: Practical End-to-End Diversification in\n  Search and Recommender Systems",
        "url": "http://arxiv.org/abs/2305.15534v1",
        "pub_date": "2023-05-24",
        "summary": "As the use of online platforms continues to grow across all demographics,\nusers often express a desire to feel represented in the content. To improve\nrepresentation in search results and recommendations, we introduce end-to-end\ndiversification, ensuring that diverse content flows throughout the various\nstages of these systems, from retrieval to ranking. We develop, experiment, and\ndeploy scalable diversification mechanisms in multiple production surfaces on\nthe Pinterest platform, including Search, Related Products, and New User\nHomefeed, to improve the representation of different skin tones in beauty and\nfashion content. Diversification in production systems includes three\ncomponents: identifying requests that will trigger diversification, ensuring\ndiverse content is retrieved from the large content corpus during the retrieval\nstage, and finally, balancing the diversity-utility trade-off in a\nself-adjusting manner in the ranking stage. Our approaches, which evolved from\nusing Strong-OR logical operator to bucketized retrieval at the retrieval stage\nand from greedy re-rankers to multi-objective optimization using determinantal\npoint processes for the ranking stage, balances diversity and utility while\nenabling fast iterations and scalable expansion to diversification over\nmultiple dimensions. Our experiments indicate that these approaches\nsignificantly improve diversity metrics, with a neutral to a positive impact on\nutility metrics and improved user satisfaction, both qualitatively and\nquantitatively, in production.",
        "translated": ""
    },
    {
        "title": "Large Language Models for User Interest Journeys",
        "url": "http://arxiv.org/abs/2305.15498v1",
        "pub_date": "2023-05-24",
        "summary": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
        "translated": ""
    },
    {
        "title": "Adversarial Attacks on Online Learning to Rank with Click Feedback",
        "url": "http://arxiv.org/abs/2305.17071v1",
        "pub_date": "2023-05-26",
        "summary": "Online learning to rank (OLTR) is a sequential decision-making problem where\na learning agent selects an ordered list of items and receives feedback through\nuser clicks. Although potential attacks against OLTR algorithms may cause\nserious losses in real-world applications, little is known about adversarial\nattacks on OLTR. This paper studies attack strategies against multiple variants\nof OLTR. Our first result provides an attack strategy against the UCB algorithm\non classical stochastic bandits with binary feedback, which solves the key\nissues caused by bounded and discrete feedback that previous works can not\nhandle. Building on this result, we design attack algorithms against UCB-based\nOLTR algorithms in position-based and cascade models. Finally, we propose a\ngeneral attack strategy against any algorithm under the general click model.\nEach attack algorithm manipulates the learning agent into choosing the target\nattack item $T-o(T)$ times, incurring a cumulative cost of $o(T)$. Experiments\non synthetic and real data further validate the effectiveness of our proposed\nattack algorithms.",
        "translated": ""
    },
    {
        "title": "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System",
        "url": "http://arxiv.org/abs/2305.17034v1",
        "pub_date": "2023-05-26",
        "summary": "Significant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type.",
        "translated": ""
    },
    {
        "title": "Is googling risky? A study on risk perception and experiences of adverse\n  consequences in web search",
        "url": "http://arxiv.org/abs/2305.16990v1",
        "pub_date": "2023-05-26",
        "summary": "Search engines, such as Google, have a considerable impact on society.\nTherefore, undesirable consequences, such as retrieving incorrect search\nresults, pose a risk to users. Although previous research has reported the\nadverse outcomes of web search, little is known about how search engine users\nevaluate those outcomes. In this study, we show which aspects of web search are\nperceived as risky using a sample (N = 3,884) representative of the German\nInternet population. We found that many participants are often concerned with\nadverse consequences immediately appearing on the search engine result page.\nMoreover, participants' experiences with adverse consequences are directly\nrelated to their risk perception. Our results demonstrate that people perceive\nrisks related to web search. In addition to our study, there is a need for more\nindependent research on the possible detrimental outcomes of web search to\nmonitor and mitigate risks. Apart from risks for individuals, search engines\nwith a massive number of users have an extraordinary impact on society;\ntherefore, the acceptable risks of web search should be discussed.",
        "translated": ""
    },
    {
        "title": "Efficient Decoding of Compositional Structure in Holistic\n  Representations",
        "url": "http://arxiv.org/abs/2305.16873v1",
        "pub_date": "2023-05-26",
        "summary": "We investigate the task of retrieving information from compositional\ndistributed representations formed by Hyperdimensional Computing/Vector\nSymbolic Architectures and present novel techniques which achieve new\ninformation rate bounds. First, we provide an overview of the decoding\ntechniques that can be used to approach the retrieval task. The techniques are\ncategorized into four groups. We then evaluate the considered techniques in\nseveral settings that involve, e.g., inclusion of external noise and storage\nelements with reduced precision. In particular, we find that the decoding\ntechniques from the sparse coding and compressed sensing literature (rarely\nused for Hyperdimensional Computing/Vector Symbolic Architectures) are also\nwell-suited for decoding information from the compositional distributed\nrepresentations. Combining these decoding techniques with interference\ncancellation ideas from communications improves previously reported bounds\n(Hersche et al., 2021) of the information rate of the distributed\nrepresentations from 1.20 to 1.40 bits per dimension for smaller codebooks and\nfrom 0.60 to 1.26 bits per dimension for larger codebooks.",
        "translated": ""
    },
    {
        "title": "Automating the Analysis of Institutional Design in International\n  Agreements",
        "url": "http://arxiv.org/abs/2305.16750v1",
        "pub_date": "2023-05-26",
        "summary": "This paper explores the automatic knowledge extraction of formal\ninstitutional design - norms, rules, and actors - from international\nagreements. The focus was to analyze the relationship between the visibility\nand centrality of actors in the formal institutional design in regulating\ncritical aspects of cultural heritage relations. The developed tool utilizes\ntechniques such as collecting legal documents, annotating them with\nInstitutional Grammar, and using graph analysis to explore the formal\ninstitutional design. The system was tested against the 2003 UNESCO Convention\nfor the Safeguarding of the Intangible Cultural Heritage.",
        "translated": ""
    },
    {
        "title": "The Search for Stability: Learning Dynamics of Strategic Publishers with\n  Initial Documents",
        "url": "http://arxiv.org/abs/2305.16695v1",
        "pub_date": "2023-05-26",
        "summary": "We study a game-theoretic model of information retrieval, in which strategic\npublishers aim to maximize their chances of being ranked first by the search\nengine, while maintaining the integrity of their original documents. We show\nthat the commonly used PRP ranking scheme results in an unstable environment\nwhere games often fail to reach pure Nash equilibrium. We propose the Relative\nRanking Principle (RRP) as an alternative ranking principle, and introduce two\nranking functions that are instances of the RRP. We provide both theoretical\nand empirical evidence that these methods lead to a stable search ecosystem, by\nproviding positive results on the learning dynamics convergence. We also define\nthe publishers' and users' welfare, and demonstrate a possible publisher-user\ntrade-off, which highlights the complexity of determining which ranking\nfunction should be selected by the search engine designer.",
        "translated": ""
    },
    {
        "title": "Multiview Identifiers Enhanced Generative Retrieval",
        "url": "http://arxiv.org/abs/2305.16675v1",
        "pub_date": "2023-05-26",
        "summary": "Instead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
        "translated": ""
    },
    {
        "title": "FARA: Future-aware Ranking Algorithm for Fairness Optimization",
        "url": "http://arxiv.org/abs/2305.16637v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking systems are the key components of modern Information Retrieval (IR)\napplications, such as search engines and recommender systems. Besides the\nranking relevance to users, the exposure fairness to item providers has also\nbeen considered an important factor in ranking optimization. Many fair ranking\nalgorithms have been proposed to jointly optimize both ranking relevance and\nfairness. However, we find that most existing fair ranking methods adopt greedy\nalgorithms that only optimize rankings for the next immediate session or\nrequest. As shown in this paper, such a myopic paradigm could limit the upper\nbound of ranking optimization and lead to suboptimal performance in the long\nterm. To this end, we propose FARA, a novel Future-Aware Ranking Algorithm for\nranking relevance and fairness optimization. Instead of greedily optimizing\nrankings for the next immediate session, FARA plans ahead by jointly optimizing\nmultiple ranklists together and saving them for future sessions. Particularly,\nFARA first uses the Taylor expansion to investigate how future ranklists will\ninfluence the overall fairness of the system. Then, based on the analysis of\nthe Taylor expansion, FARA adopts a two-phase optimization algorithm where we\nfirst solve an optimal future exposure planning problem and then construct the\noptimal ranklists according to the optimal future exposure planning.\nTheoretically, we show that FARA is optimal for ranking relevance and fairness\njoint optimization. Empirically, our extensive experiments on three\nsemi-synthesized datasets show that FARA is efficient, effective, and can\ndeliver significantly better ranking performance compared to state-of-the-art\nfair ranking methods.",
        "translated": ""
    },
    {
        "title": "DataFinder: Scientific Dataset Recommendation from Natural Language\n  Descriptions",
        "url": "http://arxiv.org/abs/2305.16636v1",
        "pub_date": "2023-05-26",
        "summary": "Modern machine learning relies on datasets to develop and validate research\nideas. Given the growth of publicly available data, finding the right dataset\nto use is increasingly difficult. Any research question imposes explicit and\nimplicit constraints on how well a given dataset will enable researchers to\nanswer this question, such as dataset size, modality, and domain. We introduce\na new task of recommending relevant datasets given a short natural language\ndescription of a research idea, to help people find relevant datasets for their\nneeds. Dataset recommendation poses unique challenges as an information\nretrieval problem; datasets are hard to directly index for search and there are\nno corpora readily available for this task. To operationalize this task, we\nbuild the DataFinder Dataset which consists of a larger\nautomatically-constructed training set (17.5K queries) and a smaller\nexpert-annotated evaluation set (392 queries). Using this data, we compare\nvarious information retrieval algorithms on our test set and present the\nfirst-ever published system for text-based dataset recommendation using machine\nlearning techniques. This system, trained on the DataFinder Dataset, finds more\nrelevant search results than existing third-party dataset search engines. To\nencourage progress on dataset recommendation, we release our dataset and models\nto the public.",
        "translated": ""
    },
    {
        "title": "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach",
        "url": "http://arxiv.org/abs/2305.16606v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
        "translated": ""
    },
    {
        "title": "Large Language Models are not Fair Evaluators",
        "url": "http://arxiv.org/abs/2305.17926v1",
        "pub_date": "2023-05-29",
        "summary": "We uncover a systematic bias in the evaluation paradigm of adopting large\nlanguage models~(LLMs), e.g., GPT-4, as a referee to score the quality of\nresponses generated by candidate models. We find that the quality ranking of\ncandidate responses can be easily hacked by simply altering their order of\nappearance in the context. This manipulation allows us to skew the evaluation\nresult, making one model appear considerably superior to the other, e.g.,\nvicuna could beat ChatGPT on 66 over 80 tested queries. To address this issue,\nwe propose two simple yet effective calibration strategies: 1) Multiple\nEvidence Calibration, which requires the evaluator model to generate multiple\ndetailed pieces of evidence before assigning ratings; 2) Balanced Position\nCalibration, which aggregates results across various orders to determine the\nfinal score. Extensive experiments demonstrate that our approach successfully\nmitigates evaluation bias, resulting in closer alignment with human judgments.\nTo facilitate future research on more robust large language model comparison,\nwe integrate the techniques in the paper into an easy-to-use toolkit\n\\emph{FairEval}, along with the human\nannotations.\\footnote{\\url{https://github.com/i-Eval/FairEval}}",
        "translated": ""
    },
    {
        "title": "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation",
        "url": "http://arxiv.org/abs/2305.17866v1",
        "pub_date": "2023-05-29",
        "summary": "Traditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "HyperFormer: Learning Expressive Sparse Feature Representations via\n  Hypergraph Transformer",
        "url": "http://arxiv.org/abs/2305.17386v1",
        "pub_date": "2023-05-27",
        "summary": "Learning expressive representations for high-dimensional yet sparse features\nhas been a longstanding problem in information retrieval. Though recent deep\nlearning methods can partially solve the problem, they often fail to handle the\nnumerous sparse features, particularly those tail feature values with\ninfrequent occurrences in the training data. Worse still, existing methods\ncannot explicitly leverage the correlations among different instances to help\nfurther improve the representation learning on sparse features since such\nrelational prior knowledge is not provided. To address these challenges, in\nthis paper, we tackle the problem of representation learning on feature-sparse\ndata from a graph learning perspective. Specifically, we propose to model the\nsparse features of different instances using hypergraphs where each node\nrepresents a data instance and each hyperedge denotes a distinct feature value.\nBy passing messages on the constructed hypergraphs based on our Hypergraph\nTransformer (HyperFormer), the learned feature representations capture not only\nthe correlations among different instances but also the correlations among\nfeatures. Our experiments demonstrate that the proposed approach can\neffectively improve feature representation learning on sparse features.",
        "translated": ""
    },
    {
        "title": "Counterfactual Evaluation of Peer-Review Assignment Policies",
        "url": "http://arxiv.org/abs/2305.17339v1",
        "pub_date": "2023-05-27",
        "summary": "Peer review assignment algorithms aim to match research papers to suitable\nexpert reviewers, working to maximize the quality of the resulting reviews. A\nkey challenge in designing effective assignment policies is evaluating how\nchanges to the assignment algorithm map to changes in review quality. In this\nwork, we leverage recently proposed policies that introduce randomness in\npeer-review assignment--in order to mitigate fraud--as a valuable opportunity\nto evaluate counterfactual assignment policies. Specifically, we exploit how\nsuch randomized assignments provide a positive probability of observing the\nreviews of many assignment policies of interest. To address challenges in\napplying standard off-policy evaluation methods, such as violations of\npositivity, we introduce novel methods for partial identification based on\nmonotonicity and Lipschitz smoothness assumptions for the mapping between\nreviewer-paper covariates and outcomes. We apply our methods to peer-review\ndata from two computer science venues: the TPDP'21 workshop (95 papers and 35\nreviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We\nconsider estimates of (i) the effect on review quality when changing weights in\nthe assignment algorithm, e.g., weighting reviewers' bids vs. textual\nsimilarity (between the review's past papers and the submission), and (ii) the\n\"cost of randomization\", capturing the difference in expected quality between\nthe perturbed and unperturbed optimal match. We find that placing higher weight\non text similarity results in higher review quality and that introducing\nrandomization in the reviewer-paper assignment only marginally reduces the\nreview quality. Our methods for partial identification may be of independent\ninterest, while our off-policy approach can likely find use evaluating a broad\nclass of algorithmic matching systems.",
        "translated": ""
    },
    {
        "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and\n  Document Deduplication",
        "url": "http://arxiv.org/abs/2305.17310v1",
        "pub_date": "2023-05-27",
        "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To\nremove duplicate results in a Web search, for example, a common approach looks\nat the Jaccard index between all pairs of pages. In social network analysis, a\nmuch-celebrated metric is the Adamic-Adar index, widely used to compare node\nneighborhood sets in the important problem of predicting links. However, with\nthe increasing amount of data to be processed, calculating the exact similarity\nbetween all pairs can be intractable. The challenge of working at this scale\nhas motivated research into efficient estimators for set similarity metrics.\nThe two most popular estimators, MinHash and SimHash, are indeed used in\napplications such as document deduplication and recommender systems where large\nvolumes of data need to be processed. Given the importance of these tasks, the\ndemand for advancing estimators is evident. We propose DotHash, an unbiased\nestimator for the intersection size of two sets. DotHash can be used to\nestimate the Jaccard index and, to the best of our knowledge, is the first\nmethod that can also estimate the Adamic-Adar index and a family of related\nmetrics. We formally define this family of metrics, provide theoretical bounds\non the probability of estimate errors, and analyze its empirical performance.\nOur experimental results indicate that DotHash is more accurate than the other\nestimators in link prediction and detecting duplicate documents with the same\ncomplexity and similar comparison time.",
        "translated": ""
    },
    {
        "title": "Table Detection for Visually Rich Document Images",
        "url": "http://arxiv.org/abs/2305.19181v1",
        "pub_date": "2023-05-30",
        "summary": "Table Detection (TD) is a fundamental task towards visually rich document\nunderstanding. Current studies usually formulate the TD problem as an object\ndetection problem, then leverage Intersection over Union (IoU) based metrics to\nevaluate the model performance and IoU-based loss functions to optimize the\nmodel. TD applications usually require the prediction results to cover all the\ntable contents and avoid information loss. However, IoU and IoU-based loss\nfunctions cannot directly reflect the degree of information loss for the\nprediction results. Therefore, we propose to decouple IoU into a ground truth\ncoverage term and a prediction coverage term, in which the former can be used\nto measure the information loss of the prediction results.\n  Besides, tables in the documents are usually large, sparsely distributed, and\nhave no overlaps because they are designed to summarize essential information\nto make it easy to read and interpret for human readers. Therefore, in this\nstudy, we use SparseR-CNN as the base model, and further improve the model by\nusing Gaussian Noise Augmented Image Size region proposals and many-to-one\nlabel assignments.\n  To demonstrate the effectiveness of proposed method and compare with\nstate-of-the-art methods fairly, we conduct experiments and use IoU-based\nevaluation metrics to evaluate the model performance. The experimental results\nshow that the proposed method can consistently outperform state-of-the-art\nmethods under different IoU-based metric on a variety of datasets. We conduct\nfurther experiments to show the superiority of the proposed decoupled IoU for\nthe TD applications by replacing the IoU-based loss functions and evaluation\nmetrics with proposed decoupled IoU counterparts. The experimental results show\nthat our proposed decoupled IoU loss can encourage the model to alleviate\ninformation loss.",
        "translated": ""
    },
    {
        "title": "Event-Centric Query Expansion in Web Search",
        "url": "http://arxiv.org/abs/2305.19019v1",
        "pub_date": "2023-05-30",
        "summary": "In search engines, query expansion (QE) is a crucial technique to improve\nsearch experience. Previous studies often rely on long-term search log mining,\nwhich leads to slow updates and is sub-optimal for time-sensitive news\nsearches. In this work, we present Event-Centric Query Expansion (EQE), a novel\nQE system that addresses these issues by mining the best expansion from a\nsignificant amount of potential events rapidly and accurately. This system\nconsists of four stages, i.e., event collection, event reformulation, semantic\nretrieval and online ranking. Specifically, we first collect and filter news\nheadlines from websites. Then we propose a generation model that incorporates\ncontrastive learning and prompt-tuning techniques to reformulate these\nheadlines to concise candidates. Additionally, we fine-tune a dual-tower\nsemantic model to function as an encoder for event retrieval and explore a\ntwo-stage contrastive training approach to enhance the accuracy of event\nretrieval. Finally, we rank the retrieved events and select the optimal one as\nQE, which is then used to improve the retrieval of event-related documents.\nThrough offline analysis and online A/B testing, we observe that the EQE system\nsignificantly improves many metrics compared to the baseline. The system has\nbeen deployed in Tencent QQ Browser Search and served hundreds of millions of\nusers. The dataset and baseline codes are available at\nhttps://open-event-hub.github.io/eqe .",
        "translated": ""
    },
    {
        "title": "A Recipe for Efficient SBIR Models: Combining Relative Triplet Loss with\n  Batch Normalization and Knowledge Distillation",
        "url": "http://arxiv.org/abs/2305.18988v1",
        "pub_date": "2023-05-30",
        "summary": "Sketch-Based Image Retrieval (SBIR) is a crucial task in multimedia\nretrieval, where the goal is to retrieve a set of images that match a given\nsketch query. Researchers have already proposed several well-performing\nsolutions for this task, but most focus on enhancing embedding through\ndifferent approaches such as triplet loss, quadruplet loss, adding data\naugmentation, and using edge extraction. In this work, we tackle the problem\nfrom various angles. We start by examining the training data quality and show\nsome of its limitations. Then, we introduce a Relative Triplet Loss (RTL), an\nadapted triplet loss to overcome those limitations through loss weighting based\non anchors similarity. Through a series of experiments, we demonstrate that\nreplacing a triplet loss with RTL outperforms previous state-of-the-art without\nthe need for any data augmentation. In addition, we demonstrate why batch\nnormalization is more suited for SBIR embeddings than l2-normalization and show\nthat it improves significantly the performance of our models. We further\ninvestigate the capacity of models required for the photo and sketch domains\nand demonstrate that the photo encoder requires a higher capacity than the\nsketch encoder, which validates the hypothesis formulated in [34]. Then, we\npropose a straightforward approach to train small models, such as ShuffleNetv2\n[22] efficiently with a marginal loss of accuracy through knowledge\ndistillation. The same approach used with larger models enabled us to\noutperform previous state-of-the-art results and achieve a recall of 62.38% at\nk = 1 on The Sketchy Database [30].",
        "translated": ""
    },
    {
        "title": "The Information Retrieval Experiment Platform",
        "url": "http://arxiv.org/abs/2305.18932v1",
        "pub_date": "2023-05-30",
        "summary": "We integrate ir_datasets, ir_measures, and PyTerrier with TIRA in the\nInformation Retrieval Experiment Platform (TIREx) to promote more standardized,\nreproducible, scalable, and even blinded retrieval experiments. Standardization\nis achieved when a retrieval approach implements PyTerrier's interfaces and the\ninput and output of an experiment are compatible with ir_datasets and\nir_measures. However, none of this is a must for reproducibility and\nscalability, as TIRA can run any dockerized software locally or remotely in a\ncloud-native execution environment. Version control and caching ensure\nefficient (re)execution. TIRA allows for blind evaluation when an experiment\nruns on a remote server or cloud not under the control of the experimenter. The\ntest data and ground truth are then hidden from public access, and the\nretrieval software has to process them in a sandbox that prevents data leaks.\n  We currently host an instance of TIREx with 15 corpora (1.9 billion\ndocuments) on which 32 shared retrieval tasks are based. Using Docker images of\n50 standard retrieval approaches, we automatically evaluated all approaches on\nall tasks (50 $\\cdot$ 32 = 1,600~runs) in less than a week on a midsize cluster\n(1,620 CPU cores and 24 GPUs). This instance of TIREx is open for submissions\nand will be integrated with the IR Anthology, as well as released open source.",
        "translated": ""
    },
    {
        "title": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light\n  Graph Convolution for Effective Multi-Criteria Recommendation",
        "url": "http://arxiv.org/abs/2305.18885v1",
        "pub_date": "2023-05-30",
        "summary": "The multi-criteria (MC) recommender system, which leverages MC rating\ninformation in a wide range of e-commerce areas, is ubiquitous nowadays.\nSurprisingly, although graph neural networks (GNNs) have been widely applied to\ndevelop various recommender systems due to GNN's high expressive capability in\nlearning graph representations, it has been still unexplored how to design MC\nrecommender systems with GNNs. In light of this, we make the first attempt\ntowards designing a GNN-aided MC recommender system. Specifically, rather than\nstraightforwardly adopting existing GNN-based recommendation methods, we devise\na novel criteria preference-aware light graph convolution CPA-LGC method, which\nis capable of precisely capturing the criteria preference of users as well as\nthe collaborative signal in complex high-order connectivities. To this end, we\nfirst construct an MC expansion graph that transforms user--item MC ratings\ninto an expanded bipartite graph to potentially learn from the collaborative\nsignal in MC ratings. Next, to strengthen the capability of criteria preference\nawareness, CPA-LGC incorporates newly characterized embeddings, including\nuser-specific criteria-preference embeddings and item-specific criterion\nembeddings, into our graph convolution model. Through comprehensive evaluations\nusing four real-world datasets, we demonstrate (a) the superiority over\nbenchmark MC recommendation methods and benchmark recommendation methods using\nGNNs with tremendous gains, (b) the effectiveness of core components in\nCPA-LGC, and (c) the computational efficiency.",
        "translated": ""
    },
    {
        "title": "Robust Reinforcement Learning Objectives for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.18820v1",
        "pub_date": "2023-05-30",
        "summary": "Attention-based sequential recommendation methods have demonstrated promising\nresults by accurately capturing users' dynamic interests from historical\ninteractions. In addition to generating superior user representations, recent\nstudies have begun integrating reinforcement learning (RL) into these models.\nFraming sequential recommendation as an RL problem with reward signals, unlocks\ndeveloping recommender systems (RS) that consider a vital aspect-incorporating\ndirect user feedback in the form of rewards to deliver a more personalized\nexperience. Nonetheless, employing RL algorithms presents challenges, including\noff-policy training, expansive combinatorial action spaces, and the scarcity of\ndatasets with sufficient reward signals. Contemporary approaches have attempted\nto combine RL and sequential modeling, incorporating contrastive-based\nobjectives and negative sampling strategies for training the RL component. In\nthis study, we further emphasize the efficacy of contrastive-based objectives\npaired with augmentation to address datasets with extended horizons.\nAdditionally, we recognize the potential instability issues that may arise\nduring the application of negative sampling. These challenges primarily stem\nfrom the data imbalance prevalent in real-world datasets, which is a common\nissue in offline RL contexts. While our established baselines attempt to\nmitigate this through various techniques, instability remains an issue.\nTherefore, we introduce an enhanced methodology aimed at providing a more\neffective solution to these challenges.",
        "translated": ""
    },
    {
        "title": "Who Would be Interested in Services? An Entity Graph Learning System for\n  User Targeting",
        "url": "http://arxiv.org/abs/2305.18780v1",
        "pub_date": "2023-05-30",
        "summary": "With the growing popularity of various mobile devices, user targeting has\nreceived a growing amount of attention, which aims at effectively and\nefficiently locating target users that are interested in specific services.\nMost pioneering works for user targeting tasks commonly perform\nsimilarity-based expansion with a few active users as seeds, suffering from the\nfollowing major issues: the unavailability of seed users for newcoming services\nand the unfriendliness of black-box procedures towards marketers. In this\npaper, we design an Entity Graph Learning (EGL) system to provide explainable\nuser targeting ability meanwhile applicable to addressing the cold-start issue.\nEGL System follows the hybrid online-offline architecture to satisfy the\nrequirements of scalability and timeliness. Specifically, in the offline stage,\nthe system focuses on the heavyweight entity graph construction and user entity\npreference learning, in which we propose a Three-stage Relation Mining\nProcedure (TRMP), breaking loose from the expensive seed users. At the online\nstage, the system offers the ability of user targeting in real-time based on\nthe entity graph from the offline stage. Since the user targeting process is\nbased on graph reasoning, the whole process is transparent and\noperation-friendly to marketers. Finally, extensive offline experiments and\nonline A/B testing demonstrate the superior performance of the proposed EGL\nSystem.",
        "translated": ""
    },
    {
        "title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts",
        "url": "http://arxiv.org/abs/2305.18727v1",
        "pub_date": "2023-05-30",
        "summary": "With a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
        "translated": ""
    },
    {
        "title": "Known by the Company it Keeps: Proximity-Based Indexing for Physical\n  Content in Archival Repositories",
        "url": "http://arxiv.org/abs/2305.18683v1",
        "pub_date": "2023-05-30",
        "summary": "Despite the plethora of born-digital content, vast troves of important\ncontent remain accessible only on physical media such as paper or microfilm.\nThe traditional approach to indexing undigitized content is using manually\ncreated metadata that describes content at some level of aggregation (e.g.,\nfolder, box, or collection). Searchers led in this way to some subset of the\ncontent often must then manually examine substantial quantities of physical\nmedia to find what they are looking for. This paper proposes a complementary\napproach, in which selective digitization of a small portion of the content is\nused as a basis for proximity-based indexing as a way of bringing the user\ncloser to the specific content for which they are looking. Experiments with 35\nboxes of partially digitized US State Department records indicate that\nbox-level indexes built in this way can provide a useful basis for search.",
        "translated": ""
    },
    {
        "title": "Improving Generalization for Multimodal Fake News Detection",
        "url": "http://arxiv.org/abs/2305.18599v1",
        "pub_date": "2023-05-29",
        "summary": "The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for fake news\ndetection. However, state-of-the-art approaches are usually trained on datasets\nof smaller size or with a limited set of specific topics. As a consequence,\nthese models lack generalization capabilities and are not applicable to\nreal-world data. In this paper, we propose three models that adopt and\nfine-tune state-of-the-art multimodal transformers for multimodal fake news\ndetection. We conduct an in-depth analysis by manipulating the input data aimed\nto explore models performance in realistic use cases on social media. Our study\nacross multiple models demonstrates that these systems suffer significant\nperformance drops against manipulated data. To reduce the bias and improve\nmodel generalization, we suggest training data augmentation to conduct more\nmeaningful experiments for fake news detection on social media. The proposed\ndata augmentation techniques enable models to generalize better and yield\nimproved state-of-the-art results.",
        "translated": ""
    },
    {
        "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on\n  Structured Data",
        "url": "http://arxiv.org/abs/2305.19912v1",
        "pub_date": "2023-05-31",
        "summary": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which\nencodes user queries and structured data in one universal embedding space for\nretrieving structured data. SANTA proposes two pretraining methods to make\nlanguage models structure-aware and learn effective representations for\nstructured data: 1) Structured Data Alignment, which utilizes the natural\nalignment relations between structured data and unstructured data for\nstructure-aware pretraining. It contrastively trains language models to\nrepresent multi-modal text data and teaches models to distinguish matched\nstructured data for unstructured texts. 2) Masked Entity Prediction, which\ndesigns an entity-oriented mask strategy and asks language models to fill in\nthe masked entities. Our experiments show that SANTA achieves state-of-the-art\non code search and product search and conducts convincing results in the\nzero-shot setting. SANTA learns tailored representations for multi-modal text\ndata by aligning structured and unstructured data pairs and capturing\nstructural semantics by masking and predicting entities in the structured data.\nAll codes are available at https://github.com/OpenMatch/OpenMatch.",
        "translated": ""
    },
    {
        "title": "Web scraping: a promising tool for geographic data acquisition",
        "url": "http://arxiv.org/abs/2305.19893v1",
        "pub_date": "2023-05-31",
        "summary": "With much of our lives taking place online, researchers are increasingly\nturning to information from the World Wide Web to gain insights into geographic\npatterns and processes. Web scraping as an online data acquisition technique\nallows us to gather intelligence especially on social and economic actions for\nwhich the Web serves as a platform. Specific opportunities relate to\nnear-real-time access to object-level geolocated data, which can be captured in\na cost-effective way. The studied geographic phenomena include, but are not\nlimited to, the rental market and associated processes such as gentrification,\nentrepreneurial ecosystems, or spatial planning processes. Since the\ninformation retrieved from the Web is not made available for that purpose, Web\nscraping faces several unique challenges, several of which relate to location.\nEthical and legal issues mainly relate to intellectual property rights,\ninformed consent and (geo-) privacy, and website integrity and contract. These\nissues also effect the practice of open science. In addition, there are\ntechnical and statistical challenges that relate to dependability and\nincompleteness, data inconsistencies and bias, as well as the limited\nhistorical coverage. Geospatial analyses furthermore usually require the\nautomated extraction and subsequent resolution of toponyms or addresses\n(geoparsing, geocoding). A study on apartment rent in Leipzig, Germany is used\nto illustrate the use of Web scraping and its challenges. We conclude that\ngeographic researchers should embrace Web scraping as a powerful and affordable\ndigital fieldwork tool while paying special attention to its legal, ethical,\nand methodological challenges.",
        "translated": ""
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.19860v1",
        "pub_date": "2023-05-31",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in the field of\nNatural Language Processing (NLP) and have recently gained significant\nattention in the domain of Recommendation Systems (RS). These models, trained\non massive amounts of data using self-supervised learning, have demonstrated\nremarkable success in learning universal representations and have the potential\nto enhance various aspects of recommendation systems by some effective transfer\ntechniques such as fine-tuning and prompt tuning, and so on. The crucial aspect\nof harnessing the power of language models in enhancing recommendation quality\nis the utilization of their high-quality representations of textual features\nand their extensive coverage of external knowledge to establish correlations\nbetween items and users. To provide a comprehensive understanding of the\nexisting LLM-based recommendation systems, this survey presents a taxonomy that\ncategorizes these models into two major paradigms, respectively Discriminative\nLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation\n(GLLM4Rec), with the latter being systematically sorted out for the first time.\nFurthermore, we systematically review and analyze existing LLM-based\nrecommendation systems within each paradigm, providing insights into their\nmethodologies, techniques, and performance. Additionally, we identify key\nchallenges and several valuable findings to provide researchers and\npractitioners with inspiration.",
        "translated": ""
    },
    {
        "title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish\n  Language",
        "url": "http://arxiv.org/abs/2305.19840v1",
        "pub_date": "2023-05-31",
        "summary": "The BEIR dataset is a large, heterogeneous benchmark for Information\nRetrieval (IR) in zero-shot settings, garnering considerable attention within\nthe research community. However, BEIR and analogous datasets are predominantly\nrestricted to the English language. Our objective is to establish extensive\nlarge-scale resources for IR in the Polish language, thereby advancing the\nresearch in this NLP area. In this work, inspired by mMARCO and Mr.~TyDi\ndatasets, we translated all accessible open IR datasets into Polish, and we\nintroduced the BEIR-PL benchmark -- a new benchmark which comprises 13\ndatasets, facilitating further development, training and evaluation of modern\nPolish language models for IR tasks. We executed an evaluation and comparison\nof numerous IR models on the newly introduced BEIR-PL benchmark. Furthermore,\nwe publish pre-trained open IR models for Polish language,d marking a\npioneering development in this field. Additionally, the evaluation revealed\nthat BM25 achieved significantly lower scores for Polish than for English,\nwhich can be attributed to high inflection and intricate morphological\nstructure of the Polish language. Finally, we trained various re-ranking models\nto enhance the BM25 retrieval, and we compared their performance to identify\ntheir unique characteristic features. To ensure accurate model comparisons, it\nis necessary to scrutinise individual results rather than to average across the\nentire benchmark. Thus, we thoroughly analysed the outcomes of IR models in\nrelation to each individual data subset encompassed by the BEIR benchmark. The\nbenchmark data is available at URL {\\bf https://huggingface.co/clarin-knext}.",
        "translated": ""
    },
    {
        "title": "Medication Recommendation via Domain Knowledge Informed Deep Learning",
        "url": "http://arxiv.org/abs/2305.19604v1",
        "pub_date": "2023-05-31",
        "summary": "Medication recommendation is a fundamental yet crucial branch of healthcare,\nwhich provides opportunities to support clinical physicians with more accurate\nmedication prescriptions for patients with complex health conditions. Learning\nfrom electronic health records (EHR) to recommend medications is the most\ncommon way in previous studies. However, most of them neglect incorporating\ndomain knowledge according to the clinical manifestations in the EHR of the\npatient. To address these issues, we propose a novel \\textbf{D}omain\n\\textbf{K}nowledge \\textbf{I}nformed \\textbf{Net}work (DKINet) to integrate\ndomain knowledge with observable clinical manifestations of the patient, which\nis the first dynamic domain knowledge informed framework toward medication\nrecommendation. In particular, we first design a knowledge-driven encoder to\ncapture the domain information and then develop a data-driven encoder to\nintegrate domain knowledge into the observable EHR. To endow the model with the\ncapability of temporal decision, we design an explicit medication encoder for\nlearning the longitudinal dependence of the patient. Extensive experiments on\nthree publicly available datasets verify the superiority of our method. The\ncode will be public upon acceptance.",
        "translated": ""
    },
    {
        "title": "Towards Semi-supervised Universal Graph Classification",
        "url": "http://arxiv.org/abs/2305.19598v1",
        "pub_date": "2023-05-31",
        "summary": "Graph neural networks have pushed state-of-the-arts in graph classifications\nrecently. Typically, these methods are studied within the context of supervised\nend-to-end training, which necessities copious task-specific labels. However,\nin real-world circumstances, labeled data could be limited, and there could be\na massive corpus of unlabeled data, even from unknown classes as a\ncomplementary. Towards this end, we study the problem of semi-supervised\nuniversal graph classification, which not only identifies graph samples which\ndo not belong to known classes, but also classifies the remaining samples into\ntheir respective classes. This problem is challenging due to a severe lack of\nlabels and potential class shifts. In this paper, we propose a novel graph\nneural network framework named UGNN, which makes the best of unlabeled data\nfrom the subgraph perspective. To tackle class shifts, we estimate the\ncertainty of unlabeled graphs using multiple subgraphs, which facilities the\ndiscovery of unlabeled data from unknown categories. Moreover, we construct\nsemantic prototypes in the embedding space for both known and unknown\ncategories and utilize posterior prototype assignments inferred from the\nSinkhorn-Knopp algorithm to learn from abundant unlabeled graphs across\ndifferent subgraph views. Extensive experiments on six datasets verify the\neffectiveness of UGNN in different settings.",
        "translated": ""
    },
    {
        "title": "Multi-Epoch Learning for Deep Click-Through Rate Prediction Models",
        "url": "http://arxiv.org/abs/2305.19531v1",
        "pub_date": "2023-05-31",
        "summary": "The one-epoch overfitting phenomenon has been widely observed in industrial\nClick-Through Rate (CTR) applications, where the model performance experiences\na significant degradation at the beginning of the second epoch. Recent advances\ntry to understand the underlying factors behind this phenomenon through\nextensive experiments. However, it is still unknown whether a multi-epoch\ntraining paradigm could achieve better results, as the best performance is\nusually achieved by one-epoch training. In this paper, we hypothesize that the\nemergence of this phenomenon may be attributed to the susceptibility of the\nembedding layer to overfitting, which can stem from the high-dimensional\nsparsity of data. To maintain feature sparsity while simultaneously avoiding\noverfitting of embeddings, we propose a novel Multi-Epoch learning with Data\nAugmentation (MEDA), which can be directly applied to most deep CTR models.\nMEDA achieves data augmentation by reinitializing the embedding layer in each\nepoch, thereby avoiding embedding overfitting and simultaneously improving\nconvergence. To our best knowledge, MEDA is the first multi-epoch training\nparadigm designed for deep CTR prediction models. We conduct extensive\nexperiments on several public datasets, and the effectiveness of our proposed\nMEDA is fully verified. Notably, the results show that MEDA can significantly\noutperform the conventional one-epoch training. Besides, MEDA has exhibited\nsignificant benefits in a real-world scene on Kuaishou.",
        "translated": ""
    },
    {
        "title": "AdANNS: A Framework for Adaptive Semantic Search",
        "url": "http://arxiv.org/abs/2305.19435v1",
        "pub_date": "2023-05-30",
        "summary": "Web-scale search systems learn an encoder to embed a given query which is\nthen hooked into an approximate nearest neighbor search (ANNS) pipeline to\nretrieve similar data points. To accurately capture tail queries and data\npoints, learned representations typically are rigid, high-dimensional vectors\nthat are generally used as-is in the entire ANNS pipeline and can lead to\ncomputationally expensive retrieval. In this paper, we argue that instead of\nrigid representations, different stages of ANNS can leverage adaptive\nrepresentations of varying capacities to achieve significantly better\naccuracy-compute trade-offs, i.e., stages of ANNS that can get away with more\napproximate computation should use a lower-capacity representation of the same\ndata point. To this end, we introduce AdANNS, a novel ANNS design framework\nthat explicitly leverages the flexibility of Matryoshka Representations. We\ndemonstrate state-of-the-art accuracy-compute trade-offs using novel\nAdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF)\nand quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is\nup to 1.5% more accurate than the rigid representations-based IVF at the same\ncompute budget; and matches accuracy while being up to 90x faster in wall-clock\ntime. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the\n64-byte OPQ baseline constructed using rigid representations -- same accuracy\nat half the cost! We further show that the gains from AdANNS translate to\nmodern-day composite ANNS indices that combine search structures and\nquantization. Finally, we demonstrate that AdANNS can enable inference-time\nadaptivity for compute-aware search on ANNS indices built non-adaptively on\nmatryoshka representations. Code is open-sourced at\nhttps://github.com/RAIVNLab/AdANNS.",
        "translated": ""
    },
    {
        "title": "DuoSearch: A Novel Search Engine for Bulgarian Historical Documents",
        "url": "http://arxiv.org/abs/2305.19392v1",
        "pub_date": "2023-05-30",
        "summary": "Search in collections of digitised historical documents is hindered by a\ntwo-prong problem, orthographic variety and optical character recognition (OCR)\nmistakes. We present a new search engine for historical documents, DuoSearch,\nwhich uses ElasticSearch and machine learning methods based on deep neural\nnetworks to offer a solution to this problem. It was tested on a collection of\nhistorical newspapers in Bulgarian from the mid-19th to the mid-20th century.\nThe system provides an interactive and intuitive interface for the end-users\nallowing them to enter search terms in modern Bulgarian and search across\nhistorical spellings. This is the first solution facilitating the use of\ndigitised historical documents in Bulgarian.",
        "translated": ""
    },
    {
        "title": "AMR4NLI: Interpretable and robust NLI measures from semantic graphs",
        "url": "http://arxiv.org/abs/2306.00936v1",
        "pub_date": "2023-06-01",
        "summary": "The task of natural language inference (NLI) asks whether a given premise\n(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human\nratings of entailment, but the meaning relationships driving these ratings are\nnot formalized. Can the underlying sentence pair relationships be made more\nexplicit in an interpretable yet robust fashion? We compare semantic structures\nto represent premise and hypothesis, including sets of contextualized\nembeddings and semantic graphs (Abstract Meaning Representations), and measure\nwhether the hypothesis is a semantic substructure of the premise, utilizing\ninterpretable metrics. Our evaluation on three English benchmarks finds value\nin both contextualized embeddings and semantic graphs; moreover, they provide\ncomplementary signals, and can be leveraged together in a hybrid model.",
        "translated": ""
    },
    {
        "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach\n  for Low-Resource Complex NER",
        "url": "http://arxiv.org/abs/2306.00928v1",
        "pub_date": "2023-06-01",
        "summary": "Complex Named Entity Recognition (NER) is the task of detecting\nlinguistically complex named entities in low-context text. In this paper, we\npresent ACLM Attention-map aware keyword selection for Conditional Language\nModel fine-tuning), a novel data augmentation approach based on conditional\ngeneration to address the data scarcity problem in low-resource complex NER.\nACLM alleviates the context-entity mismatch issue, a problem existing NER data\naugmentation techniques suffer from and often generates incoherent\naugmentations by placing complex named entities in the wrong context. ACLM\nbuilds on BART and is optimized on a novel text reconstruction or denoising\ntask - we use selective masking (aided by attention maps) to retain the named\nentities and certain keywords in the input sentence that provide contextually\nrelevant additional knowledge or hints about the named entities. Compared with\nother data augmentation strategies, ACLM can generate more diverse and coherent\naugmentations preserving the true word sense of complex entities in the\nsentence. We demonstrate the effectiveness of ACLM both qualitatively and\nquantitatively on monolingual, cross-lingual, and multilingual complex NER\nacross various low-resource settings. ACLM outperforms all our neural baselines\nby a significant margin (1%-36%). In addition, we demonstrate the application\nof ACLM to other domains that suffer from data scarcity (e.g., biomedical). In\npractice, ACLM generates more effective and factual augmentations for these\ndomains than prior methods. Code: https://github.com/Sreyan88/ACLM",
        "translated": ""
    },
    {
        "title": "SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in\n  Graph Neural Networks",
        "url": "http://arxiv.org/abs/2306.00899v1",
        "pub_date": "2023-06-01",
        "summary": "Graph Neural Networks (GNNs) have demonstrated promising outcomes across\nvarious tasks, including node classification and link prediction. Despite their\nremarkable success in various high-impact applications, we have identified\nthree common pitfalls in message passing for link prediction. Particularly, in\nprevalent GNN frameworks (e.g., DGL and PyTorch-Geometric), the target edges\n(i.e., the edges being predicted) consistently exist as message passing edges\nin the graph during training. Consequently, this results in overfitting and\ndistribution shift, both of which adversely impact the generalizability to test\nthe target edges. Additionally, during test time, the failure to exclude the\ntest target edges leads to implicit test leakage caused by neighborhood\naggregation. In this paper, we analyze these three pitfalls and investigate the\nimpact of including or excluding target edges on the performance of nodes with\nvarying degrees during training and test phases. Our theoretical and empirical\nanalysis demonstrates that low-degree nodes are more susceptible to these\npitfalls. These pitfalls can have detrimental consequences when GNNs are\nimplemented in production systems. To systematically address these pitfalls, we\npropose SpotTarget, an effective and efficient GNN training framework. During\ntraining, SpotTarget leverages our insight regarding low-degree nodes and\nexcludes train target edges connected to at least one low-degree node. During\ntest time, it emulates real-world scenarios of GNN usage in production and\nexcludes all test target edges. Our experiments conducted on diverse real-world\ndatasets, demonstrate that SpotTarget significantly enhances GNNs, achieving up\nto a 15x increase in accuracy in sparse graphs. Furthermore, SpotTarget\nconsistently and dramatically improves the performance for low-degree nodes in\ndense graphs.",
        "translated": ""
    },
    {
        "title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection",
        "url": "http://arxiv.org/abs/2306.00765v1",
        "pub_date": "2023-06-01",
        "summary": "Stance Detection is concerned with identifying the attitudes expressed by an\nauthor towards a target of interest. This task spans a variety of domains\nranging from social media opinion identification to detecting the stance for a\nlegal claim. However, the framing of the task varies within these domains, in\nterms of the data collection protocol, the label dictionary and the number of\navailable annotations. Furthermore, these stance annotations are significantly\nimbalanced on a per-topic and inter-topic basis. These make multi-domain stance\ndetection a challenging task, requiring standardization and domain adaptation.\nTo overcome this challenge, we propose $\\textbf{T}$opic $\\textbf{E}$fficient\n$\\textbf{St}$anc$\\textbf{E}$ $\\textbf{D}$etection (TESTED), consisting of a\ntopic-guided diversity sampling technique and a contrastive objective that is\nused for fine-tuning a stance classifier. We evaluate the method on an existing\nbenchmark of $16$ datasets with in-domain, i.e. all topics seen and\nout-of-domain, i.e. unseen topics, experiments. The results show that our\nmethod outperforms the state-of-the-art with an average of $3.5$ F1 points\nincrease in-domain, and is more generalizable with an averaged increase of\n$10.2$ F1 on out-of-domain evaluation while using $\\leq10\\%$ of the training\ndata. We show that our sampling technique mitigates both inter- and per-topic\nclass imbalances. Finally, our analysis demonstrates that the contrastive\nlearning objective allows the model a more pronounced segmentation of samples\nwith varying labels.",
        "translated": ""
    },
    {
        "title": "End-to-End Document Classification and Key Information Extraction using\n  Assignment Optimization",
        "url": "http://arxiv.org/abs/2306.00750v1",
        "pub_date": "2023-06-01",
        "summary": "We propose end-to-end document classification and key information extraction\n(KIE) for automating document processing in forms. Through accurate document\nclassification we harness known information from templates to enhance KIE from\nforms. We use text and layout encoding with a cosine similarity measure to\nclassify visually-similar documents. We then demonstrate a novel application of\nmixed integer programming by using assignment optimization to extract key\ninformation from documents. Our approach is validated on an in-house dataset of\nnoisy scanned forms. The best performing document classification approach\nachieved 0.97 f1 score. A mean f1 score of 0.94 for the KIE task suggests there\nis significant potential in applying optimization techniques. Abation results\nshow that the method relies on document preprocessing techniques to mitigate\nType II errors and achieve optimal performance.",
        "translated": ""
    },
    {
        "title": "Class Anchor Margin Loss for Content-Based Image Retrieval",
        "url": "http://arxiv.org/abs/2306.00630v1",
        "pub_date": "2023-06-01",
        "summary": "The performance of neural networks in content-based image retrieval (CBIR) is\nhighly influenced by the chosen loss (objective) function. The majority of\nobjective functions for neural models can be divided into metric learning and\nstatistical learning. Metric learning approaches require a pair mining strategy\nthat often lacks efficiency, while statistical learning approaches are not\ngenerating highly compact features due to their indirect feature optimization.\nTo this end, we propose a novel repeller-attractor loss that falls in the\nmetric learning paradigm, yet directly optimizes for the L2 metric without the\nneed of generating pairs. Our loss is formed of three components. One leading\nobjective ensures that the learned features are attracted to each designated\nlearnable class anchor. The second loss component regulates the anchors and\nforces them to be separable by a margin, while the third objective ensures that\nthe anchors do not collapse to zero. Furthermore, we develop a more efficient\ntwo-stage retrieval system by harnessing the learned class anchors during the\nfirst stage of the retrieval process, eliminating the need of comparing the\nquery with every image in the database. We establish a set of four datasets\n(CIFAR-100, Food-101, SVHN, and Tiny ImageNet) and evaluate the proposed\nobjective in the context of few-shot and full-set training on the CBIR task, by\nusing both convolutional and transformer architectures. Compared to existing\nobjective functions, our empirical evidence shows that the proposed objective\nis generating superior and more consistent results.",
        "translated": ""
    },
    {
        "title": "End-to-end Knowledge Retrieval with Multi-modal Queries",
        "url": "http://arxiv.org/abs/2306.00424v1",
        "pub_date": "2023-06-01",
        "summary": "We investigate knowledge retrieval with multi-modal queries, i.e. queries\ncontaining information split across image and text inputs, a challenging task\nthat differs from previous work on cross-modal retrieval. We curate a new\ndataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a\nsystem to retrieve knowledge from a large corpus by integrating contents from\nboth text and image queries. We introduce a retriever model ``ReViz'' that can\ndirectly process input text and images to retrieve relevant knowledge in an\nend-to-end fashion without being dependent on intermediate modules such as\nobject detectors or caption generators. We introduce a new pretraining task\nthat is effective for learning knowledge retrieval with multimodal queries and\nalso improves performance on downstream tasks. We demonstrate superior\nperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot\nsettings as well as further improvements when finetuned on these datasets.",
        "translated": ""
    },
    {
        "title": "A Survey on Fairness-aware Recommender Systems",
        "url": "http://arxiv.org/abs/2306.00403v1",
        "pub_date": "2023-06-01",
        "summary": "As information filtering services, recommender systems have extremely\nenriched our daily life by providing personalized suggestions and facilitating\npeople in decision-making, which makes them vital and indispensable to human\nsociety in the information era. However, as people become more dependent on\nthem, recent studies show that recommender systems potentially own\nunintentional impacts on society and individuals because of their unfairness\n(e.g., gender discrimination in job recommendations). To develop trustworthy\nservices, it is crucial to devise fairness-aware recommender systems that can\nmitigate these bias issues. In this survey, we summarise existing methodologies\nand practices of fairness in recommender systems. Firstly, we present concepts\nof fairness in different recommendation scenarios, comprehensively categorize\ncurrent advances, and introduce typical methods to promote fairness in\ndifferent stages of recommender systems. Next, after introducing datasets and\nevaluation metrics applied to assess the fairness of recommender systems, we\nwill delve into the significant influence that fairness-aware recommender\nsystems exert on real-world industrial applications. Subsequently, we highlight\nthe connection between fairness and other principles of trustworthy recommender\nsystems, aiming to consider trustworthiness principles holistically while\nadvocating for fairness. Finally, we summarize this review, spotlighting\npromising opportunities in comprehending concepts, frameworks, the balance\nbetween accuracy and fairness, and the ties with trustworthiness, with the\nultimate goal of fostering the development of fairness-aware recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "Explicit Feature Interaction-aware Uplift Network for Online Marketing",
        "url": "http://arxiv.org/abs/2306.00315v1",
        "pub_date": "2023-06-01",
        "summary": "As a key component in online marketing, uplift modeling aims to accurately\ncapture the degree to which different treatments motivate different users, such\nas coupons or discounts, also known as the estimation of individual treatment\neffect (ITE). In an actual business scenario, the options for treatment may be\nnumerous and complex, and there may be correlations between different\ntreatments. In addition, each marketing instance may also have rich user and\ncontextual features. However, existing methods still fall short in both fully\nexploiting treatment information and mining features that are sensitive to a\nparticular treatment. In this paper, we propose an explicit feature\ninteraction-aware uplift network (EFIN) to address these two problems. Our EFIN\nincludes four customized modules: 1) a feature encoding module encodes not only\nthe user and contextual features, but also the treatment features; 2) a\nself-interaction module aims to accurately model the user's natural response\nwith all but the treatment features; 3) a treatment-aware interaction module\naccurately models the degree to which a particular treatment motivates a user\nthrough interactions between the treatment features and other features, i.e.,\nITE; and 4) an intervention constraint module is used to balance the ITE\ndistribution of users between the control and treatment groups so that the\nmodel would still achieve a accurate uplift ranking on data collected from a\nnon-random intervention marketing scenario. We conduct extensive experiments on\ntwo public datasets and one product dataset to verify the effectiveness of our\nEFIN. In addition, our EFIN has been deployed in a credit card bill payment\nscenario of a large online financial platform with a significant improvement.",
        "translated": ""
    },
    {
        "title": "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest",
        "url": "http://arxiv.org/abs/2306.00248v1",
        "pub_date": "2023-05-31",
        "summary": "Sequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
        "translated": ""
    },
    {
        "title": "Fresh Content Needs More Attention: Multi-funnel Fresh Content\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.01720v1",
        "pub_date": "2023-06-02",
        "summary": "Recommendation system serves as a conduit connecting users to an incredibly\nlarge, diverse and ever growing collection of contents. In practice, missing\ninformation on fresh (and tail) contents needs to be filled in order for them\nto be exposed and discovered by their audience. We here share our success\nstories in building a dedicated fresh content recommendation stack on a large\ncommercial platform. To nominate fresh contents, we built a multi-funnel\nnomination system that combines (i) a two-tower model with strong\ngeneralization power for coverage, and (ii) a sequence model with near\nreal-time update on user feedback for relevance. The multi-funnel setup\neffectively balances between coverage and relevance. An in-depth study uncovers\nthe relationship between user activity level and their proximity toward fresh\ncontents, which further motivates a contextual multi-funnel setup. Nominated\nfresh candidates are then scored and ranked by systems considering prediction\nuncertainty to further bootstrap content with less exposure. We evaluate the\nbenefits of the dedicated fresh content recommendation stack, and the\nmulti-funnel nomination system in particular, through user corpus co-diverted\nlive experiments. We conduct multiple rounds of live experiments on a\ncommercial platform serving billion of users demonstrating efficacy of our\nproposed methods.",
        "translated": ""
    },
    {
        "title": "Pretrained Language Model based Web Search Ranking: From Relevance to\n  Satisfaction",
        "url": "http://arxiv.org/abs/2306.01599v1",
        "pub_date": "2023-06-02",
        "summary": "Search engine plays a crucial role in satisfying users' diverse information\nneeds. Recently, Pretrained Language Models (PLMs) based text ranking models\nhave achieved huge success in web search. However, many state-of-the-art text\nranking approaches only focus on core relevance while ignoring other dimensions\nthat contribute to user satisfaction, e.g., document quality, recency,\nauthority, etc. In this work, we focus on ranking user satisfaction rather than\nrelevance in web search, and propose a PLM-based framework, namely SAT-Ranker,\nwhich comprehensively models different dimensions of user satisfaction in a\nunified manner. In particular, we leverage the capacities of PLMs on both\ntextual and numerical inputs, and apply a multi-field input that modularizes\neach dimension of user satisfaction as an input field. Overall, SAT-Ranker is\nan effective, extensible, and data-centric framework that has huge potential\nfor industrial applications. On rigorous offline and online experiments,\nSAT-Ranker obtains remarkable gains on various evaluation sets targeting\ndifferent dimensions of user satisfaction. It is now fully deployed online to\nimprove the usability of our search engine.",
        "translated": ""
    },
    {
        "title": "Influence Maximization with Fairness at Scale (Extended Version)",
        "url": "http://arxiv.org/abs/2306.01587v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we revisit the problem of influence maximization with\nfairness, which aims to select k influential nodes to maximise the spread of\ninformation in a network, while ensuring that selected sensitive user\nattributes are fairly affected, i.e., are proportionally similar between the\noriginal network and the affected users. Recent studies on this problem focused\nonly on extremely small networks, hence the challenge remains on how to achieve\na scalable solution, applicable to networks with millions or billions of nodes.\nWe propose an approach that is based on learning node representations for fair\nspread from diffusion cascades, instead of the social connectivity s.t. we can\ndeal with very large graphs. We propose two data-driven approaches: (a)\nfairness-based participant sampling (FPS), and (b) fairness as context (FAC).\nSpread related user features, such as the probability of diffusing information\nto others, are derived from the historical information cascades, using a deep\nneural network. The extracted features are then used in selecting influencers\nthat maximize the influence spread, while being also fair with respect to the\nchosen sensitive attributes. In FPS, fairness and cascade length information\nare considered independently in the decision-making process, while FAC\nconsiders these information facets jointly and considers correlations between\nthem. The proposed algorithms are generic and represent the first policy-driven\nsolutions that can be applied to arbitrary sets of sensitive attributes at\nscale. We evaluate the performance of our solutions on a real-world public\ndataset (Sina Weibo) and on a hybrid real-synthethic dataset (Digg), which\nexhibit all the facets that we exploit, namely diffusion network, diffusion\ntraces, and user profiles. These experiments show that our methods outperform\nthe state-the-art solutions in terms of spread, fairness, and scalability.",
        "translated": ""
    },
    {
        "title": "Système de recommandations basé sur les contraintes pour les\n  simulations de gestion de crise",
        "url": "http://arxiv.org/abs/2306.01504v1",
        "pub_date": "2023-06-02",
        "summary": "In the context of the evacuation of populations, some citizens/volunteers may\nwant and be able to participate in the evacuation of populations in difficulty\nby coming to lend a hand to emergency/evacuation vehicles with their own\nvehicles. One way of framing these impulses of solidarity would be to be able\nto list in real-time the citizens/volunteers available with their vehicles\n(land, sea, air, etc.), to be able to geolocate them according to the risk\nareas to be evacuated, and adding them to the evacuation/rescue vehicles.\nBecause it is difficult to propose an effective real-time operational system on\nthe field in a real crisis situation, in this work, we propose to add a module\nfor recommending driver/vehicle pairs (with their specificities) to a system of\ncrisis management simulation. To do that, we chose to model and develop an\nontology-supported constraint-based recommender system for crisis management\nsimulations.",
        "translated": ""
    },
    {
        "title": "Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking\n  Intent in Recommender Systems",
        "url": "http://arxiv.org/abs/2306.01476v1",
        "pub_date": "2023-06-02",
        "summary": "Recommending novel content, which expands user horizons by introducing them\nto new interests, has been shown to improve users' long-term experience on\nrecommendation platforms \\cite{chen2021values}. Users however are not\nconstantly looking to explore novel content. It is therefore crucial to\nunderstand their novelty-seeking intent and adjust the recommendation policy\naccordingly. Most existing literature models a user's propensity to choose\nnovel content or to prefer a more diverse set of recommendations at individual\ninteractions. Hierarchical structure, on the other hand, exists in a user's\nnovelty-seeking intent, which is manifested as a static and intrinsic user\npreference for seeking novelty along with a dynamic session-based propensity.\nTo this end, we propose a novel hierarchical reinforcement learning-based\nmethod to model the hierarchical user novelty-seeking intent, and to adapt the\nrecommendation policy accordingly based on the extracted user novelty-seeking\npropensity. We further incorporate diversity and novelty-related measurement in\nthe reward function of the hierarchical RL (HRL) agent to encourage user\nexploration \\cite{chen2021values}. We demonstrate the benefits of explicitly\nmodeling hierarchical user novelty-seeking intent in recommendations through\nextensive experiments on simulated and real-world datasets. In particular, we\ndemonstrate that the effectiveness of our proposed hierarchical RL-based method\nlies in its ability to capture such hierarchically-structured intent. As a\nresult, the proposed HRL model achieves superior performance on several public\ndatasets, compared with state-of-art baselines.",
        "translated": ""
    },
    {
        "title": "Prompt Tuning Large Language Models on Personalized Aspect Extraction\n  for Recommendations",
        "url": "http://arxiv.org/abs/2306.01475v1",
        "pub_date": "2023-06-02",
        "summary": "Existing aspect extraction methods mostly rely on explicit or ground truth\naspect information, or using data mining or machine learning approaches to\nextract aspects from implicit user feedback such as user reviews. It however\nremains under-explored how the extracted aspects can help generate more\nmeaningful recommendations to the users. Meanwhile, existing research on\naspect-based recommendations often relies on separate aspect extraction models\nor assumes the aspects are given, without accounting for the fact the optimal\nset of aspects could be dependent on the recommendation task at hand.\n  In this work, we propose to combine aspect extraction together with\naspect-based recommendations in an end-to-end manner, achieving the two goals\ntogether in a single framework. For the aspect extraction component, we\nleverage the recent advances in large language models and design a new prompt\nlearning mechanism to generate aspects for the end recommendation task. For the\naspect-based recommendation component, the extracted aspects are concatenated\nwith the usual user and item features used by the recommendation model. The\nrecommendation task mediates the learning of the user embeddings and item\nembeddings, which are used as soft prompts to generate aspects. Therefore, the\nextracted aspects are personalized and contextualized by the recommendation\ntask. We showcase the effectiveness of our proposed method through extensive\nexperiments on three industrial datasets, where our proposed framework\nsignificantly outperforms state-of-the-art baselines in both the personalized\naspect extraction and aspect-based recommendation tasks. In particular, we\ndemonstrate that it is necessary and beneficial to combine the learning of\naspect extraction and aspect-based recommendation together. We also conduct\nextensive ablation studies to understand the contribution of each design\ncomponent in our framework.",
        "translated": ""
    },
    {
        "title": "An OPC UA-based industrial Big Data architecture",
        "url": "http://arxiv.org/abs/2306.01418v1",
        "pub_date": "2023-06-02",
        "summary": "Industry 4.0 factories are complex and data-driven. Data is yielded from many\nsources, including sensors, PLCs, and other devices, but also from IT, like ERP\nor CRM systems. We ask how to collect and process this data in a way, such that\nit includes metadata and can be used for industrial analytics or to derive\nintelligent support systems. This paper describes a new, query model based\napproach, which uses a big data architecture to capture data from various\nsources using OPC UA as a foundation. It buffers and preprocesses the\ninformation for the purpose of harmonizing and providing a holistic state space\nof a factory, as well as mappings to the current state of a production site.\nThat information can be made available to multiple processing sinks, decoupled\nfrom the data sources, which enables them to work with the information without\ninterfering with devices of the production, disturbing the network devices they\nare working in, or influencing the production process negatively. Metadata and\nconnected semantic information is kept throughout the process, allowing to feed\nalgorithms with meaningful data, so that it can be accessed in its entirety to\nperform time series analysis, machine learning or similar evaluations as well\nas replaying the data from the buffer for repeatable simulations.",
        "translated": ""
    },
    {
        "title": "DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG\n  2000 Compressed Documents",
        "url": "http://arxiv.org/abs/2306.01359v1",
        "pub_date": "2023-06-02",
        "summary": "For any digital application with document images such as retrieval, the\nclassification of document images becomes an essential stage. Conventionally\nfor the purpose, the full versions of the documents, that is the uncompressed\ndocument images make the input dataset, which poses a threat due to the big\nvolume required to accommodate the full versions of the documents. Therefore,\nit would be novel, if the same classification task could be accomplished\ndirectly (with some partial decompression) with the compressed representation\nof documents in order to make the whole process computationally more efficient.\nIn this research work, a novel deep learning model, DWT CompCNN is proposed for\nclassification of documents that are compressed using High Throughput JPEG 2000\n(HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional\nlayers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each\nincreasing layer to improve learning from the wavelet coefficients extracted\nfrom the compressed images. Experiments are performed on two benchmark\ndatasets- Tobacco-3482 and RVL-CDIP, which demonstrate that the proposed model\nis time and space efficient, and also achieves a better classification accuracy\nin compressed domain.",
        "translated": ""
    },
    {
        "title": "Reducing Popularity Bias in Recommender Systems through AUC-Optimal\n  Negative Sampling",
        "url": "http://arxiv.org/abs/2306.01348v1",
        "pub_date": "2023-06-02",
        "summary": "Popularity bias is a persistent issue associated with recommendation systems,\nposing challenges to both fairness and efficiency. Existing literature widely\nacknowledges that reducing popularity bias often requires sacrificing\nrecommendation accuracy. In this paper, we challenge this commonly held belief.\nOur analysis under general bias-variance decomposition framework shows that\nreducing bias can actually lead to improved model performance under certain\nconditions. To achieve this win-win situation, we propose to intervene in model\ntraining through negative sampling thereby modifying model predictions.\nSpecifically, we provide an optimal negative sampling rule that maximizes\npartial AUC to preserve the accuracy of any given model, while correcting\nsample information and prior information to reduce popularity bias in a\nflexible and principled way. Our experimental results on real-world datasets\ndemonstrate the superiority of our approach in improving recommendation\nperformance and reducing popularity bias.",
        "translated": ""
    },
    {
        "title": "LyricSIM: A novel Dataset and Benchmark for Similarity Detection in\n  Spanish Song LyricS",
        "url": "http://arxiv.org/abs/2306.01325v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we present a new dataset and benchmark tailored to the task of\nsemantic similarity in song lyrics. Our dataset, originally consisting of 2775\npairs of Spanish songs, was annotated in a collective annotation experiment by\n63 native annotators. After collecting and refining the data to ensure a high\ndegree of consensus and data integrity, we obtained 676 high-quality annotated\npairs that were used to evaluate the performance of various state-of-the-art\nmonolingual and multilingual language models. Consequently, we established\nbaseline results that we hope will be useful to the community in all future\nacademic and industrial applications conducted in this context.",
        "translated": ""
    },
    {
        "title": "Learning Similarity among Users for Personalized Session-Based\n  Recommendation from hierarchical structure of User-Session-Item",
        "url": "http://arxiv.org/abs/2306.03040v1",
        "pub_date": "2023-06-05",
        "summary": "The task of the session-based recommendation is to predict the next\ninteraction of the user based on the anonymized user's behavior pattern. And\npersonalized version of this system is a promising research field due to its\navailability to deal with user information. However, there's a problem that the\nuser's preferences and historical sessions were not considered in the typical\nsession-based recommendation since it concentrates only on user-item\ninteraction. In addition, the existing personalized session-based\nrecommendation model has a limited capability in that it only considers the\npreference of the current user without considering those of similar users. It\nmeans there can be the loss of information included within the hierarchical\ndata structure of the user-session-item. To tackle with this problem, we\npropose USP-SBR(abbr. of User Similarity Powered - Session Based Recommender).\nTo model global historical sessions of users, we propose UserGraph that has two\ntypes of nodes - ItemNode and UserNode. We then connect the nodes with three\ntypes of edges. The first type of edges connects ItemNode as chronological\norder, and the second connects ItemNode to UserNode, and the last connects\nUserNode to ItemNode. With these user embeddings, we propose additional\ncontrastive loss, that makes users with similar intention be close to each\nother in the vector space. we apply graph neural network on these UserGraph and\nupdate nodes. Experimental results on two real-world datasets demonstrate that\nour method outperforms some state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Gen-IR @ SIGIR 2023: The First Workshop on Generative Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2306.02887v1",
        "pub_date": "2023-06-05",
        "summary": "Generative information retrieval (IR) has experienced substantial growth\nacross multiple research communities (e.g., information retrieval, computer\nvision, natural language processing, and machine learning), and has been highly\nvisible in the popular press. Theoretical, empirical, and actual user-facing\nproducts have been released that retrieve documents (via generation) or\ndirectly generate answers given an input request. We would like to investigate\nwhether end-to-end generative models are just another trend or, as some claim,\na paradigm change for IR. This necessitates new metrics, theoretical grounding,\nevaluation methods, task definitions, models, user interfaces, etc. The goal of\nthis workshop (https://coda.io/@sigir/gen-ir) is to focus on previously\nexplored Generative IR techniques like document retrieval and direct Grounded\nAnswer Generation, while also offering a venue for the discussion and\nexploration of how Generative IR can be applied to new domains like\nrecommendation systems, summarization, etc. The format of the workshop is\ninteractive, including roundtable and keynote sessions and tends to avoid the\none-sided dialogue of a mini-conference.",
        "translated": ""
    },
    {
        "title": "Benchmarking Middle-Trained Language Models for Neural Search",
        "url": "http://arxiv.org/abs/2306.02867v1",
        "pub_date": "2023-06-05",
        "summary": "Middle training methods aim to bridge the gap between the Masked Language\nModel (MLM) pre-training and the final finetuning for retrieval. Recent models\nsuch as CoCondenser, RetroMAE, and LexMAE argue that the MLM task is not\nsufficient enough to pre-train a transformer network for retrieval and hence\npropose various tasks to do so. Intrigued by those novel methods, we noticed\nthat all these models used different finetuning protocols, making it hard to\nassess the benefits of middle training. We propose in this paper a benchmark of\nCoCondenser, RetroMAE, and LexMAE, under the same finetuning conditions. We\ncompare both dense and sparse approaches under various finetuning protocols and\nmiddle training on different collections (MS MARCO, Wikipedia or Tripclick). We\nuse additional middle training baselines, such as a standard MLM finetuning on\nthe retrieval collection, optionally augmented by a CLS predicting the passage\nterm frequency. For the sparse approach, our study reveals that there is almost\nno statistical difference between those methods: the more effective the\nfinetuning procedure is, the less difference there is between those models. For\nthe dense approach, RetroMAE using MS MARCO as middle-training collection shows\nexcellent results in almost all the settings. Finally, we show that middle\ntraining on the retrieval collection, thus adapting the language model to it,\nis a critical factor. Overall, a better experimental setup should be adopted to\nevaluate middle training methods. Code available at\nhttps://github.com/naver/splade/tree/benchmarch-SIGIR23",
        "translated": ""
    },
    {
        "title": "CTRL: Connect Tabular and Language Model for CTR Prediction",
        "url": "http://arxiv.org/abs/2306.02841v1",
        "pub_date": "2023-06-05",
        "summary": "Traditional click-through rate (CTR) prediction models convert the tabular\ndata into one-hot vectors and leverage the collaborative relations among\nfeatures for inferring user's preference over items. This modeling paradigm\ndiscards the essential semantic information. Though some recent works like P5\nand M6-Rec have explored the potential of using Pre-trained Language Models\n(PLMs) to extract semantic signals for CTR prediction, they are computationally\nexpensive and suffer from low efficiency. Besides, the beneficial collaborative\nrelations are not considered, hindering the recommendation performance. To\nsolve these problems, in this paper, we propose a novel framework\n\\textbf{CTRL}, which is industrial friendly and model-agnostic with high\ntraining and inference efficiency. Specifically, the original tabular data is\nfirst converted into textual data. Both tabular data and converted textual data\nare regarded as two different modalities and are separately fed into the\ncollaborative CTR model and pre-trained language model. A cross-modal knowledge\nalignment procedure is performed to fine-grained align and integrate the\ncollaborative and semantic signals, and the lightweight collaborative model can\nbe deployed online for efficient serving after fine-tuned with supervised\nsignals. Experimental results on three public datasets show that CTRL\noutperforms the SOTA CTR models significantly. Moreover, we further verify its\neffectiveness on a large-scale industrial recommender system.",
        "translated": ""
    },
    {
        "title": "Path-Specific Counterfactual Fairness for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02615v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender systems (RSs) have become an indispensable part of online\nplatforms. With the growing concerns of algorithmic fairness, RSs are not only\nexpected to deliver high-quality personalized content, but are also demanded\nnot to discriminate against users based on their demographic information.\nHowever, existing RSs could capture undesirable correlations between sensitive\nfeatures and observed user behaviors, leading to biased recommendations. Most\nfair RSs tackle this problem by completely blocking the influences of sensitive\nfeatures on recommendations. But since sensitive features may also affect user\ninterests in a fair manner (e.g., race on culture-based preferences),\nindiscriminately eliminating all the influences of sensitive features\ninevitably degenerate the recommendations quality and necessary diversities. To\naddress this challenge, we propose a path-specific fair RS (PSF-RS) for\nrecommendations. Specifically, we summarize all fair and unfair correlations\nbetween sensitive features and observed ratings into two latent proxy\nmediators, where the concept of path-specific bias (PS-Bias) is defined based\non path-specific counterfactual inference. Inspired by Pearl's minimal change\nprinciple, we address the PS-Bias by minimally transforming the biased factual\nworld into a hypothetically fair world, where a fair RS model can be learned\naccordingly by solving a constrained optimization problem. For the technical\npart, we propose a feasible implementation of PSF-RS, i.e., PSF-VAE, with\nweakly-supervised variational inference, which robustly infers the latent\nmediators such that unfairness can be mitigated while necessary recommendation\ndiversities can be maximally preserved simultaneously. Experiments conducted on\nsemi-simulated and real-world datasets demonstrate the effectiveness of PSF-RS.",
        "translated": ""
    },
    {
        "title": "Learning to Relate to Previous Turns in Conversational Search",
        "url": "http://arxiv.org/abs/2306.02553v1",
        "pub_date": "2023-06-05",
        "summary": "Conversational search allows a user to interact with a search system in\nmultiple turns. A query is strongly dependent on the conversation context. An\neffective way to improve retrieval effectiveness is to expand the current query\nwith historical queries. However, not all the previous queries are related to,\nand useful for expanding the current query. In this paper, we propose a new\nmethod to select relevant historical queries that are useful for the current\nquery. To cope with the lack of labeled training data, we use a pseudo-labeling\napproach to annotate useful historical queries based on their impact on the\nretrieval results. The pseudo-labeled data are used to train a selection model.\nWe further propose a multi-task learning framework to jointly train the\nselector and the retriever during fine-tuning, allowing us to mitigate the\npossible inconsistency between the pseudo labels and the changed retriever.\nExtensive experiments on four conversational search datasets demonstrate the\neffectiveness and broad applicability of our method compared with several\nstrong baselines.",
        "translated": ""
    },
    {
        "title": "RecAgent: A Novel Simulation Paradigm for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02552v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender system has deeply revolutionized people's daily life and\nproduction, bringing a large amount of business value. In the recommendation\ndomain, simulation and real data-based studies are two typical research\nparadigms, with each having different advantages. Previously, real data-based\nstudies occupy more important positions, since accurately simulating the user\npreference is quite difficult. Recently, large language models (LLM) have shown\ngreat potential to achieve human-like intelligence, which provides new\nopportunities to overcome the shortcomings of simulation-based studies and thus\nhighlight their advantages, such as much more application scenarios and cheaper\ndata acquisition strategies. To shed lights on this direction, in this paper,\nwe introduce an LLM-based recommender simulator called RecAgent. Our simulator\nis composed of two modules: (1) the user module and (2) the recommender module.\nThe user module can browse the recommendation website, communicate with other\nusers and broadcast messages on the social media. The recommender module is\ndesigned to provide search or recommendation lists to the users, and one can\ndesign different models to implement the recommender. All the users take\nactions based on LLMs, and can freely evolve like in the real world. We present\nseveral case studies to demonstrate that the users in our simulator can indeed\nbehave in a reasonable manner as expected. Our project has been released at\nhttps://github.com/RUC-GSAI/YuLan-Rec.",
        "translated": ""
    },
    {
        "title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions",
        "url": "http://arxiv.org/abs/2306.02549v1",
        "pub_date": "2023-06-05",
        "summary": "This paper investigates the use of artificial intelligence chatbots for\npatient-specific question answering (QA) from clinical notes using several\nlarge language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google\nBard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and\ncoherence of the answers generated by each model using a 5-point Likert scale\non a set of patient-specific questions.",
        "translated": ""
    },
    {
        "title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models\n  with Same Tower Negatives",
        "url": "http://arxiv.org/abs/2306.02516v1",
        "pub_date": "2023-06-05",
        "summary": "Dual encoders have been used for retrieval tasks and representation learning\nwith good results. A standard way to train dual encoders is using a contrastive\nloss with in-batch negatives. In this work, we propose an improved contrastive\nlearning objective by adding queries or documents from the same encoder towers\nto the negatives, for which we name it as \"contrastive loss with SAMe TOwer\nNEgatives\" (SamToNe). By evaluating on question answering retrieval benchmarks\nfrom MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval\nbenchmarks (BEIR), we demonstrate that SamToNe can effectively improve the\nretrieval quality for both symmetric and asymmetric dual encoders. By directly\nprobing the embedding spaces of the two encoding towers via the t-SNE algorithm\n(van der Maaten and Hinton, 2008), we observe that SamToNe ensures the\nalignment between the embedding spaces from the two encoder towers. Based on\nthe analysis of the embedding distance distributions of the top-$1$ retrieved\nresults, we further explain the efficacy of the method from the perspective of\nregularisation.",
        "translated": ""
    },
    {
        "title": "I^3 Retriever: Incorporating Implicit Interaction in Pre-trained\n  Language Models for Passage Retrieval",
        "url": "http://arxiv.org/abs/2306.02371v1",
        "pub_date": "2023-06-04",
        "summary": "Passage retrieval is a fundamental task in many information systems, such as\nweb search and question answering, where both efficiency and effectiveness are\ncritical concerns. In recent years, neural retrievers based on pre-trained\nlanguage models (PLM), such as dual-encoders, have achieved huge success. Yet,\nstudies have found that the performance of dual-encoders are often limited due\nto the neglecting of the interaction information between queries and candidate\npassages. Therefore, various interaction paradigms have been proposed to\nimprove the performance of vanilla dual-encoders. Particularly, recent\nstate-of-the-art methods often introduce late-interaction during the model\ninference process. However, such late-interaction based methods usually bring\nextensive computation and storage cost on large corpus. Despite their\neffectiveness, the concern of efficiency and space footprint is still an\nimportant factor that limits the application of interaction-based neural\nretrieval models. To tackle this issue, we incorporate implicit interaction\ninto dual-encoders, and propose I^3 retriever. In particular, our implicit\ninteraction paradigm leverages generated pseudo-queries to simulate\nquery-passage interaction, which jointly optimizes with query and passage\nencoders in an end-to-end manner. It can be fully pre-computed and cached, and\nits inference process only involves simple dot product operation of the query\nvector and passage vector, which makes it as efficient as the vanilla dual\nencoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep\nLearning Datasets, demonstrating the I^3 retriever's superiority in terms of\nboth effectiveness and efficiency. Moreover, the proposed implicit interaction\nis compatible with special pre-training and knowledge distillation for passage\nretrieval, which brings a new state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "On Manipulating Signals of User-Item Graph: A Jacobi Polynomial-based\n  Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2306.03624v1",
        "pub_date": "2023-06-06",
        "summary": "Collaborative filtering (CF) is an important research direction in\nrecommender systems that aims to make recommendations given the information on\nuser-item interactions. Graph CF has attracted more and more attention in\nrecent years due to its effectiveness in leveraging high-order information in\nthe user-item bipartite graph for better recommendations. Specifically, recent\nstudies show the success of graph neural networks (GNN) for CF is attributed to\nits low-pass filtering effects. However, current researches lack a study of how\ndifferent signal components contributes to recommendations, and how to design\nstrategies to properly use them well. To this end, from the view of spectral\ntransformation, we analyze the important factors that a graph filter should\nconsider to achieve better performance. Based on the discoveries, we design\nJGCF, an efficient and effective method for CF based on Jacobi polynomial bases\nand frequency decomposition strategies. Extensive experiments on four widely\nused public datasets show the effectiveness and efficiency of the proposed\nmethods, which brings at most 27.06% performance gain on Alibaba-iFashion.\nBesides, the experimental results also show that JGCF is better at handling\nsparse datasets, which shows potential in making recommendations for cold-start\nusers.",
        "translated": ""
    },
    {
        "title": "Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR\n  Prediction in Taobao",
        "url": "http://arxiv.org/abs/2306.03527v1",
        "pub_date": "2023-06-06",
        "summary": "Click-Through Rate (CTR) prediction serves as a fundamental component in\nonline advertising. A common practice is to train a CTR model on advertisement\n(ad) impressions with user feedback. Since ad impressions are purposely\nselected by the model itself, their distribution differs from the inference\ndistribution and thus exhibits sample selection bias (SSB) that affects model\nperformance. Existing studies on SSB mainly employ sample re-weighting\ntechniques which suffer from high variance and poor model calibration. Another\nline of work relies on costly uniform data that is inadequate to train\nindustrial models. Thus mitigating SSB in industrial models with a\nuniform-data-free framework is worth exploring. Fortunately, many platforms\ndisplay mixed results of organic items (i.e., recommendations) and sponsored\nitems (i.e., ads) to users, where impressions of ads and recommendations are\nselected by different systems but share the same user decision rationales.\nBased on the above characteristics, we propose to leverage recommendations\nsamples as a free lunch to mitigate SSB for ads CTR model (Rec4Ad). After\nelaborating data augmentation, Rec4Ad learns disentangled representations with\nalignment and decorrelation modules for enhancement. When deployed in Taobao\ndisplay advertising system, Rec4Ad achieves substantial gains in key business\nmetrics, with a lift of up to +6.6\\% CTR and +2.9\\% RPM.",
        "translated": ""
    },
    {
        "title": "COPR: Consistency-Oriented Pre-Ranking for Online Advertising",
        "url": "http://arxiv.org/abs/2306.03516v1",
        "pub_date": "2023-06-06",
        "summary": "Cascading architecture has been widely adopted in large-scale advertising\nsystems to balance efficiency and effectiveness. In this architecture, the\npre-ranking model is expected to be a lightweight approximation of the ranking\nmodel, which handles more candidates with strict latency requirements. Due to\nthe gap in model capacity, the pre-ranking and ranking models usually generate\ninconsistent ranked results, thus hurting the overall system effectiveness. The\nparadigm of score alignment is proposed to regularize their raw scores to be\nconsistent. However, it suffers from inevitable alignment errors and error\namplification by bids when applied in online advertising. To this end, we\nintroduce a consistency-oriented pre-ranking framework for online advertising,\nwhich employs a chunk-based sampling module and a plug-and-play rank alignment\nmodule to explicitly optimize consistency of ECPM-ranked results. A $\\Delta\nNDCG$-based weighting mechanism is adopted to better distinguish the importance\nof inter-chunk samples in optimization. Both online and offline experiments\nhave validated the superiority of our framework. When deployed in Taobao\ndisplay advertising system, it achieves an improvement of up to +12.3\\% CTR and\n+5.6\\% RPM.",
        "translated": ""
    },
    {
        "title": "Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search",
        "url": "http://arxiv.org/abs/2306.03411v1",
        "pub_date": "2023-06-06",
        "summary": "Customers interacting with product search engines are increasingly\nformulating information-seeking queries. Frequently Asked Question (FAQ)\nretrieval aims to retrieve common question-answer pairs for a user query with\nquestion intent. Integrating FAQ retrieval in product search can not only\nempower users to make more informed purchase decisions, but also enhance user\nretention through efficient post-purchase support. Determining when an FAQ\nentry can satisfy a user's information need within product search, without\ndisrupting their shopping experience, represents an important challenge. We\npropose an intent-aware FAQ retrieval system consisting of (1) an intent\nclassifier that predicts when a user's information need can be answered by an\nFAQ; (2) a reformulation model that rewrites a query into a natural question.\nOffline evaluation demonstrates that our approach improves Hit@1 by 13% on\nretrieving ground-truth FAQs, while reducing latency by 95% compared to\nbaseline systems. These improvements are further validated by real user\nfeedback, where 71% of displayed FAQs on top of product search results received\nexplicit positive user feedback. Overall, our findings show promising\ndirections for integrating FAQ retrieval into product search at scale.",
        "translated": ""
    },
    {
        "title": "Computational Technologies for Fashion Recommendation: A Survey",
        "url": "http://arxiv.org/abs/2306.03395v1",
        "pub_date": "2023-06-06",
        "summary": "Fashion recommendation is a key research field in computational fashion\nresearch and has attracted considerable interest in the computer vision,\nmultimedia, and information retrieval communities in recent years. Due to the\ngreat demand for applications, various fashion recommendation tasks, such as\npersonalized fashion product recommendation, complementary (mix-and-match)\nrecommendation, and outfit recommendation, have been posed and explored in the\nliterature. The continuing research attention and advances impel us to look\nback and in-depth into the field for a better understanding. In this paper, we\ncomprehensively review recent research efforts on fashion recommendation from a\ntechnological perspective. We first introduce fashion recommendation at a macro\nlevel and analyse its characteristics and differences with general\nrecommendation tasks. We then clearly categorize different fashion\nrecommendation efforts into several sub-tasks and focus on each sub-task in\nterms of its problem formulation, research focus, state-of-the-art methods, and\nlimitations. We also summarize the datasets proposed in the literature for use\nin fashion recommendation studies to give readers a brief illustration.\nFinally, we discuss several promising directions for future research in this\nfield. Overall, this survey systematically reviews the development of fashion\nrecommendation research. It also discusses the current limitations and gaps\nbetween academic research and the real needs of the fashion industry. In the\nprocess, we offer a deep insight into how the fashion industry could benefit\nfrom fashion recommendation technologies. the computational technologies of\nfashion recommendation.",
        "translated": ""
    },
    {
        "title": "Tree based Progressive Regression Model for Watch-Time Prediction in\n  Short-video Recommendation",
        "url": "http://arxiv.org/abs/2306.03392v1",
        "pub_date": "2023-06-06",
        "summary": "An accurate prediction of watch time has been of vital importance to enhance\nuser engagement in video recommender systems. To achieve this, there are four\nproperties that a watch time prediction framework should satisfy: first,\ndespite its continuous value, watch time is also an ordinal variable and the\nrelative ordering between its values reflects the differences in user\npreferences. Therefore the ordinal relations should be reflected in watch time\npredictions. Second, the conditional dependence between the video-watching\nbehaviors should be captured in the model. For instance, one has to watch half\nof the video before he/she finishes watching the whole video. Third, modeling\nwatch time with a point estimation ignores the fact that models might give\nresults with high uncertainty and this could cause bad cases in recommender\nsystems. Therefore the framework should be aware of prediction uncertainty.\nForth, the real-life recommender systems suffer from severe bias amplifications\nthus an estimation without bias amplification is expected. Therefore we propose\nTPM for watch time prediction. Specifically, the ordinal ranks of watch time\nare introduced into TPM and the problem is decomposed into a series of\nconditional dependent classification tasks which are organized into a tree\nstructure. The expectation of watch time can be generated by traversing the\ntree and the variance of watch time predictions is explicitly introduced into\nthe objective function as a measurement for uncertainty. Moreover, we\nillustrate that backdoor adjustment can be seamlessly incorporated into TPM,\nwhich alleviates bias amplifications. Extensive offline evaluations have been\nconducted in public datasets and TPM have been deployed in a real-world video\napp Kuaishou with over 300 million DAUs. The results indicate that TPM\noutperforms state-of-the-art approaches and indeed improves video consumption\nsignificantly.",
        "translated": ""
    },
    {
        "title": "Towards Alleviating the Object Bias in Prompt Tuning-based Factual\n  Knowledge Extraction",
        "url": "http://arxiv.org/abs/2306.03378v1",
        "pub_date": "2023-06-06",
        "summary": "Many works employed prompt tuning methods to automatically optimize prompt\nqueries and extract the factual knowledge stored in Pretrained Language Models.\nIn this paper, we observe that the optimized prompts, including discrete\nprompts and continuous prompts, exhibit undesirable object bias. To handle this\nproblem, we propose a novel prompt tuning method called MeCoD. consisting of\nthree modules: Prompt Encoder, Object Equalization and Biased Object\nObstruction. Experimental results show that MeCoD can significantly reduce the\nobject bias and at the same time improve accuracy of factual knowledge\nextraction.",
        "translated": ""
    },
    {
        "title": "Construction d'un système de recommandation basé sur des contraintes\n  via des graphes de connaissances",
        "url": "http://arxiv.org/abs/2306.03247v1",
        "pub_date": "2023-06-05",
        "summary": "Knowledge graphs in RDF model entities and their relations using ontologies,\nand have gained popularity for information modeling. In recommender systems,\nknowledge graphs help represent more links and relationships between users and\nitems. Constraint-based recommender systems leverage deep recommendation\nknowledge to identify relevant suggestions. When combined with knowledge\ngraphs, they offer benefits in constraint sets. This paper explores a\nconstraint-based recommender system using RDF knowledge graphs for the vehicle\npurchase/sale domain. Our experiments demonstrate that the proposed approach\nefficiently identifies recommendations based on user preferences.",
        "translated": ""
    },
    {
        "title": "Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time\n  Light Patterns",
        "url": "http://arxiv.org/abs/2306.03195v1",
        "pub_date": "2023-06-05",
        "summary": "We introduce NightPulse, an interactive tool for Night-time light (NTL) data\nvisualization and analytics, which enables researchers and stakeholders to\nexplore and analyze NTL data with a user-friendly platform. Powered by\nefficient system architecture, NightPulse supports image segmentation,\nclustering, and change pattern detection to identify urban development and\nsprawl patterns. It captures temporal trends of NTL and semantics of cities,\nanswering questions about demographic factors, city boundaries, and unusual\ndifferences.",
        "translated": ""
    },
    {
        "title": "Personalized Federated Domain Adaptation for Item-to-Item Recommendation",
        "url": "http://arxiv.org/abs/2306.03191v1",
        "pub_date": "2023-06-05",
        "summary": "Item-to-Item (I2I) recommendation is an important function in most\nrecommendation systems, which generates replacement or complement suggestions\nfor a particular item based on its semantic similarities to other cataloged\nitems. Given that subsets of items in a recommendation system might be\nco-interacted with by the same set of customers, graph-based models, such as\ngraph neural networks (GNNs), provide a natural framework to combine, ingest\nand extract valuable insights from such high-order relational interactions\nbetween cataloged items, as well as their metadata features, as has been shown\nin many recent studies. However, learning GNNs effectively for I2I requires\ningesting a large amount of relational data, which might not always be\navailable, especially in new, emerging market segments. To mitigate this data\nbottleneck, we postulate that recommendation patterns learned from existing\nmature market segments (with private data) could be adapted to build effective\nwarm-start models for emerging ones. To achieve this, we propose and\ninvestigate a personalized federated modeling framework based on GNNs to\nsummarize, assemble and adapt recommendation patterns across market segments\nwith heterogeneous customer behaviors into effective local models. Our key\ncontribution is a personalized graph adaptation model that bridges the gap\nbetween recent literature on federated GNNs and (non-graph) personalized\nfederated learning, which either does not optimize for the adaptability of the\nfederated model or is restricted to local models with homogeneous\nparameterization, excluding GNNs with heterogeneous local graphs.",
        "translated": ""
    },
    {
        "title": "MarineVRS: Marine Video Retrieval System with Explainability via\n  Semantic Understanding",
        "url": "http://arxiv.org/abs/2306.04593v1",
        "pub_date": "2023-06-07",
        "summary": "Building a video retrieval system that is robust and reliable, especially for\nthe marine environment, is a challenging task due to several factors such as\ndealing with massive amounts of dense and repetitive data, occlusion,\nblurriness, low lighting conditions, and abstract queries. To address these\nchallenges, we present MarineVRS, a novel and flexible video retrieval system\ndesigned explicitly for the marine domain. MarineVRS integrates\nstate-of-the-art methods for visual and linguistic object representation to\nenable efficient and accurate search and analysis of vast volumes of underwater\nvideo data. In addition, unlike the conventional video retrieval system, which\nonly permits users to index a collection of images or videos and search using a\nfree-form natural language sentence, our retrieval system includes an\nadditional Explainability module that outputs the segmentation masks of the\nobjects that the input query referred to. This feature allows users to identify\nand isolate specific objects in the video footage, leading to more detailed\nanalysis and understanding of their behavior and movements. Finally, with its\nadaptability, explainability, accuracy, and scalability, MarineVRS is a\npowerful tool for marine researchers and scientists to efficiently and\naccurately process vast amounts of data and gain deeper insights into the\nbehavior and movements of marine species.",
        "translated": ""
    },
    {
        "title": "Constraint-based recommender system for crisis management simulations",
        "url": "http://arxiv.org/abs/2306.04553v1",
        "pub_date": "2023-06-07",
        "summary": "In the context of the evacuation of populations, some citizens/volunteers may\nwant and be able to participate in the evacuation of populations in difficulty\nby coming to lend a hand to emergency/evacuation vehicles with their own\nvehicles. One way of framing these impulses of solidarity would be to be able\nto list in real-time the citizens/volunteers available with their vehicles\n(land, sea, air, etc.), to be able to geolocate them according to the risk\nareas to be evacuated, and adding them to the evacuation/rescue vehicles.\nBecause it is difficult to propose an effective real-time operational system on\nthe field in a real crisis situation, in this work, we propose to add a module\nfor recommending driver/vehicle pairs (with their specificities) to a system of\ncrisis management simulation. To do that, we chose to model and develop an\nontology-supported constraint-based recommender system for crisis management\nsimulations.",
        "translated": ""
    },
    {
        "title": "Embracing Uncertainty: Adaptive Vague Preference Policy Learning for\n  Multi-round Conversational Recommendation",
        "url": "http://arxiv.org/abs/2306.04487v1",
        "pub_date": "2023-06-07",
        "summary": "Conversational recommendation systems (CRS) effectively address information\nasymmetry by dynamically eliciting user preferences through multi-turn\ninteractions. Existing CRS widely assumes that users have clear preferences.\nUnder this assumption, the agent will completely trust the user feedback and\ntreat the accepted or rejected signals as strong indicators to filter items and\nreduce the candidate space, which may lead to the problem of over-filtering.\nHowever, in reality, users' preferences are often vague and volatile, with\nuncertainty about their desires and changing decisions during interactions.\n  To address this issue, we introduce a novel scenario called Vague Preference\nMulti-round Conversational Recommendation (VPMCR), which considers users' vague\nand volatile preferences in CRS.VPMCR employs a soft estimation mechanism to\nassign a non-zero confidence score for all candidate items to be displayed,\nnaturally avoiding the over-filtering problem. In the VPMCR setting, we\nintroduce an solution called Adaptive Vague Preference Policy Learning (AVPPL),\nwhich consists of two main components: Uncertainty-aware Soft Estimation (USE)\nand Uncertainty-aware Policy Learning (UPL). USE estimates the uncertainty of\nusers' vague feedback and captures their dynamic preferences using a\nchoice-based preferences extraction module and a time-aware decaying strategy.\nUPL leverages the preference distribution estimated by USE to guide the\nconversation and adapt to changes in users' preferences to make recommendations\nor ask for attributes.\n  Our extensive experiments demonstrate the effectiveness of our method in the\nVPMCR scenario, highlighting its potential for practical applications and\nimproving the overall performance and applicability of CRS in real-world\nsettings, particularly for users with vague or dynamic preferences.",
        "translated": ""
    },
    {
        "title": "RD-Suite: A Benchmark for Ranking Distillation",
        "url": "http://arxiv.org/abs/2306.04455v1",
        "pub_date": "2023-06-07",
        "summary": "The distillation of ranking models has become an important topic in both\nacademia and industry. In recent years, several advanced methods have been\nproposed to tackle this problem, often leveraging ranking information from\nteacher rankers that is absent in traditional classification settings. To date,\nthere is no well-established consensus on how to evaluate this class of models.\nMoreover, inconsistent benchmarking on a wide range of tasks and datasets make\nit difficult to assess or invigorate advances in this field. This paper first\nexamines representative prior arts on ranking distillation, and raises three\nquestions to be answered around methodology and reproducibility. To that end,\nwe propose a systematic and unified benchmark, Ranking Distillation Suite\n(RD-Suite), which is a suite of tasks with 4 large real-world datasets,\nencompassing two major modalities (textual and numeric) and two applications\n(standard distillation and distillation transfer). RD-Suite consists of\nbenchmark results that challenge some of the common wisdom in the field, and\nthe release of datasets with teacher scores and evaluation scripts for future\nresearch. RD-Suite paves the way towards better understanding of ranking\ndistillation, facilities more research in this direction, and presents new\nchallenges.",
        "translated": ""
    },
    {
        "title": "Modeling Dual Period-Varying Preferences for Takeaway Recommendation",
        "url": "http://arxiv.org/abs/2306.04370v1",
        "pub_date": "2023-06-07",
        "summary": "Takeaway recommender systems, which aim to accurately provide stores that\noffer foods meeting users' interests, have served billions of users in our\ndaily life. Different from traditional recommendation, takeaway recommendation\nfaces two main challenges: (1) Dual Interaction-Aware Preference Modeling.\nTraditional recommendation commonly focuses on users' single preferences for\nitems while takeaway recommendation needs to comprehensively consider users'\ndual preferences for stores and foods. (2) Period-Varying Preference Modeling.\nConventional recommendation generally models continuous changes in users'\npreferences from a session-level or day-level perspective. However, in\npractical takeaway systems, users' preferences vary significantly during the\nmorning, noon, night, and late night periods of the day. To address these\nchallenges, we propose a Dual Period-Varying Preference modeling (DPVP) for\ntakeaway recommendation. Specifically, we design a dual interaction-aware\nmodule, aiming to capture users' dual preferences based on their interactions\nwith stores and foods. Moreover, to model various preferences in different time\nperiods of the day, we propose a time-based decomposition module as well as a\ntime-aware gating mechanism. Extensive offline and online experiments\ndemonstrate that our model outperforms state-of-the-art methods on real-world\ndatasets and it is capable of modeling the dual period-varying preferences.\nMoreover, our model has been deployed online on Meituan Takeaway platform,\nleading to an average improvement in GMV (Gross Merchandise Value) of 0.70%.",
        "translated": ""
    },
    {
        "title": "An Overview of Challenges in Egocentric Text-Video Retrieval",
        "url": "http://arxiv.org/abs/2306.04345v1",
        "pub_date": "2023-06-07",
        "summary": "Text-video retrieval contains various challenges, including biases coming\nfrom diverse sources. We highlight some of them supported by illustrations to\nopen a discussion. Besides, we address one of the biases, frame length bias,\nwith a simple method which brings a very incremental but promising increase. We\nconclude with future directions.",
        "translated": ""
    },
    {
        "title": "Phrase Retrieval for Open-Domain Conversational Question Answering with\n  Conversational Dependency Modeling via Contrastive Learning",
        "url": "http://arxiv.org/abs/2306.04293v1",
        "pub_date": "2023-06-07",
        "summary": "Open-Domain Conversational Question Answering (ODConvQA) aims at answering\nquestions through a multi-turn conversation based on a retriever-reader\npipeline, which retrieves passages and then predicts answers with them.\nHowever, such a pipeline approach not only makes the reader vulnerable to the\nerrors propagated from the retriever, but also demands additional effort to\ndevelop both the retriever and the reader, which further makes it slower since\nthey are not runnable in parallel. In this work, we propose a method to\ndirectly predict answers with a phrase retrieval scheme for a sequence of\nwords, reducing the conventional two distinct subtasks into a single one. Also,\nfor the first time, we study its capability for ODConvQA tasks. However, simply\nadopting it is largely problematic, due to the dependencies between previous\nand current turns in a conversation. To address this problem, we further\nintroduce a novel contrastive learning strategy, making sure to reflect\nprevious turns when retrieving the phrase for the current context, by\nmaximizing representational similarities of consecutive turns in a conversation\nwhile minimizing irrelevant conversational contexts. We validate our model on\ntwo ODConvQA datasets, whose experimental results show that it substantially\noutperforms the relevant baselines with the retriever-reader. Code is available\nat: https://github.com/starsuzi/PRO-ConvQA.",
        "translated": ""
    },
    {
        "title": "Set-to-Sequence Ranking-based Concept-aware Learning Path Recommendation",
        "url": "http://arxiv.org/abs/2306.04234v1",
        "pub_date": "2023-06-07",
        "summary": "With the development of the online education system, personalized education\nrecommendation has played an essential role. In this paper, we focus on\ndeveloping path recommendation systems that aim to generating and recommending\nan entire learning path to the given user in each session. Noticing that\nexisting approaches fail to consider the correlations of concepts in the path,\nwe propose a novel framework named Set-to-Sequence Ranking-based Concept-aware\nLearning Path Recommendation (SRC), which formulates the recommendation task\nunder a set-to-sequence paradigm. Specifically, we first design a concept-aware\nencoder module which can capture the correlations among the input learning\nconcepts. The outputs are then fed into a decoder module that sequentially\ngenerates a path through an attention mechanism that handles correlations\nbetween the learning and target concepts. Our recommendation policy is\noptimized by policy gradient. In addition, we also introduce an auxiliary\nmodule based on knowledge tracing to enhance the model's stability by\nevaluating students' learning effects on learning concepts. We conduct\nextensive experiments on two real-world public datasets and one industrial\ndataset, and the experimental results demonstrate the superiority and\neffectiveness of SRC. Code will be available at\nhttps://gitee.com/mindspore/models/tree/master/research/recommend/SRC.",
        "translated": ""
    },
    {
        "title": "SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet",
        "url": "http://arxiv.org/abs/2306.04148v1",
        "pub_date": "2023-06-07",
        "summary": "It is very important to access a rich music dataset that is useful in a wide\nvariety of applications. Currently, available datasets are mostly focused on\nstoring vocal or instrumental recording data and ignoring the requirement of\nits visual representation and retrieval. This paper attempts to build an\nXML-based public dataset, called SANGEET, that stores comprehensive information\nof Hindustani Sangeet (North Indian Classical Music) compositions written by\nfamous musicologist Pt. Vishnu Narayan Bhatkhande. SANGEET preserves all the\nrequired information of any given composition including metadata, structural,\nnotational, rhythmic, and melodic information in a standardized way for easy\nand efficient storage and extraction of musical information. The dataset is\nintended to provide the ground truth information for music information research\ntasks, thereby supporting several data-driven analysis from a machine learning\nperspective. We present the usefulness of the dataset by demonstrating its\napplication on music information retrieval using XQuery, visualization through\nOmenad rendering system. Finally, we propose approaches to transform the\ndataset for performing statistical and machine learning tasks for a better\nunderstanding of Hindustani Sangeet. The dataset can be found at\nhttps://github.com/cmisra/Sangeet.",
        "translated": ""
    },
    {
        "title": "Answering Compositional Queries with Set-Theoretic Embeddings",
        "url": "http://arxiv.org/abs/2306.04133v1",
        "pub_date": "2023-06-07",
        "summary": "The need to compactly and robustly represent item-attribute relations arises\nin many important tasks, such as faceted browsing and recommendation systems. A\npopular machine learning approach for this task denotes that an item has an\nattribute by a high dot-product between vectors for the item and attribute -- a\nrepresentation that is not only dense, but also tends to correct noisy and\nincomplete data. While this method works well for queries retrieving items by a\nsingle attribute (such as \\emph{movies that are comedies}), we find that vector\nembeddings do not so accurately support compositional queries (such as movies\nthat are comedies and British but not romances). To address these set-theoretic\ncompositions, this paper proposes to replace vectors with box embeddings, a\nregion-based representation that can be thought of as learnable Venn diagrams.\nWe introduce a new benchmark dataset for compositional queries, and present\nexperiments and analysis providing insights into the behavior of both. We find\nthat, while vector and box embeddings are equally suited to single attribute\nqueries, for compositional queries box embeddings provide substantial\nadvantages over vectors, particularly at the moderate and larger retrieval set\nsizes that are most useful for users' search and browsing.",
        "translated": ""
    },
    {
        "title": "Safe Collaborative Filtering",
        "url": "http://arxiv.org/abs/2306.05292v1",
        "pub_date": "2023-06-08",
        "summary": "Excellent tail performance is crucial for modern machine learning tasks, such\nas algorithmic fairness, class imbalance, and risk-sensitive decision making,\nas it ensures the effective handling of challenging samples within a dataset.\nTail performance is also a vital determinant of success for personalised\nrecommender systems to reduce the risk of losing users with low satisfaction.\nThis study introduces a \"safe\" collaborative filtering method that prioritises\nrecommendation quality for less-satisfied users rather than focusing on the\naverage performance. Our approach minimises the conditional value at risk\n(CVaR), which represents the average risk over the tails of users' loss. To\novercome computational challenges for web-scale recommender systems, we develop\na robust yet practical algorithm that extends the most scalable method,\nimplicit alternating least squares (iALS). Empirical evaluation on real-world\ndatasets demonstrates the excellent tail performance of our approach while\nmaintaining competitive computational efficiency.",
        "translated": ""
    },
    {
        "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
        "url": "http://arxiv.org/abs/2306.05212v1",
        "pub_date": "2023-06-08",
        "summary": "Although Large Language Models (LLMs) have demonstrated extraordinary\ncapabilities in many domains, they still have a tendency to hallucinate and\ngenerate fictitious responses to user requests. This problem can be alleviated\nby augmenting LLMs with information retrieval (IR) systems (also known as\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\nfactual texts in response to user input according to the relevant content\nretrieved by IR systems from external corpora as references. In addition, by\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\nquestions that cannot be answered by solely relying on the world knowledge\nstored in parameters. To support research in this area and facilitate the\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\nto help researchers and users build their customized in-domain LLM-based\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\nprovides more plug-and-play modules to support better interaction between IR\nsystems and LLMs, including {request rewriting, document retrieval, passage\nextraction, answer generation, and fact checking} modules. Our toolkit is\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",
        "translated": ""
    },
    {
        "title": "Controllable Multi-Objective Re-ranking with Policy Hypernetworks",
        "url": "http://arxiv.org/abs/2306.05118v1",
        "pub_date": "2023-06-08",
        "summary": "Multi-stage ranking pipelines have become widely used strategies in modern\nrecommender systems, where the final stage aims to return a ranked list of\nitems that balances a number of requirements such as user preference,\ndiversity, novelty etc. Linear scalarization is arguably the most widely used\ntechnique to merge multiple requirements into one optimization objective, by\nsumming up the requirements with certain preference weights. Existing\nfinal-stage ranking methods often adopt a static model where the preference\nweights are determined during offline training and kept unchanged during online\nserving. Whenever a modification of the preference weights is needed, the model\nhas to be re-trained, which is time and resources inefficient. Meanwhile, the\nmost appropriate weights may vary greatly for different groups of targeting\nusers or at different time periods (e.g., during holiday promotions). In this\npaper, we propose a framework called controllable multi-objective re-ranking\n(CMR) which incorporates a hypernetwork to generate parameters for a re-ranking\nmodel according to different preference weights. In this way, CMR is enabled to\nadapt the preference weights according to the environment changes in an online\nmanner, without retraining the models. Moreover, we classify practical\nbusiness-oriented tasks into four main categories and seamlessly incorporate\nthem in a new proposed re-ranking model based on an Actor-Evaluator framework,\nwhich serves as a reliable real-world testbed for CMR. Offline experiments\nbased on the dataset collected from Taobao App showed that CMR improved several\npopular re-ranking models by using them as underlying models. Online A/B tests\nalso demonstrated the effectiveness and trustworthiness of CMR.",
        "translated": ""
    },
    {
        "title": "Attention Weighted Mixture of Experts with Contrastive Learning for\n  Personalized Ranking in E-commerce",
        "url": "http://arxiv.org/abs/2306.05011v1",
        "pub_date": "2023-06-08",
        "summary": "Ranking model plays an essential role in e-commerce search and\nrecommendation. An effective ranking model should give a personalized ranking\nlist for each user according to the user preference. Existing algorithms\nusually extract a user representation vector from the user behavior sequence,\nthen feed the vector into a feed-forward network (FFN) together with other\nfeatures for feature interactions, and finally produce a personalized ranking\nscore. Despite tremendous progress in the past, there is still room for\nimprovement. Firstly, the personalized patterns of feature interactions for\ndifferent users are not explicitly modeled. Secondly, most of existing\nalgorithms have poor personalized ranking results for long-tail users with few\nhistorical behaviors due to the data sparsity. To overcome the two challenges,\nwe propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive\nlearning for personalized ranking. Firstly, AW-MoE leverages the MoE framework\nto capture personalized feature interactions for different users. To model the\nuser preference, the user behavior sequence is simultaneously fed into expert\nnetworks and the gate network. Within the gate network, one gate unit and one\nactivation unit are designed to adaptively learn the fine-grained activation\nvector for experts using an attention mechanism. Secondly, a random masking\nstrategy is applied to the user behavior sequence to simulate long-tail users,\nand an auxiliary contrastive loss is imposed to the output of the gate network\nto improve the model generalization for these users. This is validated by a\nhigher performance gain on the long-tail user test set. Experiment results on a\nJD real production dataset and a public dataset demonstrate the effectiveness\nof AW-MoE, which significantly outperforms state-of-art methods. Notably,\nAW-MoE has been successfully deployed in the JD e-commerce search engine, ...",
        "translated": ""
    },
    {
        "title": "Unified Embedding Based Personalized Retrieval in Etsy Search",
        "url": "http://arxiv.org/abs/2306.04833v1",
        "pub_date": "2023-06-07",
        "summary": "Embedding-based neural retrieval is a prevalent approach to address the\nsemantic gap problem which often arises in product search on tail queries. In\ncontrast, popular queries typically lack context and have a broad intent where\nadditional context from users historical interaction can be helpful. In this\npaper, we share our novel approach to address both: the semantic gap problem\nfollowed by an end to end trained model for personalized semantic retrieval. We\npropose learning a unified embedding model incorporating graph, transformer and\nterm-based embeddings end to end and share our design choices for optimal\ntradeoff between performance and efficiency. We share our learnings in feature\nengineering, hard negative sampling strategy, and application of transformer\nmodel, including a novel pre-training strategy and other tricks for improving\nsearch relevance and deploying such a model at industry scale. Our personalized\nretrieval model significantly improves the overall search experience, as\nmeasured by a 5.58% increase in search purchase rate and a 2.63% increase in\nsite-wide conversion rate, aggregated across multiple A/B tests - on live\ntraffic.",
        "translated": ""
    },
    {
        "title": "SKG: A Versatile Information Retrieval and Analysis Framework for\n  Academic Papers with Semantic Knowledge Graphs",
        "url": "http://arxiv.org/abs/2306.04758v1",
        "pub_date": "2023-06-07",
        "summary": "The number of published research papers has experienced exponential growth in\nrecent years, which makes it crucial to develop new methods for efficient and\nversatile information extraction and knowledge discovery. To address this need,\nwe propose a Semantic Knowledge Graph (SKG) that integrates semantic concepts\nfrom abstracts and other meta-information to represent the corpus. The SKG can\nsupport various semantic queries in academic literature thanks to the high\ndiversity and rich information content stored within. To extract knowledge from\nunstructured text, we develop a Knowledge Extraction Module that includes a\nsemi-supervised pipeline for entity extraction and entity normalization. We\nalso create an ontology to integrate the concepts with other meta information,\nenabling us to build the SKG. Furthermore, we design and develop a dataflow\nsystem that demonstrates how to conduct various semantic queries flexibly and\ninteractively over the SKG. To demonstrate the effectiveness of our approach,\nwe conduct the research based on the visualization literature and provide\nreal-world use cases to show the usefulness of the SKG.\n  The dataset and codes for this work are available at\nhttps://osf.io/aqv8p/?view_only=2c26b36e3e3941ce999df47e4616207f.",
        "translated": ""
    },
    {
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey",
        "url": "http://arxiv.org/abs/2306.05817v1",
        "pub_date": "2023-06-09",
        "summary": "Recommender systems (RS) play important roles to match users' information\nneeds for Internet applications. In natural language processing (NLP) domains,\nlarge language model (LLM) has shown astonishing emergent abilities (e.g.,\ninstruction following, reasoning), thus giving rise to the promising research\ndirection of adapting LLM to RS for performance enhancements and user\nexperience improvements. In this paper, we conduct a comprehensive survey on\nthis research direction from an application-oriented view. We first summarize\nexisting research works from two orthogonal perspectives: where and how to\nadapt LLM to RS. For the \"WHERE\" question, we discuss the roles that LLM could\nplay in different stages of the recommendation pipeline, i.e., feature\nengineering, feature encoder, scoring/ranking function, and pipeline\ncontroller. For the \"HOW\" question, we investigate the training and inference\nstrategies, resulting in two fine-grained taxonomy criteria, i.e., whether to\ntune LLMs or not, and whether to involve conventional recommendation model\n(CRM) for inference. Detailed analysis and general development trajectories are\nprovided for both questions, respectively. Then, we highlight key challenges in\nadapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and\nethics. Finally, we summarize the survey and discuss the future prospects. We\nalso actively maintain a GitHub repository for papers and other related\nresources in this rising direction:\n$\\href{https://github.com/CHIANGEL/Awesome-LLM-for-RecSys}{[GitHub\\;Link]}$.",
        "translated": ""
    },
    {
        "title": "Interactive Explanation with Varying Level of Details in an Explainable\n  Scientific Literature Recommender System",
        "url": "http://arxiv.org/abs/2306.05809v1",
        "pub_date": "2023-06-09",
        "summary": "Explainable recommender systems (RS) have traditionally followed a\none-size-fits-all approach, delivering the same explanation level of detail to\neach user, without considering their individual needs and goals. Further,\nexplanations in RS have so far been presented mostly in a static and\nnon-interactive manner. To fill these research gaps, we aim in this paper to\nadopt a user-centered, interactive explanation model that provides explanations\nwith different levels of detail and empowers users to interact with, control,\nand personalize the explanations based on their needs and preferences. We\nfollowed a user-centered approach to design interactive explanations with three\nlevels of detail (basic, intermediate, and advanced) and implemented them in\nthe transparent Recommendation and Interest Modeling Application (RIMA). We\nconducted a qualitative user study (N=14) to investigate the impact of\nproviding interactive explanations with varying level of details on the users'\nperception of the explainable RS. Our study showed qualitative evidence that\nfostering interaction and giving users control in deciding which explanation\nthey would like to see can meet the demands of users with different needs,\npreferences, and goals, and consequently can have positive effects on different\ncrucial aspects in explainable recommendation, including transparency, trust,\nsatisfaction, and user experience.",
        "translated": ""
    },
    {
        "title": "RankFormer: Listwise Learning-to-Rank Using Listwide Labels",
        "url": "http://arxiv.org/abs/2306.05808v1",
        "pub_date": "2023-06-09",
        "summary": "Web applications where users are presented with a limited selection of items\nhave long employed ranking models to put the most relevant results first. Any\nfeedback received from users is typically assumed to reflect a relative\njudgement on the utility of items, e.g. a user clicking on an item only implies\nit is better than items not clicked in the same ranked list. Hence, the\nobjectives optimized in Learning-to-Rank (LTR) tend to be pairwise or listwise.\n  Yet, by only viewing feedback as relative, we neglect the user's absolute\nfeedback on the list's overall quality, e.g. when no items in the selection are\nclicked. We thus reconsider the standard LTR paradigm and argue the benefits of\nlearning from this listwide signal. To this end, we propose the RankFormer as\nan architecture that, with a Transformer at its core, can jointly optimize a\nnovel listwide assessment objective and a traditional listwise LTR objective.\n  We simulate implicit feedback on public datasets and observe that the\nRankFormer succeeds in benefitting from listwide signals. Additionally, we\nconduct experiments in e-commerce on Amazon Search data and find the RankFormer\nto be superior to all baselines offline. An online experiment shows that\nknowledge distillation can be used to find immediate practical use for the\nRankFormer.",
        "translated": ""
    },
    {
        "title": "Customizing General-Purpose Foundation Models for Medical Report\n  Generation",
        "url": "http://arxiv.org/abs/2306.05642v1",
        "pub_date": "2023-06-09",
        "summary": "Medical caption prediction which can be regarded as a task of medical report\ngeneration (MRG), requires the automatic generation of coherent and accurate\ncaptions for the given medical images. However, the scarcity of labelled\nmedical image-report pairs presents great challenges in the development of deep\nand large-scale neural networks capable of harnessing the potential artificial\ngeneral intelligence power like large language models (LLMs). In this work, we\npropose customizing off-the-shelf general-purpose large-scale pre-trained\nmodels, i.e., foundation models (FMs), in computer vision and natural language\nprocessing with a specific focus on medical report generation. Specifically,\nfollowing BLIP-2, a state-of-the-art vision-language pre-training approach, we\nintroduce our encoder-decoder-based MRG model. This model utilizes a\nlightweight query Transformer to connect two FMs: the giant vision Transformer\nEVA-ViT-g and a bilingual LLM trained to align with human intentions (referred\nto as ChatGLM-6B). Furthermore, we conduct ablative experiments on the\ntrainable components of the model to identify the crucial factors for effective\ntransfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn\nmedical image representations, followed by parameter-efficient training of\nChatGLM-6B to capture the writing styles of medical reports, is essential for\nachieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and\nthe 2nd, respectively, out of 13 participating teams, based on the BERTScore\nand ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction\nTask competition.",
        "translated": ""
    },
    {
        "title": "Bayesian Knowledge-driven Critiquing with Indirect Evidence",
        "url": "http://arxiv.org/abs/2306.05636v1",
        "pub_date": "2023-06-09",
        "summary": "Conversational recommender systems (CRS) enhance the expressivity and\npersonalization of recommendations through multiple turns of user-system\ninteraction. Critiquing is a well-known paradigm for CRS that allows users to\niteratively refine recommendations by providing feedback about attributes of\nrecommended items. While existing critiquing methodologies utilize direct\nattributes of items to address user requests such as 'I prefer Western movies',\nthe opportunity of incorporating richer contextual and side information about\nitems stored in Knowledge Graphs (KG) into the critiquing paradigm has been\noverlooked. Employing this substantial knowledge together with a\nwell-established reasoning methodology paves the way for critique-based\nrecommenders to allow for complex knowledge-based feedback (e.g., 'I like\nmovies featuring war side effects on veterans') which may arise in natural\nuser-system conversations. In this work, we aim to increase the flexibility of\ncritique-based recommendation by integrating KGs and propose a novel Bayesian\ninference framework that enables reasoning with relational knowledge-based\nfeedback. We study and formulate the framework considering a Gaussian\nlikelihood and evaluate it on two well-known recommendation datasets with KGs.\nOur evaluations demonstrate the effectiveness of our framework in leveraging\nindirect KG-based feedback (i.e., preferred relational properties of items\nrather than preferred items themselves), often improving personalized\nrecommendations over a one-shot recommender by more than 15%. This work enables\na new paradigm for using rich knowledge content and reasoning over indirect\nevidence as a mechanism for critiquing interactions with CRS.",
        "translated": ""
    },
    {
        "title": "CLC: Cluster Assignment via Contrastive Representation Learning",
        "url": "http://arxiv.org/abs/2306.05439v1",
        "pub_date": "2023-06-08",
        "summary": "Clustering remains an important and challenging task of grouping samples into\nclusters without manual annotations. Recent works have achieved excellent\nresults on small datasets by performing clustering on feature representations\nlearned from self-supervised learning. However, for datasets with a large\nnumber of clusters, such as ImageNet, current methods still can not achieve\nhigh clustering performance. In this paper, we propose Contrastive\nLearning-based Clustering (CLC), which uses contrastive learning to directly\nlearn cluster assignment. We decompose the representation into two parts: one\nencodes the categorical information under an equipartition constraint, and the\nother captures the instance-wise factors. We propose a contrastive loss using\nboth parts of the representation. We theoretically analyze the proposed\ncontrastive loss and reveal that CLC sets different weights for the negative\nsamples while learning cluster assignments. Further gradient analysis shows\nthat the larger weights tend to focus more on the hard negative samples.\nTherefore, the proposed loss has high expressiveness that enables us to\nefficiently learn cluster assignments. Experimental evaluation shows that CLC\nachieves overall state-of-the-art or highly competitive clustering performance\non multiple benchmark datasets. In particular, we achieve 53.4% accuracy on the\nfull ImageNet dataset and outperform existing methods by large margins (+\n10.2%).",
        "translated": ""
    },
    {
        "title": "Weakly-Supervised Scientific Document Classification via\n  Retrieval-Augmented Multi-Stage Training",
        "url": "http://arxiv.org/abs/2306.07193v1",
        "pub_date": "2023-06-12",
        "summary": "Scientific document classification is a critical task for a wide range of\napplications, but the cost of obtaining massive amounts of human-labeled data\ncan be prohibitive. To address this challenge, we propose a weakly-supervised\napproach for scientific document classification using label names only. In\nscientific domains, label names often include domain-specific concepts that may\nnot appear in the document corpus, making it difficult to match labels and\ndocuments precisely. To tackle this issue, we propose WANDER, which leverages\ndense retrieval to perform matching in the embedding space to capture the\nsemantics of label names. We further design the label name expansion module to\nenrich the label name representations. Lastly, a self-training step is used to\nrefine the predictions. The experiments on three datasets show that WANDER\noutperforms the best baseline by 11.9% on average. Our code will be published\nat https://github.com/ritaranx/wander.",
        "translated": ""
    },
    {
        "title": "Fair Learning to Rank with Distribution-free Risk Control",
        "url": "http://arxiv.org/abs/2306.07188v1",
        "pub_date": "2023-06-12",
        "summary": "Learning to Rank (LTR) methods are vital in online economies, affecting users\nand item providers. Fairness in LTR models is crucial to allocate exposure\nproportionally to item relevance. The deterministic ranking model can lead to\nunfair exposure distribution when items with the same relevance receive\nslightly different scores. Stochastic LTR models, incorporating the\nPlackett-Luce (PL) model, address fairness issues but have limitations in\ncomputational cost and performance guarantees. To overcome these limitations,\nwe propose FairLTR-RC, a novel post-hoc model-agnostic method. FairLTR-RC\nleverages a pretrained scoring function to create a stochastic LTR model,\neliminating the need for expensive training. Furthermore, FairLTR-RC provides\nfinite-sample guarantees on a user-specified utility using distribution-free\nrisk control framework. By additionally incorporating the Thresholded PL (TPL)\nmodel, we are able to achieve an effective trade-off between utility and\nfairness. Experimental results on several benchmark datasets demonstrate that\nFairLTR-RC significantly improves fairness in widely-used deterministic LTR\nmodels while guaranteeing a specified level of utility.",
        "translated": ""
    },
    {
        "title": "Video-to-Music Recommendation using Temporal Alignment of Segments",
        "url": "http://arxiv.org/abs/2306.07187v1",
        "pub_date": "2023-06-12",
        "summary": "We study cross-modal recommendation of music tracks to be used as soundtracks\nfor videos. This problem is known as the music supervision task. We build on a\nself-supervised system that learns a content association between music and\nvideo. In addition to the adequacy of content, adequacy of structure is crucial\nin music supervision to obtain relevant recommendations. We propose a novel\napproach to significantly improve the system's performance using\nstructure-aware recommendation. The core idea is to consider not only the full\naudio-video clips, but rather shorter segments for training and inference. We\nfind that using semantic segments and ranking the tracks according to sequence\nalignment costs significantly improves the results. We investigate the impact\nof different ranking metrics and segmentation methods.",
        "translated": ""
    },
    {
        "title": "Adversarial Constrained Bidding via Minimax Regret Optimization with\n  Causality-Aware Reinforcement Learning",
        "url": "http://arxiv.org/abs/2306.07106v1",
        "pub_date": "2023-06-12",
        "summary": "The proliferation of the Internet has led to the emergence of online\nadvertising, driven by the mechanics of online auctions. In these repeated\nauctions, software agents participate on behalf of aggregated advertisers to\noptimize for their long-term utility. To fulfill the diverse demands, bidding\nstrategies are employed to optimize advertising objectives subject to different\nspending constraints. Existing approaches on constrained bidding typically rely\non i.i.d. train and test conditions, which contradicts the adversarial nature\nof online ad markets where different parties possess potentially conflicting\nobjectives. In this regard, we explore the problem of constrained bidding in\nadversarial bidding environments, which assumes no knowledge about the\nadversarial factors. Instead of relying on the i.i.d. assumption, our insight\nis to align the train distribution of environments with the potential test\ndistribution meanwhile minimizing policy regret. Based on this insight, we\npropose a practical Minimax Regret Optimization (MiRO) approach that\ninterleaves between a teacher finding adversarial environments for tutoring and\na learner meta-learning its policy over the given distribution of environments.\nIn addition, we pioneer to incorporate expert demonstrations for learning\nbidding strategies. Through a causality-aware policy design, we improve upon\nMiRO by distilling knowledge from the experts. Extensive experiments on both\nindustrial data and synthetic data show that our method, MiRO with\nCausality-aware reinforcement Learning (MiROCL), outperforms prior methods by\nover 30%.",
        "translated": ""
    },
    {
        "title": "Imbalanced Multi-label Classification for Business-related Text with\n  Moderately Large Label Spaces",
        "url": "http://arxiv.org/abs/2306.07046v1",
        "pub_date": "2023-06-12",
        "summary": "In this study, we compared the performance of four different methods for\nmulti label text classification using a specific imbalanced business dataset.\nThe four methods we evaluated were fine tuned BERT, Binary Relevance,\nClassifier Chains, and Label Powerset. The results show that fine tuned BERT\noutperforms the other three methods by a significant margin, achieving high\nvalues of accuracy, F1 Score, Precision, and Recall. Binary Relevance also\nperforms well on this dataset, while Classifier Chains and Label Powerset\ndemonstrate relatively poor performance. These findings highlight the\neffectiveness of fine tuned BERT for multi label text classification tasks, and\nsuggest that it may be a useful tool for businesses seeking to analyze complex\nand multifaceted texts.",
        "translated": ""
    },
    {
        "title": "Skellam Rank: Fair Learning to Rank Algorithm Based on Poisson Process\n  and Skellam Distribution for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.06607v1",
        "pub_date": "2023-06-11",
        "summary": "Recommender system is a widely adopted technology in a diversified class of\nproduct lines. Modern day recommender system approaches include matrix\nfactorization, learning to rank and deep learning paradigms, etc. Unlike many\nother approaches, learning to rank builds recommendation results based on\nmaximization of the probability of ranking orders. There are intrinsic issues\nrelated to recommender systems such as selection bias, exposure bias and\npopularity bias. In this paper, we propose a fair recommender system algorithm\nthat uses Poisson process and Skellam distribution. We demonstrate in our\nexperiments that our algorithm is competitive in accuracy metrics and far more\nsuperior than other modern algorithms in fairness metrics.",
        "translated": ""
    },
    {
        "title": "Mean-Variance Efficient Collaborative Filtering for Stock Recommendation",
        "url": "http://arxiv.org/abs/2306.06590v1",
        "pub_date": "2023-06-11",
        "summary": "The rise of FinTech has transformed financial services onto online platforms,\nyet stock investment recommender systems have received limited attention\ncompared to other industries. Personalized stock recommendations can\nsignificantly impact customer engagement and satisfaction within the industry.\nHowever, traditional investment recommendations focus on high-return stocks or\nhighly diversified portfolios based on the modern portfolio theory, often\nneglecting user preferences. On the other hand, collaborative filtering (CF)\nmethods also may not be directly applicable to stock recommendations, because\nit is inappropriate to just recommend stocks that users like. The key is to\noptimally blend users preference with the portfolio theory. However, research\non stock recommendations within the recommender system domain remains\ncomparatively limited, and no existing model considers both the preference of\nusers and the risk-return characteristics of stocks. In this regard, we propose\na mean-variance efficient collaborative filtering (MVECF) model for stock\nrecommendations that consider both aspects. Our model is specifically designed\nto improve the pareto optimality (mean-variance efficiency) in a trade-off\nbetween the risk (variance of return) and return (mean return) by systemically\nhandling uncertainties in stock prices. Such improvements are incorporated into\nthe MVECF model using regularization, and the model is restructured to fit into\nthe ordinary matrix factorization scheme to boost computational efficiency.\nExperiments on real-world fund holdings data show that our model can increase\nthe mean-variance efficiency of suggested portfolios while sacrificing just a\nsmall amount of mean average precision and recall. Finally, we further show\nMVECF is easily applicable to the state-of-the-art graph-based ranking models.",
        "translated": ""
    },
    {
        "title": "GuP: Fast Subgraph Matching by Guard-based Pruning",
        "url": "http://arxiv.org/abs/2306.06557v1",
        "pub_date": "2023-06-11",
        "summary": "Subgraph matching, which finds subgraphs isomorphic to a query, is the key to\ninformation retrieval from data represented as a graph. To avoid redundant\nexploration in the data, existing methods restrict the search space by\nextracting candidate vertices and candidate edges that may constitute\nisomorphic subgraphs. However, it still requires expensive computation because\ncandidate vertices induce many subgraphs that are not isomorphic to the query.\nIn this paper, we propose GuP, a subgraph matching algorithm with pruning based\non guards. Guards are a pattern of intermediate search states that never find\nisomorphic subgraphs. GuP attaches a guard on each candidate vertex and edge\nand filters out them adaptively to the search state. The experimental results\nshowed that GuP can efficiently solve various queries, including those that the\nstate-of-the-art methods could not solve in practical time.",
        "translated": ""
    },
    {
        "title": "Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain\n  Recommendation in an AI Assistant Application",
        "url": "http://arxiv.org/abs/2306.06302v1",
        "pub_date": "2023-06-09",
        "summary": "Recommender systems have found significant commercial success but still\nstruggle with integrating new users. Since users often interact with content in\ndifferent domains, it is possible to leverage a user's interactions in previous\ndomains to improve that user's recommendations in a new one (multi-domain\nrecommendation). A separate research thread on knowledge graph enhancement uses\nexternal knowledge graphs to improve single domain recommendations (knowledge\ngraph enhancement). Both research threads incorporate related information to\nimprove predictions in a new domain. We propose in this work to unify these\napproaches: Using information from interactions in other domains as well as\nexternal knowledge graphs to make predictions in a new domain that would be\nimpossible with either information source alone. We apply these ideas to a\ndataset derived from millions of users' requests for content across three\ndomains (videos, music, and books) in a live virtual assistant application. We\ndemonstrate the advantage of combining knowledge graph enhancement with\nprevious multi-domain recommendation techniques to provide better overall\nrecommendations as well as for better recommendations on new users of a domain.",
        "translated": ""
    },
    {
        "title": "Open Data on GitHub: Unlocking the Potential of AI",
        "url": "http://arxiv.org/abs/2306.06191v1",
        "pub_date": "2023-06-09",
        "summary": "GitHub is the world's largest platform for collaborative software\ndevelopment, with over 100 million users. GitHub is also used extensively for\nopen data collaboration, hosting more than 800 million open data files,\ntotaling 142 terabytes of data. This study highlights the potential of open\ndata on GitHub and demonstrates how it can accelerate AI research. We analyze\nthe existing landscape of open data on GitHub and the patterns of how users\nshare datasets. Our findings show that GitHub is one of the largest hosts of\nopen data in the world and has experienced an accelerated growth of open data\nassets over the past four years. By examining the open data landscape on\nGitHub, we aim to empower users and organizations to leverage existing open\ndatasets and improve their discoverability -- ultimately contributing to the\nongoing AI revolution to help address complex societal issues. We release the\nthree datasets that we have collected to support this analysis as open datasets\nat https://github.com/github/open-data-on-github.",
        "translated": ""
    },
    {
        "title": "Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal\n  Rank with Lexicographic Precision",
        "url": "http://arxiv.org/abs/2306.07908v1",
        "pub_date": "2023-06-13",
        "summary": "Across a variety of ranking tasks, researchers use reciprocal rank to measure\nthe effectiveness for users interested in exactly one relevant item. Despite\nits widespread use, evidence suggests that reciprocal rank is brittle when\ndiscriminating between systems. This brittleness, in turn, is compounded in\nmodern evaluation settings where current, high-precision systems may be\ndifficult to distinguish. We address the lack of sensitivity of reciprocal rank\nby introducing and connecting it to the concept of best-case retrieval, an\nevaluation method focusing on assessing the quality of a ranking for the most\nsatisfied possible user across possible recall requirements. This perspective\nallows us to generalize reciprocal rank and define a new preference-based\nevaluation we call lexicographic precision or lexiprecision. By mathematical\nconstruction, we ensure that lexiprecision preserves differences detected by\nreciprocal rank, while empirically improving sensitivity and robustness across\na broad set of retrieval and recommendation tasks.",
        "translated": ""
    },
    {
        "title": "ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support\n  Lateral Reading",
        "url": "http://arxiv.org/abs/2306.07875v1",
        "pub_date": "2023-06-13",
        "summary": "With the rapid growth and spread of online misinformation, people need tools\nto help them evaluate the credibility and accuracy of online information.\nLateral reading, a strategy that involves cross-referencing information with\nmultiple sources, may be an effective approach to achieving this goal. In this\npaper, we present ReadProbe, a tool to support lateral reading, powered by\ngenerative large language models from OpenAI and the Bing search engine. Our\ntool is able to generate useful questions for lateral reading, scour the web\nfor relevant documents, and generate well-attributed answers to help people\nbetter evaluate online information. We made a web-based application to\ndemonstrate how ReadProbe can help reduce the risk of being misled by false\ninformation. The code is available at\nhttps://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won\nthe first prize in a national AI misinformation hackathon.",
        "translated": ""
    },
    {
        "title": "KuaiSAR: A Unified Search And Recommendation Dataset",
        "url": "http://arxiv.org/abs/2306.07705v1",
        "pub_date": "2023-06-13",
        "summary": "The confluence of Search and Recommendation services is a vital aspect of\nonline content platforms like Kuaishou and TikTok. The integration of S&amp;R\nmodeling is a highly intuitive approach adopted by industry practitioners.\nHowever, there is a noticeable lack of research conducted in this area within\nthe academia, primarily due to the absence of publicly available datasets.\nConsequently, a substantial gap has emerged between academia and industry\nregarding research endeavors in this field. To bridge this gap, we introduce\nthe first large-scale, real-world dataset KuaiSAR of integrated Search And\nRecommendation behaviors collected from Kuaishou, a leading short-video app in\nChina with over 300 million daily active users. Previous research in this field\nhas predominantly employed publicly available datasets that are semi-synthetic\nand simulated, with artificially fabricated search behaviors. Distinct from\nprevious datasets, KuaiSAR records genuine user behaviors, the occurrence of\neach interaction within either search or recommendation service, and the users'\ntransitions between the two services. This work aids in joint modeling of S&amp;R,\nand the utilization of search data for recommenders (and recommendation data\nfor search engines). Additionally, due to the diverse feedback labels of\nuser-video interactions, KuaiSAR also supports a wide range of other tasks,\nincluding intent recommendation, multi-task learning, and long sequential\nmulti-behavior modeling etc. We believe this dataset will facilitate innovative\nresearch and enrich our understanding of S&amp;R services integration in real-world\napplications.",
        "translated": ""
    },
    {
        "title": "Practice with Graph-based ANN Algorithms on Sparse Data: Chi-square\n  Two-tower model, HNSW, Sign Cauchy Projections",
        "url": "http://arxiv.org/abs/2306.07607v1",
        "pub_date": "2023-06-13",
        "summary": "Sparse data are common. The traditional ``handcrafted'' features are often\nsparse. Embedding vectors from trained models can also be very sparse, for\nexample, embeddings trained via the ``ReLu'' activation function. In this\npaper, we report our exploration of efficient search in sparse data with\ngraph-based ANN algorithms (e.g., HNSW, or SONG which is the GPU version of\nHNSW), which are popular in industrial practice, e.g., search and ads\n(advertising).\n  We experiment with the proprietary ads targeting application, as well as\nbenchmark public datasets. For ads targeting, we train embeddings with the\nstandard ``cosine two-tower'' model and we also develop the ``chi-square\ntwo-tower'' model. Both models produce (highly) sparse embeddings when they are\nintegrated with the ``ReLu'' activation function. In EBR (embedding-based\nretrieval) applications, after we the embeddings are trained, the next crucial\ntask is the approximate near neighbor (ANN) search for serving. While there are\nmany ANN algorithms we can choose from, in this study, we focus on the\ngraph-based ANN algorithm (e.g., HNSW-type).\n  Sparse embeddings should help improve the efficiency of EBR. One benefit is\nthe reduced memory cost for the embeddings. The other obvious benefit is the\nreduced computational time for evaluating similarities, because, for\ngraph-based ANN algorithms such as HNSW, computing similarities is often the\ndominating cost. In addition to the effort on leveraging data sparsity for\nstorage and computation, we also integrate ``sign cauchy random projections''\n(SignCRP) to hash vectors to bits, to further reduce the memory cost and speed\nup the ANN search. In NIPS'13, SignCRP was proposed to hash the chi-square\nsimilarity, which is a well-adopted nonlinear kernel in NLP and computer\nvision. Therefore, the chi-square two-tower model, SignCRP, and HNSW are now\ntightly integrated.",
        "translated": ""
    },
    {
        "title": "Unified Off-Policy Learning to Rank: a Reinforcement Learning\n  Perspective",
        "url": "http://arxiv.org/abs/2306.07528v1",
        "pub_date": "2023-06-13",
        "summary": "Off-policy Learning to Rank (LTR) aims to optimize a ranker from data\ncollected by a deployed logging policy. However, existing off-policy learning\nto rank methods often make strong assumptions about how users generate the\nclick data, i.e., the click model, and hence need to tailor their methods\nspecifically under different click models. In this paper, we unified the\nranking process under general stochastic click models as a Markov Decision\nProcess (MDP), and the optimal ranking could be learned with offline\nreinforcement learning (RL) directly. Building upon this, we leverage offline\nRL techniques for off-policy LTR and propose the Click Model-Agnostic Unified\nOff-policy Learning to Rank (CUOLR) method, which could be easily applied to a\nwide range of click models. Through a dedicated formulation of the MDP, we show\nthat offline RL algorithms can adapt to various click models without complex\ndebiasing techniques and prior knowledge of the model. Results on various\nlarge-scale datasets demonstrate that CUOLR consistently outperforms the\nstate-of-the-art off-policy learning to rank algorithms while maintaining\nconsistency and robustness under different click models.",
        "translated": ""
    },
    {
        "title": "Topic-Centric Explanations for News Recommendation",
        "url": "http://arxiv.org/abs/2306.07506v1",
        "pub_date": "2023-06-13",
        "summary": "News recommender systems (NRS) have been widely applied for online news\nwebsites to help users find relevant articles based on their interests. Recent\nmethods have demonstrated considerable success in terms of recommendation\nperformance. However, the lack of explanation for these recommendations can\nlead to mistrust among users and lack of acceptance of recommendations. To\naddress this issue, we propose a new explainable news model to construct a\ntopic-aware explainable recommendation approach that can both accurately\nidentify relevant articles and explain why they have been recommended, using\ninformation from associated topics. Additionally, our model incorporates two\ncoherence metrics applied to assess topic quality, providing measure of the\ninterpretability of these explanations. The results of our experiments on the\nMIND dataset indicate that the proposed explainable NRS outperforms several\nother baseline systems, while it is also capable of producing interpretable\ntopics compared to those generated by a classical LDA topic model. Furthermore,\nwe present a case study through a real-world example showcasing the usefulness\nof our NRS for generating explanations.",
        "translated": ""
    },
    {
        "title": "Incentivizing High-Quality Content in Online Recommender Systems",
        "url": "http://arxiv.org/abs/2306.07479v1",
        "pub_date": "2023-06-13",
        "summary": "For content recommender systems such as TikTok and YouTube, the platform's\ndecision algorithm shapes the incentives of content producers, including how\nmuch effort the content producers invest in the quality of their content. Many\nplatforms employ online learning, which creates intertemporal incentives, since\ncontent produced today affects recommendations of future content. In this\npaper, we study the incentives arising from online learning, analyzing the\nquality of content produced at a Nash equilibrium. We show that classical\nonline learning algorithms, such as Hedge and EXP3, unfortunately incentivize\nproducers to create low-quality content. In particular, the quality of content\nis upper bounded in terms of the learning rate and approaches zero for typical\nlearning rate schedules. Motivated by this negative result, we design a\ndifferent learning algorithm -- based on punishing producers who create\nlow-quality content -- that correctly incentivizes producers to create\nhigh-quality content. At a conceptual level, our work illustrates the\nunintended impact that a platform's learning algorithm can have on content\nquality and opens the door towards designing platform learning algorithms that\nincentivize the creation of high-quality content.",
        "translated": ""
    },
    {
        "title": "Resources for Brewing BEIR: Reproducible Reference Models and an\n  Official Leaderboard",
        "url": "http://arxiv.org/abs/2306.07471v1",
        "pub_date": "2023-06-13",
        "summary": "BEIR is a benchmark dataset for zero-shot evaluation of information retrieval\nmodels across 18 different domain/task combinations. In recent years, we have\nwitnessed the growing popularity of a representation learning approach to\nbuilding retrieval models, typically using pretrained transformers in a\nsupervised setting. This naturally begs the question: How effective are these\nmodels when presented with queries and documents that differ from the training\ndata? Examples include searching in different domains (e.g., medical or legal\ntext) and with different types of queries (e.g., keywords vs. well-formed\nquestions). While BEIR was designed to answer these questions, our work\naddresses two shortcomings that prevent the benchmark from achieving its full\npotential: First, the sophistication of modern neural methods and the\ncomplexity of current software infrastructure create barriers to entry for\nnewcomers. To this end, we provide reproducible reference implementations that\ncover the two main classes of approaches: learned dense and sparse models.\nSecond, there does not exist a single authoritative nexus for reporting the\neffectiveness of different models on BEIR, which has led to difficulty in\ncomparing different methods. To remedy this, we present an official\nself-service BEIR leaderboard that provides fair and consistent comparisons of\nretrieval models. By addressing both shortcomings, our work facilitates future\nexplorations in a range of interesting research questions that BEIR enables.",
        "translated": ""
    },
    {
        "title": "Web of Things and Trends in Agriculture: A Systematic Literature Review",
        "url": "http://arxiv.org/abs/2306.09079v1",
        "pub_date": "2023-06-15",
        "summary": "In the past few years, the Web of Things (WOT) became a beneficial\ngame-changing technology within the Agriculture domain as it introduces\ninnovative and promising solutions to the Internet of Things (IoT) agricultural\napplications problems by providing its services. WOT provides the support for\nintegration, interoperability for heterogeneous devices, infrastructures,\nplatforms, and the emergence of various other technologies. The main aim of\nthis study is about understanding and providing a growing and existing research\ncontent, issues, and directions for the future regarding WOT-based agriculture.\nTherefore, a systematic literature review (SLR) of research articles is\npresented by categorizing the selected studies published between 2010 and 2020\ninto the following categories: research type, approaches, and their application\ndomains. Apart from reviewing the state-of-the-art articles on WOT solutions\nfor the agriculture field, a taxonomy of WOT-base agriculture application\ndomains has also been presented in this study. A model has also presented to\nshow the picture of WOT based Smart Agriculture. Lastly, the findings of this\nSLR and the research gaps in terms of open issues have been presented to\nprovide suggestions on possible future directions for the researchers for\nfuture research.",
        "translated": ""
    },
    {
        "title": "Fast and Examination-agnostic Reciprocal Recommendation in Matching\n  Markets",
        "url": "http://arxiv.org/abs/2306.09060v1",
        "pub_date": "2023-06-15",
        "summary": "In matching markets such as job posting and online dating platforms, the\nrecommender system plays a critical role in the success of the platform. Unlike\nstandard recommender systems that suggest items to users, reciprocal\nrecommender systems (RRSs) that suggest other users must take into account the\nmutual interests of users. In addition, ensuring that recommendation\nopportunities do not disproportionately favor popular users is essential for\nthe total number of matches and for fairness among users. Existing\nrecommendation methods in matching markets, however, face computational\nchallenges on large-scale platforms and depend on specific examination\nfunctions in the position-based model (PBM). In this paper, we introduce the\nreciprocal recommendation method based on the matching with transferable\nutility (TU matching) model in the context of ranking recommendations in\nmatching markets and propose a fast and examination-model-free algorithm.\nFurthermore, we evaluate our approach on experiments with synthetic data and\nreal-world data from an online dating platform in Japan. Our method performs\nbetter than or as well as existing methods in terms of the total number of\nmatches and works well even in a large-scale dataset for which one existing\nmethod does not work.",
        "translated": ""
    },
    {
        "title": "Mapping Researcher Activity based on Publication Data by means of\n  Transformers",
        "url": "http://arxiv.org/abs/2306.09049v1",
        "pub_date": "2023-06-15",
        "summary": "Modern performance on several natural language processing (NLP) tasks has\nbeen enhanced thanks to the Transformer-based pre-trained language model BERT.\nWe employ this concept to investigate a local publication database. Research\npapers are encoded and clustered to form a landscape view of the scientific\ntopics, in which research is active. Authors working on similar topics can be\nidentified by calculating the similarity between their papers. Based on this,\nwe define a similarity metric between authors. Additionally we introduce the\nconcept of self-similarity to indicate the topical variety of authors.",
        "translated": ""
    },
    {
        "title": "RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation",
        "url": "http://arxiv.org/abs/2306.08947v1",
        "pub_date": "2023-06-15",
        "summary": "In this paper we propose RecFusion, which comprise a set of diffusion models\nfor recommendation. Unlike image data which contain spatial correlations, a\nuser-item interaction matrix, commonly utilized in recommendation, lacks\nspatial relationships between users and items. We formulate diffusion on a 1D\nvector and propose binomial diffusion, which explicitly models binary user-item\ninteractions with a Bernoulli process. We show that RecFusion approaches the\nperformance of complex VAE baselines on the core recommendation setting (top-n\nrecommendation for binary non-sequential feedback) and the most common datasets\n(MovieLens and Netflix). Our proposed diffusion models that are specialized for\n1D and/or binary setups have implications beyond recommendation systems, such\nas in the medical domain with MRI and CT scans.",
        "translated": ""
    },
    {
        "title": "Document Entity Retrieval with Massive and Noisy Pre-training",
        "url": "http://arxiv.org/abs/2306.08937v1",
        "pub_date": "2023-06-15",
        "summary": "Visually-Rich Document Entity Retrieval (VDER) is a type of machine learning\ntask that aims at recovering text spans in the documents for each of the\nentities in question. VDER has gained significant attention in recent years\nthanks to its broad applications in enterprise AI. Unfortunately, as document\nimages often contain personally identifiable information (PII), publicly\navailable data have been scarce, not only because of privacy constraints but\nalso the costs of acquiring annotations. To make things worse, each dataset\nwould often define its own sets of entities, and the non-overlapping entity\nspaces between datasets make it difficult to transfer knowledge between\ndocuments. In this paper, we propose a method to collect massive-scale, noisy,\nand weakly labeled data from the web to benefit the training of VDER models.\nSuch a method will generate a huge amount of document image data to compensate\nfor the lack of training data in many VDER settings. Moreover, the collected\ndataset named DocuNet would not need to be dependent on specific document types\nor entity sets, making it universally applicable to all VDER tasks. Empowered\nby DocuNet, we present a lightweight multimodal architecture named UniFormer,\nwhich can learn a unified representation from text, layout, and image crops\nwithout needing extra visual pertaining. We experiment with our methods on\npopular VDER models in various settings and show the improvements when this\nmassive dataset is incorporated with UniFormer on both classic entity retrieval\nand few-shot learning settings.",
        "translated": ""
    },
    {
        "title": "Community Detection Attack against Collaborative Learning-based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2306.08929v1",
        "pub_date": "2023-06-15",
        "summary": "Collaborative-learning based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while keeping their history of consumed items on their\ndevices. While these solutions seemed appealing for preserving the privacy of\nthe participants at a first glance, recent studies have shown that\ncollaborative learning can be vulnerable to a variety of privacy attacks. In\nthis paper we propose a novel privacy attack called Community Detection Attack\n(CDA), which allows an adversary to discover the members of a community based\non a set of items of her choice (e.g., discovering users interested in LGBT\ncontent). Through experiments on three real recommendation datasets and by\nusing two state-of-the-art recommendation models, we assess the sensitivity of\nan FL-based recommender system as well as two flavors of Gossip Learning-based\nrecommender systems to CDA. Results show that on all models and all datasets,\nthe FL setting is more vulnerable to CDA than Gossip settings. We further\nevaluated two off-the-shelf mitigation strategies, namely differential privacy\n(DP) and a share less policy, which consists in sharing a subset of model\nparameters. Results show a better privacy-utility trade-off for the share less\npolicy compared to DP especially in the Gossip setting.",
        "translated": ""
    },
    {
        "title": "Prompt Performance Prediction for Generative IR",
        "url": "http://arxiv.org/abs/2306.08915v1",
        "pub_date": "2023-06-15",
        "summary": "The ability to predict the performance of a query in Information Retrieval\n(IR) systems has been a longstanding challenge. In this paper, we introduce a\nnovel task called \"Prompt Performance Prediction\" that aims to predict the\nperformance of a query, referred to as a prompt, before obtaining the actual\nsearch results. The context of our task leverages a generative model as an IR\nengine to evaluate the prompts' performance on image retrieval tasks. We\ndemonstrate the plausibility of our task by measuring the correlation\ncoefficient between predicted and actual performance scores across three\ndatasets containing pairs of prompts and generated images. Our results show\npromising performance prediction capabilities, suggesting potential\napplications for optimizing generative IR systems.",
        "translated": ""
    },
    {
        "title": "Description-Enhanced Label Embedding Contrastive Learning for Text\n  Classification",
        "url": "http://arxiv.org/abs/2306.08817v1",
        "pub_date": "2023-06-15",
        "summary": "Text Classification is one of the fundamental tasks in natural language\nprocessing, which requires an agent to determine the most appropriate category\nfor input sentences. Recently, deep neural networks have achieved impressive\nperformance in this area, especially Pre-trained Language Models (PLMs).\nUsually, these methods concentrate on input sentences and corresponding\nsemantic embedding generation. However, for another essential component:\nlabels, most existing works either treat them as meaningless one-hot vectors or\nuse vanilla embedding methods to learn label representations along with model\ntraining, underestimating the semantic information and guidance that these\nlabels reveal. To alleviate this problem and better exploit label information,\nin this paper, we employ Self-Supervised Learning (SSL) in model learning\nprocess and design a novel self-supervised Relation of Relation (R2)\nclassification task for label utilization from a one-hot manner perspective.\nThen, we propose a novel Relation of Relation Learning Network (R2-Net) for\ntext classification, in which text classification and R2 classification are\ntreated as optimization targets. Meanwhile, triplet loss is employed to enhance\nthe analysis of differences and connections among labels. Moreover, considering\nthat one-hot usage is still short of exploiting label information, we\nincorporate external knowledge from WordNet to obtain multi-aspect descriptions\nfor label semantic learning and extend R2-Net to a novel Description-Enhanced\nLabel Embedding network (DELE) from a label embedding perspective. ...",
        "translated": ""
    },
    {
        "title": "ReLoop2: Building Self-Adaptive Recommendation Models via Responsive\n  Error Compensation Loop",
        "url": "http://arxiv.org/abs/2306.08808v1",
        "pub_date": "2023-06-15",
        "summary": "Industrial recommender systems face the challenge of operating in\nnon-stationary environments, where data distribution shifts arise from evolving\nuser behaviors over time. To tackle this challenge, a common approach is to\nperiodically re-train or incrementally update deployed deep models with newly\nobserved data, resulting in a continual training process. However, the\nconventional learning paradigm of neural networks relies on iterative\ngradient-based updates with a small learning rate, making it slow for large\nrecommendation models to adapt. In this paper, we introduce ReLoop2, a\nself-correcting learning loop that facilitates fast model adaptation in online\nrecommender systems through responsive error compensation. Inspired by the\nslow-fast complementary learning system observed in human brains, we propose an\nerror memory module that directly stores error samples from incoming data\nstreams. These stored samples are subsequently leveraged to compensate for\nmodel prediction errors during testing, particularly under distribution shifts.\nThe error memory module is designed with fast access capabilities and undergoes\ncontinual refreshing with newly observed data samples during the model serving\nphase to support fast model adaptation. We evaluate the effectiveness of\nReLoop2 on three open benchmark datasets as well as a real-world production\ndataset. The results demonstrate the potential of ReLoop2 in enhancing the\nresponsiveness and adaptiveness of recommender systems operating in\nnon-stationary environments.",
        "translated": ""
    },
    {
        "title": "Learning to Rank when Grades Matter",
        "url": "http://arxiv.org/abs/2306.08650v1",
        "pub_date": "2023-06-14",
        "summary": "Graded labels are ubiquitous in real-world learning-to-rank applications,\nespecially in human rated relevance data. Traditional learning-to-rank\ntechniques aim to optimize the ranked order of documents. They typically,\nhowever, ignore predicting actual grades. This prevents them from being adopted\nin applications where grades matter, such as filtering out ``poor'' documents.\nAchieving both good ranking performance and good grade prediction performance\nis still an under-explored problem. Existing research either focuses only on\nranking performance by not calibrating model outputs, or treats grades as\nnumerical values, assuming labels are on a linear scale and failing to leverage\nthe ordinal grade information. In this paper, we conduct a rigorous study of\nlearning to rank with grades, where both ranking performance and grade\nprediction performance are important. We provide a formal discussion on how to\nperform ranking with non-scalar predictions for grades, and propose a\nmultiobjective formulation to jointly optimize both ranking and grade\npredictions. In experiments, we verify on several public datasets that our\nmethods are able to push the Pareto frontier of the tradeoff between ranking\nand grade prediction performance, showing the benefit of leveraging ordinal\ngrade information.",
        "translated": ""
    },
    {
        "title": "GRM: Generative Relevance Modeling Using Relevance-Aware Sample\n  Estimation for Document Retrieval",
        "url": "http://arxiv.org/abs/2306.09938v1",
        "pub_date": "2023-06-16",
        "summary": "Recent studies show that Generative Relevance Feedback (GRF), using text\ngenerated by Large Language Models (LLMs), can enhance the effectiveness of\nquery expansion. However, LLMs can generate irrelevant information that harms\nretrieval effectiveness. To address this, we propose Generative Relevance\nModeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more\naccurate weighting of expansion terms. Specifically, we identify similar real\ndocuments for each generated document and use a neural re-ranker to estimate\ntheir relevance. Experiments on three standard document ranking benchmarks show\nthat GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.",
        "translated": ""
    },
    {
        "title": "Smart Sentiment Analysis-based Search Engine Classification Intelligence",
        "url": "http://arxiv.org/abs/2306.09777v1",
        "pub_date": "2023-06-16",
        "summary": "Search engines are widely used for finding information on the internet.\nHowever, there are limitations in the current search approach, such as\nproviding popular but not necessarily relevant results. This research addresses\nthe issue of polysemy in search results by implementing a search function that\ndetermines the sentimentality of the retrieved information. The study utilizes\na web crawler to collect data from the British Broadcasting Corporation (BBC)\nnews site, and the sentimentality of the news articles is determined using the\nSentistrength program. The results demonstrate that the proposed search\nfunction improves recall value while accurately retrieving nonpolysemous news.\nFurthermore, Sentistrength outperforms deep learning and clustering methods in\nclassifying search results. The methodology presented in this article can be\napplied to analyze the sentimentality and reputation of entities on the\ninternet.",
        "translated": ""
    },
    {
        "title": "Online Distillation for Pseudo-Relevance Feedback",
        "url": "http://arxiv.org/abs/2306.09657v1",
        "pub_date": "2023-06-16",
        "summary": "Model distillation has emerged as a prominent technique to improve neural\nsearch models. To date, distillation taken an offline approach, wherein a new\nneural model is trained to predict relevance scores between arbitrary queries\nand documents. In this paper, we explore a departure from this offline\ndistillation strategy by investigating whether a model for a specific query can\nbe effectively distilled from neural re-ranking results (i.e., distilling in an\nonline setting). Indeed, we find that a lexical model distilled online can\nreasonably replicate the re-ranking of a neural model. More importantly, these\nmodels can be used as queries that execute efficiently on indexes. This second\nretrieval stage can enrich the pool of documents for re-ranking by identifying\ndocuments that were missed in the first retrieval stage. Empirically, we show\nthat this approach performs favourably when compared with established pseudo\nrelevance feedback techniques, dense retrieval methods, and sparse-dense\nensemble \"hybrid\" approaches.",
        "translated": ""
    },
    {
        "title": "I Want This, Not That: Personalized Summarization of Scientific\n  Scholarly Texts",
        "url": "http://arxiv.org/abs/2306.09604v1",
        "pub_date": "2023-06-16",
        "summary": "In this paper, we present a proposal for an unsupervised algorithm, P-Summ,\nthat generates an extractive summary of scientific scholarly text to meet the\npersonal knowledge needs of the user. The method delves into the latent\nsemantic space of the document exposed by Weighted Non-negative Matrix\nFactorization, and scores sentences in consonance with the knowledge needs of\nthe user. The novelty of the algorithm lies in its ability to include desired\nknowledge and eliminate unwanted knowledge in the personal summary.\n  We also propose a multi-granular evaluation framework, which assesses the\nquality of generated personal summaries at three levels of granularity -\nsentence, terms and semantic. The framework uses system generated generic\nsummary instead of human generated summary as gold standard for evaluating the\nquality of personal summary generated by the algorithm. The effectiveness of\nthe algorithm at the semantic level is evaluated by taking into account the\nreference summary and the knowledge signals. We evaluate the performance of\nP-Summ algorithm over four data-sets consisting of scientific articles. Our\nempirical investigations reveal that the proposed method has the capability to\nmeet negative (or positive) knowledge preferences of the user.",
        "translated": ""
    },
    {
        "title": "Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive\n  Volume Lines",
        "url": "http://arxiv.org/abs/2306.11612v1",
        "pub_date": "2023-06-20",
        "summary": "To visually compare ensembles of volumes, dynamic volume lines (DVLs)\nrepresent each ensemble member as a 1D polyline. To compute these, the volume\ncells are sorted on a space-filling curve and scaled by the ensemble's local\nvariation. The resulting 1D plot can augment or serve as an alternative to a 3D\nvolume visualization free of visual clutter and occlusion. Interactively\ncomputing DVLs is challenging when the data is large, and the volume grid is\nnot structured/regular, as is often the case with computational fluid dynamics\nsimulations. We extend DVLs to support large-scale, multi-field adaptive mesh\nrefinement (AMR) data that can be explored interactively. Our GPU-based system\nupdates the DVL representation whenever the data or the alpha transfer function\nchanges. We demonstrate and evaluate our interactive prototype using large AMR\nvolumes from astrophysics simulations.",
        "translated": ""
    },
    {
        "title": "Mining Interest Trends and Adaptively Assigning SampleWeight for\n  Session-based Recommendation",
        "url": "http://arxiv.org/abs/2306.11610v1",
        "pub_date": "2023-06-20",
        "summary": "Session-based Recommendation (SR) aims to predict users' next click based on\ntheir behavior within a short period, which is crucial for online platforms.\nHowever, most existing SR methods somewhat ignore the fact that user preference\nis not necessarily strongly related to the order of interactions. Moreover,\nthey ignore the differences in importance between different samples, which\nlimits the model-fitting performance. To tackle these issues, we put forward\nthe method, Mining Interest Trends and Adaptively Assigning Sample Weight,\nabbreviated as MTAW. Specifically, we model users' instant interest based on\ntheir present behavior and all their previous behaviors. Meanwhile, we\ndiscriminatively integrate instant interests to capture the changing trend of\nuser interest to make more personalized recommendations. Furthermore, we devise\na novel loss function that dynamically weights the samples according to their\nprediction difficulty in the current epoch. Extensive experimental results on\ntwo benchmark datasets demonstrate the effectiveness and superiority of our\nmethod.",
        "translated": ""
    },
    {
        "title": "Polytope: An Algorithm for Efficient Feature Extraction on Hypercubes",
        "url": "http://arxiv.org/abs/2306.11553v1",
        "pub_date": "2023-06-20",
        "summary": "Data extraction algorithms on data hypercubes, or datacubes, are\ntraditionally only capable of cutting boxes of data along the datacube axes.\nFor many use cases however, this is not a sufficient approach and returns more\ndata than users might actually need. This not only forces users to apply\npost-processing after extraction, but more importantly this consumes more I/O\nresources than is necessary. When considering very large datacubes from which\nusers only want to extract small non-rectangular subsets, the box approach does\nnot scale well. Indeed, with this traditional approach, I/O systems quickly\nreach capacity, trying to read and return unwanted data to users. In this\npaper, we propose a novel technique, based on computational geometry concepts,\nwhich instead carefully pre-selects the precise bytes of data which the user\nneeds in order to then only read those from the datacube. As we discuss later\non, this novel extraction method will considerably help scale access to large\npetabyte size data hypercubes in a variety of scientific fields.",
        "translated": ""
    },
    {
        "title": "Generative Retrieval as Dense Retrieval",
        "url": "http://arxiv.org/abs/2306.11397v1",
        "pub_date": "2023-06-20",
        "summary": "Generative retrieval is a promising new neural retrieval paradigm that aims\nto optimize the retrieval pipeline by performing both indexing and retrieval\nwith a single transformer model. However, this new paradigm faces challenges\nwith updating the index and scaling to large collections. In this paper, we\nanalyze two prominent variants of generative retrieval and show that they can\nbe conceptually viewed as bi-encoders for dense retrieval. Specifically, we\nanalytically demonstrate that the generative retrieval process can be\ndecomposed into dot products between query and document vectors, similar to\ndense retrieval. This analysis leads us to propose a new variant of generative\nretrieval, called Tied-Atomic, which addresses the updating and scaling issues\nby incorporating techniques from dense retrieval. In experiments on two\ndatasets, NQ320k and the full MSMARCO, we confirm that this approach does not\nreduce retrieval effectiveness while enabling the model to scale to large\ncollections.",
        "translated": ""
    },
    {
        "title": "CAPRI: Context-Aware Interpretable Point-of-Interest Recommendation\n  Framework",
        "url": "http://arxiv.org/abs/2306.11395v1",
        "pub_date": "2023-06-20",
        "summary": "Point-of-Interest (POI ) recommendation systems have gained popularity for\ntheir unique ability to suggest geographical destinations with the\nincorporation of contextual information such as time, location, and user-item\ninteraction. Existing recommendation frameworks lack the contextual fusion\nrequired for POI systems. This paper presents CAPRI, a novel POI recommendation\nframework that effectively integrates context-aware models, such as GeoSoCa,\nLORE, and USG, and introduces a novel strategy for the efficient merging of\ncontextual information. CAPRI integrates an evaluation module that expands the\nevaluation scope beyond accuracy to include novelty, personalization,\ndiversity, and fairness. With an aim to establish a new industry standard for\nreproducible results in the realm of POI recommendation systems, we have made\nCAPRI openly accessible on GitHub, facilitating easy access and contribution to\nthe continued development and refinement of this innovative framework.",
        "translated": ""
    },
    {
        "title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF\n  Synthesis",
        "url": "http://arxiv.org/abs/2306.11296v1",
        "pub_date": "2023-06-20",
        "summary": "We use prompt engineering to guide ChatGPT in the automation of text mining\nof metal-organic frameworks (MOFs) synthesis conditions from diverse formats\nand styles of the scientific literature. This effectively mitigates ChatGPT's\ntendency to hallucinate information -- an issue that previously made the use of\nLarge Language Models (LLMs) in scientific fields challenging. Our approach\ninvolves the development of a workflow implementing three different processes\nfor text mining, programmed by ChatGPT itself. All of them enable parsing,\nsearching, filtering, classification, summarization, and data unification with\ndifferent tradeoffs between labor, speed, and accuracy. We deploy this system\nto extract 26,257 distinct synthesis parameters pertaining to approximately 800\nMOFs sourced from peer-reviewed research articles. This process incorporates\nour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,\nresulting in impressive precision, recall, and F1 scores of 90-99%.\nFurthermore, with the dataset built by text mining, we constructed a\nmachine-learning model with over 86% accuracy in predicting MOF experimental\ncrystallization outcomes and preliminarily identifying important factors in MOF\ncrystallization. We also developed a reliable data-grounded MOF chatbot to\nanswer questions on chemical reactions and synthesis procedures. Given that the\nprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesis\ninformation in a unified format, while using only narrative language requiring\nno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be\nvery useful across various other chemistry sub-disciplines.",
        "translated": ""
    },
    {
        "title": "Representation Sparsification with Hybrid Thresholding for Fast\n  SPLADE-based Document Retrieval",
        "url": "http://arxiv.org/abs/2306.11293v1",
        "pub_date": "2023-06-20",
        "summary": "Learned sparse document representations using a transformer-based neural\nmodel has been found to be attractive in both relevance effectiveness and time\nefficiency. This paper describes a representation sparsification scheme based\non hard and soft thresholding with an inverted index approximation for faster\nSPLADE-based document retrieval. It provides analytical and experimental\nresults on the impact of this learnable hybrid thresholding scheme.",
        "translated": ""
    },
    {
        "title": "Less Can Be More: Exploring Population Rating Dispositions with\n  Partitioned Models in Recommender Systems",
        "url": "http://arxiv.org/abs/2306.11279v1",
        "pub_date": "2023-06-20",
        "summary": "In this study, we partition users by rating disposition - looking first at\ntheir percentage of negative ratings, and then at the general use of the rating\nscale. We hypothesize that users with different rating dispositions may use the\nrecommender system differently and therefore the agreement with their past\nratings may be less predictive of the future agreement.\n  We use data from a large movie rating website to explore whether users should\nbe grouped by disposition, focusing on identifying their various rating\ndistributions that may hurt recommender effectiveness. We find that such\npartitioning not only improves computational efficiency but also improves top-k\nperformance and predictive accuracy. Though such effects are largest for the\nuser-based KNN CF, smaller for item-based KNN CF, and smallest for latent\nfactor algorithms such as SVD.",
        "translated": ""
    },
    {
        "title": "Hybrid Multi-Criteria Preference Ranking by Subsorting",
        "url": "http://arxiv.org/abs/2306.11233v1",
        "pub_date": "2023-06-20",
        "summary": "Multi-criteria recommender systems can improve the quality of recommendations\nby considering user preferences on multiple criteria. One promising approach\nproposed recently is multi-criteria ranking, which uses Pareto ranking to\nassign a ranking score based on the dominance relationship between predicted\nratings across criteria. However, applying Pareto ranking to all criteria may\nresult in non-differentiable ranking scores. To alleviate this issue, we\nproposed a hybrid multi-criteria ranking method by using subsorting. More\nspecifically, we utilize one ranking method as the major sorting approach,\nwhile we apply another preference ordering method as subsorting. Our\nexperimental results on the OpenTable and Yahoo!Movies data present the\nadvantages of this hybrid ranking approach. In addition, the experiments also\nreveal more insights about the sustainability of the multi-criteria ranking for\ntop-N item recommendations.",
        "translated": ""
    },
    {
        "title": "Co-design Hardware and Algorithm for Vector Search",
        "url": "http://arxiv.org/abs/2306.11182v1",
        "pub_date": "2023-06-19",
        "summary": "Vector search has emerged as the foundation for large-scale information\nretrieval and machine learning systems, with search engines like Google and\nBing processing tens of thousands of queries per second on petabyte-scale\ndocument datasets by evaluating vector similarities between encoded query texts\nand web documents. As performance demands for vector search systems surge,\naccelerated hardware offers a promising solution in the post-Moore's Law era.\nWe introduce \\textit{FANNS}, an end-to-end and scalable vector search framework\non FPGAs. Given a user-provided recall requirement on a dataset and a hardware\nresource budget, \\textit{FANNS} automatically co-designs hardware and\nalgorithm, subsequently generating the corresponding accelerator. The framework\nalso supports scale-out by incorporating a hardware TCP/IP stack in the\naccelerator. \\textit{FANNS} attains up to 23.0$\\times$ and 37.2$\\times$ speedup\ncompared to FPGA and CPU baselines, respectively, and demonstrates superior\nscalability to GPUs, achieving 5.5$\\times$ and 7.6$\\times$ speedup in median\nand 95\\textsuperscript{th} percentile (P95) latency within an eight-accelerator\nconfiguration. The remarkable performance of \\textit{FANNS} lays a robust\ngroundwork for future FPGA integration in data centers and AI supercomputers.",
        "translated": ""
    },
    {
        "title": "Knowledge-based Multimodal Music Similarity",
        "url": "http://arxiv.org/abs/2306.12249v1",
        "pub_date": "2023-06-21",
        "summary": "Music similarity is an essential aspect of music retrieval, recommendation\nsystems, and music analysis. Moreover, similarity is of vital interest for\nmusic experts, as it allows studying analogies and influences among composers\nand historical periods. Current approaches to musical similarity rely mainly on\nsymbolic content, which can be expensive to produce and is not always readily\navailable. Conversely, approaches using audio signals typically fail to provide\nany insight about the reasons behind the observed similarity. This research\naddresses the limitations of current approaches by focusing on the study of\nmusical similarity using both symbolic and audio content. The aim of this\nresearch is to develop a fully explainable and interpretable system that can\nprovide end-users with more control and understanding of music similarity and\nclassification systems.",
        "translated": ""
    },
    {
        "title": "CompMix: A Benchmark for Heterogeneous Question Answering",
        "url": "http://arxiv.org/abs/2306.12235v1",
        "pub_date": "2023-06-21",
        "summary": "Fact-centric question answering (QA) often requires access to multiple,\nheterogeneous, information sources. By jointly considering several sources like\na knowledge base (KB), a text collection, and tables from the web, QA systems\ncan enhance their answer coverage and confidence. However, existing QA\nbenchmarks are mostly constructed with a single source of knowledge in mind.\nThis limits capabilities of these benchmarks to fairly evaluate QA systems that\ncan tap into more than one information repository. To bridge this gap, we\nrelease CompMix, a crowdsourced QA benchmark which naturally demands the\nintegration of a mixture of input sources. CompMix has a total of 9,410\nquestions, and features several complex intents like joins and temporal\nconditions. Evaluation of a range of QA systems on CompMix highlights the need\nfor further research on leveraging information from heterogeneous sources.",
        "translated": ""
    },
    {
        "title": "STAN: Stage-Adaptive Network for Multi-Task Recommendation by Learning\n  User Lifecycle-Based Representation",
        "url": "http://arxiv.org/abs/2306.12232v1",
        "pub_date": "2023-06-21",
        "summary": "Recommendation systems play a vital role in many online platforms, with their\nprimary objective being to satisfy and retain users. As directly optimizing\nuser retention is challenging, multiple evaluation metrics are often employed.\nExisting methods generally formulate the optimization of these evaluation\nmetrics as a multitask learning problem, but often overlook the fact that user\npreferences for different tasks are personalized and change over time.\nIdentifying and tracking the evolution of user preferences can lead to better\nuser retention. To address this issue, we introduce the concept of \"user\nlifecycle\", consisting of multiple stages characterized by users' varying\npreferences for different tasks. We propose a novel Stage-Adaptive Network\n(STAN) framework for modeling user lifecycle stages. STAN first identifies\nlatent user lifecycle stages based on learned user preferences, and then\nemploys the stage representation to enhance multi-task learning performance.\nOur experimental results using both public and industrial datasets demonstrate\nthat the proposed model significantly improves multi-task prediction\nperformance compared to state-of-the-art methods, highlighting the importance\nof considering user lifecycle stages in recommendation systems. Furthermore,\nonline A/B testing reveals that our model outperforms the existing model,\nachieving a significant improvement of 3.05% in staytime per user and 0.88% in\nCVR. These results indicate that our approach effectively improves the overall\nefficiency of the multi-task recommendation system.",
        "translated": ""
    },
    {
        "title": "Post-hoc Selection of Pareto-Optimal Solutions in Search and\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.12165v1",
        "pub_date": "2023-06-21",
        "summary": "Information Retrieval (IR) and Recommender Systems (RS) tasks are moving from\ncomputing a ranking of final results based on a single metric to\nmulti-objective problems. Solving these problems leads to a set of\nPareto-optimal solutions, known as Pareto frontier, in which no objective can\nbe further improved without hurting the others. In principle, all the points on\nthe Pareto frontier are potential candidates to represent the best model\nselected with respect to the combination of two, or more, metrics. To our\nknowledge, there are no well-recognized strategies to decide which point should\nbe selected on the frontier. In this paper, we propose a novel, post-hoc,\ntheoretically-justified technique, named \"Population Distance from Utopia\"\n(PDU), to identify and select the one-best Pareto-optimal solution from the\nfrontier. In detail, PDU analyzes the distribution of the points by\ninvestigating how far each point is from its utopia point (the ideal\nperformance for the objectives). The possibility of considering fine-grained\nutopia points allows PDU to select solutions tailored to individual user\npreferences, a novel feature we call \"calibration\". We compare PDU against\nexisting state-of-the-art strategies through extensive experiments on tasks\nfrom both IR and RS. Experimental results show that PDU and combined with\ncalibration notably impact the solution selection. Furthermore, the results\nshow that the proposed framework selects a solution in a principled way,\nirrespective of its position on the frontier, thus overcoming the limits of\nother strategies.",
        "translated": ""
    },
    {
        "title": "Visualizing Relation Between (De)Motivating Topics and Public Stance\n  toward COVID-19 Vaccine",
        "url": "http://arxiv.org/abs/2306.12118v1",
        "pub_date": "2023-06-21",
        "summary": "While social media plays a vital role in communication nowadays,\nmisinformation and trolls can easily take over the conversation and steer\npublic opinion on these platforms. We saw the effect of misinformation during\nthe {COVID-19} pandemic when public health officials faced significant\npush-back while trying to motivate the public to vaccinate. To tackle the\ncurrent and any future threats in emergencies and motivate the public towards a\ncommon goal, it is essential to understand how public motivation shifts and\nwhich topics resonate among the general population. In this study, we proposed\nan interactive visualization tool to inspect and analyze the topics that\nresonated among Twitter-sphere during the {COVID-19} pandemic and understand\nthe key factors that shifted public stance for vaccination. This tool can\neasily be generalized for any scenario for visual analysis and to increase the\ntransparency of social media data for researchers and the general population\nalike.",
        "translated": ""
    },
    {
        "title": "Comparative analysis of various web crawler algorithms",
        "url": "http://arxiv.org/abs/2306.12027v1",
        "pub_date": "2023-06-21",
        "summary": "This presentation focuses on the importance of web crawling and page ranking\nalgorithms in dealing with the massive amount of data present on the World Wide\nWeb. As the web continues to grow exponentially, efficient search and retrieval\nmethods become crucial. Web crawling is a process that converts unstructured\ndata into structured data, enabling effective information retrieval.\nAdditionally, page ranking algorithms play a significant role in assessing the\nquality and popularity of web pages. The presentation explores the background\nof these algorithms and evaluates five different crawling algorithms: Shark\nSearch, Priority-Based Queue, Naive Bayes, Breadth-First, and Depth-First. The\ngoal is to identify the most effective algorithm for crawling web pages. By\nunderstanding these algorithms, we can enhance our ability to navigate the web\nand extract valuable information efficiently.",
        "translated": ""
    },
    {
        "title": "Addressing the Rank Degeneration in Sequential Recommendation via\n  Singular Spectrum Smoothing",
        "url": "http://arxiv.org/abs/2306.11986v1",
        "pub_date": "2023-06-21",
        "summary": "Sequential recommendation (SR) investigates the dynamic user preferences\nmodeling and generates the next-item prediction. The next item preference is\ntypically generated by the affinity between the sequence and item\nrepresentations. However, both sequence and item representations suffer from\nthe rank degeneration issue due to the data sparsity problem. The rank\ndegeneration issue significantly impairs the representations for SR. This\nmotivates us to measure how severe is the rank degeneration issue and alleviate\nthe sequence and item representation rank degeneration issues simultaneously\nfor SR.\n  In this work, we theoretically connect the sequence representation\ndegeneration issue with the item rank degeneration, particularly for short\nsequences and cold items. We also identify the connection between the fast\nsingular value decay phenomenon and the rank collapse issue in transformer\nsequence output and item embeddings. We propose the area under the singular\nvalue curve metric to evaluate the severity of the singular value decay\nphenomenon and use it as an indicator of rank degeneration. We further\nintroduce a novel singular spectrum smoothing regularization to alleviate the\nrank degeneration on both sequence and item sides, which is the Singular\nsPectrum sMoothing for sequential Recommendation (SPMRec). We also establish a\ncorrelation between the ranks of sequence and item embeddings and the rank of\nthe user-item preference prediction matrix, which can affect recommendation\ndiversity. We conduct experiments on four benchmark datasets to demonstrate the\nsuperiority of SPMRec over the state-of-the-art recommendation methods,\nespecially in short sequences. The experiments also demonstrate a strong\nconnection between our proposed singular spectrum smoothing and recommendation\ndiversity.",
        "translated": ""
    },
    {
        "title": "Sampling Individually-Fair Rankings that are Always Group Fair",
        "url": "http://arxiv.org/abs/2306.11964v1",
        "pub_date": "2023-06-21",
        "summary": "Rankings on online platforms help their end-users find the relevant\ninformation -- people, news, media, and products -- quickly. Fair ranking\ntasks, which ask to rank a set of items to maximize utility subject to\nsatisfying group-fairness constraints, have gained significant interest in the\nAlgorithmic Fairness, Information Retrieval, and Machine Learning literature.\nRecent works, however, identify uncertainty in the utilities of items as a\nprimary cause of unfairness and propose introducing randomness in the output.\nThis randomness is carefully chosen to guarantee an adequate representation of\neach item (while accounting for the uncertainty). However, due to this\nrandomness, the output rankings may violate group fairness constraints. We give\nan efficient algorithm that samples rankings from an individually-fair\ndistribution while ensuring that every output ranking is group fair. The\nexpected utility of the output ranking is at least $\\alpha$ times the utility\nof the optimal fair solution. Here, $\\alpha$ depends on the utilities,\nposition-discounts, and constraints -- it approaches 1 as the range of\nutilities or the position-discounts shrinks, or when utilities satisfy\ndistributional assumptions. Empirically, we observe that our algorithm achieves\nindividual and group fairness and that Pareto dominates the state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "Multimodality Fusion for Smart Healthcare: a Journey from Data,\n  Information, Knowledge to Wisdom",
        "url": "http://arxiv.org/abs/2306.11963v1",
        "pub_date": "2023-06-21",
        "summary": "Multimodal medical data fusion has emerged as a transformative approach in\nsmart healthcare, enabling a comprehensive understanding of patient health and\npersonalized treatment plans. In this paper, a journey from data, information,\nand knowledge to wisdom (DIKW) is explored through multimodal fusion for smart\nhealthcare. A comprehensive review of multimodal medical data fusion focuses on\nthe integration of various data modalities are presented. It explores different\napproaches such as Feature selection, Rule-based systems, Machine learning,\nDeep learning, and Natural Language Processing for fusing and analyzing\nmultimodal data. The paper also highlights the challenges associated with\nmultimodal fusion in healthcare. By synthesizing the reviewed frameworks and\ninsights, a generic framework for multimodal medical data fusion is proposed\nwhile aligning with the DIKW mechanism. Moreover, it discusses future\ndirections aligned with the four pillars of healthcare: Predictive, Preventive,\nPersonalized, and Participatory approaches based on the DIKW and the generic\nframework. The components from this comprehensive survey form the foundation\nfor the successful implementation of multimodal fusion in smart healthcare. The\nfindings of this survey can guide researchers and practitioners in leveraging\nthe power of multimodal fusion with the approaches to revolutionize healthcare\nand improve patient outcomes.",
        "translated": ""
    },
    {
        "title": "Retrieval-Based Transformer for Table Augmentation",
        "url": "http://arxiv.org/abs/2306.11843v1",
        "pub_date": "2023-06-20",
        "summary": "Data preparation, also called data wrangling, is considered one of the most\nexpensive and time-consuming steps when performing analytics or building\nmachine learning models. Preparing data typically involves collecting and\nmerging data from complex heterogeneous, and often large-scale data sources,\nsuch as data lakes. In this paper, we introduce a novel approach toward\nautomatic data wrangling in an attempt to alleviate the effort of end-users,\ne.g. data analysts, in structuring dynamic views from data lakes in the form of\ntabular data. We aim to address table augmentation tasks, including row/column\npopulation and data imputation. Given a corpus of tables, we propose a\nretrieval augmented self-trained transformer model. Our self-learning strategy\nconsists in randomly ablating tables from the corpus and training the\nretrieval-based model to reconstruct the original values or headers given the\npartial tables as input. We adopt this strategy to first train the dense neural\nretrieval model encoding table-parts to vectors, and then the end-to-end model\ntrained to perform table augmentation tasks. We test on EntiTables, the\nstandard benchmark for table augmentation, as well as introduce a new benchmark\nto advance further research: WebTables. Our model consistently and\nsubstantially outperforms both supervised statistical methods and the current\nstate-of-the-art transformer-based models.",
        "translated": ""
    },
    {
        "title": "Data augmentation for recommender system: A semi-supervised approach\n  using maximum margin matrix factorization",
        "url": "http://arxiv.org/abs/2306.13050v1",
        "pub_date": "2023-06-22",
        "summary": "Collaborative filtering (CF) has become a popular method for developing\nrecommender systems (RS) where ratings of a user for new items is predicted\nbased on her past preferences and available preference information of other\nusers. Despite the popularity of CF-based methods, their performance is often\ngreatly limited by the sparsity of observed entries. In this study, we explore\nthe data augmentation and refinement aspects of Maximum Margin Matrix\nFactorization (MMMF), a widely accepted CF technique for the rating\npredictions, which have not been investigated before. We exploit the inherent\ncharacteristics of CF algorithms to assess the confidence level of individual\nratings and propose a semi-supervised approach for rating augmentation based on\nself-training. We hypothesize that any CF algorithm's predictions with low\nconfidence are due to some deficiency in the training data and hence, the\nperformance of the algorithm can be improved by adopting a systematic data\naugmentation strategy. We iteratively use some of the ratings predicted with\nhigh confidence to augment the training data and remove low-confidence entries\nthrough a refinement process. By repeating this process, the system learns to\nimprove prediction accuracy. Our method is experimentally evaluated on several\nstate-of-the-art CF algorithms and leads to informative rating augmentation,\nimproving the performance of the baseline approaches.",
        "translated": ""
    },
    {
        "title": "Efficient Partitioning Method of Large-Scale Public Safety\n  Spatio-Temporal Data based on Information Loss Constraints",
        "url": "http://arxiv.org/abs/2306.12857v1",
        "pub_date": "2023-06-22",
        "summary": "The storage, management, and application of massive spatio-temporal data are\nwidely applied in various practical scenarios, including public safety.\nHowever, due to the unique spatio-temporal distribution characteristics of\nre-al-world data, most existing methods have limitations in terms of the\nspatio-temporal proximity of data and load balancing in distributed storage.\nThere-fore, this paper proposes an efficient partitioning method of large-scale\npublic safety spatio-temporal data based on information loss constraints\n(IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal\npoint da-ta by combining the spatio-temporal partitioning module (STPM) with\nthe graph partitioning module (GPM). This approach can significantly reduce the\nscale of data while maintaining the model's accuracy, in order to improve the\npartitioning efficiency. It can also ensure the load balancing of distributed\nstorage while maintaining spatio-temporal proximity of the data partitioning\nresults. This method provides a new solution for distributed storage of\nmas-sive spatio-temporal data. The experimental results on multiple real-world\nda-tasets demonstrate the effectiveness and superiority of IFL-LSTP.",
        "translated": ""
    },
    {
        "title": "HypeRS: Building a Hypergraph-driven ensemble Recommender System",
        "url": "http://arxiv.org/abs/2306.12800v1",
        "pub_date": "2023-06-22",
        "summary": "Recommender systems are designed to predict user preferences over collections\nof items. These systems process users' previous interactions to decide which\nitems should be ranked higher to satisfy their desires. An ensemble recommender\nsystem can achieve great recommendation performance by effectively combining\nthe decisions generated by individual models. In this paper, we propose a novel\nensemble recommender system that combines predictions made by different models\ninto a unified hypergraph ranking framework. This is the first time that\nhypergraph ranking has been employed to model an ensemble of recommender\nsystems. Hypergraphs are generalizations of graphs where multiple vertices can\nbe connected via hyperedges, efficiently modeling high-order relations. We\ndifferentiate real and predicted connections between users and items by\nassigning different hyperedge weights to individual recommender systems. We\nperform experiments using four datasets from the fields of movie, music and\nnews media recommendation. The obtained results show that the ensemble\nhypergraph ranking method generates more accurate recommendations compared to\nthe individual models and a weighted hybrid approach. The assignment of\ndifferent hyperedge weights to the ensemble hypergraph further improves the\nperformance compared to a setting with identical hyperedge weights.",
        "translated": ""
    },
    {
        "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution\n  Perspective",
        "url": "http://arxiv.org/abs/2306.12756v1",
        "pub_date": "2023-06-22",
        "summary": "Recently, we have witnessed generative retrieval increasingly gaining\nattention in the information retrieval (IR) field, which retrieves documents by\ndirectly generating their identifiers. So far, much effort has been devoted to\ndeveloping effective generative retrieval models. There has been less attention\npaid to the robustness perspective. When a new retrieval paradigm enters into\nthe real-world application, it is also critical to measure the\nout-of-distribution (OOD) generalization, i.e., how would generative retrieval\nmodels generalize to new distributions. To answer this question, firstly, we\ndefine OOD robustness from three perspectives in retrieval problems: 1) The\nquery variations; 2) The unforeseen query types; and 3) The unforeseen tasks.\nBased on this taxonomy, we conduct empirical studies to analyze the OOD\nrobustness of several representative generative retrieval models against dense\nretrieval models. The empirical results indicate that the OOD robustness of\ngenerative retrieval models requires enhancement. We hope studying the OOD\nrobustness of generative retrieval models would be advantageous to the IR\ncommunity.",
        "translated": ""
    },
    {
        "title": "Vec2Vec: A Compact Neural Network Approach for Transforming Text\n  Embeddings with High Fidelity",
        "url": "http://arxiv.org/abs/2306.12689v1",
        "pub_date": "2023-06-22",
        "summary": "Vector embeddings have become ubiquitous tools for many language-related\ntasks. A leading embedding model is OpenAI's text-ada-002 which can embed\napproximately 6,000 words into a 1,536-dimensional vector. While powerful,\ntext-ada-002 is not open source and is only available via API. We trained a\nsimple neural network to convert open-source 768-dimensional MPNet embeddings\ninto text-ada-002 embeddings. We compiled a subset of 50,000 online food\nreviews. We calculated MPNet and text-ada-002 embeddings for each review and\ntrained a simple neural network to for 75 epochs. The neural network was\ndesigned to predict the corresponding text-ada-002 embedding for a given MPNET\nembedding. Our model achieved an average cosine similarity of 0.932 on 10,000\nunseen reviews in our held-out test dataset. We manually assessed the quality\nof our predicted embeddings for vector search over text-ada-002-embedded\nreviews. While not as good as real text-ada-002 embeddings, predicted\nembeddings were able to retrieve highly relevant reviews. Our final model,\nVec2Vec, is lightweight (&lt;80 MB) and fast. Future steps include training a\nneural network with a more sophisticated architecture and a larger dataset of\npaired embeddings to achieve greater performance. The ability to convert\nbetween and align embedding spaces may be helpful for interoperability,\nlimiting dependence on proprietary models, protecting data privacy, reducing\ncosts, and offline operations.",
        "translated": ""
    },
    {
        "title": "Recent Developments in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2306.12680v1",
        "pub_date": "2023-06-22",
        "summary": "In this technical survey, we comprehensively summarize the latest\nadvancements in the field of recommender systems. The objective of this study\nis to provide an overview of the current state-of-the-art in the field and\nhighlight the latest trends in the development of recommender systems. The\nstudy starts with a comprehensive summary of the main taxonomy of recommender\nsystems, including personalized and group recommender systems, and then delves\ninto the category of knowledge-based recommender systems. In addition, the\nsurvey analyzes the robustness, data bias, and fairness issues in recommender\nsystems, summarizing the evaluation metrics used to assess the performance of\nthese systems. Finally, the study provides insights into the latest trends in\nthe development of recommender systems and highlights the new directions for\nfuture research in the field.",
        "translated": ""
    },
    {
        "title": "Resources and Evaluations for Multi-Distribution Dense Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2306.12601v1",
        "pub_date": "2023-06-21",
        "summary": "We introduce and define the novel problem of multi-distribution information\nretrieval (IR) where given a query, systems need to retrieve passages from\nwithin multiple collections, each drawn from a different distribution. Some of\nthese collections and distributions might not be available at training time. To\nevaluate methods for multi-distribution retrieval, we design three benchmarks\nfor this task from existing single-distribution datasets, namely, a dataset\nbased on question answering and two based on entity matching. We propose simple\nmethods for this task which allocate the fixed retrieval budget (top-k\npassages) strategically across domains to prevent the known domains from\nconsuming most of the budget. We show that our methods lead to an average of\n3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and\nthat improvements are consistent when fine-tuning different base retrieval\nmodels. Our benchmarks are made publicly available.",
        "translated": ""
    },
    {
        "title": "Fuzzification-based Feature Selection for Enhanced Website Content\n  Encryption",
        "url": "http://arxiv.org/abs/2306.13548v1",
        "pub_date": "2023-06-23",
        "summary": "We propose a novel approach that utilizes fuzzification theory to perform\nfeature selection on website content for encryption purposes. Our objective is\nto identify and select the most relevant features from the website by\nharnessing the principles of fuzzy logic. Fuzzification allows us to transform\nthe crisp website content into fuzzy representations, enabling a more nuanced\nanalysis of their characteristics. By considering the degree of membership of\neach feature in different fuzzy categories, we can evaluate their importance\nand relevance for encryption. This approach enables us to prioritize and focus\non the features that exhibit higher membership degrees, indicating their\nsignificance in the encryption process. By employing fuzzification-based\nfeature selection, we aim to enhance the effectiveness and efficiency of\nwebsite content encryption, ultimately improving the overall internet security.",
        "translated": ""
    },
    {
        "title": "OptMSM: Optimizing Multi-Scenario Modeling for Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2306.13382v1",
        "pub_date": "2023-06-23",
        "summary": "A large-scale industrial recommendation platform typically consists of\nmultiple associated scenarios, requiring a unified click-through rate (CTR)\nprediction model to serve them simultaneously. Existing approaches for\nmulti-scenario CTR prediction generally consist of two main modules: i) a\nscenario-aware learning module that learns a set of multi-functional\nrepresentations with scenario-shared and scenario-specific information from\ninput features, and ii) a scenario-specific prediction module that serves each\nscenario based on these representations. However, most of these approaches\nprimarily focus on improving the former module and neglect the latter module.\nThis can result in challenges such as increased model parameter size, training\ndifficulty, and performance bottlenecks for each scenario. To address these\nissues, we propose a novel framework called OptMSM (\\textbf{Opt}imizing\n\\textbf{M}ulti-\\textbf{S}cenario \\textbf{M}odeling). First, we introduce a\nsimplified yet effective scenario-enhanced learning module to alleviate the\naforementioned challenges. Specifically, we partition the input features into\nscenario-specific and scenario-shared features, which are mapped to specific\ninformation embedding encodings and a set of shared information embeddings,\nrespectively. By imposing an orthogonality constraint on the shared information\nembeddings to facilitate the disentanglement of shared information\ncorresponding to each scenario, we combine them with the specific information\nembeddings to obtain multi-functional representations. Second, we introduce a\nscenario-specific hypernetwork in the scenario-specific prediction module to\ncapture interactions within each scenario more effectively, thereby alleviating\nthe performance bottlenecks. Finally, we conduct extensive offline experiments\nand an online A/B test to demonstrate the effectiveness of OptMSM.",
        "translated": ""
    },
    {
        "title": "Human Activity Behavioural Pattern Recognition in Smarthome with\n  Long-hour Data Collection",
        "url": "http://arxiv.org/abs/2306.13374v1",
        "pub_date": "2023-06-23",
        "summary": "The research on human activity recognition has provided novel solutions to\nmany applications like healthcare, sports, and user profiling. Considering the\ncomplex nature of human activities, it is still challenging even after\neffective and efficient sensors are available. The existing works on human\nactivity recognition using smartphone sensors focus on recognizing basic human\nactivities like sitting, sleeping, standing, stair up and down and running.\nHowever, more than these basic activities is needed to analyze human\nbehavioural pattern. The proposed framework recognizes basic human activities\nusing deep learning models. Also, ambient sensors like PIR, pressure sensors,\nand smartphone-based sensors like accelerometers and gyroscopes are combined to\nmake it hybrid-sensor-based human activity recognition. The hybrid approach\nhelped derive more activities than the basic ones, which also helped derive\nhuman activity patterns or user profiling. User profiling provides sufficient\ninformation to identify daily living activity patterns and predict whether any\nanomaly exists. The framework provides the base for applications such as\nelderly monitoring when they are alone at home. The GRU model's accuracy of\n95\\% is observed to recognize the basic activities. Finally, Human activity\npatterns over time are recognized based on the duration and frequency of the\nactivities. It is observed that human activity pattern, like, morning walking\nduration, varies depending on the day of the week.",
        "translated": ""
    },
    {
        "title": "A Decade of Scholarly Research on Open Knowledge Graphs",
        "url": "http://arxiv.org/abs/2306.13186v1",
        "pub_date": "2023-06-22",
        "summary": "The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.",
        "translated": ""
    },
    {
        "title": "An overview on the evaluated video retrieval tasks at TRECVID 2022",
        "url": "http://arxiv.org/abs/2306.13118v1",
        "pub_date": "2023-06-22",
        "summary": "The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis\nand retrieval evaluation with the goal of promoting progress in research and\ndevelopment of content-based exploitation and retrieval of information from\ndigital video via open, tasks-based evaluation supported by metrology. Over the\nlast twenty-one years this effort has yielded a better understanding of how\nsystems can effectively accomplish such processing and how one can reliably\nbenchmark their performance. TRECVID has been funded by NIST (National\nInstitute of Standards and Technology) and other US government agencies. In\naddition, many organizations and individuals worldwide contribute significant\ntime and effort. TRECVID 2022 planned for the following six tasks: Ad-hoc video\nsearch, Video to text captioning, Disaster scene description and indexing,\nActivity in extended videos, deep video understanding, and movie summarization.\nIn total, 35 teams from various research organizations worldwide signed up to\njoin the evaluation campaign this year. This paper introduces the tasks,\ndatasets used, evaluation frameworks and metrics, as well as a high-level\nresults overview.",
        "translated": ""
    },
    {
        "title": "Scalable Neural Contextual Bandit for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.14834v1",
        "pub_date": "2023-06-26",
        "summary": "High-quality recommender systems ought to deliver both innovative and\nrelevant content through effective and exploratory interactions with users.\nYet, supervised learning-based neural networks, which form the backbone of many\nexisting recommender systems, only leverage recognized user interests, falling\nshort when it comes to efficiently uncovering unknown user preferences. While\nthere has been some progress with neural contextual bandit algorithms towards\nenabling online exploration through neural networks, their onerous\ncomputational demands hinder widespread adoption in real-world recommender\nsystems. In this work, we propose a scalable sample-efficient neural contextual\nbandit algorithm for recommender systems. To do this, we design an epistemic\nneural network architecture, Epistemic Neural Recommendation (ENR), that\nenables Thompson sampling at a large scale. In two distinct large-scale\nexperiments with real-world tasks, ENR significantly boosts click-through rates\nand user ratings by at least 9% and 6% respectively compared to\nstate-of-the-art neural contextual bandit algorithms. Furthermore, it achieves\nequivalent performance with at least 29% fewer user interactions compared to\nthe best-performing baseline algorithm. Remarkably, while accomplishing these\nimprovements, ENR demands orders of magnitude fewer computational resources\nthan neural contextual bandit baseline algorithms.",
        "translated": ""
    },
    {
        "title": "Reciprocal Sequential Recommendation",
        "url": "http://arxiv.org/abs/2306.14712v1",
        "pub_date": "2023-06-26",
        "summary": "Reciprocal recommender system (RRS), considering a two-way matching between\ntwo parties, has been widely applied in online platforms like online dating and\nrecruitment. Existing RRS models mainly capture static user preferences, which\nhave neglected the evolving user tastes and the dynamic matching relation\nbetween the two parties. Although dynamic user modeling has been well-studied\nin sequential recommender systems, existing solutions are developed in a\nuser-oriented manner. Therefore, it is non-trivial to adapt sequential\nrecommendation algorithms to reciprocal recommendation. In this paper, we\nformulate RRS as a distinctive sequence matching task, and further propose a\nnew approach ReSeq for RRS, which is short for Reciprocal Sequential\nrecommendation. To capture dual-perspective matching, we propose to learn\nfine-grained sequence similarities by co-attention mechanism across different\ntime steps. Further, to improve the inference efficiency, we introduce the\nself-distillation technique to distill knowledge from the fine-grained matching\nmodule into the more efficient student module. In the deployment stage, only\nthe efficient student module is used, greatly speeding up the similarity\ncomputation. Extensive experiments on five real-world datasets from two\nscenarios demonstrate the effectiveness and efficiency of the proposed method.\nOur code is available at https://github.com/RUCAIBox/ReSeq/.",
        "translated": ""
    },
    {
        "title": "PTVD: A Large-Scale Plot-Oriented Multimodal Dataset Based on Television\n  Dramas",
        "url": "http://arxiv.org/abs/2306.14644v1",
        "pub_date": "2023-06-26",
        "summary": "Art forms such as movies and television (TV) dramas are reflections of the\nreal world, which have attracted much attention from the multimodal learning\ncommunity recently. However, existing corpora in this domain share three\nlimitations: (1) annotated in a scene-oriented fashion, they ignore the\ncoherence within plots; (2) their text lacks empathy and seldom mentions\nsituational context; (3) their video clips fail to cover long-form relationship\ndue to short duration. To address these fundamental issues, using 1,106 TV\ndrama episodes and 24,875 informative plot-focused sentences written by\nprofessionals, with the help of 449 human annotators, we constructed PTVD, the\nfirst plot-oriented multimodal dataset in the TV domain. It is also the first\nnon-English dataset of its kind. Additionally, PTVD contains more than 26\nmillion bullet screen comments (BSCs), powering large-scale pre-training. Next,\naiming to open-source a strong baseline for follow-up works, we developed the\nmultimodal algorithm that attacks different cinema/TV modelling problems with a\nunified architecture. Extensive experiments on three cognitive-inspired tasks\nyielded a number of novel observations (some of them being quite\ncounter-intuition), further validating the value of PTVD in promoting\nmultimodal research. The dataset and codes are released at\n\\url{https://ptvd.github.io/}.",
        "translated": ""
    },
    {
        "title": "Multi-task Item-attribute Graph Pre-training for Strict Cold-start Item\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.14462v1",
        "pub_date": "2023-06-26",
        "summary": "Recommendation systems suffer in the strict cold-start (SCS) scenario, where\nthe user-item interactions are entirely unavailable. The ID-based approaches\ncompletely fail to work. Cold-start recommenders, on the other hand, leverage\nitem contents to map the new items to the existing ones. However, the existing\nSCS recommenders explore item contents in coarse-grained manners that introduce\nnoise or information loss. Moreover, informative data sources other than item\ncontents, such as users' purchase sequences and review texts, are ignored. We\nexplore the role of the fine-grained item attributes in bridging the gaps\nbetween the existing and the SCS items and pre-train a knowledgeable\nitem-attribute graph for SCS item recommendation. Our proposed framework,\nColdGPT, models item-attribute correlations into an item-attribute graph by\nextracting fine-grained attributes from item contents. ColdGPT then transfers\nknowledge into the item-attribute graph from various available data sources,\ni.e., item contents, historical purchase sequences, and review texts of the\nexisting items, via multi-task learning. To facilitate the positive transfer,\nColdGPT designs submodules according to the natural forms of the data sources\nand coordinates the multiple pre-training tasks via unified\nalignment-and-uniformity losses. Our pre-trained item-attribute graph acts as\nan implicit, extendable item embedding matrix, which enables the SCS item\nembeddings to be easily acquired by inserting these items and propagating their\nattributes' embeddings. We carefully process three public datasets, i.e., Yelp,\nAmazon-home, and Amazon-sports, to guarantee the SCS setting for evaluation.\nExtensive experiments show that ColdGPT consistently outperforms the existing\nSCS recommenders by large margins and even surpasses models that are\npre-trained on 75-224 times more, cross-domain data on two out of four\ndatasets.",
        "translated": ""
    },
    {
        "title": "Contrastive Multi-view Framework for Customer Lifetime Value Prediction",
        "url": "http://arxiv.org/abs/2306.14400v1",
        "pub_date": "2023-06-26",
        "summary": "Accurate customer lifetime value (LTV) prediction can help service providers\noptimize their marketing policies in customer-centric applications. However,\nthe heavy sparsity of consumption events and the interference of data variance\nand noise obstruct LTV estimation. Many existing LTV prediction methods\ndirectly train a single-view LTV predictor on consumption samples, which may\nyield inaccurate and even biased knowledge extraction. In this paper, we\npropose a contrastive multi-view framework for LTV prediction, which is a\nplug-and-play solution compatible with various backbone models. It synthesizes\nmultiple heterogeneous LTV regressors with complementary knowledge to improve\nmodel robustness and captures sample relatedness via contrastive learning to\nmitigate the dependency on data abundance. Concretely, we use a decomposed\nscheme that converts the LTV prediction problem into a combination of\nestimating consumption probability and payment amount. To alleviate the impact\nof noisy data on model learning, we propose a multi-view framework that jointly\noptimizes multiple types of regressors with diverse characteristics and\nadvantages to encode and fuse comprehensive knowledge. To fully exploit the\npotential of limited training samples, we propose a hybrid contrastive learning\nmethod to help capture the relatedness between samples in both classification\nand regression tasks. We conduct extensive experiments on a real-world game LTV\nprediction dataset and the results validate the effectiveness of our method. We\nhave deployed our solution online in Huawei's mobile game center and achieved\n32.26% of total payment amount gains.",
        "translated": ""
    },
    {
        "title": "G-STO: Sequential Main Shopping Intention Detection via\n  Graph-Regularized Stochastic Transformer",
        "url": "http://arxiv.org/abs/2306.14314v1",
        "pub_date": "2023-06-25",
        "summary": "Sequential recommendation requires understanding the dynamic patterns of\nusers' behaviors, contexts, and preferences from their historical interactions.\nMost existing works focus on modeling user-item interactions only from the item\nlevel, ignoring that they are driven by latent shopping intentions (e.g.,\nballpoint pens, miniatures, etc). The detection of the underlying shopping\nintentions of users based on their historical interactions is a crucial aspect\nfor e-commerce platforms, such as Amazon, to enhance the convenience and\nefficiency of their customers' shopping experiences. Despite its significance,\nthe area of main shopping intention detection remains under-investigated in the\nacademic literature. To fill this gap, we propose a graph-regularized\nstochastic Transformer method, G-STO. By considering intentions as sets of\nproducts and user preferences as compositions of intentions, we model both of\nthem as stochastic Gaussian embeddings in the latent representation space.\nInstead of training the stochastic representations from scratch, we develop a\nglobal intention relational graph as prior knowledge for regularization,\nallowing relevant shopping intentions to be distributionally close. Finally, we\nfeed the newly regularized stochastic embeddings into Transformer-based models\nto encode sequential information from the intention transitions. We evaluate\nour main shopping intention identification model on three different real-world\ndatasets, where G-STO achieves significantly superior performances to the\nbaselines by 18.08% in Hit@1, 7.01% in Hit@10, and 6.11% in NDCG@10 on average.",
        "translated": ""
    },
    {
        "title": "RecBaselines2023: a new dataset for choosing baselines for recommender\n  models",
        "url": "http://arxiv.org/abs/2306.14292v1",
        "pub_date": "2023-06-25",
        "summary": "The number of proposed recommender algorithms continues to grow. The authors\npropose new approaches and compare them with existing models, called baselines.\nDue to the large number of recommender models, it is difficult to estimate\nwhich algorithms to choose in the article. To solve this problem, we have\ncollected and published a dataset containing information about the recommender\nmodels used in 903 papers, both as baselines and as proposed approaches. This\ndataset can be seen as a typical dataset with interactions between papers and\npreviously proposed models. In addition, we provide a descriptive analysis of\nthe dataset and highlight possible challenges to be investigated with the data.\nFurthermore, we have conducted extensive experiments using a well-established\nmethodology to build a good recommender algorithm under the dataset. Our\nexperiments show that the selection of the best baselines for proposing new\nrecommender approaches can be considered and successfully solved by existing\nstate-of-the-art collaborative filtering models. Finally, we discuss\nlimitations and future work.",
        "translated": ""
    },
    {
        "title": "Mining Stable Preferences: Adaptive Modality Decorrelation for\n  Multimedia Recommendation",
        "url": "http://arxiv.org/abs/2306.14179v1",
        "pub_date": "2023-06-25",
        "summary": "Multimedia content is of predominance in the modern Web era. In real\nscenarios, multiple modalities reveal different aspects of item attributes and\nusually possess different importance to user purchase decisions. However, it is\ndifficult for models to figure out users' true preference towards different\nmodalities since there exists strong statistical correlation between\nmodalities. Even worse, the strong statistical correlation might mislead models\nto learn the spurious preference towards inconsequential modalities. As a\nresult, when data (modal features) distribution shifts, the learned spurious\npreference might not guarantee to be as effective on the inference set as on\nthe training set. We propose a novel MOdality DEcorrelating STable learning\nframework, MODEST for brevity, to learn users' stable preference. Inspired by\nsample re-weighting techniques, the proposed method aims to estimate a weight\nfor each item, such that the features from different modalities in the weighted\ndistribution are decorrelated. We adopt Hilbert Schmidt Independence Criterion\n(HSIC) as independence testing measure which is a kernel-based method capable\nof evaluating the correlation degree between two multi-dimensional and\nnon-linear variables. Our method could be served as a play-and-plug module for\nexisting multimedia recommendation backbones. Extensive experiments on four\npublic datasets and four state-of-the-art multimedia recommendation backbones\nunequivocally show that our proposed method can improve the performances by a\nlarge margin.",
        "translated": ""
    },
    {
        "title": "Enhancing Dynamic Image Advertising with Vision-Language Pre-training",
        "url": "http://arxiv.org/abs/2306.14112v1",
        "pub_date": "2023-06-25",
        "summary": "In the multimedia era, image is an effective medium in search advertising.\nDynamic Image Advertising (DIA), a system that matches queries with ad images\nand generates multimodal ads, is introduced to improve user experience and ad\nrevenue. The core of DIA is a query-image matching module performing ad image\nretrieval and relevance modeling. Current query-image matching suffers from\nlimited and inconsistent data, and insufficient cross-modal interaction. Also,\nthe separate optimization of retrieval and relevance models affects overall\nperformance. To address this issue, we propose a vision-language framework\nconsisting of two parts. First, we train a base model on large-scale image-text\npairs to learn general multimodal representation. Then, we fine-tune the base\nmodel on advertising business data, unifying relevance modeling and retrieval\nthrough multi-objective learning. Our framework has been implemented in Baidu\nsearch advertising system \"Phoneix Nest\". Online evaluation shows that it\nimproves cost per mille (CPM) and click-through rate (CTR) by 1.04% and 1.865%.",
        "translated": ""
    },
    {
        "title": "Cross-domain Recommender Systems via Multimodal Domain Adaptation",
        "url": "http://arxiv.org/abs/2306.13887v1",
        "pub_date": "2023-06-24",
        "summary": "Collaborative Filtering (CF) has emerged as one of the most prominent\nimplementation strategies for building recommender systems. The key idea is to\nexploit the usage patterns of individuals to generate personalized\nrecommendations. CF techniques, especially for newly launched platforms, often\nface a critical issue known as the data sparsity problem, which greatly limits\ntheir performance. Several approaches have been proposed in the literature to\ntackle the problem of data sparsity, among which cross-domain collaborative\nfiltering (CDCF) has gained significant attention in the recent past. In order\nto compensate for the scarcity of available feedback in a target domain, the\nCDCF approach makes use of information available in other auxiliary domains.\nMost of the traditional CDCF approach aim is to find a common set of entities\n(users or items) across the domains and then use them as a bridge for knowledge\ntransfer. However, most real-world datasets are collected from different\ndomains, so they often lack information about anchor points or reference\ninformation for entity alignment. In this paper, we propose a domain adaptation\ntechnique to align the embeddings of users and items across the two domains.\nOur approach first exploits the available textual and visual information to\nindependently learn a multi-view latent representation for each user and item\nin the auxiliary and target domains. The different representations of a user or\nitem are then fused to generate the corresponding unified representation. A\ndomain classifier is then trained to learn the embedding for the domain\nalignment by fixing the unified features as the anchor points. Experiments on\ntwo publicly benchmark datasets indicate the effectiveness of our proposed\napproach.",
        "translated": ""
    },
    {
        "title": "Unleashing the Power of User Reviews: Exploring Airline Choices at\n  Catania Airport, Italy",
        "url": "http://arxiv.org/abs/2306.15541v1",
        "pub_date": "2023-06-27",
        "summary": "This study aims to investigate the possible relationship between the\nmechanisms of social influence and the choice of airline, through the use of\nnew tools, with the aim of understanding whether they can contribute to a\nbetter understanding of the factors influencing the decisions of consumers in\nthe aviation sector. We have chosen to extract user reviews from well-known\nplatforms: Trustpilot, Google, and Twitter. By combining web scraping\ntechniques, we have been able to collect a comprehensive dataset comprising a\nwide range of user opinions, feedback, and ratings. We then refined the BERT\nmodel to focus on insightful sentiment in the context of airline reviews.\nThrough our analysis, we observed an intriguing trend of average negative\nsentiment scores across various airlines, giving us deeper insight into the\ndynamics between airlines and helping us identify key partnerships, popular\nroutes, and airlines that play a central role in the aeronautical ecosystem of\nCatania airport during the specified period. Our investigation led us to find\nthat, despite an airline having received prestigious awards as a low-cost\nleader in Europe for two consecutive years 2021 and 2022, the \"Catanese\" user\ntends to suffer the dominant position of other companies. Understanding the\nimpact of positive reviews and leveraging sentiment analysis can help airlines\nimprove their reputation, attract more customers, and ultimately gain a\ncompetitive edge in the marketplace.",
        "translated": ""
    },
    {
        "title": "Learning to Rank in Generative Retrieval",
        "url": "http://arxiv.org/abs/2306.15222v1",
        "pub_date": "2023-06-27",
        "summary": "Generative retrieval is a promising new paradigm in text retrieval that\ngenerates identifier strings of relevant passages as the retrieval target. This\nparadigm leverages powerful generation models and represents a new paradigm\ndistinct from traditional learning-to-rank methods. However, despite its rapid\ndevelopment, current generative retrieval methods are still limited. They\ntypically rely on a heuristic function to transform predicted identifiers into\na passage rank list, which creates a gap between the learning objective of\ngenerative retrieval and the desired passage ranking target. Moreover, the\ninherent exposure bias problem of text generation also persists in generative\nretrieval. To address these issues, we propose a novel framework, called LTRGR,\nthat combines generative retrieval with the classical learning-to-rank\nparadigm. Our approach involves training an autoregressive model using a\npassage rank loss, which directly optimizes the autoregressive model toward the\noptimal passage ranking. This framework only requires an additional training\nstep to enhance current generative retrieval systems and does not add any\nburden to the inference stage. We conducted experiments on three public\ndatasets, and our results demonstrate that LTRGR achieves state-of-the-art\nperformance among generative retrieval methods, indicating its effectiveness\nand robustness.",
        "translated": ""
    },
    {
        "title": "Off-Policy Evaluation of Ranking Policies under Diverse User Behavior",
        "url": "http://arxiv.org/abs/2306.15098v1",
        "pub_date": "2023-06-26",
        "summary": "Ranking interfaces are everywhere in online platforms. There is thus an ever\ngrowing interest in their Off-Policy Evaluation (OPE), aiming towards an\naccurate performance evaluation of ranking policies using logged data. A\nde-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides\nan unbiased and consistent value estimate. However, it becomes extremely\ninaccurate in the ranking setup due to its high variance under large action\nspaces. To deal with this problem, previous studies assume either independent\nor cascade user behavior, resulting in some ranking versions of IPS. While\nthese estimators are somewhat effective in reducing the variance, all existing\nestimators apply a single universal assumption to every user, causing excessive\nbias and variance. Therefore, this work explores a far more general formulation\nwhere user behavior is diverse and can vary depending on the user context. We\nshow that the resulting estimator, which we call Adaptive IPS (AIPS), can be\nunbiased under any complex user behavior. Moreover, AIPS achieves the minimum\nvariance among all unbiased estimators based on IPS. We further develop a\nprocedure to identify the appropriate user behavior model to minimize the mean\nsquared error (MSE) of AIPS in a data-driven fashion. Extensive experiments\ndemonstrate that the empirical accuracy improvement can be significant,\nenabling effective OPE of ranking systems even under diverse user behavior.",
        "translated": ""
    },
    {
        "title": "Efficient High-Resolution Template Matching with Vector Quantized\n  Nearest Neighbour Fields",
        "url": "http://arxiv.org/abs/2306.15010v1",
        "pub_date": "2023-06-26",
        "summary": "Template matching is a fundamental problem in computer vision and has\napplications in various fields, such as object detection, image registration,\nand object tracking. The current state-of-the-art methods rely on\nnearest-neighbour (NN) matching in which the query feature space is converted\nto NN space by representing each query pixel with its NN in the template\npixels. The NN-based methods have been shown to perform better in occlusions,\nchanges in appearance, illumination variations, and non-rigid transformations.\nHowever, NN matching scales poorly with high-resolution data and high feature\ndimensions. In this work, we present an NN-based template-matching method which\nefficiently reduces the NN computations and introduces filtering in the NN\nfields to consider deformations. A vector quantization step first represents\nthe template with $k$ features, then filtering compares the template and query\ndistributions over the $k$ features. We show that state-of-the-art performance\nwas achieved in low-resolution data, and our method outperforms previous\nmethods at higher resolution showing the robustness and scalability of the\napproach.",
        "translated": ""
    },
    {
        "title": "SE-PQA: Personalized Community Question Answering",
        "url": "http://arxiv.org/abs/2306.16261v1",
        "pub_date": "2023-06-28",
        "summary": "Personalization in Information Retrieval is a topic studied for a long time.\nNevertheless, there is still a lack of high-quality, real-world datasets to\nconduct large-scale experiments and evaluate models for personalized search.\nThis paper contributes to filling this gap by introducing SE-PQA (StackExchange\n- Personalized Question Answering), a new curated resource to design and\nevaluate personalized models related to the task of community Question\nAnswering (cQA). The contributed dataset includes more than 1 million queries\nand 2 million answers, annotated with a rich set of features modeling the\nsocial interactions among the users of a popular cQA platform. We describe the\ncharacteristics of SE-PQA and detail the features associated with questions and\nanswers. We also provide reproducible baseline methods for the cQA task based\non the resource, including deep learning models and personalization approaches.\nThe results of the preliminary experiments conducted show the appropriateness\nof SE-PQA to train effective cQA models; they also show that personalization\nremarkably improves the effectiveness of all the methods tested. Furthermore,\nwe show the benefits in terms of robustness and generalization of combining\ndata from multiple communities for personalization purposes.",
        "translated": ""
    },
    {
        "title": "Query Understanding in the Age of Large Language Models",
        "url": "http://arxiv.org/abs/2306.16004v1",
        "pub_date": "2023-06-28",
        "summary": "Querying, conversing, and controlling search and information-seeking\ninterfaces using natural language are fast becoming ubiquitous with the rise\nand adoption of large-language models (LLM). In this position paper, we\ndescribe a generic framework for interactive query-rewriting using LLMs. Our\nproposal aims to unfold new opportunities for improved and transparent intent\nunderstanding while building high-performance retrieval systems using LLMs. A\nkey aspect of our framework is the ability of the rewriter to fully specify the\nmachine intent by the search engine in natural language that can be further\nrefined, controlled, and edited before the final retrieval phase. The ability\nto present, interact, and reason over the underlying machine intent in natural\nlanguage has profound implications on transparency, ranking performance, and a\ndeparture from the traditional way in which supervised signals were collected\nfor understanding intents. We detail the concept, backed by initial\nexperiments, along with open questions for this interactive query understanding\nframework.",
        "translated": ""
    },
    {
        "title": "Streamlining Social Media Information Retrieval for Public Health\n  Research with Deep Learning",
        "url": "http://arxiv.org/abs/2306.16001v1",
        "pub_date": "2023-06-28",
        "summary": "The utilization of social media in epidemic surveillance has been well\nestablished. Nonetheless, bias is often introduced when pre-defined lexicons\nare used to retrieve relevant corpus. This study introduces a framework aimed\nat curating extensive dictionaries of medical colloquialisms and Unified\nMedical Language System (UMLS) concepts. The framework comprises three modules:\na BERT-based Named Entity Recognition (NER) model that identifies medical\nentities from social media content, a deep-learning powered normalization\nmodule that standardizes the extracted entities, and a semi-supervised\nclustering module that assigns the most probable UMLS concept to each\nstandardized entity. We applied this framework to COVID-19-related tweets from\nFebruary 1, 2020, to April 30, 2022, generating a symptom dictionary (available\nat https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249\nstandardized entities mapped to 876 UMLS concepts and 38,175 colloquial\nexpressions. This framework demonstrates encouraging potential in addressing\nthe constraints of keyword matching information retrieval in social media-based\npublic health research.",
        "translated": ""
    },
    {
        "title": "Disentangled Variational Auto-encoder Enhanced by Counterfactual Data\n  for Debiasing Recommendation",
        "url": "http://arxiv.org/abs/2306.15961v1",
        "pub_date": "2023-06-28",
        "summary": "Recommender system always suffers from various recommendation biases,\nseriously hindering its development. In this light, a series of debias methods\nhave been proposed in the recommender system, especially for two most common\nbiases, i.e., popularity bias and amplified subjective bias. However, exsisting\ndebias methods usually concentrate on correcting a single bias. Such\nsingle-functionality debiases neglect the bias-coupling issue in which the\nrecommended items are collectively attributed to multiple biases. Besides,\nprevious work cannot tackle the lacking supervised signals brought by sparse\ndata, yet which has become a commonplace in the recommender system. In this\nwork, we introduce a disentangled debias variational auto-encoder\nframework(DB-VAE) to address the single-functionality issue as well as a\ncounterfactual data enhancement method to mitigate the adverse effect due to\nthe data sparsity. In specific, DB-VAE first extracts two types of extreme\nitems only affected by a single bias based on the collier theory, which are\nrespectively employed to learn the latent representation of corresponding\nbiases, thereby realizing the bias decoupling. In this way, the exact unbiased\nuser representation can be learned by these decoupled bias representations.\nFurthermore, the data generation module employs Pearl's framework to produce\nmassive counterfactual data, making up the lacking supervised signals due to\nthe sparse data. Extensive experiments on three real-world datasets demonstrate\nthe effectiveness of our proposed model. Besides, the counterfactual data can\nfurther improve DB-VAE, especially on the dataset with low sparsity.",
        "translated": ""
    },
    {
        "title": "Pb-Hash: Partitioned b-bit Hashing",
        "url": "http://arxiv.org/abs/2306.15944v1",
        "pub_date": "2023-06-28",
        "summary": "Many hashing algorithms including minwise hashing (MinHash), one permutation\nhashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$\nbits. With $k$ hashes for each data vector, the storage would be $B\\times k$\nbits; and when used for large-scale learning, the model size would be\n$2^B\\times k$, which can be expensive. A standard strategy is to use only the\nlowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of\nhashes. In this study, we propose to re-use the hashes by partitioning the $B$\nbits into $m$ chunks, e.g., $b\\times m =B$. Correspondingly, the model size\nbecomes $m\\times 2^b \\times k$, which can be substantially smaller than the\noriginal $2^B\\times k$.\n  Our theoretical analysis reveals that by partitioning the hash values into\n$m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$\nbits would not be as accurate as directly using $B$ bits. This is due to the\ncorrelation from re-using the same hash. On the other hand, our analysis also\nshows that the accuracy would not drop much for (e.g.,) $m=2\\sim 4$. In some\nregions, Pb-Hash still works well even for $m$ much larger than 4. We expect\nPb-Hash would be a good addition to the family of hashing methods/applications\nand benefit industrial practitioners.\n  We verify the effectiveness of Pb-Hash in machine learning tasks, for linear\nSVM models as well as deep learning models. Since the hashed data are\nessentially categorical (ID) features, we follow the standard practice of using\nembedding tables for each hash. With Pb-Hash, we need to design an effective\nstrategy to combine $m$ embeddings. Our study provides an empirical evaluation\non four pooling schemes: concatenation, max pooling, mean pooling, and product\npooling. There is no definite answer which pooling would be always better and\nwe leave that for future study.",
        "translated": ""
    },
    {
        "title": "Confidence-Calibrated Ensemble Dense Phrase Retrieval",
        "url": "http://arxiv.org/abs/2306.15917v1",
        "pub_date": "2023-06-28",
        "summary": "In this paper, we consider the extent to which the transformer-based Dense\nPassage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can\nbe optimized without further pre-training. Our method involves two particular\ninsights: we apply the DPR context encoder at various phrase lengths (e.g.\none-sentence versus five-sentence segments), and we take a\nconfidence-calibrated ensemble prediction over all of these different\nsegmentations. This somewhat exhaustive approach achieves start-of-the-art\nresults on benchmark datasets such as Google NQ and SQuAD. We also apply our\nmethod to domain-specific datasets, and the results suggest how different\ngranularities are optimal for different domains",
        "translated": ""
    },
    {
        "title": "Dimension Independent Mixup for Hard Negative Sample in Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2306.15905v1",
        "pub_date": "2023-06-28",
        "summary": "Collaborative filtering (CF) is a widely employed technique that predicts\nuser preferences based on past interactions. Negative sampling plays a vital\nrole in training CF-based models with implicit feedback. In this paper, we\npropose a novel perspective based on the sampling area to revisit existing\nsampling methods. We point out that current sampling methods mainly focus on\nPoint-wise or Line-wise sampling, lacking flexibility and leaving a significant\nportion of the hard sampling area un-explored. To address this limitation, we\npropose Dimension Independent Mixup for Hard Negative Sampling (DINS), which is\nthe first Area-wise sampling method for training CF-based models. DINS\ncomprises three modules: Hard Boundary Definition, Dimension Independent Mixup,\nand Multi-hop Pooling. Experiments with real-world datasets on both matrix\nfactorization and graph-based models demonstrate that DINS outperforms other\nnegative sampling methods, establishing its effectiveness and superiority. Our\nwork contributes a new perspective, introduces Area-wise sampling, and presents\nDINS as a novel approach that achieves state-of-the-art performance for\nnegative sampling. Our implementations are available in PyTorch.",
        "translated": ""
    },
    {
        "title": "Blockwise Feature Interaction in Recommendation Systems",
        "url": "http://arxiv.org/abs/2306.15881v1",
        "pub_date": "2023-06-28",
        "summary": "Feature interactions can play a crucial role in recommendation systems as\nthey capture complex relationships between user preferences and item\ncharacteristics. Existing methods such as Deep &amp; Cross Network (DCNv2) may\nsuffer from high computational requirements due to their cross-layer\noperations. In this paper, we propose a novel approach called blockwise feature\ninteraction (BFI) to help alleviate this issue. By partitioning the feature\ninteraction process into smaller blocks, we can significantly reduce both the\nmemory footprint and the computational burden. Four variants (denoted by P, Q,\nT, S, respectively) of BFI have been developed and empirically compared. Our\nexperimental results demonstrate that the proposed algorithms achieves close\naccuracy compared to the standard DCNv2, while greatly reducing the\ncomputational overhead and the number of parameters. This paper contributes to\nthe development of efficient recommendation systems by providing a practical\nsolution for improving feature interaction efficiency.",
        "translated": ""
    },
    {
        "title": "Ducho: A Unified Framework for the Extraction of Multimodal Features in\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.17125v1",
        "pub_date": "2023-06-29",
        "summary": "In multimodal-aware recommendation, the extraction of meaningful multimodal\nfeatures is at the basis of high-quality recommendations. Generally, each\nrecommendation framework implements its multimodal extraction procedures with\nspecific strategies and tools. This is limiting for two reasons: (i) different\nextraction strategies do not ease the interdependence among multimodal\nrecommendation frameworks; thus, they cannot be efficiently and fairly\ncompared; (ii) given the large plethora of pre-trained deep learning models\nmade available by different open source tools, model designers do not have\naccess to shared interfaces to extract features. Motivated by the outlined\naspects, we propose Ducho, a unified framework for the extraction of multimodal\nfeatures in recommendation. By integrating three widely-adopted deep learning\nlibraries as backends, namely, TensorFlow, PyTorch, and Transformers, we\nprovide a shared interface to extract and process features where each backend's\nspecific methods are abstracted to the end user. Noteworthy, the extraction\npipeline is easily configurable with a YAML-based file where the user can\nspecify, for each modality, the list of models (and their specific\nbackends/parameters) to perform the extraction. Finally, to make Ducho\naccessible to the community, we build a public Docker image equipped with a\nready-to-use CUDA environment and propose three demos to test its\nfunctionalities for different scenarios and tasks. The GitHub repository and\nthe documentation is accessible at this link:\nhttps://github.com/sisinflab/Ducho.",
        "translated": ""
    },
    {
        "title": "Re-Rank - Expand - Repeat: Adaptive Query Expansion for Document\n  Retrieval Using Words and Entities",
        "url": "http://arxiv.org/abs/2306.17082v1",
        "pub_date": "2023-06-29",
        "summary": "Sparse and dense pseudo-relevance feedback (PRF) approaches perform poorly on\nchallenging queries due to low precision in first-pass retrieval. However,\nrecent advances in neural language models (NLMs) can re-rank relevant documents\nto top ranks, even when few are in the re-ranking pool. This paper first\naddresses the problem of poor pseudo-relevance feedback by simply applying\nre-ranking prior to query expansion and re-executing this query. We find that\nthis change alone can improve the retrieval effectiveness of sparse and dense\nPRF approaches by 5-8%. Going further, we propose a new expansion model, Latent\nEntity Expansion (LEE), a fine-grained word and entity-based relevance\nmodelling incorporating localized features. Finally, we include an \"adaptive\"\ncomponent to the retrieval process, which iteratively refines the re-ranking\npool during scoring using the expansion model, i.e. we \"re-rank - expand -\nrepeat\". Using LEE, we achieve (to our knowledge) the best NDCG, MAP and R@1000\nresults on the TREC Robust 2004 and CODEC adhoc document datasets,\ndemonstrating a significant advancement in expansion effectiveness.",
        "translated": ""
    },
    {
        "title": "Harnessing the Power of Hugging Face Transformers for Predicting Mental\n  Health Disorders in Social Networks",
        "url": "http://arxiv.org/abs/2306.16891v1",
        "pub_date": "2023-06-29",
        "summary": "Early diagnosis of mental disorders and intervention can facilitate the\nprevention of severe injuries and the improvement of treatment results. Using\nsocial media and pre-trained language models, this study explores how\nuser-generated data can be used to predict mental disorder symptoms. Our study\ncompares four different BERT models of Hugging Face with standard machine\nlearning techniques used in automatic depression diagnosis in recent\nliterature. The results show that new models outperform the previous approach\nwith an accuracy rate of up to 97%. Analyzing the results while complementing\npast findings, we find that even tiny amounts of data (like users' bio\ndescriptions) have the potential to predict mental disorders. We conclude that\nsocial media data is an excellent source of mental health screening, and\npre-trained models can effectively automate this critical task.",
        "translated": ""
    },
    {
        "title": "Computing all-vs-all MEMs in grammar-compressed text",
        "url": "http://arxiv.org/abs/2306.16815v1",
        "pub_date": "2023-06-29",
        "summary": "We describe a compression-aware method to compute all-vs-all maximal exact\nmatches (MEM) among strings of a repetitive collection $\\mathcal{T}$. The key\nconcept in our work is the construction of a fully-balanced grammar\n$\\mathcal{G}$ from $\\mathcal{T}$ that meets a property that we call\n\\emph{fix-free}: the expansions of the nonterminals that have the same height\nin the parse tree form a fix-free set (i.e., prefix-free and suffix-free). The\nfix-free property allows us to compute the MEMs of $\\mathcal{T}$ incrementally\nover $\\mathcal{G}$ using a standard suffix-tree-based MEM algorithm, which runs\non a subset of grammar rules at a time and does not decompress nonterminals. By\nmodifying the locally-consistent grammar of Christiansen et al 2020., we show\nhow we can build $\\mathcal{G}$ from $\\mathcal{T}$ in linear time and space. We\nalso demonstrate that our MEM algorithm runs on top of $\\mathcal{G}$ in $O(G\n+occ)$ time and uses $O(\\log G(G+occ))$ bits, where $G$ is the grammar size,\nand $occ$ is the number of MEMs in $\\mathcal{T}$. In the conclusions, we\ndiscuss how our idea can be modified to implement approximate pattern matching\nin compressed space.",
        "translated": ""
    },
    {
        "title": "Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall\n  Classification",
        "url": "http://arxiv.org/abs/2306.16760v1",
        "pub_date": "2023-06-29",
        "summary": "We present working notes on transfer learning with semi-supervised dataset\nannotation for the BirdCLEF 2023 competition, focused on identifying African\nbird species in recorded soundscapes. Our approach utilizes existing\noff-the-shelf models, BirdNET and MixIT, to address representation and labeling\nchallenges in the competition. We explore the embedding space learned by\nBirdNET and propose a process to derive an annotated dataset for supervised\nlearning. Our experiments involve various models and feature engineering\napproaches to maximize performance on the competition leaderboard. The results\ndemonstrate the effectiveness of our approach in classifying bird species and\nhighlight the potential of transfer learning and semi-supervised dataset\nannotation in similar tasks.",
        "translated": ""
    },
    {
        "title": "Multi-Scenario Ranking with Adaptive Feature Learning",
        "url": "http://arxiv.org/abs/2306.16732v1",
        "pub_date": "2023-06-29",
        "summary": "Recently, Multi-Scenario Learning (MSL) is widely used in recommendation and\nretrieval systems in the industry because it facilitates transfer learning from\ndifferent scenarios, mitigating data sparsity and reducing maintenance cost.\nThese efforts produce different MSL paradigms by searching more optimal network\nstructure, such as Auxiliary Network, Expert Network, and Multi-Tower Network.\nIt is intuitive that different scenarios could hold their specific\ncharacteristics, activating the user's intents quite differently. In other\nwords, different kinds of auxiliary features would bear varying importance\nunder different scenarios. With more discriminative feature representations\nrefined in a scenario-aware manner, better ranking performance could be easily\nobtained without expensive search for the optimal network structure.\nUnfortunately, this simple idea is mainly overlooked but much desired in\nreal-world systems.Further analysis also validates the rationality of adaptive\nfeature learning under a multi-scenario scheme. Moreover, our A/B test results\non the Alibaba search advertising platform also demonstrate that Maria is\nsuperior in production environments.",
        "translated": ""
    },
    {
        "title": "Exploring the Representation Power of SPLADE Models",
        "url": "http://arxiv.org/abs/2306.16680v1",
        "pub_date": "2023-06-29",
        "summary": "The SPLADE (SParse Lexical AnD Expansion) model is a highly effective\napproach to learned sparse retrieval, where documents are represented by term\nimpact scores derived from large language models. During training, SPLADE\napplies regularization to ensure postings lists are kept sparse -- with the aim\nof mimicking the properties of natural term distributions -- allowing efficient\nand effective lexical matching and ranking. However, we hypothesize that SPLADE\nmay encode additional signals into common postings lists to further improve\neffectiveness. To explore this idea, we perform a number of empirical analyses\nwhere we re-train SPLADE with different, controlled vocabularies and measure\nhow effective it is at ranking passages. Our findings suggest that SPLADE can\neffectively encode useful ranking signals in documents even when the vocabulary\nis constrained to terms that are not traditionally useful for ranking, such as\nstopwords or even random words.",
        "translated": ""
    },
    {
        "title": "Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of\n  Information Retrieval Models",
        "url": "http://arxiv.org/abs/2306.16668v1",
        "pub_date": "2023-06-29",
        "summary": "As in other fields of artificial intelligence, the information retrieval\ncommunity has grown interested in investigating the power consumption\nassociated with neural models, particularly models of search. This interest has\nbecome particularly relevant as the energy consumption of information retrieval\nmodels has risen with new neural models based on large language models, leading\nto an associated increase of CO2 emissions, albeit relatively low compared to\nfields such as natural language processing.",
        "translated": ""
    },
    {
        "title": "Event Detection from Social Media Stream: Methods, Datasets and\n  Opportunities",
        "url": "http://arxiv.org/abs/2306.16495v1",
        "pub_date": "2023-06-28",
        "summary": "Social media streams contain large and diverse amount of information, ranging\nfrom daily-life stories to the latest global and local events and news.\nTwitter, especially, allows a fast spread of events happening real time, and\nenables individuals and organizations to stay informed of the events happening\nnow. Event detection from social media data poses different challenges from\ntraditional text and is a research area that has attracted much attention in\nrecent years. In this paper, we survey a wide range of event detection methods\nfor Twitter data stream, helping readers understand the recent development in\nthis area. We present the datasets available to the public. Furthermore, a few\nresearch opportunities",
        "translated": ""
    },
    {
        "title": "Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual\n  Question Answering",
        "url": "http://arxiv.org/abs/2306.16478v1",
        "pub_date": "2023-06-28",
        "summary": "This paper studies a category of visual question answering tasks, in which\naccessing external knowledge is necessary for answering the questions. This\ncategory is called outside-knowledge visual question answering (OK-VQA). A\nmajor step in developing OK-VQA systems is to retrieve relevant documents for\nthe given multi-modal query. Current state-of-the-art asymmetric dense\nretrieval model for this task uses an architecture with a multi-modal query\nencoder and a uni-modal document encoder. Such an architecture requires a large\namount of training data for effective performance. We propose an automatic data\ngeneration pipeline for pre-training passage retrieval models for OK-VQA tasks.\nThe proposed approach leads to 26.9% Precision@5 improvements compared to the\ncurrent state-of-the-art asymmetric architecture. Additionally, the proposed\npre-training approach exhibits a good ability in zero-shot retrieval scenarios.",
        "translated": ""
    },
    {
        "title": "Precision Anti-Cancer Drug Selection via Neural Ranking",
        "url": "http://arxiv.org/abs/2306.17771v1",
        "pub_date": "2023-06-30",
        "summary": "Personalized cancer treatment requires a thorough understanding of complex\ninteractions between drugs and cancer cell lines in varying genetic and\nmolecular contexts. To address this, high-throughput screening has been used to\ngenerate large-scale drug response data, facilitating data-driven computational\nmodels. Such models can capture complex drug-cell line interactions across\nvarious contexts in a fully data-driven manner. However, accurately\nprioritizing the most sensitive drugs for each cell line still remains a\nsignificant challenge. To address this, we developed neural ranking approaches\nthat leverage large-scale drug response data across multiple cell lines from\ndiverse cancer types. Unlike existing approaches that primarily utilize\nregression and classification techniques for drug response prediction, we\nformulated the objective of drug selection and prioritization as a drug ranking\nproblem. In this work, we proposed two neural listwise ranking methods that\nlearn latent representations of drugs and cell lines, and then use those\nrepresentations to score drugs in each cell line via a learnable scoring\nfunction. Specifically, we developed a neural listwise ranking method,\nList-One, on top of the existing method ListNet. Additionally, we proposed a\nnovel listwise ranking method, List-All, that focuses on all the sensitive\ndrugs instead of the top sensitive drug, unlike List-One. Our results\ndemonstrate that List-All outperforms the best baseline with significant\nimprovements of as much as 8.6% in hit@20 across 50% test cell lines.\nFurthermore, our analyses suggest that the learned latent spaces from our\nproposed methods demonstrate informative clustering structures and capture\nrelevant underlying biological features. Moreover, our comprehensive empirical\nevaluation provides a thorough and objective comparison of the performance of\ndifferent methods (including our proposed ones).",
        "translated": ""
    },
    {
        "title": "Outcome-based Evaluation of Systematic Review Automation",
        "url": "http://arxiv.org/abs/2306.17614v1",
        "pub_date": "2023-06-30",
        "summary": "Current methods of evaluating search strategies and automated citation\nscreening for systematic literature reviews typically rely on counting the\nnumber of relevant and not relevant publications. This established practice,\nhowever, does not accurately reflect the reality of conducting a systematic\nreview, because not all included publications have the same influence on the\nfinal outcome of the systematic review. More specifically, if an important\npublication gets excluded or included, this might significantly change the\noverall review outcome, while not including or excluding less influential\nstudies may only have a limited impact. However, in terms of evaluation\nmeasures, all inclusion and exclusion decisions are treated equally and,\ntherefore, failing to retrieve publications with little to no impact on the\nreview outcome leads to the same decrease in recall as failing to retrieve\ncrucial publications. We propose a new evaluation framework that takes into\naccount the impact of the reported study on the overall systematic review\noutcome. We demonstrate the framework by extracting review meta-analysis data\nand estimating outcome effects using predictions from ranking runs on\nsystematic reviews of interventions from CLEF TAR 2019 shared task. We further\nmeasure how closely the obtained outcomes are to the outcomes of the original\nreview if the arbitrary rankings were used. We evaluate 74 runs using the\nproposed framework and compare the results with those obtained using standard\nIR measures. We find that accounting for the difference in review outcomes\nleads to a different assessment of the quality of a system than if traditional\nevaluation measures were used. Our analysis provides new insights into the\nevaluation of retrieval results in the context of systematic review automation,\nemphasising the importance of assessing the usefulness of each document beyond\nbinary relevance.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting",
        "url": "http://arxiv.org/abs/2306.17563v1",
        "pub_date": "2023-06-30",
        "summary": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, there has been limited success so far, as researchers have\nfound it difficult to outperform fine-tuned baseline rankers on benchmark\ndatasets. We analyze pointwise and listwise ranking prompts used by existing\nmethods and argue that off-the-shelf LLMs do not fully understand these ranking\nformulations, possibly due to the nature of how LLMs are trained. In this\npaper, we propose to significantly reduce the burden on LLMs by using a new\ntechnique called Pairwise Ranking Prompting (PRP). Our results are the first in\nthe literature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on\nthe Flan-UL2 model with 20B parameters outperforms the previous best approach\nin the literature, which is based on the blackbox commercial GPT-4 that has 50x\n(estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only\ninferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while\noutperforming other existing solutions, such as InstructGPT which has 175B\nparameters, by over 10% for nearly all ranking metrics. Furthermore, we propose\nseveral variants of PRP to improve efficiency and show that it is possible to\nachieve competitive results even with linear complexity. We also discuss other\nbenefits of PRP, such as supporting both generation and scoring LLM APIs, as\nwell as being insensitive to input ordering.",
        "translated": ""
    },
    {
        "title": "Leveraging Watch-time Feedback for Short-Video Recommendations: A Causal\n  Labeling Framework",
        "url": "http://arxiv.org/abs/2306.17426v1",
        "pub_date": "2023-06-30",
        "summary": "With the proliferation of short video applications, the significance of short\nvideo recommendations has vastly increased. Unlike other recommendation\nscenarios, short video recommendation systems heavily rely on feedback from\nwatch time. Existing approaches simply treat watch time as a direct label,\nfailing to effectively harness its extensive semantics and introduce bias,\nthereby limiting the potential for modeling user interests based on watch time.\nTo overcome this challenge, we propose a framework named Debiasied\nMultiple-semantics-extracting Labeling (DML). DML constructs labels that\nencompass various semantics by utilizing quantiles derived from the\ndistribution of watch time, prioritizing relative order rather than absolute\nlabel values. This approach facilitates easier model learning while aligning\nwith the ranking objective of recommendations. Furthermore, we introduce a\nmethod inspired by causal adjustment to refine label definitions, thereby\nreducing the impact of bias on the label and directly mitigating bias at the\nlabel level. We substantiate the effectiveness of our DML framework through\nboth online and offline experiments. Extensive results demonstrate that our DML\ncould effectively leverage watch time to discover users' real interests,\nenhancing their engagement in our application.",
        "translated": ""
    },
    {
        "title": "Audio Embeddings as Teachers for Music Classification",
        "url": "http://arxiv.org/abs/2306.17424v1",
        "pub_date": "2023-06-30",
        "summary": "Music classification has been one of the most popular tasks in the field of\nmusic information retrieval. With the development of deep learning models, the\nlast decade has seen impressive improvements in a wide range of classification\ntasks. However, the increasing model complexity makes both training and\ninference computationally expensive. In this paper, we integrate the ideas of\ntransfer learning and feature-based knowledge distillation and systematically\ninvestigate using pre-trained audio embeddings as teachers to guide the\ntraining of low-complexity student networks. By regularizing the feature space\nof the student networks with the pre-trained embeddings, the knowledge in the\nteacher embeddings can be transferred to the students. We use various\npre-trained audio embeddings and test the effectiveness of the method on the\ntasks of musical instrument classification and music auto-tagging. Results show\nthat our method significantly improves the results in comparison to the\nidentical model trained without the teacher's knowledge. This technique can\nalso be combined with classical knowledge distillation approaches to further\nimprove the model's performance.",
        "translated": ""
    },
    {
        "title": "DeepTagger: Knowledge Enhanced Named Entity Recognition for Web-Based\n  Ads Queries",
        "url": "http://arxiv.org/abs/2306.17413v1",
        "pub_date": "2023-06-30",
        "summary": "Named entity recognition (NER) is a crucial task for online advertisement.\nState-of-the-art solutions leverage pre-trained language models for this task.\nHowever, three major challenges remain unresolved: web queries differ from\nnatural language, on which pre-trained models are trained; web queries are\nshort and lack contextual information; and labeled data for NER is scarce. We\npropose DeepTagger, a knowledge-enhanced NER model for web-based ads queries.\nThe proposed knowledge enhancement framework leverages both model-free and\nmodel-based approaches. For model-free enhancement, we collect unlabeled web\nqueries to augment domain knowledge; and we collect web search results to\nenrich the information of ads queries. We further leverage effective prompting\nmethods to automatically generate labels using large language models such as\nChatGPT. Additionally, we adopt a model-based knowledge enhancement method\nbased on adversarial data augmentation. We employ a three-stage training\nframework to train DeepTagger models. Empirical results in various NER tasks\ndemonstrate the effectiveness of the proposed framework.",
        "translated": ""
    },
    {
        "title": "Towards Personalized Cold-Start Recommendation with Prompts",
        "url": "http://arxiv.org/abs/2306.17256v1",
        "pub_date": "2023-06-29",
        "summary": "Recommender systems play a crucial role in helping users discover information\nthat aligns with their interests based on their past behaviors. However,\ndeveloping personalized recommendation systems becomes challenging when\nhistorical records of user-item interactions are unavailable, leading to what\nis known as the system cold-start recommendation problem. This issue is\nparticularly prominent in start-up businesses or platforms with insufficient\nuser engagement history. Previous studies focus on user or item cold-start\nscenarios, where systems could make recommendations for new users or items but\nare still trained with historical user-item interactions in the same domain,\nwhich cannot solve our problem. To bridge the gap, our research introduces an\ninnovative and effective approach, capitalizing on the capabilities of\npre-trained language models. We transform the recommendation process into\nsentiment analysis of natural languages containing information of user profiles\nand item attributes, where the sentiment polarity is predicted with prompt\nlearning. By harnessing the extensive knowledge housed within language models,\nthe prediction can be made without historical user-item interaction records. A\nbenchmark is also introduced to evaluate the proposed method under the\ncold-start setting, and the results demonstrate the effectiveness of our\nmethod. To the best of our knowledge, this is the first study to tackle the\nsystem cold-start recommendation problem. The benchmark and implementation of\nthe method are available at https://github.com/JacksonWuxs/PromptRec.",
        "translated": ""
    },
    {
        "title": "ChatGPT vs. Google: A Comparative Study of Search Performance and User\n  Experience",
        "url": "http://arxiv.org/abs/2307.01135v1",
        "pub_date": "2023-07-03",
        "summary": "The advent of ChatGPT, a large language model-powered chatbot, has prompted\nquestions about its potential implications for traditional search engines. In\nthis study, we investigate the differences in user behavior when employing\nsearch engines and chatbot tools for information-seeking tasks. We carry out a\nrandomized online experiment, dividing participants into two groups: one using\na ChatGPT-like tool and the other using a Google Search-like tool. Our findings\nreveal that the ChatGPT group consistently spends less time on all tasks, with\nno significant difference in overall task performance between the groups.\nNotably, ChatGPT levels user search performance across different education\nlevels and excels in answering straightforward questions and providing general\nsolutions but falls short in fact-checking tasks. Users perceive ChatGPT's\nresponses as having higher information quality compared to Google Search,\ndespite displaying a similar level of trust in both tools. Furthermore,\nparticipants using ChatGPT report significantly better user experiences in\nterms of usefulness, enjoyment, and satisfaction, while perceived ease of use\nremains comparable between the two tools. However, ChatGPT may also lead to\noverreliance and generate or replicate misinformation, yielding inconsistent\nresults. Our study offers valuable insights for search engine management and\nhighlights opportunities for integrating chatbot technologies into search\nengine designs.",
        "translated": ""
    },
    {
        "title": "OpenSiteRec: An Open Dataset for Site Recommendation",
        "url": "http://arxiv.org/abs/2307.00856v1",
        "pub_date": "2023-07-03",
        "summary": "As a representative information retrieval task, site recommendation, which\naims at predicting the optimal sites for a brand or an institution to open new\nbranches in an automatic data-driven way, is beneficial and crucial for brand\ndevelopment in modern business. However, there is no publicly available dataset\nso far and most existing approaches are limited to an extremely small scope of\nbrands, which seriously hinders the research on site recommendation. Therefore,\nwe collect, construct and release an open comprehensive dataset, namely\nOpenSiteRec, to facilitate and promote the research on site recommendation.\nSpecifically, OpenSiteRec leverages a heterogeneous graph schema to represent\nvarious types of real-world entities and relations in four international\nmetropolises. To evaluate the performance of the existing general methods on\nthe site recommendation task, we conduct benchmarking experiments of several\nrepresentative recommendation models on OpenSiteRec. Furthermore, we also\nhighlight the potential application directions to demonstrate the wide\napplicability of OpenSiteRec. We believe that our OpenSiteRec dataset is\nsignificant and anticipated to encourage the development of advanced methods\nfor site recommendation. OpenSiteRec is available online at\nhttps://OpenSiteRec.github.io/.",
        "translated": ""
    },
    {
        "title": "Looks Can Be Deceiving: Linking User-Item Interactions and User's\n  Propensity Towards Multi-Objective Recommendations",
        "url": "http://arxiv.org/abs/2307.00654v1",
        "pub_date": "2023-07-02",
        "summary": "Multi-objective recommender systems (MORS) provide suggestions to users\naccording to multiple (and possibly conflicting) goals. When a system optimizes\nits results at the individual-user level, it tailors them on a user's\npropensity towards the different objectives. Hence, the capability to\nunderstand users' fine-grained needs towards each goal is crucial. In this\npaper, we present the results of a user study in which we monitored the way\nusers interacted with recommended items, as well as their self-proclaimed\npropensities towards relevance, novelty and diversity objectives. The study was\ndivided into several sessions, where users evaluated recommendation lists\noriginating from a relevance-only single-objective baseline as well as MORS. We\nshow that despite MORS-based recommendations attracted less selections, its\npresence in the early sessions is crucial for users' satisfaction in the later\nstages. Surprisingly, the self-proclaimed willingness of users to interact with\nnovel and diverse items is not always reflected in the recommendations they\naccept. Post-study questionnaires provide insights on how to deal with this\nmatter, suggesting that MORS-based results should be accompanied by elements\nthat allow users to understand the recommendations, so as to facilitate their\nacceptance.",
        "translated": ""
    },
    {
        "title": "BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed\n  Search Logs for Zero-shot Biomedical Information Retrieval",
        "url": "http://arxiv.org/abs/2307.00589v1",
        "pub_date": "2023-07-02",
        "summary": "Information retrieval (IR) is essential in biomedical knowledge acquisition\nand clinical decision support. While recent progress has shown that language\nmodel encoders perform better semantic retrieval, training such models requires\nabundant query-article annotations that are difficult to obtain in biomedicine.\nAs a result, most biomedical IR systems only conduct lexical matching. In\nresponse, we introduce BioCPT, a first-of-its-kind Contrastively Pre-trained\nTransformer model for zero-shot biomedical IR. To train BioCPT, we collected an\nunprecedented scale of 255 million user click logs from PubMed. With such data,\nwe use contrastive learning to train a pair of closely-integrated retriever and\nre-ranker. Experimental results show that BioCPT sets new state-of-the-art\nperformance on five biomedical IR tasks, outperforming various baselines\nincluding much larger models such as GPT-3-sized cpt-text-XL. In addition,\nBioCPT also generates better biomedical article and sentence representations\nfor semantic evaluations. As such, BioCPT can be readily applied to various\nreal-world biomedical IR tasks. BioCPT API and code are publicly available at\nhttps://github.com/ncbi/BioCPT.",
        "translated": ""
    },
    {
        "title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text",
        "url": "http://arxiv.org/abs/2307.00509v1",
        "pub_date": "2023-07-02",
        "summary": "The task of textual geolocation - retrieving the coordinates of a place based\non a free-form language description - calls for not only grounding but also\nnatural language understanding and geospatial reasoning. Even though there are\nquite a few datasets in English used for geolocation, they are currently based\non open-source data (Wikipedia and Twitter), where the location of the\ndescribed place is mostly implicit, such that the location retrieval resolution\nis limited. Furthermore, there are no datasets available for addressing the\nproblem of textual geolocation in morphologically rich and resource-poor\nlanguages, such as Hebrew. In this paper, we present the Hebrew Geo-Location\n(HeGeL) corpus, designed to collect literal place descriptions and analyze\nlingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place\ndescriptions of various place types in three cities in Israel. Qualitative and\nempirical analysis show that the data exhibits abundant use of geospatial\nreasoning and requires a novel environmental representation.",
        "translated": ""
    },
    {
        "title": "Text based Large Language Model for Recommendation",
        "url": "http://arxiv.org/abs/2307.00457v1",
        "pub_date": "2023-07-02",
        "summary": "In recent years, large language models (LLM) have emerged as powerful tools\nfor diverse natural language processing tasks. However, their potential for\nrecommendation systems remains relatively unexplored. This paper presents an\ninnovative approach to recommendation systems using large language models\n(LLMs) based on text data. In this paper, we present a novel text-based large\nlanguage model for recommendation (TBLLMR) that utilized the expressive power\nof LLM to generate personalized recommendation. TBLLMR uses LLM's understanding\nability to interpret context, learn user preferences, and generate relevant\nrecommendation. Our proposed approach leverages the vast knowledge encoded in\nlarge language models to accomplish recommendation tasks. We first we formulate\nspecialized prompts to enhance the ability of LLM to comprehend recommendation\ntasks. Subsequently, we use these prompts to fine-tune the model on a dataset\nof user-item interactions, represented by textual data, to capture user\npreferences and item characteristics. Our research underscores the potential of\ntext-based LLMs in revolutionizing the domain of recommendation systems and\noffers a foundational framework for future explorations in this field. We\nconduct extensive experiments on benchmark datasets, and the experiments shows\nthat our TBLLMR has significant better results on large dataset.",
        "translated": ""
    },
    {
        "title": "One Copy Is All You Need: Resource-Efficient Streaming of Medical\n  Imaging Data at Scale",
        "url": "http://arxiv.org/abs/2307.00438v1",
        "pub_date": "2023-07-01",
        "summary": "Large-scale medical imaging datasets have accelerated development of\nartificial intelligence tools for clinical decision support. However, the large\nsize of these datasets is a bottleneck for users with limited storage and\nbandwidth. Many users may not even require such large datasets as AI models are\noften trained on lower resolution images. If users could directly download at\ntheir desired resolution, storage and bandwidth requirements would\nsignificantly decrease. However, it is impossible to anticipate every users'\nrequirements and impractical to store the data at multiple resolutions. What if\nwe could store images at a single resolution but send them at different ones?\nWe propose MIST, an open-source framework to operationalize progressive\nresolution for streaming medical images at multiple resolutions from a single\nhigh-resolution copy. We demonstrate that MIST can dramatically reduce imaging\ninfrastructure inefficiencies for hosting and streaming medical images by &gt;90%,\nwhile maintaining diagnostic quality for deep learning applications.",
        "translated": ""
    },
    {
        "title": "Effective Matching of Patients to Clinical Trials using Entity\n  Extraction and Neural Re-ranking",
        "url": "http://arxiv.org/abs/2307.00381v1",
        "pub_date": "2023-07-01",
        "summary": "Clinical trials (CTs) often fail due to inadequate patient recruitment. This\npaper tackles the challenges of CT retrieval by presenting an approach that\naddresses the patient-to-trials paradigm. Our approach involves two key\ncomponents in a pipeline-based model: (i) a data enrichment technique for\nenhancing both queries and documents during the first retrieval stage, and (ii)\na novel re-ranking schema that uses a Transformer network in a setup adapted to\nthis task by leveraging the structure of the CT documents. We use named entity\nrecognition and negation detection in both patient description and the\neligibility section of CTs. We further classify patient descriptions and CT\neligibility criteria into current, past, and family medical conditions. This\nextracted information is used to boost the importance of disease and drug\nmentions in both query and index for lexical retrieval. Furthermore, we propose\na two-step training schema for the Transformer network used to re-rank the\nresults from the lexical retrieval. The first step focuses on matching patient\ninformation with the descriptive sections of trials, while the second step aims\nto determine eligibility by matching patient information with the criteria\nsection. Our findings indicate that the inclusion criteria section of the CT\nhas a great influence on the relevance score in lexical models, and that the\nenrichment techniques for queries and documents improve the retrieval of\nrelevant trials. The re-ranking strategy, based on our training schema,\nconsistently enhances CT retrieval and shows improved performance by 15\\% in\nterms of precision at retrieving eligible trials. The results of our\nexperiments suggest the benefit of making use of extracted entities. Moreover,\nour proposed re-ranking schema shows promising effectiveness compared to larger\nneural models, even with limited training data.",
        "translated": ""
    },
    {
        "title": "Improving Text Matching in E-Commerce Search with A Rationalizable,\n  Intervenable and Fast Entity-Based Relevance Model",
        "url": "http://arxiv.org/abs/2307.00370v1",
        "pub_date": "2023-07-01",
        "summary": "Discovering the intended items of user queries from a massive repository of\nitems is one of the main goals of an e-commerce search system. Relevance\nprediction is essential to the search system since it helps improve\nperformance. When online serving a relevance model, the model is required to\nperform fast and accurate inference. Currently, the widely used models such as\nBi-encoder and Cross-encoder have their limitations in accuracy or inference\nspeed respectively. In this work, we propose a novel model called the\nEntity-Based Relevance Model (EBRM). We identify the entities contained in an\nitem and decompose the QI (query-item) relevance problem into multiple QE\n(query-entity) relevance problems; we then aggregate their results to form the\nQI prediction using a soft logic formulation. The decomposition allows us to\nuse a Cross-encoder QE relevance module for high accuracy as well as cache QE\npredictions for fast online inference. Utilizing soft logic makes the\nprediction procedure interpretable and intervenable. We also show that\npretraining the QE module with auto-generated QE data from user logs can\nfurther improve the overall performance. The proposed method is evaluated on\nlabeled data from e-commerce websites. Empirical results show that it achieves\npromising improvements with computation efficiency.",
        "translated": ""
    },
    {
        "title": "Improving Multitask Retrieval by Promoting Task Specialization",
        "url": "http://arxiv.org/abs/2307.00342v1",
        "pub_date": "2023-07-01",
        "summary": "In multitask retrieval, a single retriever is trained to retrieve relevant\ncontexts for multiple tasks. Despite its practical appeal, naive multitask\nretrieval lags behind task-specific retrieval in which a separate retriever is\ntrained for each task. We show that it is possible to train a multitask\nretriever that outperforms task-specific retrievers by promoting task\nspecialization. The main ingredients are: (1) a better choice of pretrained\nmodel (one that is explicitly optimized for multitasking) along with compatible\nprompting, and (2) a novel adaptive learning method that encourages each\nparameter to specialize in a particular task. The resulting multitask retriever\nis highly performant on the KILT benchmark. Upon analysis, we find that the\nmodel indeed learns parameters that are more task-specialized compared to naive\nmultitasking without prompting or adaptive learning.",
        "translated": ""
    },
    {
        "title": "MultiVENT: Multilingual Videos of Events with Aligned Natural Text",
        "url": "http://arxiv.org/abs/2307.03153v1",
        "pub_date": "2023-07-06",
        "summary": "Everyday news coverage has shifted from traditional broadcasts towards a wide\nrange of presentation formats such as first-hand, unedited video footage.\nDatasets that reflect the diverse array of multimodal, multilingual news\nsources available online could be used to teach models to benefit from this\nshift, but existing news video datasets focus on traditional news broadcasts\nproduced for English-speaking audiences. We address this limitation by\nconstructing MultiVENT, a dataset of multilingual, event-centric videos\ngrounded in text documents across five target languages. MultiVENT includes\nboth news broadcast videos and non-professional event footage, which we use to\nanalyze the state of online news videos and how they can be leveraged to build\nrobust, factually accurate models. Finally, we provide a model for complex,\nmultilingual video retrieval to serve as a baseline for information retrieval\nusing MultiVENT.",
        "translated": ""
    },
    {
        "title": "Track Mix Generation on Music Streaming Services using Transformers",
        "url": "http://arxiv.org/abs/2307.03045v1",
        "pub_date": "2023-07-06",
        "summary": "This paper introduces Track Mix, a personalized playlist generation system\nreleased in 2022 on the music streaming service Deezer. Track Mix automatically\ngenerates \"mix\" playlists inspired by initial music tracks, allowing users to\ndiscover music similar to their favorite content. To generate these mixes, we\nconsider a Transformer model trained on millions of track sequences from user\nplaylists. In light of the growing popularity of Transformers in recent years,\nwe analyze the advantages, drawbacks, and technical challenges of using such a\nmodel for mix generation on the service, compared to a more traditional\ncollaborative filtering approach. Since its release, Track Mix has been\ngenerating playlists for millions of users daily, enhancing their music\ndiscovery experience on Deezer.",
        "translated": ""
    },
    {
        "title": "Improving Retrieval-Augmented Large Language Models via Data Importance\n  Learning",
        "url": "http://arxiv.org/abs/2307.03027v1",
        "pub_date": "2023-07-06",
        "summary": "Retrieval augmentation enables large language models to take advantage of\nexternal knowledge, for example on tasks like question answering and data\nimputation. However, the performance of such retrieval-augmented models is\nlimited by the data quality of their underlying retrieval corpus. In this\npaper, we propose an algorithm based on multilinear extension for evaluating\nthe data importance of retrieved data points. There are exponentially many\nterms in the multilinear extension, and one key contribution of this paper is a\npolynomial time algorithm that computes exactly, given a retrieval-augmented\nmodel with an additive utility function and a validation set, the data\nimportance of data points in the retrieval corpus using the multilinear\nextension of the model's utility function. We further proposed an even more\nefficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental\nresults illustrate that we can enhance the performance of large language models\nby only pruning or reweighting the retrieval corpus, without requiring further\ntraining. For some tasks, this even allows a small model (e.g., GPT-JT),\naugmented with a search engine API, to outperform GPT-3.5 (without retrieval\naugmentation). Moreover, we show that weights based on multilinear extension\ncan be computed efficiently in practice (e.g., in less than ten minutes for a\ncorpus with 100 million elements).",
        "translated": ""
    },
    {
        "title": "A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System\n  Ranking Consistency and Discriminative Power",
        "url": "http://arxiv.org/abs/2307.02936v1",
        "pub_date": "2023-07-06",
        "summary": "Recently, Moffat et al. proposed an analytic framework, namely C/W/L/A, for\noffline evaluation metrics. This framework allows information retrieval (IR)\nresearchers to design evaluation metrics through the flexible combination of\nuser browsing models and user gain aggregations. However, the statistical\nstability of C/W/L/A metrics with different aggregations is not yet\ninvestigated. In this study, we investigate the statistical stability of\nC/W/L/A metrics from the perspective of: (1) the system ranking similarity\namong aggregations, (2) the system ranking consistency of aggregations and (3)\nthe discriminative power of aggregations. More specifically, we combined\nvarious aggregation functions with the browsing model of Precision, Discounted\nCumulative Gain (DCG), Rank-Biased Precision (RBP), INST, Average Precision\n(AP) and Expected Reciprocal Rank (ERR), examing their performances in terms of\nsystem ranking similarity, system ranking consistency and discriminative power\non two offline test collections. Our experimental result suggests that, in\nterms of system ranking consistency and discriminative power, the aggregation\nfunction of expected rate of gain (ERG) has an outstanding performance while\nthe aggregation function of maximum relevance usually has an insufficient\nperformance. The result also suggests that Precision, DCG, RBP, INST and AP\nwith their canonical aggregation all have favourable performances in system\nranking consistency and discriminative power; but for ERR, replacing its\ncanonical aggregation with ERG can further strengthen the discriminative power\nwhile obtaining a system ranking list similar to the canonical version at the\nsame time.",
        "translated": ""
    },
    {
        "title": "PLIERS: a Popularity-Based Recommender System for Content Dissemination\n  in Online Social Networks",
        "url": "http://arxiv.org/abs/2307.02865v1",
        "pub_date": "2023-07-06",
        "summary": "In this paper, we propose a novel tag-based recommender system called PLIERS,\nwhich relies on the assumption that users are mainly interested in items and\ntags with similar popularity to those they already own. PLIERS is aimed at\nreaching a good tradeoff between algorithmic complexity and the level of\npersonalization of recommended items. To evaluate PLIERS, we performed a set of\nexperiments on real OSN datasets, demonstrating that it outperforms\nstate-of-the-art solutions in terms of personalization, relevance, and novelty\nof recommendations.",
        "translated": ""
    },
    {
        "title": "BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by\n  Eliminating Ideological Segregation in Knowledge-based Recommendations",
        "url": "http://arxiv.org/abs/2307.02797v1",
        "pub_date": "2023-07-06",
        "summary": "In the realm of personalized recommendation systems, the increasing concern\nis the amplification of belief imbalance and user biases, a phenomenon\nprimarily attributed to the filter bubble. Addressing this critical issue, we\nintroduce an innovative intermediate agency (BHEISR) between users and existing\nrecommendation systems to attenuate the negative repercussions of the filter\nbubble effect in extant recommendation systems. The main objective is to strike\na belief balance for users while minimizing the detrimental influence caused by\nfilter bubbles. The BHEISR model amalgamates principles from nudge theory while\nupholding democratic and transparent principles. It harnesses user-specific\ncategory information to stimulate curiosity, even in areas users might\ninitially deem uninteresting. By progressively stimulating interest in novel\ncategories, the model encourages users to broaden their belief horizons and\nexplore the information they typically overlook. Our model is time-sensitive\nand operates on a user feedback loop. It utilizes the existing recommendation\nalgorithm of the model and incorporates user feedback from the prior time\nframe. This approach endeavors to transcend the constraints of the filter\nbubble, enrich recommendation diversity, and strike a belief balance among\nusers while also catering to user preferences and system-specific business\nrequirements. To validate the effectiveness and reliability of the BHEISR\nmodel, we conducted a series of comprehensive experiments with real-world\ndatasets. These experiments compared the performance of the BHEISR model\nagainst several baseline models using nearly 200 filter bubble-impacted users\nas test subjects. Our experimental results conclusively illustrate the superior\nperformance of the BHEISR model in mitigating filter bubbles and balancing user\nperspectives.",
        "translated": ""
    },
    {
        "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start\n  Recommendation",
        "url": "http://arxiv.org/abs/2307.02761v1",
        "pub_date": "2023-07-06",
        "summary": "Multimedia recommendation aims to fuse the multi-modal information of items\nfor feature enrichment to improve the recommendation performance. However,\nexisting methods typically introduce multi-modal information based on\ncollaborative information to improve the overall recommendation precision,\nwhile failing to explore its cold-start recommendation performance. Meanwhile,\nthese above methods are only applicable when such multi-modal data is\navailable. To address this problem, this paper proposes a recommendation\nframework, named Cross-modal Content Inference and Feature Enrichment\nRecommendation (CIERec), which exploits the multi-modal information to improve\nits cold-start recommendation performance. Specifically, CIERec first\nintroduces image annotation as the privileged information to help guide the\nmapping of unified features from the visual space to the semantic space in the\ntraining phase. And then CIERec enriches the content representation with the\nfusion of collaborative, visual, and cross-modal inferred representations, so\nas to improve its cold-start recommendation performance. Experimental results\non two real-world datasets show that the content representations learned by\nCIERec are able to achieve superior cold-start recommendation performance over\nexisting visually-aware recommendation algorithms. More importantly, CIERec can\nconsistently achieve significant improvements with different conventional\nvisually-aware backbones, which verifies its universality and effectiveness.",
        "translated": ""
    },
    {
        "title": "Knowledge Graph Self-Supervised Rationalization for Recommendation",
        "url": "http://arxiv.org/abs/2307.02759v1",
        "pub_date": "2023-07-06",
        "summary": "In this paper, we introduce a new self-supervised rationalization method,\ncalled KGRec, for knowledge-aware recommender systems. To effectively identify\ninformative knowledge connections, we propose an attentive knowledge\nrationalization mechanism that generates rational scores for knowledge\ntriplets. With these scores, KGRec integrates generative and contrastive\nself-supervised tasks for recommendation through rational masking. To highlight\nrationales in the knowledge graph, we design a novel generative task in the\nform of masking-reconstructing. By masking important knowledge with high\nrational scores, KGRec is trained to rebuild and highlight useful knowledge\nconnections that serve as rationales. To further rationalize the effect of\ncollaborative interactions on knowledge graph learning, we introduce a\ncontrastive learning task that aligns signals from knowledge and user-item\ninteraction views. To ensure noise-resistant contrasting, potential noisy edges\nin both graphs judged by the rational scores are masked. Extensive experiments\non three real-world datasets demonstrate that KGRec outperforms\nstate-of-the-art methods. We also provide the implementation codes for our\napproach at https://github.com/HKUDS/KGRec.",
        "translated": ""
    },
    {
        "title": "Dense Retrieval Adaptation using Target Domain Description",
        "url": "http://arxiv.org/abs/2307.02740v1",
        "pub_date": "2023-07-06",
        "summary": "In information retrieval (IR), domain adaptation is the process of adapting a\nretrieval model to a new domain whose data distribution is different from the\nsource domain. Existing methods in this area focus on unsupervised domain\nadaptation where they have access to the target document collection or\nsupervised (often few-shot) domain adaptation where they additionally have\naccess to (limited) labeled data in the target domain. There also exists\nresearch on improving zero-shot performance of retrieval models with no\nadaptation. This paper introduces a new category of domain adaptation in IR\nthat is as-yet unexplored. Here, similar to the zero-shot setting, we assume\nthe retrieval model does not have access to the target document collection. In\ncontrast, it does have access to a brief textual description that explains the\ntarget domain. We define a taxonomy of domain attributes in retrieval tasks to\nunderstand different properties of a source domain that can be adapted to a\ntarget domain. We introduce a novel automatic data construction pipeline that\nproduces a synthetic document collection, query set, and pseudo relevance\nlabels, given a textual domain description. Extensive experiments on five\ndiverse target domains show that adapting dense retrieval models using the\nconstructed synthetic data leads to effective retrieval performance on the\ntarget domain.",
        "translated": ""
    },
    {
        "title": "Improving Address Matching using Siamese Transformer Networks",
        "url": "http://arxiv.org/abs/2307.02300v1",
        "pub_date": "2023-07-05",
        "summary": "Matching addresses is a critical task for companies and post offices involved\nin the processing and delivery of packages. The ramifications of incorrectly\ndelivering a package to the wrong recipient are numerous, ranging from harm to\nthe company's reputation to economic and environmental costs. This research\nintroduces a deep learning-based model designed to increase the efficiency of\naddress matching for Portuguese addresses. The model comprises two parts: (i) a\nbi-encoder, which is fine-tuned to create meaningful embeddings of Portuguese\npostal addresses, utilized to retrieve the top 10 likely matches of the\nun-normalized target address from a normalized database, and (ii) a\ncross-encoder, which is fine-tuned to accurately rerank the 10 addresses\nobtained by the bi-encoder. The model has been tested on a real-case scenario\nof Portuguese addresses and exhibits a high degree of accuracy, exceeding 95%\nat the door level. When utilized with GPU computations, the inference speed is\nabout 4.5 times quicker than other traditional approaches such as BM25. An\nimplementation of this system in a real-world scenario would substantially\nincrease the effectiveness of the distribution process. Such an implementation\nis currently under investigation.",
        "translated": ""
    },
    {
        "title": "A Network Resource Allocation Recommendation Method with An Improved\n  Similarity Measure",
        "url": "http://arxiv.org/abs/2307.03399v1",
        "pub_date": "2023-07-07",
        "summary": "Recommender systems have been acknowledged as efficacious tools for managing\ninformation overload. Nevertheless, conventional algorithms adopted in such\nsystems primarily emphasize precise recommendations and, consequently, overlook\nother vital aspects like the coverage, diversity, and novelty of items. This\napproach results in less exposure for long-tail items. In this paper, to\npersonalize the recommendations and allocate recommendation resources more\npurposively, a method named PIM+RA is proposed. This method utilizes a\nbipartite network that incorporates self-connecting edges and weights.\nFurthermore, an improved Pearson correlation coefficient is employed for better\nredistribution. The evaluation of PIM+RA demonstrates a significant enhancement\nnot only in accuracy but also in coverage, diversity, and novelty of the\nrecommendation. It leads to a better balance in recommendation frequency by\nproviding effective exposure to long-tail items, while allowing customized\nparameters to adjust the recommendation list bias.",
        "translated": ""
    },
    {
        "title": "InfoSync: Information Synchronization across Multilingual\n  Semi-structured Tables",
        "url": "http://arxiv.org/abs/2307.03313v1",
        "pub_date": "2023-07-06",
        "summary": "Information Synchronization of semi-structured data across languages is\nchallenging. For instance, Wikipedia tables in one language should be\nsynchronized across languages. To address this problem, we introduce a new\ndataset InfoSyncC and a two-step method for tabular synchronization. InfoSync\ncontains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages,\nof which a subset (3.5K pairs) are manually annotated. The proposed method\nincludes 1) Information Alignment to map rows and 2) Information Update for\nupdating missing/outdated information for aligned tables across multilingual\ntables. When evaluated on InfoSync, information alignment achieves an F1 score\nof 87.91 (en &lt;-&gt; non-en). To evaluate information updation, we perform\nhuman-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach\nobtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of\nthe proposed method.",
        "translated": ""
    },
    {
        "title": "Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph\n  Reasoning",
        "url": "http://arxiv.org/abs/2307.03591v1",
        "pub_date": "2023-07-06",
        "summary": "Multimodal knowledge graphs (MKGs), which intuitively organize information in\nvarious modalities, can benefit multiple practical downstream tasks, such as\nrecommendation systems, and visual question answering. However, most MKGs are\nstill far from complete, which motivates the flourishing of MKG reasoning\nmodels. Recently, with the development of general artificial architectures, the\npretrained transformer models have drawn increasing attention, especially for\nmultimodal scenarios. However, the research of multimodal pretrained\ntransformer (MPT) for knowledge graph reasoning (KGR) is still at an early\nstage. As the biggest difference between MKG and other multimodal data, the\nrich structural information underlying the MKG still cannot be fully leveraged\nin existing MPT models. Most of them only utilize the graph structure as a\nretrieval map for matching images and texts connected with the same entity.\nThis manner hinders their reasoning performances. To this end, we propose the\ngraph Structure Guided Multimodal Pretrained Transformer for knowledge graph\nreasoning, termed SGMPT. Specifically, the graph structure encoder is adopted\nfor structural feature encoding. Then, a structure-guided fusion module with\ntwo different strategies, i.e., weighted summation and alignment constraint, is\nfirst designed to inject the structural information into both the textual and\nvisual features. To the best of our knowledge, SGMPT is the first MPT model for\nmultimodal KGR, which mines the structural information underlying the knowledge\ngraph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that\nour SGMPT outperforms existing state-of-the-art models, and prove the\neffectiveness of the designed strategies.",
        "translated": ""
    },
    {
        "title": "Undecimated Wavelet Transform for Word Embedded Semantic Marginal\n  Autoencoder in Security improvement and Denoising different Languages",
        "url": "http://arxiv.org/abs/2307.03679v1",
        "pub_date": "2023-07-06",
        "summary": "By combining the undecimated wavelet transform within a Word Embedded\nSemantic Marginal Autoencoder (WESMA), this research study provides a novel\nstrategy for improving security measures and denoising multiple languages. The\nincorporation of these strategies is intended to address the issues of\nrobustness, privacy, and multilingualism in data processing applications. The\nundecimated wavelet transform is used as a feature extraction tool to identify\nprominent language patterns and structural qualities in the input data. The\nproposed system may successfully capture significant information while\npreserving the temporal and geographical links within the data by employing\nthis transform. This improves security measures by increasing the system's\nability to detect abnormalities, discover hidden patterns, and distinguish\nbetween legitimate content and dangerous threats. The Word Embedded Semantic\nMarginal Autoencoder also functions as an intelligent framework for\ndimensionality and noise reduction. The autoencoder effectively learns the\nunderlying semantics of the data and reduces noise components by exploiting\nword embeddings and semantic context. As a result, data quality and accuracy\nare increased in following processing stages. The suggested methodology is\ntested using a diversified dataset that includes several languages and security\nscenarios. The experimental results show that the proposed approach is\neffective in attaining security enhancement and denoising capabilities across\nmultiple languages. The system is strong in dealing with linguistic variances,\nproducing consistent outcomes regardless of the language used. Furthermore,\nincorporating the undecimated wavelet transform considerably improves the\nsystem's ability to efficiently address complex security concerns",
        "translated": ""
    },
    {
        "title": "Fairness and Diversity in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2307.04644v1",
        "pub_date": "2023-07-10",
        "summary": "Recommender systems are effective tools for mitigating information overload\nand have seen extensive applications across various domains. However, the\nsingle focus on utility goals proves to be inadequate in addressing real-world\nconcerns, leading to increasing attention to fairness-aware and diversity-aware\nrecommender systems. While most existing studies explore fairness and diversity\nindependently, we identify strong connections between these two domains. In\nthis survey, we first discuss each of them individually and then dive into\ntheir connections. Additionally, motivated by the concepts of user-level and\nitem-level fairness, we broaden the understanding of diversity to encompass not\nonly the item level but also the user level. With this expanded perspective on\nuser and item-level diversity, we re-interpret fairness studies from the\nviewpoint of diversity. This fresh perspective enhances our understanding of\nfairness-related work and paves the way for potential future research\ndirections. Papers discussed in this survey along with public code links are\navailable at\nhttps://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems .",
        "translated": ""
    },
    {
        "title": "InPars Toolkit: A Unified and Reproducible Synthetic Data Generation\n  Pipeline for Neural Information Retrieval",
        "url": "http://arxiv.org/abs/2307.04601v1",
        "pub_date": "2023-07-10",
        "summary": "Recent work has explored Large Language Models (LLMs) to overcome the lack of\ntraining data for Information Retrieval (IR) tasks. The generalization\nabilities of these models have enabled the creation of synthetic in-domain data\nby providing instructions and a few examples on a prompt. InPars and\nPromptagator have pioneered this approach and both methods have demonstrated\nthe potential of using LLMs as synthetic data generators for IR tasks. This\nmakes them an attractive solution for IR tasks that suffer from a lack of\nannotated data. However, the reproducibility of these methods was limited,\nbecause InPars' training scripts are based on TPUs -- which are not widely\naccessible -- and because the code for Promptagator was not released and its\nproprietary LLM is not publicly accessible. To fully realize the potential of\nthese methods and make their impact more widespread in the research community,\nthe resources need to be accessible and easy to reproduce by researchers and\npractitioners. Our main contribution is a unified toolkit for end-to-end\nreproducible synthetic data generation research, which includes generation,\nfiltering, training and evaluation. Additionally, we provide an interface to IR\nlibraries widely used by the community and support for GPU. Our toolkit not\nonly reproduces the InPars method and partially reproduces Promptagator, but\nalso provides a plug-and-play functionality allowing the use of different LLMs,\nexploring filtering methods and finetuning various reranker models on the\ngenerated data. We also made available all the synthetic data generated in this\nwork for the 18 different datasets in the BEIR benchmark which took more than\n2,000 GPU hours to be generated as well as the reranker models finetuned on the\nsynthetic data. Code and data are available at\nhttps://github.com/zetaalphavector/InPars",
        "translated": ""
    },
    {
        "title": "A Semi-Automated Solution Approach Selection Tool for Any Use Case via\n  Scopus and OpenAI: a Case Study for AI/ML in Oncology",
        "url": "http://arxiv.org/abs/2307.04573v1",
        "pub_date": "2023-07-10",
        "summary": "In today's vast literature landscape, a manual review is very time-consuming.\nTo address this challenge, this paper proposes a semi-automated tool for\nsolution method review and selection. It caters to researchers, practitioners,\nand decision-makers while serving as a benchmark for future work. The tool\ncomprises three modules: (1) paper selection and scoring, using a keyword\nselection scheme to query Scopus API and compute relevancy; (2) solution method\nextraction in papers utilizing OpenAI API; (3) sensitivity analysis and\npost-analyzes. It reveals trends, relevant papers, and methods. AI in the\noncology case study and several use cases are presented with promising results,\ncomparing the tool to manual ground truth.",
        "translated": ""
    },
    {
        "title": "Alleviating Matthew Effect of Offline Reinforcement Learning in\n  Interactive Recommendation",
        "url": "http://arxiv.org/abs/2307.04571v1",
        "pub_date": "2023-07-10",
        "summary": "Offline reinforcement learning (RL), a technology that offline learns a\npolicy from logged data without the need to interact with online environments,\nhas become a favorable choice in decision-making processes like interactive\nrecommendation. Offline RL faces the value overestimation problem. To address\nit, existing methods employ conservatism, e.g., by constraining the learned\npolicy to be close to behavior policies or punishing the rarely visited\nstate-action pairs. However, when applying such offline RL to recommendation,\nit will cause a severe Matthew effect, i.e., the rich get richer and the poor\nget poorer, by promoting popular items or categories while suppressing the less\npopular ones. It is a notorious issue that needs to be addressed in practical\nrecommender systems.\n  In this paper, we aim to alleviate the Matthew effect in offline RL-based\nrecommendation. Through theoretical analyses, we find that the conservatism of\nexisting methods fails in pursuing users' long-term satisfaction. It inspires\nus to add a penalty term to relax the pessimism on states with high entropy of\nthe logging policy and indirectly penalizes actions leading to less diverse\nstates. This leads to the main technical contribution of the work: Debiased\nmodel-based Offline RL (DORL) method. Experiments show that DORL not only\ncaptures user interests well but also alleviates the Matthew effect. The\nimplementation is available via https://github.com/chongminggao/DORL-codes.",
        "translated": ""
    },
    {
        "title": "Counterfactual Explanation for Fairness in Recommendation",
        "url": "http://arxiv.org/abs/2307.04386v1",
        "pub_date": "2023-07-10",
        "summary": "Fairness-aware recommendation eliminates discrimination issues to build\ntrustworthy recommendation systems.Explaining the causes of unfair\nrecommendations is critical, as it promotes fairness diagnostics, and thus\nsecures users' trust in recommendation models. Existing fairness explanation\nmethods suffer high computation burdens due to the large-scale search space and\nthe greedy nature of the explanation search process. Besides, they perform\nscore-based optimizations with continuous values, which are not applicable to\ndiscrete attributes such as gender and race. In this work, we adopt the novel\nparadigm of counterfactual explanation from causal inference to explore how\nminimal alterations in explanations change model fairness, to abandon the\ngreedy search for explanations. We use real-world attributes from Heterogeneous\nInformation Networks (HINs) to empower counterfactual reasoning on discrete\nattributes. We propose a novel Counterfactual Explanation for Fairness\n(CFairER) that generates attribute-level counterfactual explanations from HINs\nfor recommendation fairness. Our CFairER conducts off-policy reinforcement\nlearning to seek high-quality counterfactual explanations, with an attentive\naction pruning reducing the search space of candidate counterfactuals. The\ncounterfactual explanations help to provide rational and proximate explanations\nfor model fairness, while the attentive action pruning narrows the search space\nof attributes. Extensive experiments demonstrate our proposed model can\ngenerate faithful explanations while maintaining favorable recommendation\nperformance.",
        "translated": ""
    },
    {
        "title": "Causal Neural Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2307.04384v1",
        "pub_date": "2023-07-10",
        "summary": "Graph collaborative filtering (GCF) has gained considerable attention in\nrecommendation systems by leveraging graph learning techniques to enhance\ncollaborative filtering (CF) models. One classical approach in GCF is to learn\nuser and item embeddings by modeling complex graph relations and utilizing\nthese embeddings for CF models. However, the quality of the embeddings\nsignificantly impacts the recommendation performance of GCF models. In this\npaper, we argue that existing graph learning methods are insufficient in\ngenerating satisfactory embeddings for CF models. This is because they\naggregate neighboring node messages directly, which can result in incorrect\nestimations of user-item correlations. To overcome this limitation, we propose\na novel approach that incorporates causal modeling to explicitly encode the\ncausal effects of neighboring nodes on the target node. This approach enables\nus to identify spurious correlations and uncover the root causes of user\npreferences. We introduce Causal Neural Graph Collaborative Filtering (CNGCF),\nthe first causality-aware graph learning framework for CF. CNGCF integrates\ncausal modeling into the graph representation learning process, explicitly\ncoupling causal effects between node pairs into the core message-passing\nprocess of graph learning. As a result, CNGCF yields causality-aware embeddings\nthat promote robust recommendations. Our extensive experiments demonstrate that\nCNGCF provides precise recommendations that align with user preferences.\nTherefore, our proposed framework can address the limitations of existing GCF\nmodels and offer a more effective solution for recommendation systems.",
        "translated": ""
    },
    {
        "title": "Graph Contrastive Learning with Multi-Objective for Personalized Product\n  Retrieval in Taobao Search",
        "url": "http://arxiv.org/abs/2307.04322v1",
        "pub_date": "2023-07-10",
        "summary": "In e-commerce search, personalized retrieval is a crucial technique for\nimproving user shopping experience. Recent works in this domain have achieved\nsignificant improvements by the representation learning paradigm, e.g.,\nembedding-based retrieval (EBR) and collaborative filtering (CF). EBR methods\ndo not sufficiently exploit the useful collaborative signal and are difficult\nto learn the representations of long-tail item well. Graph-based CF methods\nimprove personalization by modeling collaborative signal within the user click\ngraph. However, existing Graph-based methods ignore user's multiple behaviours,\nsuch as click/purchase and the relevance constraint between user behaviours and\nitems.In this paper, we propose a Graph Contrastive Learning with\nMulti-Objective (GCL-MO) collaborative filtering model, which solves the\nproblems of weak relevance and incomplete personalization in e-commerce search.\nSpecifically, GCL-MO builds a homogeneous graph of items and then optimizes a\nmulti-objective function of personalization and relevance. Moreover, we propose\na modified contrastive loss for multi-objectives graph learning, which avoids\nthe mutual suppression among positive samples and thus improves the\ngeneralization and robustness of long-tail item representations. These learned\nitem embeddings are then used for personalized retrieval by constructing an\nefficient offline-to-online inverted table. GCL-MO outperforms the online\ncollaborative filtering baseline in both offline/online experimental metrics\nand shows a significant improvement in the online A/B testing of Taobao search.",
        "translated": ""
    },
    {
        "title": "DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge\n  Graphs",
        "url": "http://arxiv.org/abs/2307.04090v1",
        "pub_date": "2023-07-09",
        "summary": "Recent work within the Argument Mining community has shown the applicability\nof Natural Language Processing systems for solving problems found within\ncompetitive debate. One of the most important tasks within competitive debate\nis for debaters to create high quality debate cases. We show that effective\ndebate cases can be constructed using constrained shortest path traversals on\nArgumentative Semantic Knowledge Graphs. We study this potential in the context\nof a type of American Competitive Debate, called Policy Debate, which already\nhas a large scale dataset targeting it called DebateSum. We significantly\nimprove upon DebateSum by introducing 53180 new examples, as well as further\nuseful metadata for every example, to the dataset. We leverage the txtai\nsemantic search and knowledge graph toolchain to produce and contribute 9\nsemantic knowledge graphs built on this dataset. We create a unique method for\nevaluating which knowledge graphs are better in the context of producing policy\ndebate cases. A demo which automatically generates debate cases, along with all\nother code and the Knowledge Graphs, are open-sourced and made available to the\npublic here: https://github.com/Hellisotherpeople/DebateKG",
        "translated": ""
    },
    {
        "title": "Fairness-Aware Graph Neural Networks: A Survey",
        "url": "http://arxiv.org/abs/2307.03929v1",
        "pub_date": "2023-07-08",
        "summary": "Graph Neural Networks (GNNs) have become increasingly important due to their\nrepresentational power and state-of-the-art predictive performance on many\nfundamental learning tasks. Despite this success, GNNs suffer from fairness\nissues that arise as a result of the underlying graph data and the fundamental\naggregation mechanism that lies at the heart of the large class of GNN models.\nIn this article, we examine and categorize fairness techniques for improving\nthe fairness of GNNs. Previous work on fair GNN models and techniques are\ndiscussed in terms of whether they focus on improving fairness during a\npreprocessing step, during training, or in a post-processing phase.\nFurthermore, we discuss how such techniques can be used together whenever\nappropriate, and highlight the advantages and intuition as well. We also\nintroduce an intuitive taxonomy for fairness evaluation metrics including\ngraph-level fairness, neighborhood-level fairness, embedding-level fairness,\nand prediction-level fairness metrics. In addition, graph datasets that are\nuseful for benchmarking the fairness of GNN models are summarized succinctly.\nFinally, we highlight key open problems and challenges that remain to be\naddressed.",
        "translated": ""
    },
    {
        "title": "Embedding Mental Health Discourse for Community Recommendation",
        "url": "http://arxiv.org/abs/2307.03892v1",
        "pub_date": "2023-07-08",
        "summary": "Our paper investigates the use of discourse embedding techniques to develop a\ncommunity recommendation system that focuses on mental health support groups on\nsocial media. Social media platforms provide a means for users to anonymously\nconnect with communities that cater to their specific interests. However, with\nthe vast number of online communities available, users may face difficulties in\nidentifying relevant groups to address their mental health concerns. To address\nthis challenge, we explore the integration of discourse information from\nvarious subreddit communities using embedding techniques to develop an\neffective recommendation system. Our approach involves the use of content-based\nand collaborative filtering techniques to enhance the performance of the\nrecommendation system. Our findings indicate that the proposed approach\noutperforms the use of each technique separately and provides interpretability\nin the recommendation process.",
        "translated": ""
    },
    {
        "title": "Duncode Characters Shorter",
        "url": "http://arxiv.org/abs/2307.05414v1",
        "pub_date": "2023-07-11",
        "summary": "This paper investigates the employment of various encoders in text\ntransformation, converting characters into bytes. It discusses local encoders\nsuch as ASCII and GB-2312, which encode specific characters into shorter bytes,\nand universal encoders like UTF-8 and UTF-16, which can encode the complete\nUnicode set with greater space requirements and are gaining widespread\nacceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,\nhowever, lack self-synchronizing capabilities. Duncode is introduced as an\ninnovative encoding method that aims to encode the entire Unicode character set\nwith high space efficiency, akin to local encoders. It has the potential to\ncompress multiple characters of a string into a Duncode unit using fewer bytes.\nDespite offering less self-synchronizing identification information, Duncode\nsurpasses UTF8 in terms of space efficiency. The application is available at\n\\url{https://github.com/laohur/duncode}. Additionally, we have developed a\nbenchmark for evaluating character encoders across different languages. It\nencompasses 179 languages and can be accessed at\n\\url{https://github.com/laohur/wiki2txt}.",
        "translated": ""
    },
    {
        "title": "Temporal Graphs Anomaly Emergence Detection: Benchmarking For Social\n  Media Interactions",
        "url": "http://arxiv.org/abs/2307.05268v1",
        "pub_date": "2023-07-11",
        "summary": "Temporal graphs have become an essential tool for analyzing complex dynamic\nsystems with multiple agents. Detecting anomalies in temporal graphs is crucial\nfor various applications, including identifying emerging trends, monitoring\nnetwork security, understanding social dynamics, tracking disease outbreaks,\nand understanding financial dynamics. In this paper, we present a comprehensive\nbenchmarking study that compares 12 data-driven methods for anomaly detection\nin temporal graphs. We conduct experiments on two temporal graphs extracted\nfrom Twitter and Facebook, aiming to identify anomalies in group interactions.\nSurprisingly, our study reveals an unclear pattern regarding the best method\nfor such tasks, highlighting the complexity and challenges involved in anomaly\nemergence detection in large and dynamic systems. The results underscore the\nneed for further research and innovative approaches to effectively detect\nemerging anomalies in dynamic systems represented as temporal graphs.",
        "translated": ""
    },
    {
        "title": "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion",
        "url": "http://arxiv.org/abs/2307.05260v1",
        "pub_date": "2023-07-11",
        "summary": "The task of Prior Case Retrieval (PCR) in the legal domain is about\nautomatically citing relevant (based on facts and precedence) prior legal cases\nin a given query case. To further promote research in PCR, in this paper, we\npropose a new large benchmark (in English) for the PCR task: IL-PCR (Indian\nLegal Prior Case Retrieval) corpus. Given the complex nature of case relevance\nand the long size of legal documents, BM25 remains a strong baseline for\nranking the cited prior documents. In this work, we explore the role of events\nin legal case retrieval and propose an unsupervised retrieval method-based\npipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find\nthat the proposed unsupervised retrieval method significantly increases\nperformance compared to BM25 and makes retrieval faster by a considerable\nmargin, making it applicable to real-time case retrieval systems. Our proposed\nsystem is generic, we show that it generalizes across two different legal\nsystems (Indian and Canadian), and it shows state-of-the-art performance on the\nbenchmarks for both the legal systems (IL-PCR and COLIEE corpora).",
        "translated": ""
    },
    {
        "title": "Generative Contrastive Graph Learning for Recommendation",
        "url": "http://arxiv.org/abs/2307.05100v1",
        "pub_date": "2023-07-11",
        "summary": "By treating users' interactions as a user-item graph, graph learning models\nhave been widely deployed in Collaborative Filtering(CF) based recommendation.\nRecently, researchers have introduced Graph Contrastive Learning(GCL)\ntechniques into CF to alleviate the sparse supervision issue, which first\nconstructs contrastive views by data augmentations and then provides\nself-supervised signals by maximizing the mutual information between\ncontrastive views. Despite the effectiveness, we argue that current GCL-based\nrecommendation models are still limited as current data augmentation\ntechniques, either structure augmentation or feature augmentation. First,\nstructure augmentation randomly dropout nodes or edges, which is easy to\ndestroy the intrinsic nature of the user-item graph. Second, feature\naugmentation imposes the same scale noise augmentation on each node, which\nneglects the unique characteristics of nodes on the graph. To tackle the above\nlimitations, we propose a novel Variational Graph Generative-Contrastive\nLearning(VGCL) framework for recommendation. Specifically, we leverage\nvariational graph reconstruction to estimate a Gaussian distribution of each\nnode, then generate multiple contrastive views through multiple samplings from\nthe estimated distributions, which builds a bridge between generative and\ncontrastive learning. Besides, the estimated variances are tailored to each\nnode, which regulates the scale of contrastive loss for each node on\noptimization. Considering the similarity of the estimated distributions, we\npropose a cluster-aware twofold contrastive learning, a node-level to encourage\nconsistency of a node's contrastive views and a cluster-level to encourage\nconsistency of nodes in a cluster. Finally, extensive experimental results on\nthree public datasets clearly demonstrate the effectiveness of the proposed\nmodel.",
        "translated": ""
    },
    {
        "title": "Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with\n  Sample-aware Prompting and Dynamic Revision Chain",
        "url": "http://arxiv.org/abs/2307.05074v1",
        "pub_date": "2023-07-11",
        "summary": "Text-to-SQL aims at generating SQL queries for the given natural language\nquestions and thus helping users to query databases. Prompt learning with large\nlanguage models (LLMs) has emerged as a recent approach, which designs prompts\nto lead LLMs to understand the input question and generate the corresponding\nSQL. However, it faces challenges with strict SQL syntax requirements. Existing\nwork prompts the LLMs with a list of demonstration examples (i.e. question-SQL\npairs) to generate SQL, but the fixed prompts can hardly handle the scenario\nwhere the semantic gap between the retrieved demonstration and the input\nquestion is large. In this paper, we propose a retrieval-augmented prompting\nmethod for a LLM-based Text-to-SQL framework, involving sample-aware prompting\nand a dynamic revision chain. Our approach incorporates sample-aware\ndemonstrations, which include the composition of SQL operators and fine-grained\ninformation related to the given question. To retrieve questions sharing\nsimilar intents with input questions, we propose two strategies for assisting\nretrieval. Firstly, we leverage LLMs to simplify the original questions,\nunifying the syntax and thereby clarifying the users' intentions. To generate\nexecutable and accurate SQLs without human intervention, we design a dynamic\nrevision chain which iteratively adapts fine-grained feedback from the\npreviously generated SQL. Experimental results on three Text-to-SQL benchmarks\ndemonstrate the superiority of our method over strong baseline models.",
        "translated": ""
    },
    {
        "title": "Mining for Unknown Unknowns",
        "url": "http://arxiv.org/abs/2307.05071v1",
        "pub_date": "2023-07-11",
        "summary": "Unknown unknowns are future relevant contingencies that lack an ex ante\ndescription. While there are numerous retrospective accounts showing that\nsignificant gains or losses might have been achieved or avoided had such\ncontingencies been previously uncovered, getting hold of unknown unknowns still\nremains elusive, both in practice and conceptually. Using Formal Concept\nAnalysis (FCA) - a subfield of lattice theory which is increasingly applied for\nmining and organizing data - this paper introduces a simple framework to\nsystematically think out of the box and direct the search for unknown unknowns.",
        "translated": ""
    },
    {
        "title": "Neural-Symbolic Recommendation with Graph-Enhanced Information",
        "url": "http://arxiv.org/abs/2307.05036v1",
        "pub_date": "2023-07-11",
        "summary": "The recommendation system is not only a problem of inductive statistics from\ndata but also a cognitive task that requires reasoning ability. The most\nadvanced graph neural networks have been widely used in recommendation systems\nbecause they can capture implicit structured information from graph-structured\ndata. However, like most neural network algorithms, they only learn matching\npatterns from a perception perspective. Some researchers use user behavior for\nlogic reasoning to achieve recommendation prediction from the perspective of\ncognitive reasoning, but this kind of reasoning is a local one and ignores\nimplicit information on a global scale. In this work, we combine the advantages\nof graph neural networks and propositional logic operations to construct a\nneuro-symbolic recommendation model with both global implicit reasoning ability\nand local explicit logic reasoning ability. We first build an item-item graph\nbased on the principle of adjacent interaction and use graph neural networks to\ncapture implicit information in global data. Then we transform user behavior\ninto propositional logic expressions to achieve recommendations from the\nperspective of cognitive reasoning. Extensive experiments on five public\ndatasets show that our proposed model outperforms several state-of-the-art\nmethods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].",
        "translated": ""
    },
    {
        "title": "Empowering recommender systems using automatically generated Knowledge\n  Graphs and Reinforcement Learning",
        "url": "http://arxiv.org/abs/2307.04996v1",
        "pub_date": "2023-07-11",
        "summary": "Personalized recommendations have a growing importance in direct marketing,\nwhich motivates research to enhance customer experiences by knowledge graph\n(KG) applications. For example, in financial services, companies may benefit\nfrom providing relevant financial articles to their customers to cultivate\nrelationships, foster client engagement and promote informed financial\ndecisions. While several approaches center on KG-based recommender systems for\nimproved content, in this study we focus on interpretable KG-based recommender\nsystems for decision making.To this end, we present two knowledge graph-based\napproaches for personalized article recommendations for a set of customers of a\nlarge multinational financial services company. The first approach employs\nReinforcement Learning and the second approach uses the XGBoost algorithm for\nrecommending articles to the customers. Both approaches make use of a KG\ngenerated from both structured (tabular data) and unstructured data (a large\nbody of text data).Using the Reinforcement Learning-based recommender system we\ncould leverage the graph traversal path leading to the recommendation as a way\nto generate interpretations (Path Directed Reasoning (PDR)). In the\nXGBoost-based approach, one can also provide explainable results using post-hoc\nmethods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I\nam Five).Importantly, our approach offers explainable results, promoting better\ndecision-making. This study underscores the potential of combining advanced\nmachine learning techniques with KG-driven insights to bolster experience in\ncustomer relationship management.",
        "translated": ""
    },
    {
        "title": "Ranking with Long-Term Constraints",
        "url": "http://arxiv.org/abs/2307.04923v1",
        "pub_date": "2023-07-10",
        "summary": "The feedback that users provide through their choices (e.g., clicks,\npurchases) is one of the most common types of data readily available for\ntraining search and recommendation algorithms. However, myopically training\nsystems based on choice data may only improve short-term engagement, but not\nthe long-term sustainability of the platform and the long-term benefits to its\nusers, content providers, and other stakeholders. In this paper, we thus\ndevelop a new framework in which decision makers (e.g., platform operators,\nregulators, users) can express long-term goals for the behavior of the platform\n(e.g., fairness, revenue distribution, legal requirements). These goals take\nthe form of exposure or impact targets that go well beyond individual sessions,\nand we provide new control-based algorithms to achieve these goals. In\nparticular, the controllers are designed to achieve the stated long-term goals\nwith minimum impact on short-term engagement. Beyond the principled theoretical\nderivation of the controllers, we evaluate the algorithms on both synthetic and\nreal-world data. While all controllers perform well, we find that they provide\ninteresting trade-offs in efficiency, robustness, and the ability to plan\nahead.",
        "translated": ""
    },
    {
        "title": "Testing different Log Bases For Vector Model Weighting Technique",
        "url": "http://arxiv.org/abs/2307.06213v1",
        "pub_date": "2023-07-12",
        "summary": "Information retrieval systems retrieves relevant documents based on a query\nsubmitted by the user. The documents are initially indexed and the words in the\ndocuments are assigned weights using a weighting technique called TFIDF which\nis the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF\nrepresents the number of occurrences of a term in a document. IDF measures\nwhether the term is common or rare across all documents. It is computed by\ndividing the total number of documents in the system by the number of documents\ncontaining the term and then computing the logarithm of the quotient. By\ndefault, we use base 10 to calculate the logarithm. In this paper, we are going\nto test this weighting technique by using a range of log bases from 0.1 to\n100.0 to calculate the IDF. Testing different log bases for vector model\nweighting technique is to highlight the importance of understanding the\nperformance of the system at different weighting values. We use the documents\nof MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled\nexplicitly for experiments in data information retrieval systems.",
        "translated": ""
    },
    {
        "title": "DDNAS: Discretized Differentiable Neural Architecture Search for Text\n  Classification",
        "url": "http://arxiv.org/abs/2307.06005v1",
        "pub_date": "2023-07-12",
        "summary": "Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.",
        "translated": ""
    },
    {
        "title": "Contrastive Learning for Conversion Rate Prediction",
        "url": "http://arxiv.org/abs/2307.05974v1",
        "pub_date": "2023-07-12",
        "summary": "Conversion rate (CVR) prediction plays an important role in advertising\nsystems. Recently, supervised deep neural network-based models have shown\npromising performance in CVR prediction. However, they are data hungry and\nrequire an enormous amount of training data. In online advertising systems,\nalthough there are millions to billions of ads, users tend to click only a\nsmall set of them and to convert on an even smaller set. This data sparsity\nissue restricts the power of these deep models. In this paper, we propose the\nContrastive Learning for CVR prediction (CL4CVR) framework. It associates the\nsupervised CVR prediction task with a contrastive learning task, which can\nlearn better data representations exploiting abundant unlabeled data and\nimprove the CVR prediction performance. To tailor the contrastive learning task\nto the CVR prediction problem, we propose embedding masking (EM), rather than\nfeature masking, to create two views of augmented samples. We also propose a\nfalse negative elimination (FNE) component to eliminate samples with the same\nfeature as the anchor sample, to account for the natural property in user\nbehavior data. We further propose a supervised positive inclusion (SPI)\ncomponent to include additional positive samples for each anchor sample, in\norder to make full use of sparse but precious user conversion events.\nExperimental results on two real-world conversion datasets demonstrate the\nsuperior performance of CL4CVR. The source code is available at\nhttps://github.com/DongRuiHust/CL4CVR.",
        "translated": ""
    },
    {
        "title": "Relational Extraction on Wikipedia Tables using Convolutional and Memory\n  Networks",
        "url": "http://arxiv.org/abs/2307.05827v1",
        "pub_date": "2023-07-11",
        "summary": "Relation extraction (RE) is the task of extracting relations between entities\nin text. Most RE methods extract relations from free-form running text and\nleave out other rich data sources, such as tables. We explore RE from the\nperspective of applying neural methods on tabularly organized data. We\nintroduce a new model consisting of Convolutional Neural Network (CNN) and\nBidirectional-Long Short Term Memory (BiLSTM) network to encode entities and\nlearn dependencies among them, respectively. We evaluate our model on a large\nand recent dataset and compare results with previous neural methods.\nExperimental results show that our model consistently outperforms the previous\nmodel for the task of relation extraction on tabular data. We perform\ncomprehensive error analyses and ablation study to show the contribution of\nvarious components of our model. Finally, we discuss the usefulness and\ntrade-offs of our approach, and provide suggestions for fostering further\nresearch.",
        "translated": ""
    },
    {
        "title": "Parmesan: mathematical concept extraction for education",
        "url": "http://arxiv.org/abs/2307.06699v1",
        "pub_date": "2023-07-13",
        "summary": "Mathematics is a highly specialized domain with its own unique set of\nchallenges that has seen limited study in natural language processing. However,\nmathematics is used in a wide variety of fields and multidisciplinary research\nin many different domains often relies on an understanding of mathematical\nconcepts. To aid researchers coming from other fields, we develop a prototype\nsystem for searching for and defining mathematical concepts in context,\nfocusing on the field of category theory. This system, Parmesan, depends on\nnatural language processing components including concept extraction, relation\nextraction, definition extraction, and entity linking. In developing this\nsystem, we show that existing techniques cannot be applied directly to the\ncategory theory domain, and suggest hybrid techniques that do perform well,\nthough we expect the system to evolve over time. We also provide two cleaned\nmathematical corpora that power the prototype system, which are based on\njournal articles and wiki pages, respectively. The corpora have been annotated\nwith dependency trees, lemmas, and part-of-speech tags.",
        "translated": ""
    },
    {
        "title": "Going Beyond Local: Global Graph-Enhanced Personalized News\n  Recommendations",
        "url": "http://arxiv.org/abs/2307.06576v1",
        "pub_date": "2023-07-13",
        "summary": "Precisely recommending candidate news articles to users has always been a\ncore challenge for personalized news recommendation systems. Most recent works\nprimarily focus on using advanced natural language processing techniques to\nextract semantic information from rich textual data, employing content-based\nmethods derived from local historical news. However, this approach lacks a\nglobal perspective, failing to account for users' hidden motivations and\nbehaviors beyond semantic information. To address this challenge, we propose a\nnovel model called GLORY (Global-LOcal news Recommendation sYstem), which\ncombines global representations learned from other users with local\nrepresentations to enhance personalized recommendation systems. We accomplish\nthis by constructing a Global-aware Historical News Encoder, which includes a\nglobal news graph and employs gated graph neural networks to enrich news\nrepresentations, thereby fusing historical news representations by a historical\nnews aggregator. Similarly, we extend this approach to a Global Candidate News\nEncoder, utilizing a global entity graph and a candidate news aggregator to\nenhance candidate news representation. Evaluation results on two public news\ndatasets demonstrate that our method outperforms existing approaches.\nFurthermore, our model offers more diverse recommendations.",
        "translated": ""
    },
    {
        "title": "Assessing the Ability of ChatGPT to Screen Articles for Systematic\n  Reviews",
        "url": "http://arxiv.org/abs/2307.06464v1",
        "pub_date": "2023-07-12",
        "summary": "By organizing knowledge within a research field, Systematic Reviews (SR)\nprovide valuable leads to steer research. Evidence suggests that SRs have\nbecome first-class artifacts in software engineering. However, the tedious\nmanual effort associated with the screening phase of SRs renders these studies\na costly and error-prone endeavor. While screening has traditionally been\nconsidered not amenable to automation, the advent of generative AI-driven\nchatbots, backed with large language models is set to disrupt the field. In\nthis report, we propose an approach to leverage these novel technological\ndevelopments for automating the screening of SRs. We assess the consistency,\nclassification performance, and generalizability of ChatGPT in screening\narticles for SRs and compare these figures with those of traditional\nclassifiers used in SR automation. Our results indicate that ChatGPT is a\nviable option to automate the SR processes, but requires careful considerations\nfrom developers when integrating ChatGPT into their SR tools.",
        "translated": ""
    },
    {
        "title": "Streaming CTR Prediction: Rethinking Recommendation Task for Real-World\n  Streaming Data",
        "url": "http://arxiv.org/abs/2307.07509v1",
        "pub_date": "2023-07-14",
        "summary": "The Click-Through Rate (CTR) prediction task is critical in industrial\nrecommender systems, where models are usually deployed on dynamic streaming\ndata in practical applications. Such streaming data in real-world recommender\nsystems face many challenges, such as distribution shift, temporal\nnon-stationarity, and systematic biases, which bring difficulties to the\ntraining and utilizing of recommendation models. However, most existing studies\napproach the CTR prediction as a classification task on static datasets,\nassuming that the train and test sets are independent and identically\ndistributed (a.k.a, i.i.d. assumption). To bridge this gap, we formulate the\nCTR prediction problem in streaming scenarios as a Streaming CTR Prediction\ntask. Accordingly, we propose dedicated benchmark settings and metrics to\nevaluate and analyze the performance of the models in streaming data. To better\nunderstand the differences compared to traditional CTR prediction tasks, we\ndelve into the factors that may affect the model performance, such as parameter\nscale, normalization, regularization, etc. The results reveal the existence of\nthe ''streaming learning dilemma'', whereby the same factor may have different\neffects on model performance in the static and streaming scenarios. Based on\nthe findings, we propose two simple but inspiring methods (i.e., tuning key\nparameters and exemplar replay) that significantly improve the effectiveness of\nthe CTR models in the new streaming scenario. We hope our work will inspire\nfurther research on streaming CTR prediction and help improve the robustness\nand adaptability of recommender systems.",
        "translated": ""
    },
    {
        "title": "PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language\n  Pre-training via Prompting",
        "url": "http://arxiv.org/abs/2307.07341v1",
        "pub_date": "2023-07-14",
        "summary": "Vision-language (VL) Pre-training (VLP) has shown to well generalize VL\nmodels over a wide range of VL downstream tasks, especially for cross-modal\nretrieval. However, it hinges on a huge amount of image-text pairs, which\nrequires tedious and costly curation. On the contrary, weakly-supervised VLP\n(W-VLP) explores means with object tags generated by a pre-trained object\ndetector (OD) from images. Yet, they still require paired information, i.e.\nimages and object-level annotations, as supervision to train an OD.\n  To further reduce the amount of supervision, we propose Prompts-in-The-Loop\n(PiTL) that prompts knowledge from large language models (LLMs) to describe\nimages. Concretely, given a category label of an image, e.g. refinery, the\nknowledge, e.g. a refinery could be seen with large storage tanks, pipework,\nand ..., extracted by LLMs is used as the language counterpart. The knowledge\nsupplements, e.g. the common relations among entities most likely appearing in\na scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of\n14K categories from ImageNet21K with PiTL. Empirically, the VL models\npre-trained with PiTL-generated pairs are strongly favored over other W-VLP\nworks on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with less\nsupervision. The results reveal the effectiveness of PiTL-generated pairs for\nVLP.",
        "translated": ""
    },
    {
        "title": "Hybrid moderation in the newsroom: Recommending featured posts to\n  content moderators",
        "url": "http://arxiv.org/abs/2307.07317v1",
        "pub_date": "2023-07-14",
        "summary": "Online news outlets are grappling with the moderation of user-generated\ncontent within their comment section. We present a recommender system based on\nranking class probabilities to support and empower the moderator in choosing\nfeatured posts, a time-consuming task. By combining user and textual content\nfeatures we obtain an optimal classification F1-score of 0.44 on the test set.\nFurthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of\nvalidation articles. As an expert evaluation, content moderators assessed the\noutput of a random selection of articles by choosing comments to feature based\non the recommendations, which resulted in a NDCG score of 0.83. We conclude\nthat first, adding text features yields the best score and second, while\nchoosing featured content remains somewhat subjective, content moderators found\nsuitable comments in all but one evaluated recommendations. We end the paper by\nanalyzing our best-performing model, a step towards transparency and\nexplainability in hybrid content moderation.",
        "translated": ""
    },
    {
        "title": "Learning to Retrieve In-Context Examples for Large Language Models",
        "url": "http://arxiv.org/abs/2307.07164v1",
        "pub_date": "2023-07-14",
        "summary": "Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of 30 tasks demonstrate that our framework significantly\nenhances in-context learning performance. Furthermore, we show the\ngeneralization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes.",
        "translated": ""
    },
    {
        "title": "Digital Health Discussion Through Articles Published Until the Year\n  2021: A Digital Topic Modeling Approach",
        "url": "http://arxiv.org/abs/2307.07130v1",
        "pub_date": "2023-07-14",
        "summary": "The digital health industry has grown in popularity since the 2010s, but\nthere has been limited analysis of the topics discussed in the field across\nacademic disciplines. This study aims to analyze the research trends of digital\nhealth-related articles published on the Web of Science until 2021, in order to\nunderstand the concentration, scope, and characteristics of the research.\n15,950 digital health-related papers from the top 10 academic fields were\nanalyzed using the Web of Science. The papers were grouped into three domains:\npublic health, medicine, and electrical engineering and computer science\n(EECS). Two time periods (2012-2016 and 2017-2021) were compared using Latent\nDirichlet Allocation (LDA) for topic modeling. The number of topics was\ndetermined based on coherence score, and topic compositions were compared using\na homogeneity test. The number of optimal topics varied across domains and time\nperiods. For public health, the first and second halves had 13 and 19 topics,\nrespectively. Medicine had 14 and 25 topics, and EECS had 7 and 21 topics. Text\nanalysis revealed shared topics among the domains, but with variations in\ncomposition. The homogeneity test confirmed significant differences between the\ngroups (p&lt;2.2e-16). Six dominant themes emerged, including journal article\nmethodology, information technology, medical issues, population demographics,\nsocial phenomena, and healthcare. Digital health research is expanding and\nevolving, particularly in relation to Covid-19, where topics such as depression\nand mental disorders, education, and physical activity have gained prominence.\nThere was no bias in topic composition among the three domains, but other\nfields like kinesiology or psychology could contribute to future digital health\nresearch. Exploring expanded topics that reflect people's needs for digital\nhealth over time will be crucial.",
        "translated": ""
    },
    {
        "title": "Making the Most Out of the Limited Context Length: Predictive Power\n  Varies with Clinical Note Type and Note Section",
        "url": "http://arxiv.org/abs/2307.07051v1",
        "pub_date": "2023-07-13",
        "summary": "Recent advances in large language models have led to renewed interest in\nnatural language processing in healthcare using the free text of clinical\nnotes. One distinguishing characteristic of clinical notes is their long time\nspan over multiple long documents. The unique structure of clinical notes\ncreates a new design choice: when the context length for a language model\npredictor is limited, which part of clinical notes should we choose as the\ninput? Existing studies either choose the inputs with domain knowledge or\nsimply truncate them. We propose a framework to analyze the sections with high\npredictive power. Using MIMIC-III, we show that: 1) predictive power\ndistribution is different between nursing notes and discharge notes and 2)\ncombining different types of notes could improve performance when the context\nlength is large. Our findings suggest that a carefully selected sampling\nfunction could enable more efficient information extraction from clinical\nnotes.",
        "translated": ""
    },
    {
        "title": "Towards Populating Generalizable Engineering Design Knowledge",
        "url": "http://arxiv.org/abs/2307.06985v1",
        "pub_date": "2023-07-13",
        "summary": "Aiming to populate generalizable engineering design knowledge, we propose a\nmethod to extract facts of the form head entity :: relationship :: tail entity\nfrom sentences found in patent documents. These facts could be combined within\nand across patent documents to form knowledge graphs that serve as schemes for\nrepresenting as well as storing design knowledge. Existing methods in\nengineering design literature often utilise a set of predefined relationships\nto populate triples that are statistical approximations rather than facts. In\nour method, we train a tagger to identify both entities and relationships from\na sentence. Given a pair of entities thus identified, we train another tagger\nto identify the relationship tokens that specifically denote the relationship\nbetween the pair. For training these taggers, we manually construct a dataset\nof 44,227 sentences and corresponding facts. We also compare the performance of\nthe method against typically recommended approaches, wherein, we predict the\nedges among tokens by pairing the tokens independently and as part of a graph.\nWe apply our method to sentences found in patents related to fan systems and\nbuild a domain knowledge base. Upon providing an overview of the knowledge\nbase, we search for solutions relevant to some key issues prevailing in fan\nsystems. We organize the responses into knowledge graphs and hold a comparative\ndiscussion against the opinions from ChatGPT.",
        "translated": ""
    },
    {
        "title": "NS4AR: A new, focused on sampling areas sampling method in graphical\n  recommendation Systems",
        "url": "http://arxiv.org/abs/2307.07321v1",
        "pub_date": "2023-07-13",
        "summary": "The effectiveness of graphical recommender system depends on the quantity and\nquality of negative sampling. This paper selects some typical recommender\nsystem models, as well as some latest negative sampling strategies on the\nmodels as baseline. Based on typical graphical recommender model, we divide\nsample region into assigned-n areas and use AdaSim to give different weight to\nthese areas to form positive set and negative set. Because of the volume and\nsignificance of negative items, we also proposed a subset selection model to\nnarrow the core negative samples.",
        "translated": ""
    },
    {
        "title": "Leveraging Recommender Systems to Reduce Content Gaps on Peer Production\n  Platforms",
        "url": "http://arxiv.org/abs/2307.08669v1",
        "pub_date": "2023-07-17",
        "summary": "Peer production platforms like Wikipedia commonly suffer from content gaps.\nPrior research suggests recommender systems can help solve this problem, by\nguiding editors towards underrepresented topics. However, it remains unclear\nwhether this approach would result in less relevant recommendations, leading to\nreduced overall engagement with recommended items. To answer this question, we\nfirst conducted offline analyses (Study 1) on SuggestBot, a task-routing\nrecommender system for Wikipedia, then did a three-month controlled experiment\n(Study 2). Our results show that presenting users with articles from\nunderrepresented topics increased the proportion of work done on those articles\nwithout significantly reducing overall recommendation uptake. We discuss the\nimplications of our results, including how ignoring the article discovery\nprocess can artificially narrow recommendations. We draw parallels between this\nphenomenon and the common issue of ``filter bubbles'' to show how any platform\nthat employs recommender systems is susceptible to it.",
        "translated": ""
    },
    {
        "title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2307.08303v1",
        "pub_date": "2023-07-17",
        "summary": "Dense retrieval (DR) converts queries and documents into dense embeddings and\nmeasures the similarity between queries and documents in vector space. One of\nthe challenges in DR is the lack of domain-specific training data. While DR\nmodels can learn from large-scale public datasets like MS MARCO through\ntransfer learning, evidence shows that not all DR models and domains can\nbenefit from transfer learning equally. Recently, some researchers have\nresorted to large language models (LLMs) to improve the zero-shot and few-shot\nDR models. However, the hard prompts or human-written prompts utilized in these\nworks cannot guarantee the good quality of generated weak queries. To tackle\nthis, we propose soft prompt tuning for augmenting DR (SPTAR): For each task,\nwe leverage soft prompt-tuning to optimize a task-specific soft prompt on\nlimited ground truth data and then prompt the LLMs to tag unlabeled documents\nwith weak queries, yielding enough weak document-query pairs to train\ntask-specific dense retrievers. We design a filter to select high-quality\nexample document-query pairs in the prompt to further improve the quality of\nweak tagged queries. To the best of our knowledge, there is no prior work\nutilizing soft prompt tuning to augment DR models. The experiments demonstrate\nthat SPTAR outperforms the unsupervised baselines BM25 and the recently\nproposed LLMs-based augmentation method for DR.",
        "translated": ""
    },
    {
        "title": "Measuring Item Global Residual Value for Fair Recommendation",
        "url": "http://arxiv.org/abs/2307.08259v1",
        "pub_date": "2023-07-17",
        "summary": "In the era of information explosion, numerous items emerge every day,\nespecially in feed scenarios. Due to the limited system display slots and user\nbrowsing attention, various recommendation systems are designed not only to\nsatisfy users' personalized information needs but also to allocate items'\nexposure. However, recent recommendation studies mainly focus on modeling user\npreferences to present satisfying results and maximize user interactions, while\npaying little attention to developing item-side fair exposure mechanisms for\nrational information delivery. This may lead to serious resource allocation\nproblems on the item side, such as the Snowball Effect. Furthermore, unfair\nexposure mechanisms may hurt recommendation performance. In this paper, we call\nfor a shift of attention from modeling user preferences to developing fair\nexposure mechanisms for items. We first conduct empirical analyses of feed\nscenarios to explore exposure problems between items with distinct uploaded\ntimes. This points out that unfair exposure caused by the time factor may be\nthe major cause of the Snowball Effect. Then, we propose to explicitly model\nitem-level customized timeliness distribution, Global Residual Value (GRV), for\nfair resource allocation. This GRV module is introduced into recommendations\nwith the designed Timeliness-aware Fair Recommendation Framework (TaFR).\nExtensive experiments on two datasets demonstrate that TaFR achieves consistent\nimprovements with various backbone recommendation models. By modeling item-side\ncustomized Global Residual Value, we achieve a fairer distribution of resources\nand, at the same time, improve recommendation performance.",
        "translated": ""
    },
    {
        "title": "Data Discovery for the SDGs: A Systematic Rule-based Approach",
        "url": "http://arxiv.org/abs/2307.07983v1",
        "pub_date": "2023-07-16",
        "summary": "In 2015, the United Nations put forward 17 Sustainable Development Goals\n(SDGs) to be achieved by 2030, where data has been promoted as a focus to\ninnovating sustainable development and as a means to measuring progress towards\nachieving the SDGs. In this study, we propose a systematic approach towards\ndiscovering data types and sources that can be used for SDG research. The\nproposed method integrates a systematic mapping approach using manual\nqualitative coding over a corpus of SDG-related research literature followed by\nan automated process that applies rules to perform data entity extraction\ncomputationally. This approach is exemplified by an analysis of literature\nrelating to SDG 7, the results of which are also presented in this paper. The\npaper concludes with a discussion of the approach and suggests future work to\nextend the method with more advance NLP and machine learning techniques.",
        "translated": ""
    },
    {
        "title": "Opinion mining using Double Channel CNN for Recommender System",
        "url": "http://arxiv.org/abs/2307.07798v1",
        "pub_date": "2023-07-15",
        "summary": "Much unstructured data has been produced with the growth of the Internet and\nsocial media. A significant volume of textual data includes users' opinions\nabout products in online stores and social media. By exploring and categorizing\nthem, helpful information can be acquired, including customer satisfaction,\nuser feedback about a particular event, predicting the sale of a specific\nproduct, and other similar cases. In this paper, we present an approach for\nsentiment analysis with a deep learning model and use it to recommend products.\nA two-channel convolutional neural network model has been used for opinion\nmining, which has five layers and extracts essential features from the data. We\nincreased the number of comments by applying the SMOTE algorithm to the initial\ndataset and balanced the data. Then we proceed to cluster the aspects. We also\nassign a weight to each cluster using tensor decomposition algorithms that\nimprove the recommender system's performance. Our proposed method has reached\n91.6% accuracy, significantly improved compared to previous aspect-based\napproaches.",
        "translated": ""
    },
    {
        "title": "Improving Trace Link Recommendation by Using Non-Isotropic Distances and\n  Combinations",
        "url": "http://arxiv.org/abs/2307.07781v1",
        "pub_date": "2023-07-15",
        "summary": "The existence of trace links between artifacts of the software development\nlife cycle can improve the efficiency of many activities during software\ndevelopment, maintenance and operations. Unfortunately, the creation and\nmaintenance of trace links is time-consuming and error-prone. Research efforts\nhave been spent to automatically compute trace links and lately gained\nmomentum, e.g., due to the availability of powerful tools in the area of\nnatural language processing. In this paper, we report on some observations that\nwe made during studying non-linear similarity measures for computing trace\nlinks. We argue, that taking a geometric viewpoint on semantic similarity can\nbe helpful for future traceability research. We evaluated our observations on a\ndataset of four open source projects and two industrial projects. We\nfurthermore point out that our findings are more general and can build the\nbasis for other information retrieval problems as well.",
        "translated": ""
    },
    {
        "title": "Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model",
        "url": "http://arxiv.org/abs/2307.07740v1",
        "pub_date": "2023-07-15",
        "summary": "Sentiment analysis is the process of identifying and categorizing people's\nemotions or opinions regarding various topics. The analysis of Twitter\nsentiment has become an increasingly popular topic in recent years. In this\npaper, we present several machine learning and a deep learning model to\nanalysis sentiment of Persian political tweets. Our analysis was conducted\nusing Bag of Words and ParsBERT for word representation. We applied Gaussian\nNaive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random\nForests, as well as a combination of CNN and LSTM to classify the polarities of\ntweets. The results of this study indicate that deep learning with ParsBERT\nembedding performs better than machine learning. The CNN-LSTM model had the\nhighest classification accuracy with 89 percent on the first dataset with three\nclasses and 71 percent on the second dataset with seven classes. Due to the\ncomplexity of Persian, it was a difficult task to achieve this level of\nefficiency.",
        "translated": ""
    },
    {
        "title": "On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit\n  Mechanisms",
        "url": "http://arxiv.org/abs/2307.07675v1",
        "pub_date": "2023-07-15",
        "summary": "Efficient learning in multi-armed bandit mechanisms such as pay-per-click\n(PPC) auctions typically involves three challenges: 1) inducing truthful\nbidding behavior (incentives), 2) using personalization in the users (context),\nand 3) circumventing manipulations in click patterns (corruptions). Each of\nthese challenges has been studied orthogonally in the literature; incentives\nhave been addressed by a line of work on truthful multi-armed bandit\nmechanisms, context has been extensively tackled by contextual bandit\nalgorithms, while corruptions have been discussed via a recent line of work on\nbandits with adversarial corruptions. Since these challenges co-exist, it is\nimportant to understand the robustness of each of these approaches in\naddressing the other challenges, provide algorithms that can handle all\nsimultaneously, and highlight inherent limitations in this combination. In this\nwork, we show that the most prominent contextual bandit algorithm,\n$\\epsilon$-greedy can be extended to handle the challenges introduced by\nstrategic arms in the contextual multi-arm bandit mechanism setting. We further\nshow that $\\epsilon$-greedy is inherently robust to adversarial data corruption\nattacks and achieves performance that degrades linearly with the amount of\ncorruption.",
        "translated": ""
    },
    {
        "title": "Deep Neural Aggregation for Recommending Items to Group of Users",
        "url": "http://arxiv.org/abs/2307.09447v1",
        "pub_date": "2023-07-18",
        "summary": "Modern society devotes a significant amount of time to digital interaction.\nMany of our daily actions are carried out through digital means. This has led\nto the emergence of numerous Artificial Intelligence tools that assist us in\nvarious aspects of our lives. One key tool for the digital society is\nRecommender Systems, intelligent systems that learn from our past actions to\npropose new ones that align with our interests. Some of these systems have\nspecialized in learning from the behavior of user groups to make\nrecommendations to a group of individuals who want to perform a joint task. In\nthis article, we analyze the current state of Group Recommender Systems and\npropose two new models that use emerging Deep Learning architectures.\nExperimental results demonstrate the improvement achieved by employing the\nproposed models compared to the state-of-the-art models using four different\ndatasets. The source code of the models, as well as that of all the experiments\nconducted, is available in a public repository.",
        "translated": ""
    },
    {
        "title": "Zero-shot Query Reformulation for Conversational Search",
        "url": "http://arxiv.org/abs/2307.09384v1",
        "pub_date": "2023-07-18",
        "summary": "As the popularity of voice assistants continues to surge, conversational\nsearch has gained increased attention in Information Retrieval. However, data\nsparsity issues in conversational search significantly hinder the progress of\nsupervised conversational search methods. Consequently, researchers are\nfocusing more on zero-shot conversational search approaches. Nevertheless,\nexisting zero-shot methods face three primary limitations: they are not\nuniversally applicable to all retrievers, their effectiveness lacks sufficient\nexplainability, and they struggle to resolve common conversational ambiguities\ncaused by omission. To address these limitations, we introduce a novel\nZero-shot Query Reformulation (ZeQR) framework that reformulates queries based\non previous dialogue contexts without requiring supervision from conversational\nsearch data. Specifically, our framework utilizes language models designed for\nmachine reading comprehension tasks to explicitly resolve two common\nambiguities: coreference and omission, in raw queries. In comparison to\nexisting zero-shot methods, our approach is universally applicable to any\nretriever without additional adaptation or indexing. It also provides greater\nexplainability and effectively enhances query intent understanding because\nambiguities are explicitly and proactively resolved. Through extensive\nexperiments on four TREC conversational datasets, we demonstrate the\neffectiveness of our method, which consistently outperforms state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via\n  Parameter Constraint",
        "url": "http://arxiv.org/abs/2307.09193v1",
        "pub_date": "2023-07-18",
        "summary": "Large-scale online recommender system spreads all over the Internet being in\ncharge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion\nRate (CVR) estimations. However, traditional CVR estimators suffer from\nwell-known Sample Selection Bias and Data Sparsity issues. Entire space models\nwere proposed to address the two issues via tracing the decision-making path of\n\"exposure_click_purchase\". Further, some researchers observed that there are\npurchase-related behaviors between click and purchase, which can better draw\nthe user's decision-making intention and improve the recommendation\nperformance. Thus, the decision-making path has been extended to\n\"exposure_click_in-shop action_purchase\" and can be modeled with conditional\nprobability approach. Nevertheless, we observe that the chain rule of\nconditional probability does not always hold. We report Probability Space\nConfusion (PSC) issue and give a derivation of difference between ground-truth\nand estimation mathematically. We propose a novel Entire Space Multi-Task Model\nfor Post-Click Conversion Rate via Parameter Constraint (ESMC) and two\nalternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and\nEntire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.\nSpecifically, we handle \"exposure_click_in-shop action\" and \"in-shop\naction_purchase\" separately in the light of characteristics of in-shop action.\nThe first path is still treated with conditional probability while the second\none is treated with parameter constraint strategy. Experiments on both offline\nand online environments in a large-scale recommendation system illustrate the\nsuperiority of our proposed methods over state-of-the-art models. The\nreal-world datasets will be released.",
        "translated": ""
    },
    {
        "title": "Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance\n  Detection and Feature Matching for Image Retrieval for Arguments",
        "url": "http://arxiv.org/abs/2307.09172v1",
        "pub_date": "2023-07-18",
        "summary": "Participating in the shared task \"Image Retrieval for arguments\", we used\ndifferent pipelines for image retrieval containing Image Generation, Stance\nDetection, Preselection and Feature Matching. We submitted four different runs\nwith different pipeline layout and compare them to given baseline. Our\npipelines perform similarly to the baseline.",
        "translated": ""
    },
    {
        "title": "Modeling Orders of User Behaviors via Differentiable Sorting: A\n  Multi-task Framework to Predicting User Post-click Conversion",
        "url": "http://arxiv.org/abs/2307.09089v1",
        "pub_date": "2023-07-18",
        "summary": "User post-click conversion prediction is of high interest to researchers and\ndevelopers. Recent studies employ multi-task learning to tackle the selection\nbias and data sparsity problem, two severe challenges in post-click behavior\nprediction, by incorporating click data. However, prior works mainly focused on\npointwise learning and the orders of labels (i.e., click and post-click) are\nnot well explored, which naturally poses a listwise learning problem. Inspired\nby recent advances on differentiable sorting, in this paper, we propose a novel\nmulti-task framework that leverages orders of user behaviors to predict user\npost-click conversion in an end-to-end approach. Specifically, we define an\naggregation operator to combine predicted outputs of different tasks to a\nunified score, then we use the computed scores to model the label relations via\ndifferentiable sorting. Extensive experiments on public and industrial datasets\nshow the superiority of our proposed model against competitive baselines.",
        "translated": ""
    },
    {
        "title": "GraphCL-DTA: a graph contrastive learning with molecular semantics for\n  drug-target binding affinity prediction",
        "url": "http://arxiv.org/abs/2307.08989v1",
        "pub_date": "2023-07-18",
        "summary": "Drug-target binding affinity prediction plays an important role in the early\nstages of drug discovery, which can infer the strength of interactions between\nnew drugs and new targets. However, the performance of previous computational\nmodels is limited by the following drawbacks. The learning of drug\nrepresentation relies only on supervised data, without taking into account the\ninformation contained in the molecular graph itself. Moreover, most previous\nstudies tended to design complicated representation learning module, while\nuniformity, which is used to measure representation quality, is ignored. In\nthis study, we propose GraphCL-DTA, a graph contrastive learning with molecular\nsemantics for drug-target binding affinity prediction. In GraphCL-DTA, we\ndesign a graph contrastive learning framework for molecular graphs to learn\ndrug representations, so that the semantics of molecular graphs are preserved.\nThrough this graph contrastive framework, a more essential and effective drug\nrepresentation can be learned without additional supervised data. Next, we\ndesign a new loss function that can be directly used to smoothly adjust the\nuniformity of drug and target representations. By directly optimizing the\nuniformity of representations, the representation quality of drugs and targets\ncan be improved. The effectiveness of the above innovative elements is verified\non two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA\non the above datasets suggests its superiority to the state-of-the-art model.",
        "translated": ""
    },
    {
        "title": "Sharpness-Aware Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2307.08910v1",
        "pub_date": "2023-07-18",
        "summary": "Graph Neural Networks (GNNs) have achieved impressive performance in\ncollaborative filtering. However, GNNs tend to yield inferior performance when\nthe distributions of training and test data are not aligned well. Also,\ntraining GNNs requires optimizing non-convex neural networks with an abundance\nof local and global minima, which may differ widely in their performance at\ntest time. Thus, it is essential to choose the minima carefully. Here we\npropose an effective training schema, called {gSAM}, under the principle that\nthe \\textit{flatter} minima has a better generalization ability than the\n\\textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of\nthe weight loss landscape by forming a bi-level optimization: the outer problem\nconducts the standard model training while the inner problem helps the model\njump out of the sharp minima. Experimental results show the superiority of our\ngSAM.",
        "translated": ""
    },
    {
        "title": "An Admissible Shift-Consistent Method for Recommender Systems",
        "url": "http://arxiv.org/abs/2307.08857v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper, we propose a new constraint, called shift-consistency, for\nsolving matrix/tensor completion problems in the context of recommender\nsystems. Our method provably guarantees several key mathematical properties:\n(1) satisfies a recently established admissibility criterion for recommender\nsystems; (2) satisfies a definition of fairness that eliminates a specific\nclass of potential opportunities for users to maliciously influence system\nrecommendations; and (3) offers robustness by exploiting provable uniqueness of\nmissing-value imputation. We provide a rigorous mathematical description of the\nmethod, including its generalization from matrix to tensor form to permit\nrepresentation and exploitation of complex structural relationships among sets\nof user and product attributes. We argue that our analysis suggests a\nstructured means for defining latent-space projections that can permit provable\nperformance properties to be established for machine learning methods.",
        "translated": ""
    },
    {
        "title": "An Exploration Study of Mixed-initiative Query Reformulation in\n  Conversational Passage Retrieval",
        "url": "http://arxiv.org/abs/2307.08803v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper, we report our methods and experiments for the TREC\nConversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce\nmulti-stage retrieval pipelines and explore one of the potential benefits of\ninvolving mixed-initiative interaction in conversational passage retrieval\nscenarios: reformulating raw queries. Before the first ranking stage of a\nmulti-stage retrieval pipeline, we propose a mixed-initiative query\nreformulation module, which achieves query reformulation based on the\nmixed-initiative interaction between the users and the system, as the\nreplacement for the neural reformulation method. Specifically, we design an\nalgorithm to generate appropriate questions related to the ambiguities in raw\nqueries, and another algorithm to reformulate raw queries by parsing users'\nfeedback and incorporating it into the raw query. For the first ranking stage\nof our multi-stage pipelines, we adopt a sparse ranking function: BM25, and a\ndense retrieval method: TCT-ColBERT. For the second-ranking step, we adopt a\npointwise reranker: MonoT5, and a pairwise reranker: DuoT5. Experiments on both\nTREC CAsT 2021 and TREC CAsT 2022 datasets show the effectiveness of our\nmixed-initiative-based query reformulation method on improving retrieval\nperformance compared with two popular reformulators: a neural reformulator:\nCANARD-T5 and a rule-based reformulator: historical query reformulator(HQE).",
        "translated": ""
    },
    {
        "title": "Imposing Consistency Properties on Blackbox Systems with Applications to\n  SVD-Based Recommender Systems",
        "url": "http://arxiv.org/abs/2307.08760v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper we discuss pre- and post-processing methods to induce desired\nconsistency and/or invariance properties in blackbox systems, e.g., AI-based.\nWe demonstrate our approach in the context of blackbox SVD-based\nmatrix-completion methods commonly used in recommender system (RS)\napplications. We provide empirical results showing that enforcement of\nunit-consistency and shift-consistency, which have provable RS-relevant\nproperties relating to robustness and fairness, also lead to improved\nperformance according to generic RMSE and MAE performance metrics, irrespective\nof the initial chosen hyperparameter.",
        "translated": ""
    },
    {
        "title": "UniMatch: A Unified User-Item Matching Framework for the Multi-purpose\n  Merchant Marketing",
        "url": "http://arxiv.org/abs/2307.09989v1",
        "pub_date": "2023-07-19",
        "summary": "When doing private domain marketing with cloud services, the merchants\nusually have to purchase different machine learning models for the multiple\nmarketing purposes, leading to a very high cost. We present a unified user-item\nmatching framework to simultaneously conduct item recommendation and user\ntargeting with just one model. We empirically demonstrate that the above\nconcurrent modeling is viable via modeling the user-item interaction matrix\nwith the multinomial distribution, and propose a bidirectional bias-corrected\nNCE loss for the implementation. The proposed loss function guides the model to\nlearn the user-item joint probability $p(u,i)$ instead of the conditional\nprobability $p(i|u)$ or $p(u|i)$ through correcting both the users and items'\nbiases caused by the in-batch negative sampling. In addition, our framework is\nmodel-agnostic enabling a flexible adaptation of different model architectures.\nExtensive experiments demonstrate that our framework results in significant\nperformance gains in comparison with the state-of-the-art methods, with greatly\nreduced cost on computing resources and daily maintenance.",
        "translated": ""
    },
    {
        "title": "Our Model Achieves Excellent Performance on MovieLens: What Does it\n  Mean?",
        "url": "http://arxiv.org/abs/2307.09985v1",
        "pub_date": "2023-07-19",
        "summary": "A typical benchmark dataset for recommender system (RecSys) evaluation\nconsists of user-item interactions generated on a platform within a time\nperiod. The interaction generation mechanism partially explains why a user\ninteracts with (e.g.,like, purchase, rate) an item, and the context of when a\nparticular interaction happened. In this study, we conduct a meticulous\nanalysis on the MovieLens dataset and explain the potential impact on using the\ndataset for evaluating recommendation algorithms. We make a few main findings\nfrom our analysis. First, there are significant differences in user\ninteractions at the different stages when a user interacts with the MovieLens\nplatform. The early interactions largely define the user portrait which affect\nthe subsequent interactions. Second, user interactions are highly affected by\nthe candidate movies that are recommended by the platform's internal\nrecommendation algorithm(s). Removal of interactions that happen nearer to the\nlast few interactions of a user leads to increasing difficulty in learning user\npreference, thus deteriorating recommendation accuracy. Third, changing the\norder of user interactions makes it more difficult for sequential algorithms to\ncapture the progressive interaction process. Based on these findings, we\nfurther discuss the discrepancy between the interaction generation mechanism\nthat is employed by the MovieLens system and that of typical real world\nrecommendation scenarios. In summary, models that achieve excellent\nrecommendation accuracy on the MovieLens dataset may not demonstrate superior\nperformance in practice for at least two kinds of differences: (i) the\ndifferences in the contexts of user-item interaction generation, and (ii) the\ndifferences in user knowledge about the item collections.",
        "translated": ""
    },
    {
        "title": "Who Provides the Largest Megaphone? The Role of Google News in Promoting\n  Russian State-Affiliated News Sources",
        "url": "http://arxiv.org/abs/2307.09834v1",
        "pub_date": "2023-07-19",
        "summary": "The Internet has not only digitized but also democratized information access\nacross the globe. This gradual but path-breaking move to online information\npropagation has resulted in search engines playing an increasingly prominent\nrole in shaping access to human knowledge. When an Internet user enters a\nquery, the search engine sorts through the hundreds of billions of possible\nwebpages to determine what to show. Google dominates the search engine market,\nwith Google Search surpassing 80% market share globally every year of the last\ndecade. Only in Russia and China do Google competitors claim more market share,\nwith approximately 60% of Internet users in Russia preferring Yandex (compared\nto 40% in favor of Google) and more than 80% of China's Internet users\naccessing Baidu as of 2022. Notwithstanding this long-standing regional\nvariation in Internet search providers, there is limited research showing how\nthese providers compare in terms of propagating state-sponsored information.\nOur study fills this research gap by focusing on Russian cyberspace and\nexamining how Google and Yandex's search algorithms rank content from Russian\nstate-controlled media (hereon, RSM) outlets. This question is timely and of\npractical interest given widespread reports indicating that RSM outlets have\nactively engaged in promoting Kremlin propaganda in the lead-up to, and in the\naftermath of, the Russian invasion of Ukraine in February 2022.",
        "translated": ""
    },
    {
        "title": "DisCover: Disentangled Music Representation Learning for Cover Song\n  Identification",
        "url": "http://arxiv.org/abs/2307.09775v1",
        "pub_date": "2023-07-19",
        "summary": "In the field of music information retrieval (MIR), cover song identification\n(CSI) is a challenging task that aims to identify cover versions of a query\nsong from a massive collection. Existing works still suffer from high\nintra-song variances and inter-song correlations, due to the entangled nature\nof version-specific and version-invariant factors in their modeling. In this\nwork, we set the goal of disentangling version-specific and version-invariant\nfactors, which could make it easier for the model to learn invariant music\nrepresentations for unseen query songs. We analyze the CSI task in a\ndisentanglement view with the causal graph technique, and identify the\nintra-version and inter-version effects biasing the invariant learning. To\nblock these effects, we propose the disentangled music representation learning\nframework (DisCover) for CSI. DisCover consists of two critical components: (1)\nKnowledge-guided Disentanglement Module (KDM) and (2) Gradient-based\nAdversarial Disentanglement Module (GADM), which block intra-version and\ninter-version biased effects, respectively. KDM minimizes the mutual\ninformation between the learned representations and version-variant factors\nthat are identified with prior domain knowledge. GADM identifies\nversion-variant factors by simulating the representation transitions between\nintra-song versions, and exploits adversarial distillation for effect blocking.\nExtensive comparisons with best-performing methods and in-depth analysis\ndemonstrate the effectiveness of DisCover and the and necessity of\ndisentanglement for CSI.",
        "translated": ""
    },
    {
        "title": "Information Retrieval Meets Large Language Models: A Strategic Report\n  from Chinese IR Community",
        "url": "http://arxiv.org/abs/2307.09751v1",
        "pub_date": "2023-07-19",
        "summary": "The research field of Information Retrieval (IR) has evolved significantly,\nexpanding beyond traditional search to meet diverse user information needs.\nRecently, Large Language Models (LLMs) have demonstrated exceptional\ncapabilities in text understanding, generation, and knowledge inference,\nopening up exciting avenues for IR research. LLMs not only facilitate\ngenerative retrieval but also offer improved solutions for user understanding,\nmodel evaluation, and user-system interactions. More importantly, the\nsynergistic relationship among IR models, LLMs, and humans forms a new\ntechnical paradigm that is more powerful for information seeking. IR models\nprovide real-time and relevant information, LLMs contribute internal knowledge,\nand humans play a central role of demanders and evaluators to the reliability\nof information services. Nevertheless, significant challenges exist, including\ncomputational costs, credibility concerns, domain-specific limitations, and\nethical considerations. To thoroughly discuss the transformative impact of LLMs\non IR research, the Chinese IR community conducted a strategic workshop in\nApril 2023, yielding valuable insights. This paper provides a summary of the\nworkshop's outcomes, including the rethinking of IR's core values, the mutual\nenhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and\nopen challenges.",
        "translated": ""
    },
    {
        "title": "Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for\n  Recommendation and Text Generation",
        "url": "http://arxiv.org/abs/2307.09688v1",
        "pub_date": "2023-07-19",
        "summary": "Modeling customer shopping intentions is a crucial task for e-commerce, as it\ndirectly impacts user experience and engagement. Thus, accurately understanding\ncustomer preferences is essential for providing personalized recommendations.\nSession-based recommendation, which utilizes customer session data to predict\ntheir next interaction, has become increasingly popular. However, existing\nsession datasets have limitations in terms of item attributes, user diversity,\nand dataset scale. As a result, they cannot comprehensively capture the\nspectrum of user behaviors and preferences. To bridge this gap, we present the\nAmazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It\nis the first multilingual dataset consisting of millions of user sessions from\nsix different locales, where the major languages of products are English,\nGerman, Japanese, French, Italian, and Spanish. Remarkably, the dataset can\nhelp us enhance personalization and understanding of user preferences, which\ncan benefit various existing tasks as well as enable new tasks. To test the\npotential of the dataset, we introduce three tasks in this work: (1)\nnext-product recommendation, (2) next-product recommendation with domain\nshifts, and (3) next-product title generation. With the above tasks, we\nbenchmark a range of algorithms on our proposed dataset, drawing new insights\nfor further research and practice. In addition, based on the proposed dataset\nand tasks, we hosted a competition in the KDD CUP 2023 and have attracted\nthousands of users and submissions. The winning solutions and the associated\nworkshop can be accessed at our website https://kddcup23.github.io/.",
        "translated": ""
    },
    {
        "title": "PubMed and Beyond: Recent Advances and Best Practices in Biomedical\n  Literature Search",
        "url": "http://arxiv.org/abs/2307.09683v1",
        "pub_date": "2023-07-18",
        "summary": "Biomedical research yields a wealth of information, much of which is only\naccessible through the literature. Consequently, literature search is an\nessential tool for building on prior knowledge in clinical and biomedical\nresearch. Although recent improvements in artificial intelligence have expanded\nfunctionality beyond keyword-based search, these advances may be unfamiliar to\nclinicians and researchers. In response, we present a survey of literature\nsearch tools tailored to both general and specific information needs in\nbiomedicine, with the objective of helping readers efficiently fulfill their\ninformation needs. We first examine the widely used PubMed search engine,\ndiscussing recent improvements and continued challenges. We then describe\nliterature search tools catering to five specific information needs: 1.\nIdentifying high-quality clinical research for evidence-based medicine. 2.\nRetrieving gene-related information for precision medicine and genomics. 3.\nSearching by meaning, including natural language questions. 4. Locating related\narticles with literature recommendation. 5. Mining literature to discover\nassociations between concepts such as diseases and genetic variants.\nAdditionally, we cover practical considerations and best practices for choosing\nand using these tools. Finally, we provide a perspective on the future of\nliterature search engines, considering recent breakthroughs in large language\nmodels such as ChatGPT. In summary, our survey provides a comprehensive view of\nbiomedical literature search functionalities with 36 publicly available tools.",
        "translated": ""
    },
    {
        "title": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation",
        "url": "http://arxiv.org/abs/2307.11019v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require\na substantial amount of factual knowledge and often rely on external\ninformation for assistance. Recently, large language models (LLMs) (e.g.,\nChatGPT), have demonstrated impressive prowess in solving a wide range of tasks\nwith world knowledge, including knowledge-intensive tasks. However, it remains\nunclear how well LLMs are able to perceive their factual knowledge boundaries,\nparticularly how they behave when incorporating retrieval augmentation. In this\nstudy, we present an initial analysis of the factual knowledge boundaries of\nLLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially,\nwe focus on three primary research questions and analyze them by examining QA\nperformance, priori judgement and posteriori judgement of LLMs. We show\nevidence that LLMs possess unwavering confidence in their capabilities to\nrespond to questions and the accuracy of their responses. Furthermore,\nretrieval augmentation proves to be an effective approach in enhancing LLMs'\nawareness of knowledge boundaries, thereby improving their judgemental\nabilities. Additionally, we also find that LLMs have a propensity to rely on\nthe provided retrieval results when formulating answers, while the quality of\nthese results significantly impacts their reliance. The code to reproduce this\nwork is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
        "translated": ""
    },
    {
        "title": "Enhancing Job Recommendation through LLM-based Generative Adversarial\n  Networks",
        "url": "http://arxiv.org/abs/2307.10747v1",
        "pub_date": "2023-07-20",
        "summary": "Recommending suitable jobs to users is a critical task in online recruitment\nplatforms, as it can enhance users' satisfaction and the platforms'\nprofitability. While existing job recommendation methods encounter challenges\nsuch as the low quality of users' resumes, which hampers their accuracy and\npractical effectiveness. With the rapid development of large language models\n(LLMs), utilizing the rich external knowledge encapsulated within them, as well\nas their powerful capabilities of text processing and reasoning, is a promising\nway to complete users' resumes for more accurate recommendations. However,\ndirectly leveraging LLMs to enhance recommendation results is not a\none-size-fits-all solution, as LLMs may suffer from fabricated generation and\nfew-shot problems, which degrade the quality of resume completion. In this\npaper, we propose a novel LLM-based approach for job recommendation. To\nalleviate the limitation of fabricated generation for LLMs, we extract accurate\nand valuable information beyond users' self-description, which helps the LLMs\nbetter profile users for resume completion. Specifically, we not only extract\nusers' explicit properties (e.g., skills, interests) from their\nself-description but also infer users' implicit characteristics from their\nbehaviors for more accurate and meaningful resume completion. Nevertheless,\nsome users still suffer from few-shot problems, which arise due to scarce\ninteraction records, leading to limited guidance for the models in generating\nhigh-quality resumes. To address this issue, we propose aligning unpaired\nlow-quality with high-quality generated resumes by Generative Adversarial\nNetworks (GANs), which can refine the resume representations for better\nrecommendation results. Extensive experiments on three large real-world\nrecruitment datasets demonstrate the effectiveness of our proposed method.",
        "translated": ""
    },
    {
        "title": "A Constraint-based Recommender System via RDF Knowledge Graphs",
        "url": "http://arxiv.org/abs/2307.10702v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge graphs, represented in RDF, are able to model entities and their\nrelations by means of ontologies. The use of knowledge graphs for information\nmodeling has attracted interest in recent years. In recommender systems, items\nand users can be mapped and integrated into the knowledge graph, which can\nrepresent more links and relationships between users and items.\nConstraint-based recommender systems are based on the idea of explicitly\nexploiting deep recommendation knowledge through constraints to identify\nrelevant recommendations. When combined with knowledge graphs, a\nconstraint-based recommender system gains several benefits in terms of\nconstraint sets. In this paper, we investigate and propose the construction of\na constraint-based recommender system via RDF knowledge graphs applied to the\nvehicle purchase/sale domain. The results of our experiments show that the\nproposed approach is able to efficiently identify recommendations in accordance\nwith user preferences.",
        "translated": ""
    },
    {
        "title": "A Personalized Recommender System Based-on Knowledge Graph Embeddings",
        "url": "http://arxiv.org/abs/2307.10680v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge graphs have proven to be effective for modeling entities and their\nrelationships through the use of ontologies. The recent emergence in interest\nfor using knowledge graphs as a form of information modeling has led to their\nincreased adoption in recommender systems. By incorporating users and items\ninto the knowledge graph, these systems can better capture the implicit\nconnections between them and provide more accurate recommendations. In this\npaper, we investigate and propose the construction of a personalized\nrecommender system via knowledge graphs embedding applied to the vehicle\npurchase/sale domain. The results of our experimentation demonstrate the\nefficacy of the proposed method in providing relevant recommendations that are\nconsistent with individual users.",
        "translated": ""
    },
    {
        "title": "Language-Enhanced Session-Based Recommendation with Decoupled\n  Contrastive Learning",
        "url": "http://arxiv.org/abs/2307.10650v1",
        "pub_date": "2023-07-20",
        "summary": "Session-based recommendation techniques aim to capture dynamic user behavior\nby analyzing past interactions. However, existing methods heavily rely on\nhistorical item ID sequences to extract user preferences, leading to challenges\nsuch as popular bias and cold-start problems. In this paper, we propose a\nhybrid multimodal approach for session-based recommendation to address these\nchallenges. Our approach combines different modalities, including textual\ncontent and item IDs, leveraging the complementary nature of these modalities\nusing CatBoost. To learn universal item representations, we design a language\nrepresentation-based item retrieval architecture that extracts features from\nthe textual content utilizing pre-trained language models. Furthermore, we\nintroduce a novel Decoupled Contrastive Learning method to enhance the\neffectiveness of the language representation. This technique decouples the\nsequence representation and item representation space, facilitating\nbidirectional alignment through dual-queue contrastive learning.\nSimultaneously, the momentum queue provides a large number of negative samples,\neffectively enhancing the effectiveness of contrastive learning. Our approach\nyielded competitive results, securing a 5th place ranking in KDD CUP 2023 Task\n1. We have released the source code and pre-trained models associated with this\nwork.",
        "translated": ""
    },
    {
        "title": "Improving Semantic Similarity Measure Within a Recommender System\n  Based-on RDF Graphs",
        "url": "http://arxiv.org/abs/2307.10639v1",
        "pub_date": "2023-07-20",
        "summary": "In today's era of information explosion, more users are becoming more reliant\nupon recommender systems to have better advice, suggestions, or inspire them.\nThe measure of the semantic relatedness or likeness between terms, words, or\ntext data plays an important role in different applications dealing with\ntextual data, as in a recommender system. Over the past few years, many\nontologies have been developed and used as a form of structured representation\nof knowledge bases for information systems. The measure of semantic similarity\nfrom ontology has developed by several methods. In this paper, we propose and\ncarry on an approach for the improvement of semantic similarity calculations\nwithin a recommender system based-on RDF graphs.",
        "translated": ""
    },
    {
        "title": "Detecting deceptive reviews using text classification",
        "url": "http://arxiv.org/abs/2307.10617v1",
        "pub_date": "2023-07-20",
        "summary": "In recent years, online reviews play a vital role for promoting any kind of\nproduct or services. Businesses may embed fake reviews in order to attract\ncustomers to purchase their products. They may even highlight the benefits of\ntheir own product or criticize the competition's product. Marketers,\nadvertisers, and other online business users have incentive to create fake\npositive reviews for products which they want to promote or give fake negative\nreviews for products which they really don't like. So now-a-days writing a\ndeceptive review is inevitable thing for promoting their own business or\ndegrading competitor's reputation. Thus, identifying deceptive reviews is an\nintense and on-going research area. This research paper proposes machine\nlearning model approach to identify deceptive reviews. The paper investigates\nthe performance of the several experiments done on a Deceptive Opinion Spam\nCorpus dataset of restaurants reviews. We developed a n-gram model and max\nfeatures to identify deceptive contents with a particular focus on fake\nreviews. Further, we conduct a benchmark study to investigate the performance\nof two different features extraction techniques and apply five machine learning\nclassification techniques. The experimental results show that passive\naggressive classifier outperforms other algorithms, and it reaches the highest\naccuracy not only in text classification but also to fake reviews. We also\nstudy the data augmentation and implement different deep learning techniques.",
        "translated": ""
    },
    {
        "title": "SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot\n  Neural Sparse Retrieval",
        "url": "http://arxiv.org/abs/2307.10488v1",
        "pub_date": "2023-07-19",
        "summary": "Traditionally, sparse retrieval systems relied on lexical representations to\nretrieve documents, such as BM25, dominated information retrieval tasks. With\nthe onset of pre-trained transformer models such as BERT, neural sparse\nretrieval has led to a new paradigm within retrieval. Despite the success,\nthere has been limited software supporting different sparse retrievers running\nin a unified, common environment. This hinders practitioners from fairly\ncomparing different sparse models and obtaining realistic evaluation results.\nAnother missing piece is, that a majority of prior work evaluates sparse\nretrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO.\nHowever, a key requirement in practical retrieval systems requires models that\ncan generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In\nthis work, we provide SPRINT, a unified Python toolkit based on Pyserini and\nLucene, supporting a common interface for evaluating neural sparse retrieval.\nThe toolkit currently includes five built-in models: uniCOIL, DeepImpact,\nSPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by\ndefining their term weighting method. Using our toolkit, we establish strong\nand reproducible zero-shot sparse retrieval baselines across the\nwell-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2\nachieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural\nsparse retrievers. In this work, we further uncover the reasons behind its\nperformance gain. We show that SPLADEv2 produces sparse representations with a\nmajority of tokens outside of the original query and document which is often\ncrucial for its performance gains, i.e. a limitation among its other sparse\ncounterparts. We provide our SPRINT toolkit, models, and data used in our\nexperiments publicly here at https://github.com/thakur-nandan/sprint.",
        "translated": ""
    },
    {
        "title": "Fast Approximate Nearest Neighbor Search with a Dynamic Exploration\n  Graph using Continuous Refinement",
        "url": "http://arxiv.org/abs/2307.10479v1",
        "pub_date": "2023-07-19",
        "summary": "For approximate nearest neighbor search, graph-based algorithms have shown to\noffer the best trade-off between accuracy and search time. We propose the\nDynamic Exploration Graph (DEG) which significantly outperforms existing\nalgorithms in terms of search and exploration efficiency by combining two new\nideas: First, a single undirected even regular graph is incrementally built by\npartially replacing existing edges to integrate new vertices and to update old\nneighborhoods at the same time. Secondly, an edge optimization algorithm is\nused to continuously improve the quality of the graph. Combining this ongoing\nrefinement with the graph construction process leads to a well-organized graph\nstructure at all times, resulting in: (1) increased search efficiency, (2)\npredictable index size, (3) guaranteed connectivity and therefore reachability\nof all vertices, and (4) a dynamic graph structure. In addition we investigate\nhow well existing graph-based search systems can handle indexed queries where\nthe seed vertex of a search is the query itself. Such exploration tasks,\ndespite their good starting point, are not necessarily easy. High efficiency in\napproximate nearest neighbor search (ANNS) does not automatically imply good\nperformance in exploratory search. Extensive experiments show that our new\nDynamic Exploration Graph outperforms existing algorithms significantly for\nindexed and unindexed queries.",
        "translated": ""
    },
    {
        "title": "Classification of Visualization Types and Perspectives in Patents",
        "url": "http://arxiv.org/abs/2307.10471v1",
        "pub_date": "2023-07-19",
        "summary": "Due to the swift growth of patent applications each year, information and\nmultimedia retrieval approaches that facilitate patent exploration and\nretrieval are of utmost importance. Different types of visualizations (e.g.,\ngraphs, technical drawings) and perspectives (e.g., side view, perspective) are\nused to visualize details of innovations in patents. The classification of\nthese images enables a more efficient search and allows for further analysis.\nSo far, datasets for image type classification miss some important\nvisualization types for patents. Furthermore, related work does not make use of\nrecent deep learning approaches including transformers. In this paper, we adopt\nstate-of-the-art deep learning methods for the classification of visualization\ntypes and perspectives in patent images. We extend the CLEF-IP dataset for\nimage type classification in patents to ten classes and provide manual ground\ntruth annotations. In addition, we derive a set of hierarchical classes from a\ndataset that provides weakly-labeled data for image perspectives. Experimental\nresults have demonstrated the feasibility of the proposed approaches. Source\ncode, models, and dataset will be made publicly available.",
        "translated": ""
    },
    {
        "title": "Alleviating the Long-Tail Problem in Conversational Recommender Systems",
        "url": "http://arxiv.org/abs/2307.11650v1",
        "pub_date": "2023-07-21",
        "summary": "Conversational recommender systems (CRS) aim to provide the recommendation\nservice via natural language conversations. To develop an effective CRS,\nhigh-quality CRS datasets are very crucial. However, existing CRS datasets\nsuffer from the long-tail issue, \\ie a large proportion of items are rarely (or\neven never) mentioned in the conversations, which are called long-tail items.\nAs a result, the CRSs trained on these datasets tend to recommend frequent\nitems, and the diversity of the recommended items would be largely reduced,\nmaking users easier to get bored.\n  To address this issue, this paper presents \\textbf{LOT-CRS}, a novel\nframework that focuses on simulating and utilizing a balanced CRS dataset (\\ie\ncovering all the items evenly) for improving \\textbf{LO}ng-\\textbf{T}ail\nrecommendation performance of CRSs. In our approach, we design two pre-training\ntasks to enhance the understanding of simulated conversation for long-tail\nitems, and adopt retrieval-augmented fine-tuning with label smoothness strategy\nto further improve the recommendation of long-tail items. Extensive experiments\non two public CRS datasets have demonstrated the effectiveness and\nextensibility of our approach, especially on long-tail recommendation.",
        "translated": ""
    },
    {
        "title": "Identifying document similarity using a fast estimation of the\n  Levenshtein Distance based on compression and signatures",
        "url": "http://arxiv.org/abs/2307.11496v1",
        "pub_date": "2023-07-21",
        "summary": "Identifying document similarity has many applications, e.g., source code\nanalysis or plagiarism detection. However, identifying similarities is not\ntrivial and can be time complex. For instance, the Levenshtein Distance is a\ncommon metric to define the similarity between two documents but has quadratic\nruntime which makes it impractical for large documents where large starts with\na few hundred kilobytes. In this paper, we present a novel concept that allows\nestimating the Levenshtein Distance: the algorithm first compresses documents\nto signatures (similar to hash values) using a user-defined compression ratio.\nSignatures can then be compared against each other (some constrains apply)\nwhere the outcome is the estimated Levenshtein Distance. Our evaluation shows\npromising results in terms of runtime efficiency and accuracy. In addition, we\nintroduce a significance score allowing examiners to set a threshold and\nidentify related documents.",
        "translated": ""
    },
    {
        "title": "Analysis of Elephant Movement in Sub-Saharan Africa: Ecological,\n  Climatic, and Conservation Perspectives",
        "url": "http://arxiv.org/abs/2307.11325v1",
        "pub_date": "2023-07-21",
        "summary": "The interaction between elephants and their environment has profound\nimplications for both ecology and conservation strategies. This study presents\nan analytical approach to decipher the intricate patterns of elephant movement\nin Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal\nvariations and rainfall patterns. Despite the complexities surrounding these\ninfluential factors, our analysis provides a holistic view of elephant\nmigratory behavior in the context of the dynamic African landscape. Our\ncomprehensive approach enables us to predict the potential impact of these\necological determinants on elephant migration, a critical step in establishing\ninformed conservation strategies. This projection is particularly crucial given\nthe impacts of global climate change on seasonal and rainfall patterns, which\ncould substantially influence elephant movements in the future. The findings of\nour work aim to not only advance the understanding of movement ecology but also\nfoster a sustainable coexistence of humans and elephants in Sub-Saharan Africa.\nBy predicting potential elephant routes, our work can inform strategies to\nminimize human-elephant conflict, effectively manage land use, and enhance\nanti-poaching efforts. This research underscores the importance of integrating\nmovement ecology and climatic variables for effective wildlife management and\nconservation planning.",
        "translated": ""
    },
    {
        "title": "Jina Embeddings: A Novel Set of High-Performance Sentence Embedding\n  Models",
        "url": "http://arxiv.org/abs/2307.11224v1",
        "pub_date": "2023-07-20",
        "summary": "Jina Embeddings constitutes a set of high-performance sentence embedding\nmodels adept at translating various textual inputs into numerical\nrepresentations, thereby capturing the semantic essence of the text. While\nthese models are not exclusively designed for text generation, they excel in\napplications such as dense retrieval and semantic textual similarity. This\npaper details the development of Jina Embeddings, starting with the creation of\na high-quality pairwise and triplet dataset. It underlines the crucial role of\ndata cleaning in dataset preparation, gives in-depth insights into the model\ntraining process, and concludes with a comprehensive performance evaluation\nusing the Massive Textual Embedding Benchmark (MTEB).",
        "translated": ""
    },
    {
        "title": "RCVaR: an Economic Approach to Estimate Cyberattacks Costs using Data\n  from Industry Reports",
        "url": "http://arxiv.org/abs/2307.11140v1",
        "pub_date": "2023-07-20",
        "summary": "Digitization increases business opportunities and the risk of companies being\nvictims of devastating cyberattacks. Therefore, managing risk exposure and\ncybersecurity strategies is essential for digitized companies that want to\nsurvive in competitive markets. However, understanding company-specific risks\nand quantifying their associated costs is not trivial. Current approaches fail\nto provide individualized and quantitative monetary estimations of\ncybersecurity impacts. Due to limited resources and technical expertise, SMEs\nand even large companies are affected and struggle to quantify their\ncyberattack exposure. Therefore, novel approaches must be placed to support the\nunderstanding of the financial loss due to cyberattacks. This article\nintroduces the Real Cyber Value at Risk (RCVaR), an economical approach for\nestimating cybersecurity costs using real-world information from public\ncybersecurity reports. RCVaR identifies the most significant cyber risk factors\nfrom various sources and combines their quantitative results to estimate\nspecific cyberattacks costs for companies. Furthermore, RCVaR extends current\nmethods to achieve cost and risk estimations based on historical real-world\ndata instead of only probability-based simulations. The evaluation of the\napproach on unseen data shows the accuracy and efficiency of the RCVaR in\npredicting and managing cyber risks. Thus, it shows that the RCVaR is a\nvaluable addition to cybersecurity planning and risk management processes.",
        "translated": ""
    },
    {
        "title": "HeteFedRec: Federated Recommender Systems with Model Heterogeneity",
        "url": "http://arxiv.org/abs/2307.12810v1",
        "pub_date": "2023-07-24",
        "summary": "Owing to the nature of privacy protection, federated recommender systems\n(FedRecs) have garnered increasing interest in the realm of on-device\nrecommender systems. However, most existing FedRecs only allow participating\nclients to collaboratively train a recommendation model of the same public\nparameter size. Training a model of the same size for all clients can lead to\nsuboptimal performance since clients possess varying resources. For example,\nclients with limited training data may prefer to train a smaller recommendation\nmodel to avoid excessive data consumption, while clients with sufficient data\nwould benefit from a larger model to achieve higher recommendation accuracy. To\naddress the above challenge, this paper introduces HeteFedRec, a novel FedRec\nframework that enables the assignment of personalized model sizes to\nparticipants. In HeteFedRec, we present a heterogeneous recommendation model\naggregation strategy, including a unified dual-task learning mechanism and a\ndimensional decorrelation regularization, to allow knowledge aggregation among\nrecommender models of different sizes. Additionally, a relation-based ensemble\nknowledge distillation method is proposed to effectively distil knowledge from\nheterogeneous item embeddings. Extensive experiments conducted on three\nreal-world recommendation datasets demonstrate the effectiveness and efficiency\nof HeteFedRec in training federated recommender systems under heterogeneous\nsettings.",
        "translated": ""
    },
    {
        "title": "RRAML: Reinforced Retrieval Augmented Machine Learning",
        "url": "http://arxiv.org/abs/2307.12798v1",
        "pub_date": "2023-07-24",
        "summary": "The emergence of large language models (LLMs) has revolutionized machine\nlearning and related fields, showcasing remarkable abilities in comprehending,\ngenerating, and manipulating human language. However, their conventional usage\nthrough API-based text prompt submissions imposes certain limitations in terms\nof context constraints and external source availability. To address these\nchallenges, we propose a novel framework called Reinforced Retrieval Augmented\nMachine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs\nwith supporting information retrieved by a purpose-built retriever from a vast\nuser-provided database. By leveraging recent advancements in reinforcement\nlearning, our method effectively addresses several critical challenges.\nFirstly, it circumvents the need for accessing LLM gradients. Secondly, our\nmethod alleviates the burden of retraining LLMs for specific tasks, as it is\noften impractical or impossible due to restricted access to the model and the\ncomputational intensity involved. Additionally we seamlessly link the\nretriever's task with the reasoner, mitigating hallucinations and reducing\nirrelevant, and potentially damaging retrieved documents. We believe that the\nresearch agenda outlined in this paper has the potential to profoundly impact\nthe field of AI, democratizing access to and utilization of LLMs for a wide\nrange of entities.",
        "translated": ""
    },
    {
        "title": "Unbiased Delayed Feedback Label Correction for Conversion Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2307.12756v1",
        "pub_date": "2023-07-24",
        "summary": "Conversion rate prediction is critical to many online applications such as\ndigital display advertising. To capture dynamic data distribution, industrial\nsystems often require retraining models on recent data daily or weekly.\nHowever, the delay of conversion behavior usually leads to incorrect labeling,\nwhich is called delayed feedback problem. Existing work may fail to introduce\nthe correct information about false negative samples due to data sparsity and\ndynamic data distribution. To directly introduce the correct feedback label\ninformation, we propose an Unbiased delayed feedback Label Correction framework\n(ULC), which uses an auxiliary model to correct labels for observed negative\nfeedback samples. Firstly, we theoretically prove that the label-corrected loss\nis an unbiased estimate of the oracle loss using true labels. Then, as there\nare no ready training data for label correction, counterfactual labeling is\nused to construct artificial training data. Furthermore, since counterfactual\nlabeling utilizes only partial training data, we design an embedding-based\nalternative training method to enhance performance. Comparative experiments on\nboth public and private datasets and detailed analyses show that our proposed\napproach effectively alleviates the delayed feedback problem and consistently\noutperforms the previous state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Self-refining of Pseudo Labels for Music Source Separation with Noisy\n  Labeled Data",
        "url": "http://arxiv.org/abs/2307.12576v1",
        "pub_date": "2023-07-24",
        "summary": "Music source separation (MSS) faces challenges due to the limited\navailability of correctly-labeled individual instrument tracks. With the push\nto acquire larger datasets to improve MSS performance, the inevitability of\nencountering mislabeled individual instrument tracks becomes a significant\nchallenge to address. This paper introduces an automated technique for refining\nthe labels in a partially mislabeled dataset. Our proposed self-refining\ntechnique, employed with a noisy-labeled dataset, results in only a 1% accuracy\ndegradation in multi-label instrument recognition compared to a classifier\ntrained on a clean-labeled dataset. The study demonstrates the importance of\nrefining noisy-labeled data in MSS model training and shows that utilizing the\nrefined dataset leads to comparable results derived from a clean-labeled\ndataset. Notably, upon only access to a noisy dataset, MSS models trained on a\nself-refined dataset even outperform those trained on a dataset refined with a\nclassifier trained on clean labels.",
        "translated": ""
    },
    {
        "title": "FaFCNN: A General Disease Classification Framework Based on Feature\n  Fusion Neural Networks",
        "url": "http://arxiv.org/abs/2307.12518v1",
        "pub_date": "2023-07-24",
        "summary": "There are two fundamental problems in applying deep learning/machine learning\nmethods to disease classification tasks, one is the insufficient number and\npoor quality of training samples; another one is how to effectively fuse\nmultiple source features and thus train robust classification models. To\naddress these problems, inspired by the process of human learning knowledge, we\npropose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which\nintroduces a feature-aware interaction module and a feature alignment module\nbased on domain adversarial learning. This is a general framework for disease\nclassification, and FaFCNN improves the way existing methods obtain sample\ncorrelation features. The experimental results show that training using\naugmented features obtained by pre-training gradient boosting decision tree\nyields more performance gains than random-forest based methods. On the\nlow-quality dataset with a large amount of missing data in our setup, FaFCNN\nobtains a consistently optimal performance compared to competitive baselines.\nIn addition, extensive experiments demonstrate the robustness of the proposed\nmethod and the effectiveness of each component of the model\\footnote{Accepted\nin IEEE SMC2023}.",
        "translated": ""
    },
    {
        "title": "Interface Design to Mitigate Inflation in Recommender Systems",
        "url": "http://arxiv.org/abs/2307.12424v1",
        "pub_date": "2023-07-23",
        "summary": "Recommendation systems rely on user-provided data to learn about item quality\nand provide personalized recommendations. An implicit assumption when\naggregating ratings into item quality is that ratings are strong indicators of\nitem quality. In this work, we test this assumption using data collected from a\nmusic discovery application. Our study focuses on two factors that cause rating\ninflation: heterogeneous user rating behavior and the dynamics of personalized\nrecommendations. We show that user rating behavior substantially varies by\nuser, leading to item quality estimates that reflect the users who rated an\nitem more than the item quality itself. Additionally, items that are more\nlikely to be shown via personalized recommendations can experience a\nsubstantial increase in their exposure and potential bias toward them. To\nmitigate these effects, we analyze the results of a randomized controlled trial\nin which the rating interface was modified. The test resulted in a substantial\nimprovement in user rating behavior and a reduction in item quality inflation.\nThese findings highlight the importance of carefully considering the\nassumptions underlying recommendation systems and designing interfaces that\nencourage accurate rating behavior.",
        "translated": ""
    },
    {
        "title": "RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC",
        "url": "http://arxiv.org/abs/2307.12301v1",
        "pub_date": "2023-07-23",
        "summary": "Image outlier detection (OD) is crucial for ensuring the quality and accuracy\nof image datasets used in computer vision tasks. The majority of OD algorithms,\nhowever, have not been targeted toward image data. Consequently, the results of\napplying such algorithms to images are often suboptimal. In this work, we\npropose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for\nimages. By comparing images in a RANSAC-based approach, our algorithm\nautomatically predicts the outlier score of each image without additional\ntraining or label information. We evaluate RANSAC-NN against state-of-the-art\nOD algorithms on 15 diverse datasets. Without any hyperparameter tuning,\nRANSAC-NN consistently performs favorably in contrast to other algorithms in\nalmost every dataset category. Furthermore, we provide a detailed analysis to\nunderstand each RANSAC-NN component, and we demonstrate its potential\napplications in image mislabeled detection. Code for RANSAC-NN is provided at\nhttps://github.com/mxtsai/ransac-nn",
        "translated": ""
    },
    {
        "title": "Conformal Group Recommender System",
        "url": "http://arxiv.org/abs/2307.12034v1",
        "pub_date": "2023-07-22",
        "summary": "Group recommender systems (GRS) are critical in discovering relevant items\nfrom a near-infinite inventory based on group preferences rather than\nindividual preferences, like recommending a movie, restaurant, or tourist\ndestination to a group of individuals. The traditional models of group\nrecommendation are designed to act like a black box with a strict focus on\nimproving recommendation accuracy, and most often, they place the onus on the\nusers to interpret recommendations. In recent years, the focus of Recommender\nSystems (RS) research has shifted away from merely improving recommendation\naccuracy towards value additions such as confidence and explanation. In this\nwork, we propose a conformal prediction framework that provides a measure of\nconfidence with prediction in conjunction with a group recommender system to\naugment the system-generated plain recommendations. In the context of group\nrecommender systems, we propose various nonconformity measures that play a\nvital role in the efficiency of the conformal framework. We also show that\ndefined nonconformity satisfies the exchangeability property. Experimental\nresults demonstrate the effectiveness of the proposed approach over several\nbenchmark datasets. Furthermore, our proposed approach also satisfies validity\nand efficiency properties.",
        "translated": ""
    },
    {
        "title": "XWalk: Random Walk Based Candidate Retrieval for Product Search",
        "url": "http://arxiv.org/abs/2307.12019v1",
        "pub_date": "2023-07-22",
        "summary": "In e-commerce, head queries account for the vast majority of gross\nmerchandise sales and improvements to head queries are highly impactful to the\nbusiness. While most supervised approaches to search perform better in head\nqueries vs. tail queries, we propose a method that further improves head query\nperformance dramatically. We propose XWalk, a random-walk based graph approach\nto candidate retrieval for product search that borrows from recommendation\nsystem techniques. XWalk is highly efficient to train and inference in a\nlarge-scale high traffic e-commerce setting, and shows substantial improvements\nin head query performance over state-of-the-art neural retreivers. Ensembling\nXWalk with a neural and/or lexical retriever combines the best of both worlds\nand the resulting retrieval system outperforms all other methods in both\noffline relevance-based evaluation and in online A/B tests.",
        "translated": ""
    },
    {
        "title": "HTP: Exploiting Holistic Temporal Patterns for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2307.11994v1",
        "pub_date": "2023-07-22",
        "summary": "Sequential recommender systems have demonstrated a huge success for next-item\nrecommendation by explicitly exploiting the temporal order of users' historical\ninteractions. In practice, user interactions contain more useful temporal\ninformation beyond order, as shown by some pioneering studies. In this paper,\nwe systematically investigate various temporal information for sequential\nrecommendation and identify three types of advantageous temporal patterns\nbeyond order, including absolute time information, relative item time intervals\nand relative recommendation time intervals. We are the first to explore\nitem-oriented absolute time patterns. While existing models consider only one\nor two of these three patterns, we propose a novel holistic temporal pattern\nbased neural network, named HTP, to fully leverage all these three patterns. In\nparticular, we introduce novel components to address the subtle correlations\nbetween relative item time intervals and relative recommendation time\nintervals, which render a major technical challenge. Extensive experiments on\nthree real-world benchmark datasets show that our HTP model consistently and\nsubstantially outperforms many state-of-the-art models. Our code is publically\navailable at https://github.com/623851394/HTP/tree/main/HTP-main",
        "translated": ""
    },
    {
        "title": "Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning",
        "url": "http://arxiv.org/abs/2307.13632v1",
        "pub_date": "2023-07-25",
        "summary": "Mainstream bias, where some users receive poor recommendations because their\npreferences are uncommon or simply because they are less active, is an\nimportant aspect to consider regarding fairness in recommender systems.\nExisting methods to mitigate mainstream bias do not explicitly model the\nimportance of these non-mainstream users or, when they do, it is in a way that\nis not necessarily compatible with the data and recommendation model at hand.\nIn contrast, we use the recommendation utility as a more generic and implicit\nproxy to quantify mainstreamness, and propose a simple user-weighting approach\nto incorporate it into the training process while taking the cost of potential\nrecommendation errors into account. We provide extensive experimental results\nshowing that quantifying mainstreamness via utility is better able at\nidentifying non-mainstream users, and that they are indeed better served when\ntraining the model in a cost-sensitive way. This is achieved with negligible or\nno loss in overall recommendation accuracy, meaning that the models learn a\nbetter balance across users. In addition, we show that research of this kind,\nwhich evaluates recommendation quality at the individual user level, may not be\nreliable if not using enough interactions when assessing model performance.",
        "translated": ""
    },
    {
        "title": "Gaussian Graph with Prototypical Contrastive Learning in E-Commerce\n  Bundle Recommendation",
        "url": "http://arxiv.org/abs/2307.13468v1",
        "pub_date": "2023-07-25",
        "summary": "Bundle recommendation aims to provide a bundle of items to satisfy the user\npreference on e-commerce platform. Existing successful solutions are based on\nthe contrastive graph learning paradigm where graph neural networks (GNNs) are\nemployed to learn representations from user-level and bundle-level graph views\nwith a contrastive learning module to enhance the cooperative association\nbetween different views. Nevertheless, they ignore the uncertainty issue which\nhas a significant impact in real bundle recommendation scenarios due to the\nlack of discriminative information caused by highly sparsity or diversity. We\nfurther suggest that their instancewise contrastive learning fails to\ndistinguish the semantically similar negatives (i.e., sampling bias issue),\nresulting in performance degradation. In this paper, we propose a novel\nGaussian Graph with Prototypical Contrastive Learning (GPCL) framework to\novercome these challenges. In particular, GPCL embeds each user/bundle/item as\na Gaussian distribution rather than a fixed vector. We further design a\nprototypical contrastive learning module to capture the contextual information\nand mitigate the sampling bias issue. Extensive experiments demonstrate that\nbenefiting from the proposed components, we achieve new state-of-the-art\nperformance compared to previous methods on several public datasets. Moreover,\nGPCL has been deployed on real-world e-commerce platform and achieved\nsubstantial improvements.",
        "translated": ""
    },
    {
        "title": "Comprehensive Review on Semantic Information Retrieval and Ontology\n  Engineering",
        "url": "http://arxiv.org/abs/2307.13427v1",
        "pub_date": "2023-07-25",
        "summary": "Situation awareness is a crucial cognitive skill that enables individuals to\nperceive, comprehend, and project the current state of their environment\naccurately. It involves being conscious of relevant information, understanding\nits meaning, and using that understanding to make well-informed decisions.\nAwareness systems often need to integrate new knowledge and adapt to changing\nenvironments. Ontology reasoning facilitates knowledge integration and\nevolution, allowing for seamless updates and expansions of the ontology. With\nthe consideration of above, we are providing a quick review on semantic\ninformation retrieval and ontology engineering to understand the emerging\nchallenges and future research. In the review we have found that the ontology\nreasoning addresses the limitations of traditional systems by providing a\nformal, flexible, and scalable framework for knowledge representation,\nreasoning, and inference.",
        "translated": ""
    },
    {
        "title": "An End-to-End Workflow using Topic Segmentation and Text Summarisation\n  Methods for Improved Podcast Comprehension",
        "url": "http://arxiv.org/abs/2307.13394v1",
        "pub_date": "2023-07-25",
        "summary": "The consumption of podcast media has been increasing rapidly. Due to the\nlengthy nature of podcast episodes, users often carefully select which ones to\nlisten to. Although episode descriptions aid users by providing a summary of\nthe entire podcast, they do not provide a topic-by-topic breakdown. This study\nexplores the combined application of topic segmentation and text summarisation\nmethods to investigate how podcast episode comprehension can be improved. We\nhave sampled 10 episodes from Spotify's English-Language Podcast Dataset and\nemployed TextTiling and TextSplit to segment them. Moreover, three text\nsummarisation models, namely T5, BART, and Pegasus, were applied to provide a\nvery short title for each segment. The segmentation part was evaluated using\nour annotated sample with the $P_k$ and WindowDiff ($WD$) metrics. A survey was\nalso rolled out ($N=25$) to assess the quality of the generated summaries. The\nTextSplit algorithm achieved the lowest mean for both evaluation metrics\n($\\bar{P_k}=0.41$ and $\\bar{WD}=0.41$), while the T5 model produced the best\nsummaries, achieving a relevancy score only $8\\%$ less to the one achieved by\nthe human-written titles.",
        "translated": ""
    },
    {
        "title": "Embedding Models for Supervised Automatic Extraction and Classification\n  of Named Entities in Scientific Acknowledgements",
        "url": "http://arxiv.org/abs/2307.13377v1",
        "pub_date": "2023-07-25",
        "summary": "Acknowledgments in scientific papers may give an insight into aspects of the\nscientific community, such as reward systems, collaboration patterns, and\nhidden research trends. The aim of the paper is to evaluate the performance of\ndifferent embedding models for the task of automatic extraction and\nclassification of acknowledged entities from the acknowledgment text in\nscientific papers. We trained and implemented a named entity recognition (NER)\ntask using the Flair NLP framework. The training was conducted using three\ndefault Flair NER models with four differently-sized corpora and different\nversions of the Flair NLP framework. The Flair Embeddings model trained on the\nmedium corpus with the latest FLAIR version showed the best accuracy of 0.79.\nExpanding the size of a training corpus from very small to medium size\nmassively increased the accuracy of all training algorithms, but further\nexpansion of the training corpus did not bring further improvement. Moreover,\nthe performance of the model slightly deteriorated. Our model is able to\nrecognize six entity types: funding agency, grant number, individuals,\nuniversity, corporation, and miscellaneous. The model works more precisely for\nsome entity types than for others; thus, individuals and grant numbers showed a\nvery good F1-Score over 0.9. Most of the previous works on acknowledgment\nanalysis were limited by the manual evaluation of data and therefore by the\namount of processed data. This model can be applied for the comprehensive\nanalysis of acknowledgment texts and may potentially make a great contribution\nto the field of automated acknowledgment analysis.",
        "translated": ""
    },
    {
        "title": "An Intent Taxonomy of Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2307.13298v1",
        "pub_date": "2023-07-25",
        "summary": "Legal case retrieval is a special Information Retrieval~(IR) task focusing on\nlegal case documents. Depending on the downstream tasks of the retrieved case\ndocuments, users' information needs in legal case retrieval could be\nsignificantly different from those in Web search and traditional ad-hoc\nretrieval tasks. While there are several studies that retrieve legal cases\nbased on text similarity, the underlying search intents of legal retrieval\nusers, as shown in this paper, are more complicated than that yet mostly\nunexplored. To this end, we present a novel hierarchical intent taxonomy of\nlegal case retrieval. It consists of five intent types categorized by three\ncriteria, i.e., search for Particular Case(s), Characterization, Penalty,\nProcedure, and Interest. The taxonomy was constructed transparently and\nevaluated extensively through interviews, editorial user studies, and query log\nanalysis. Through a laboratory user study, we reveal significant differences in\nuser behavior and satisfaction under different search intents in legal case\nretrieval. Furthermore, we apply the proposed taxonomy to various downstream\nlegal retrieval tasks, e.g., result ranking and satisfaction prediction, and\ndemonstrate its effectiveness. Our work provides important insights into the\nunderstanding of user intents in legal case retrieval and potentially leads to\nbetter retrieval techniques in the legal domain, such as intent-aware ranking\nstrategies and evaluation methodologies.",
        "translated": ""
    },
    {
        "title": "Investigating the Robustness of Sequential Recommender Systems Against\n  Training Data Perturbations: an Empirical Study",
        "url": "http://arxiv.org/abs/2307.13165v1",
        "pub_date": "2023-07-24",
        "summary": "Sequential Recommender Systems (SRSs) have been widely used to model user\nbehavior over time, but their robustness in the face of perturbations to\ntraining data is a critical issue. In this paper, we conduct an empirical study\nto investigate the effects of removing items at different positions within a\ntemporally ordered sequence. We evaluate two different SRS models on multiple\ndatasets, measuring their performance using Normalized Discounted Cumulative\nGain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that\nremoving items at the end of the sequence significantly impacts performance,\nwith NDCG decreasing up to 60\\%, while removing items from the beginning or\nmiddle has no significant effect. These findings highlight the importance of\nconsidering the position of the perturbed items in the training data and shall\ninform the design of more robust SRSs.",
        "translated": ""
    },
    {
        "title": "ChatGPT and Persuasive Technologies for the Management and Delivery of\n  Personalized Recommendations in Hotel Hospitality",
        "url": "http://arxiv.org/abs/2307.14298v1",
        "pub_date": "2023-07-26",
        "summary": "Recommender systems have become indispensable tools in the hotel hospitality\nindustry, enabling personalized and tailored experiences for guests. Recent\nadvancements in large language models (LLMs), such as ChatGPT, and persuasive\ntechnologies, have opened new avenues for enhancing the effectiveness of those\nsystems. This paper explores the potential of integrating ChatGPT and\npersuasive technologies for automating and improving hotel hospitality\nrecommender systems. First, we delve into the capabilities of ChatGPT, which\ncan understand and generate human-like text, enabling more accurate and\ncontext-aware recommendations. We discuss the integration of ChatGPT into\nrecommender systems, highlighting the ability to analyze user preferences,\nextract valuable insights from online reviews, and generate personalized\nrecommendations based on guest profiles. Second, we investigate the role of\npersuasive technology in influencing user behavior and enhancing the persuasive\nimpact of hotel recommendations. By incorporating persuasive techniques, such\nas social proof, scarcity and personalization, recommender systems can\neffectively influence user decision-making and encourage desired actions, such\nas booking a specific hotel or upgrading their room. To investigate the\nefficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment\nwith a case study involving a hotel recommender system. We aim to study the\nimpact of integrating ChatGPT and persua-sive techniques on user engagement,\nsatisfaction, and conversion rates. The preliminary results demonstrate the\npotential of these technologies in enhancing the overall guest experience and\nbusiness performance. Overall, this paper contributes to the field of hotel\nhospitality by exploring the synergistic relationship between LLMs and\npersuasive technology in recommender systems, ultimately influencing guest\nsatisfaction and hotel revenue.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Competitive Near Cold-start Recommenders for\n  Language- and Item-based Preferences",
        "url": "http://arxiv.org/abs/2307.14225v1",
        "pub_date": "2023-07-26",
        "summary": "Traditional recommender systems leverage users' item preference history to\nrecommend novel content that users may like. However, modern dialog interfaces\nthat allow users to express language-based preferences offer a fundamentally\ndifferent modality for preference input. Inspired by recent successes of\nprompting paradigms for large language models (LLMs), we study their use for\nmaking recommendations from both item-based and language-based preferences in\ncomparison to state-of-the-art item-based collaborative filtering (CF) methods.\nTo support this investigation, we collect a new dataset consisting of both\nitem-based and language-based preferences elicited from users along with their\nratings on a variety of (biased) recommended items and (unbiased) random items.\nAmong numerous experimental results, we find that LLMs provide competitive\nrecommendation performance for pure language-based preferences (no item\npreferences) in the near cold-start case in comparison to item-based CF\nmethods, despite having no supervised training for this specific task\n(zero-shot) or only a few labels (few-shot). This is particularly promising as\nlanguage-based preference representations are more explainable and scrutable\nthan item-based or vector-based representations.",
        "translated": ""
    },
    {
        "title": "A Probabilistic Position Bias Model for Short-Video Recommendation Feeds",
        "url": "http://arxiv.org/abs/2307.14059v1",
        "pub_date": "2023-07-26",
        "summary": "Modern web-based platforms show ranked lists of recommendations to users,\nattempting to maximise user satisfaction or business metrics. Typically, the\ngoal of such systems boils down to maximising the exposure probability for\nitems that are deemed \"reward-maximising\" according to a metric of interest.\nThis general framing comprises streaming applications, as well as e-commerce or\njob recommendations, and even web search. Position bias or user models can be\nused to estimate exposure probabilities for each use-case, specifically\ntailored to how users interact with the presented rankings. A unifying factor\nin these diverse problem settings is that typically only one or several items\nwill be engaged with (clicked, streamed,...) before a user leaves the ranked\nlist. Short-video feeds on social media platforms diverge from this general\nframing in several ways, most notably that users do not tend to leave the feed\nafter e.g. liking a post. Indeed, seemingly infinite feeds invite users to\nscroll further down the ranked list. For this reason, existing position bias or\nuser models tend to fall short in such settings, as they do not accurately\ncapture users' interaction modalities.\n  In this work, we propose a novel and probabilistically sound personalised\nposition bias model for feed recommendations. We focus on a 1st-level feed in a\nhierarchical structure, where users may enter a 2nd-level feed via any given\n1st-level item. We posit that users come to the platform with a scrolling\nbudget drawn according to some distribution, and show how the survival function\nof said distribution can be used to obtain closed-form estimates for\npersonalised exposure probabilities. Empirical insights from a large-scale\nsocial media platform show how our probabilistic position bias model more\naccurately captures empirical exposure than existing models, and paves the way\nfor unbiased evaluation and learning-to-rank.",
        "translated": ""
    },
    {
        "title": "Multi-view Hypergraph Contrastive Policy Learning for Conversational\n  Recommendation",
        "url": "http://arxiv.org/abs/2307.14024v1",
        "pub_date": "2023-07-26",
        "summary": "Conversational recommendation systems (CRS) aim to interactively acquire user\npreferences and accordingly recommend items to users. Accurately learning the\ndynamic user preferences is of crucial importance for CRS. Previous works learn\nthe user preferences with pairwise relations from the interactive conversation\nand item knowledge, while largely ignoring the fact that factors for a\nrelationship in CRS are multiplex. Specifically, the user likes/dislikes the\nitems that satisfy some attributes (Like/Dislike view). Moreover social\ninfluence is another important factor that affects user preference towards the\nitem (Social view), while is largely ignored by previous works in CRS. The user\npreferences from these three views are inherently different but also correlated\nas a whole. The user preferences from the same views should be more similar\nthan that from different views. The user preferences from Like View should be\nsimilar to Social View while different from Dislike View. To this end, we\npropose a novel model, namely Multi-view Hypergraph Contrastive Policy Learning\n(MHCPL). Specifically, MHCPL timely chooses useful social information according\nto the interactive history and builds a dynamic hypergraph with three types of\nmultiplex relations from different views. The multiplex relations in each view\nare successively connected according to their generation order.",
        "translated": ""
    },
    {
        "title": "Domain Disentanglement with Interpolative Data Augmentation for\n  Dual-Target Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2307.13910v1",
        "pub_date": "2023-07-26",
        "summary": "The conventional single-target Cross-Domain Recommendation (CDR) aims to\nimprove the recommendation performance on a sparser target domain by\ntransferring the knowledge from a source domain that contains relatively richer\ninformation. By contrast, in recent years, dual-target CDR has been proposed to\nimprove the recommendation performance on both domains simultaneously. However,\nto this end, there are two challenges in dual-target CDR: (1) how to generate\nboth relevant and diverse augmented user representations, and (2) how to\neffectively decouple domain-independent information from domain-specific\ninformation, in addition to domain-shared information, to capture comprehensive\nuser preferences. To address the above two challenges, we propose a\nDisentanglement-based framework with Interpolative Data Augmentation for\ndual-target Cross-Domain Recommendation, called DIDA-CDR. In DIDA-CDR, we first\npropose an interpolative data augmentation approach to generating both relevant\nand diverse augmented user representations to augment sparser domain and\nexplore potential user preferences. We then propose a disentanglement module to\neffectively decouple domain-specific and domain-independent information to\ncapture comprehensive user preferences. Both steps significantly contribute to\ncapturing more comprehensive user preferences, thereby improving the\nrecommendation performance on each domain. Extensive experiments conducted on\nfive real-world datasets show the significant superiority of DIDA-CDR over the\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "ClusterSeq: Enhancing Sequential Recommender Systems with Clustering\n  based Meta-Learning",
        "url": "http://arxiv.org/abs/2307.13766v1",
        "pub_date": "2023-07-25",
        "summary": "In practical scenarios, the effectiveness of sequential recommendation\nsystems is hindered by the user cold-start problem, which arises due to limited\ninteractions for accurately determining user preferences. Previous studies have\nattempted to address this issue by combining meta-learning with user and\nitem-side information. However, these approaches face inherent challenges in\nmodeling user preference dynamics, particularly for \"minor users\" who exhibit\ndistinct preferences compared to more common or \"major users.\" To overcome\nthese limitations, we present a novel approach called ClusterSeq, a\nMeta-Learning Clustering-Based Sequential Recommender System. ClusterSeq\nleverages dynamic information in the user sequence to enhance item prediction\naccuracy, even in the absence of side information. This model preserves the\npreferences of minor users without being overshadowed by major users, and it\ncapitalizes on the collective knowledge of users within the same cluster.\nExtensive experiments conducted on various benchmark datasets validate the\neffectiveness of ClusterSeq. Empirical results consistently demonstrate that\nClusterSeq outperforms several state-of-the-art meta-learning recommenders.\nNotably, compared to existing meta-learning methods, our proposed approach\nachieves a substantial improvement of 16-39% in Mean Reciprocal Rank (MRR).",
        "translated": ""
    },
    {
        "title": "On (Normalised) Discounted Cumulative Gain as an Offline Evaluation\n  Metric for Top-$n$ Recommendation",
        "url": "http://arxiv.org/abs/2307.15053v1",
        "pub_date": "2023-07-27",
        "summary": "Approaches to recommendation are typically evaluated in one of two ways: (1)\nvia a (simulated) online experiment, often seen as the gold standard, or (2)\nvia some offline evaluation procedure, where the goal is to approximate the\noutcome of an online experiment. Several offline evaluation metrics have been\nadopted in the literature, inspired by ranking metrics prevalent in the field\nof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one\nsuch metric that has seen widespread adoption in empirical studies, and higher\n(n)DCG values have been used to present new methods as the state-of-the-art in\ntop-$n$ recommendation for many years.\n  Our work takes a critical look at this approach, and investigates when we can\nexpect such metrics to approximate the gold standard outcome of an online\nexperiment. We formally present the assumptions that are necessary to consider\nDCG an unbiased estimator of online reward and provide a derivation for this\nmetric from first principles, highlighting where we deviate from its\ntraditional uses in IR. Importantly, we show that normalising the metric\nrenders it inconsistent, in that even when DCG is unbiased, ranking competing\nmethods by their normalised DCG can invert their relative order. Through a\ncorrelation analysis between off- and on-line experiments conducted on a\nlarge-scale recommendation platform, we show that our unbiased DCG estimates\nstrongly correlate with online reward, even when some of the metric's inherent\nassumptions are violated. This statement no longer holds for its normalised\nvariant, suggesting that nDCG's practical utility may be limited.",
        "translated": ""
    },
    {
        "title": "The Effect of Third Party Implementations on Reproducibility",
        "url": "http://arxiv.org/abs/2307.14956v1",
        "pub_date": "2023-07-27",
        "summary": "Reproducibility of recommender systems research has come under scrutiny\nduring recent years. Along with works focusing on repeating experiments with\ncertain algorithms, the research community has also started discussing various\naspects of evaluation and how these affect reproducibility. We add a novel\nangle to this discussion by examining how unofficial third-party\nimplementations could benefit or hinder reproducibility. Besides giving a\ngeneral overview, we thoroughly examine six third-party implementations of a\npopular recommender algorithm and compare them to the official version on five\npublic datasets. In the light of our alarming findings we aim to draw the\nattention of the research community to this neglected aspect of\nreproducibility.",
        "translated": ""
    },
    {
        "title": "Widespread Flaws in Offline Evaluation of Recommender Systems",
        "url": "http://arxiv.org/abs/2307.14951v1",
        "pub_date": "2023-07-27",
        "summary": "Even though offline evaluation is just an imperfect proxy of online\nperformance -- due to the interactive nature of recommenders -- it will\nprobably remain the primary way of evaluation in recommender systems research\nfor the foreseeable future, since the proprietary nature of production\nrecommenders prevents independent validation of A/B test setups and\nverification of online results. Therefore, it is imperative that offline\nevaluation setups are as realistic and as flawless as they can be.\nUnfortunately, evaluation flaws are quite common in recommender systems\nresearch nowadays, due to later works copying flawed evaluation setups from\ntheir predecessors without questioning their validity. In the hope of improving\nthe quality of offline evaluation of recommender systems, we discuss four of\nthese widespread flaws and why researchers should avoid them.",
        "translated": ""
    },
    {
        "title": "Scaling Session-Based Transformer Recommendations using Optimized\n  Negative Sampling and Loss Functions",
        "url": "http://arxiv.org/abs/2307.14906v1",
        "pub_date": "2023-07-27",
        "summary": "This work introduces TRON, a scalable session-based Transformer Recommender\nusing Optimized Negative-sampling. Motivated by the scalability and performance\nlimitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates\ntop-k negative sampling and listwise loss functions to enhance its\nrecommendation accuracy. Evaluations on relevant large-scale e-commerce\ndatasets show that TRON improves upon the recommendation quality of current\nmethods while maintaining training speeds similar to SASRec. A live A/B test\nyielded an 18.14% increase in click-through rate over SASRec, highlighting the\npotential of TRON in practical settings. For further research, we provide\naccess to our source code at https://github.com/otto-de/TRON and an anonymized\ndataset at https://github.com/otto-de/recsys-dataset.",
        "translated": ""
    },
    {
        "title": "Integrating Offline Reinforcement Learning with Transformers for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2307.14450v1",
        "pub_date": "2023-07-26",
        "summary": "We consider the problem of sequential recommendation, where the current\nrecommendation is made based on past interactions. This recommendation task\nrequires efficient processing of the sequential data and aims to provide\nrecommendations that maximize the long-term reward. To this end, we train a\nfarsighted recommender by using an offline RL algorithm with the policy network\nin our model architecture that has been initialized from a pre-trained\ntransformer model. The pre-trained model leverages the superb ability of the\ntransformer to process sequential information. Compared to prior works that\nrely on online interaction via simulation, we focus on implementing a fully\noffline RL framework that is able to converge in a fast and stable way. Through\nextensive experiments on public datasets, we show that our method is robust\nacross various recommendation regimes, including e-commerce and movie\nsuggestions. Compared to state-of-the-art supervised learning algorithms, our\nalgorithm yields recommendations of higher quality, demonstrating the clear\nadvantage of combining RL and transformers.",
        "translated": ""
    },
    {
        "title": "Measuring Americanization: A Global Quantitative Study of Interest in\n  American Topics on Wikipedia",
        "url": "http://arxiv.org/abs/2307.14401v1",
        "pub_date": "2023-07-26",
        "summary": "We conducted a global comparative analysis of the coverage of American topics\nin different language versions of Wikipedia, using over 90 million Wikidata\nitems and 40 million Wikipedia articles in 58 languages. Our study aimed to\ninvestigate whether Americanization is more or less dominant in different\nregions and cultures and to determine whether interest in American topics is\nuniversal.",
        "translated": ""
    },
    {
        "title": "Framework to Automatically Determine the Quality of Open Data Catalogs",
        "url": "http://arxiv.org/abs/2307.15464v1",
        "pub_date": "2023-07-28",
        "summary": "Data catalogs play a crucial role in modern data-driven organizations by\nfacilitating the discovery, understanding, and utilization of diverse data\nassets. However, ensuring their quality and reliability is complex, especially\nin open and large-scale data environments. This paper proposes a framework to\nautomatically determine the quality of open data catalogs, addressing the need\nfor efficient and reliable quality assessment mechanisms. Our framework can\nanalyze various core quality dimensions, such as accuracy, completeness,\nconsistency, scalability, and timeliness, offer several alternatives for the\nassessment of compatibility and similarity across such catalogs as well as the\nimplementation of a set of non-core quality dimensions such as provenance,\nreadability, and licensing. The goal is to empower data-driven organizations to\nmake informed decisions based on trustworthy and well-curated data assets. The\nsource code that illustrates our approach can be downloaded from\nhttps://www.github.com/jorge-martinez-gil/dataq/.",
        "translated": ""
    },
    {
        "title": "Toward Transparent Sequence Models with Model-Based Tree Markov Model",
        "url": "http://arxiv.org/abs/2307.15367v1",
        "pub_date": "2023-07-28",
        "summary": "In this study, we address the interpretability issue in complex, black-box\nMachine Learning models applied to sequence data. We introduce the Model-Based\ntree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model\naimed at detecting high mortality risk events and discovering hidden patterns\nassociated with the mortality risk in Intensive Care Units (ICU). This model\nleverages knowledge distilled from Deep Neural Networks (DNN) to enhance\npredictive performance while offering clear explanations. Our experimental\nresults indicate the improved performance of Model-Based trees (MOB trees) via\nemploying LSTM for learning sequential patterns, which are then transferred to\nMOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in\nthe MOB-HSMM enables uncovering potential and explainable sequences using\navailable information.",
        "translated": ""
    },
    {
        "title": "Staging E-Commerce Products for Online Advertising using Retrieval\n  Assisted Image Generation",
        "url": "http://arxiv.org/abs/2307.15326v1",
        "pub_date": "2023-07-28",
        "summary": "Online ads showing e-commerce products typically rely on the product images\nin a catalog sent to the advertising platform by an e-commerce platform. In the\nbroader ads industry such ads are called dynamic product ads (DPA). It is\ncommon for DPA catalogs to be in the scale of millions (corresponding to the\nscale of products which can be bought from the e-commerce platform). However,\nnot all product images in the catalog may be appealing when directly\nre-purposed as an ad image, and this may lead to lower click-through rates\n(CTRs). In particular, products just placed against a solid background may not\nbe as enticing and realistic as a product staged in a natural environment. To\naddress such shortcomings of DPA images at scale, we propose a generative\nadversarial network (GAN) based approach to generate staged backgrounds for\nun-staged product images. Generating the entire staged background is a\nchallenging task susceptible to hallucinations. To get around this, we\nintroduce a simpler approach called copy-paste staging using retrieval assisted\nGANs. In copy paste staging, we first retrieve (from the catalog) staged\nproducts similar to the un-staged input product, and then copy-paste the\nbackground of the retrieved product in the input image. A GAN based in-painting\nmodel is used to fill the holes left after this copy-paste operation. We show\nthe efficacy of our copy-paste staging method via offline metrics, and human\nevaluation. In addition, we show how our staging approach can enable animations\nof moving products leading to a video ad from a product image.",
        "translated": ""
    },
    {
        "title": "Reconciling the accuracy-diversity trade-off in recommendations",
        "url": "http://arxiv.org/abs/2307.15142v1",
        "pub_date": "2023-07-27",
        "summary": "In recommendation settings, there is an apparent trade-off between the goals\nof accuracy (to recommend items a user is most likely to want) and diversity\n(to recommend items representing a range of categories). As such, real-world\nrecommender systems often explicitly incorporate diversity separately from\naccuracy. This approach, however, leaves a basic question unanswered: Why is\nthere a trade-off in the first place?\n  We show how the trade-off can be explained via a user's consumption\nconstraints -- users typically only consume a few of the items they are\nrecommended. In a stylized model we introduce, objectives that account for this\nconstraint induce diverse recommendations, while objectives that do not account\nfor this constraint induce homogeneous recommendations. This suggests that\naccuracy and diversity appear misaligned because standard accuracy metrics do\nnot consider consumption constraints. Our model yields precise and\ninterpretable characterizations of diversity in different settings, giving\npractical insights into the design of diverse recommendations.",
        "translated": ""
    },
    {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative\n  Information-Seeking with Attribution",
        "url": "http://arxiv.org/abs/2307.16883v1",
        "pub_date": "2023-07-31",
        "summary": "The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.",
        "translated": ""
    },
    {
        "title": "Metric@CustomerN: Evaluating Metrics at a Customer Level in E-Commerce",
        "url": "http://arxiv.org/abs/2307.16832v1",
        "pub_date": "2023-07-31",
        "summary": "Accuracy measures such as Recall, Precision, and Hit Rate have been a\nstandard way of evaluating Recommendation Systems. The assumption is to use a\nfixed Top-N to represent them. We propose that median impressions viewed from\nhistorical sessions per diner be used as a personalized value for N. We present\npreliminary exploratory results and list future steps to improve upon and\nevaluate the efficacy of these personalized metrics.",
        "translated": ""
    },
    {
        "title": "Defense of Adversarial Ranking Attack in Text Retrieval: Benchmark and\n  Baseline via Detection",
        "url": "http://arxiv.org/abs/2307.16816v1",
        "pub_date": "2023-07-31",
        "summary": "Neural ranking models (NRMs) have undergone significant development and have\nbecome integral components of information retrieval (IR) systems.\nUnfortunately, recent research has unveiled the vulnerability of NRMs to\nadversarial document manipulations, potentially exploited by malicious search\nengine optimization practitioners. While progress in adversarial attack\nstrategies aids in identifying the potential weaknesses of NRMs before their\ndeployment, the defensive measures against such attacks, like the detection of\nadversarial documents, remain inadequately explored. To mitigate this gap, this\npaper establishes a benchmark dataset to facilitate the investigation of\nadversarial ranking defense and introduces two types of detection tasks for\nadversarial documents. A comprehensive investigation of the performance of\nseveral detection baselines is conducted, which involve examining the\nspamicity, perplexity, and linguistic acceptability, and utilizing supervised\nclassifiers. Experimental results demonstrate that a supervised classifier can\neffectively mitigate known attacks, but it performs poorly against unseen\nattacks. Furthermore, such classifier should avoid using query text to prevent\nlearning the classification on relevance, as it might lead to the inadvertent\ndiscarding of relevant documents.",
        "translated": ""
    },
    {
        "title": "Lexically-Accelerated Dense Retrieval",
        "url": "http://arxiv.org/abs/2307.16779v1",
        "pub_date": "2023-07-31",
        "summary": "Retrieval approaches that score documents based on learned dense vectors\n(i.e., dense retrieval) rather than lexical signals (i.e., conventional\nretrieval) are increasingly popular. Their ability to identify related\ndocuments that do not necessarily contain the same terms as those appearing in\nthe user's query (thereby improving recall) is one of their key advantages.\nHowever, to actually achieve these gains, dense retrieval approaches typically\nrequire an exhaustive search over the document collection, making them\nconsiderably more expensive at query-time than conventional lexical approaches.\nSeveral techniques aim to reduce this computational overhead by approximating\nthe results of a full dense retriever. Although these approaches reasonably\napproximate the top results, they suffer in terms of recall -- one of the key\nadvantages of dense retrieval. We introduce 'LADR' (Lexically-Accelerated Dense\nRetrieval), a simple-yet-effective approach that improves the efficiency of\nexisting dense retrieval models without compromising on retrieval\neffectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval\nexploration that uses a document proximity graph. We explore two variants of\nLADR: a proactive approach that expands the search space to the neighbors of\nall seed documents, and an adaptive approach that selectively searches the\ndocuments with the highest estimated relevance in an iterative fashion. Through\nextensive experiments across a variety of dense retrieval models, we find that\nLADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier\namong approximate k nearest neighbor techniques. Further, we find that when\ntuned to take around 8ms per query in retrieval latency on our hardware, LADR\nconsistently achieves both precision and recall that are on par with an\nexhaustive search on standard benchmarks.",
        "translated": ""
    },
    {
        "title": "AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of\n  Autism Spectrum Disorder",
        "url": "http://arxiv.org/abs/2307.16773v1",
        "pub_date": "2023-07-31",
        "summary": "To easily obtain the knowledge about autism spectrum disorder and help its\nearly screening and diagnosis, we create AsdKB, a Chinese knowledge base on\nautism spectrum disorder. The knowledge base is built on top of various\nsources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical\ndescriptions on mental and behavioural disorders, 2) the diagnostic knowledge\nfrom DSM-5 and different screening tools recommended by social organizations\nand medical institutes, and 3) the expert knowledge on professional physicians\nand hospitals from the Web. AsdKB contains both ontological and factual\nknowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The\npotential applications of AsdKB are question answering, auxiliary diagnosis,\nand expert recommendation, and we illustrate them with a prototype which can be\naccessed at http://asdkb.org.cn/.",
        "translated": ""
    },
    {
        "title": "NEON: Living Needs Prediction System in Meituan",
        "url": "http://arxiv.org/abs/2307.16644v1",
        "pub_date": "2023-07-31",
        "summary": "Living needs refer to the various needs in human's daily lives for survival\nand well-being, including food, housing, entertainment, etc. On life service\nplatforms that connect users to service providers, such as Meituan, the problem\nof living needs prediction is fundamental as it helps understand users and\nboost various downstream applications such as personalized recommendation.\nHowever, the problem has not been well explored and is faced with two critical\nchallenges. First, the needs are naturally connected to specific locations and\ntimes, suffering from complex impacts from the spatiotemporal context. Second,\nthere is a significant gap between users' actual living needs and their\nhistorical records on the platform. To address these two challenges, we design\na system of living NEeds predictiON named NEON, consisting of three phases:\nfeature mining, feature fusion, and multi-task prediction. In the feature\nmining phase, we carefully extract individual-level user features for\nspatiotemporal modeling, and aggregated-level behavioral features for enriching\ndata, which serve as the basis for addressing two challenges, respectively.\nFurther, in the feature fusion phase, we propose a neural network that\neffectively fuses two parts of features into the user representation. Moreover,\nwe design a multi-task prediction phase, where the auxiliary task of\nneeds-meeting way prediction can enhance the modeling of spatiotemporal\ncontext. Extensive offline evaluations verify that our NEON system can\neffectively predict users' living needs. Furthermore, we deploy NEON into\nMeituan's algorithm engine and evaluate how it enhances the three downstream\nprediction applications, via large-scale online A/B testing.",
        "translated": ""
    },
    {
        "title": "When Large Language Models Meet Personalization: Perspectives of\n  Challenges and Opportunities",
        "url": "http://arxiv.org/abs/2307.16376v1",
        "pub_date": "2023-07-31",
        "summary": "The advent of large language models marks a revolutionary breakthrough in\nartificial intelligence. With the unprecedented scale of training and model\nparameters, the capability of large language models has been dramatically\nimproved, leading to human-like performances in understanding, language\nsynthesizing, and common-sense reasoning, etc. Such a major leap-forward in\ngeneral AI capacity will change the pattern of how personalization is\nconducted. For one thing, it will reform the way of interaction between humans\nand personalization systems. Instead of being a passive medium of information\nfiltering, large language models present the foundation for active user\nengagement. On top of such a new foundation, user requests can be proactively\nexplored, and user's required information can be delivered in a natural and\nexplainable way. For another thing, it will also considerably expand the scope\nof personalization, making it grow from the sole function of collecting\npersonalized information to the compound function of providing personalized\nservices. By leveraging large language models as general-purpose interface, the\npersonalization systems may compile user requests into plans, calls the\nfunctions of external tools to execute the plans, and integrate the tools'\noutputs to complete the end-to-end personalization tasks. Today, large language\nmodels are still being developed, whereas the application in personalization is\nlargely unexplored. Therefore, we consider it to be the right time to review\nthe challenges in personalization and the opportunities to address them with\nLLMs. In particular, we dedicate this perspective paper to the discussion of\nthe following aspects: the development and challenges for the existing\npersonalization system, the newly emerged capabilities of large language\nmodels, and the potential ways of making use of large language models for\npersonalization.",
        "translated": ""
    },
    {
        "title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning",
        "url": "http://arxiv.org/abs/2307.16372v1",
        "pub_date": "2023-07-31",
        "summary": "Automatic music captioning, which generates natural language descriptions for\ngiven music tracks, holds significant potential for enhancing the understanding\nand organization of large volumes of musical data. Despite its importance,\nresearchers face challenges due to the costly and time-consuming collection\nprocess of existing music-language datasets, which are limited in size. To\naddress this data scarcity issue, we propose the use of large language models\n(LLMs) to artificially generate the description sentences from large-scale tag\ndatasets. This results in approximately 2.2M captions paired with 0.5M audio\nclips. We term it Large Language Model based Pseudo music caption dataset,\nshortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale\nmusic captioning dataset with various quantitative evaluation metrics used in\nthe field of natural language processing as well as human evaluation. In\naddition, we trained a transformer-based music captioning model with the\ndataset and evaluated it under zero-shot and transfer-learning settings. The\nresults demonstrate that our proposed approach outperforms the supervised\nbaseline model.",
        "translated": ""
    },
    {
        "title": "Workshop on Document Intelligence Understanding",
        "url": "http://arxiv.org/abs/2307.16369v1",
        "pub_date": "2023-07-31",
        "summary": "Document understanding and information extraction include different tasks to\nunderstand a document and extract valuable information automatically. Recently,\nthere has been a rising demand for developing document understanding among\ndifferent domains, including business, law, and medicine, to boost the\nefficiency of work that is associated with a large number of documents. This\nworkshop aims to bring together researchers and industry developers in the\nfield of document intelligence and understanding diverse document types to\nboost automatic document processing and understanding techniques. We also\nreleased a data challenge on the recently introduced document-level VQA\ndataset, PDFVQA. The PDFVQA challenge examines the structural and contextual\nunderstandings of proposed models on the natural full document level of\nmultiple consecutive document pages by including questions with a sequence of\nanswers extracted from multi-pages of the full document. This task helps to\nboost the document understanding step from the single-page level to the full\ndocument level understanding.",
        "translated": ""
    },
    {
        "title": "Time-Aware Item Weighting for the Next Basket Recommendations",
        "url": "http://arxiv.org/abs/2307.16297v1",
        "pub_date": "2023-07-30",
        "summary": "In this paper we study the next basket recommendation problem. Recent methods\nuse different approaches to achieve better performance. However, many of them\ndo not use information about the time of prediction and time intervals between\nbaskets. To fill this gap, we propose a novel method, Time-Aware Item-based\nWeighting (TAIW), which takes timestamps and intervals into account. We provide\nexperiments on three real-world datasets, and TAIW outperforms well-tuned\nstate-of-the-art baselines for next-basket recommendations. In addition, we\nshow the results of an ablation study and a case study of a few items.",
        "translated": ""
    },
    {
        "title": "TimePool: Visually Answer \"Which and When\" Questions On Univariate Time\n  Series",
        "url": "http://arxiv.org/abs/2308.00682v1",
        "pub_date": "2023-08-01",
        "summary": "When exploring time series datasets, analysts often pose \"which and when\"\nquestions. For example, with world life expectancy data over one hundred years,\nthey may inquire about the top 10 countries in life expectancy and the time\nperiod when they achieved this status, or which countries have had longer life\nexpectancy than Ireland and when. This paper proposes TimePool, a new\nvisualization prototype, to address this need for univariate time series\nanalysis. It allows users to construct interactive \"which and when\" queries and\nvisually explore the results for insights.",
        "translated": ""
    },
    {
        "title": "Explainable Graph Spectral Clustering of Text Documents",
        "url": "http://arxiv.org/abs/2308.00504v1",
        "pub_date": "2023-08-01",
        "summary": "Spectral clustering methods are known for their ability to represent clusters\nof diverse shapes, densities etc. However, results of such algorithms, when\napplied e.g. to text documents, are hard to explain to the user, especially due\nto embedding in the spectral space which has no obvious relation to document\ncontents. Therefore there is an urgent need to elaborate methods for explaining\nthe outcome of the clustering. This paper presents a contribution towards this\ngoal. We present a proposal of explanation of results of combinatorial\nLaplacian based graph spectral clustering. It is based on showing (approximate)\nequivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in\nthis paper) and term vector space embedding. Hence a bridge is constructed\nbetween the textual contents and the clustering results. We provide theoretical\nbackground for this approach. We performed experimental study showing that\n$K$-embedding approximates well Laplacian embedding under favourable block\nmatrix conditions and show that approximation is good enough under other\nconditions.",
        "translated": ""
    },
    {
        "title": "On the Effects of Regional Spelling Conventions in Retrieval Models",
        "url": "http://arxiv.org/abs/2308.00480v1",
        "pub_date": "2023-08-01",
        "summary": "One advantage of neural ranking models is that they are meant to generalise\nwell in situations of synonymity i.e. where two words have similar or identical\nmeanings. In this paper, we investigate and quantify how well various ranking\nmodels perform in a clear-cut case of synonymity: when words are simply\nexpressed in different surface forms due to regional differences in spelling\nconventions (e.g., color vs colour). We first explore the prevalence of\nAmerican and British English spelling conventions in datasets used for the\npre-training, training and evaluation of neural retrieval methods, and find\nthat American spelling conventions are far more prevalent. Despite these biases\nin the training data, we find that retrieval models often generalise well in\nthis case of synonymity. We explore the effect of document spelling\nnormalisation in retrieval and observe that all models are affected by\nnormalising the document's spelling. While they all experience a drop in\nperformance when normalised to a different spelling convention than that of the\nquery, we observe varied behaviour when the document is normalised to share the\nquery spelling convention: lexical models show improvements, dense retrievers\nremain unaffected, and re-rankers exhibit contradictory behaviour.",
        "translated": ""
    },
    {
        "title": "Generative Query Reformulation for Effective Adhoc Search",
        "url": "http://arxiv.org/abs/2308.00415v1",
        "pub_date": "2023-08-01",
        "summary": "Performing automatic reformulations of a user's query is a popular paradigm\nused in information retrieval (IR) for improving effectiveness -- as\nexemplified by the pseudo-relevance feedback approaches, which expand the query\nin order to alleviate the vocabulary mismatch problem. Recent advancements in\ngenerative language models have demonstrated their ability in generating\nresponses that are relevant to a given prompt. In light of this success, we\nseek to study the capacity of such models to perform query reformulation and\nhow they compare with long-standing query reformulation methods that use\npseudo-relevance feedback. In particular, we investigate two representative\nquery reformulation frameworks, GenQR and GenPRF. GenQR directly reformulates\nthe user's input query, while GenPRF provides additional context for the query\nby making use of pseudo-relevance feedback information. For each reformulation\nmethod, we leverage different techniques, including fine-tuning and direct\nprompting, to harness the knowledge of language models. The reformulated\nqueries produced by the generative models are demonstrated to markedly benefit\nthe effectiveness of a state-of-the-art retrieval pipeline on four TREC test\ncollections (varying from TREC 2004 Robust to the TREC 2019 Deep Learning).\nFurthermore, our results indicate that our studied generative models can\noutperform various statistical query expansion approaches while remaining\ncomparable to other existing complex neural query reformulation models, with\nthe added benefit of being simpler to implement.",
        "translated": ""
    },
    {
        "title": "Challenging the Myth of Graph Collaborative Filtering: a Reasoned and\n  Reproducibility-driven Analysis",
        "url": "http://arxiv.org/abs/2308.00404v1",
        "pub_date": "2023-08-01",
        "summary": "The success of graph neural network-based models (GNNs) has significantly\nadvanced recommender systems by effectively modeling users and items as a\nbipartite, undirected graph. However, many original graph-based works often\nadopt results from baseline papers without verifying their validity for the\nspecific configuration under analysis. Our work addresses this issue by\nfocusing on the replicability of results. We present a code that successfully\nreplicates results from six popular and recent graph recommendation models\n(NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark\ndatasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these\ngraph models with traditional collaborative filtering models that historically\nperformed well in offline evaluations. Furthermore, we extend our study to two\nnew datasets (Allrecipes and BookCrossing) that lack established setups in\nexisting literature. As the performance on these datasets differs from the\nprevious benchmarks, we analyze the impact of specific dataset characteristics\non recommendation accuracy. By investigating the information flow from users'\nneighborhoods, we aim to identify which models are influenced by intrinsic\nfeatures in the dataset structure. The code to reproduce our experiments is\navailable at: https://github.com/sisinflab/Graph-RSs-Reproducibility.",
        "translated": ""
    },
    {
        "title": "Masked and Swapped Sequence Modeling for Next Novel Basket\n  Recommendation in Grocery Shopping",
        "url": "http://arxiv.org/abs/2308.01308v1",
        "pub_date": "2023-08-02",
        "summary": "Next basket recommendation (NBR) is the task of predicting the next set of\nitems based on a sequence of already purchased baskets. It is a recommendation\ntask that has been widely studied, especially in the context of grocery\nshopping. In next basket recommendation (NBR), it is useful to distinguish\nbetween repeat items, i.e., items that a user has consumed before, and explore\nitems, i.e., items that a user has not consumed before. Most NBR work either\nignores this distinction or focuses on repeat items. We formulate the next\nnovel basket recommendation (NNBR) task, i.e., the task of recommending a\nbasket that only consists of novel items, which is valuable for both real-world\napplication and NBR evaluation. We evaluate how existing NBR methods perform on\nthe NNBR task and find that, so far, limited progress has been made w.r.t. the\nNNBR task. To address the NNBR task, we propose a simple bi-directional\ntransformer basket recommendation model (BTBR), which is focused on directly\nmodeling item-to-item correlations within and across baskets instead of\nlearning complex basket representations. To properly train BTBR, we propose and\ninvestigate several masking strategies and training objectives: (i) item-level\nrandom masking, (ii) item-level select masking, (iii) basket-level all masking,\n(iv) basket-level explore masking, and (v) joint masking. In addition, an\nitem-basket swapping strategy is proposed to enrich the item interactions\nwithin the same baskets. We conduct extensive experiments on three open\ndatasets with various characteristics. The results demonstrate the\neffectiveness of BTBR and our masking and swapping strategies for the NNBR\ntask. BTBR with a properly selected masking and swapping strategy can\nsubstantially improve NNBR performance.",
        "translated": ""
    },
    {
        "title": "A Survey on Popularity Bias in Recommender Systems",
        "url": "http://arxiv.org/abs/2308.01118v1",
        "pub_date": "2023-08-02",
        "summary": "Recommender systems help people find relevant content in a personalized way.\nOne main promise of such systems is that they are able to increase the\nvisibility of items in the long tail, i.e., the lesser-known items in a\ncatalogue. Existing research, however, suggests that in many situations today's\nrecommendation algorithms instead exhibit a popularity bias, meaning that they\noften focus on rather popular items in their recommendations. Such a bias may\nnot only lead to limited value of the recommendations for consumers and\nproviders in the short run, but it may also cause undesired reinforcement\neffects over time. In this paper, we discuss the potential reasons for\npopularity bias and we review existing approaches to detect, quantify and\nmitigate popularity bias in recommender systems. Our survey therefore includes\nboth an overview of the computational metrics used in the literature as well as\na review of the main technical approaches to reduce the bias. We furthermore\ncritically discuss today's literature, where we observe that the research is\nalmost entirely based on computational experiments and on certain assumptions\nregarding the practical effects of including long-tail items in the\nrecommendations.",
        "translated": ""
    },
    {
        "title": "Towards Better Query Classification with Multi-Expert Knowledge\n  Condensation in JD Ads Search",
        "url": "http://arxiv.org/abs/2308.01098v1",
        "pub_date": "2023-08-02",
        "summary": "Search query classification, as an effective way to understand user intents,\nis of great importance in real-world online ads systems. To ensure a lower\nlatency, a shallow model (e.g. FastText) is widely used for efficient online\ninference. However, the representation ability of the FastText model is\ninsufficient, resulting in poor classification performance, especially on some\nlow-frequency queries and tailed categories. Using a deeper and more complex\nmodel (e.g. BERT) is an effective solution, but it will cause a higher online\ninference latency and more expensive computing costs. Thus, how to juggle both\ninference efficiency and classification performance is obviously of great\npractical importance. To overcome this challenge, in this paper, we propose\nknowledge condensation (KC), a simple yet effective knowledge distillation\nframework to boost the classification performance of the online FastText model\nunder strict low latency constraints. Specifically, we propose to train an\noffline BERT model to retrieve more potentially relevant data. Benefiting from\nits powerful semantic representation, more relevant labels not exposed in the\nhistorical data will be added into the training set for better FastText model\ntraining. Moreover, a novel distribution-diverse multi-expert learning strategy\nis proposed to further improve the mining ability of relevant data. By training\nmultiple BERT models from different data distributions, it can respectively\nperform better at high, middle, and low-frequency search queries. The model\nensemble from multi-distribution makes its retrieval ability more powerful. We\nhave deployed two versions of this framework in JD search, and both offline\nexperiments and online A/B testing from multiple datasets have validated the\neffectiveness of the proposed approach.",
        "translated": ""
    },
    {
        "title": "Rethinking Similarity Search: Embracing Smarter Mechanisms over Smarter\n  Data",
        "url": "http://arxiv.org/abs/2308.00909v1",
        "pub_date": "2023-08-02",
        "summary": "In this vision paper, we propose a shift in perspective for improving the\neffectiveness of similarity search. Rather than focusing solely on enhancing\nthe data quality, particularly machine learning-generated embeddings, we\nadvocate for a more comprehensive approach that also enhances the underpinning\nsearch mechanisms. We highlight three novel avenues that call for a\nredefinition of the similarity search problem: exploiting implicit data\nstructures and distributions, engaging users in an iterative feedback loop, and\nmoving beyond a single query vector. These novel pathways have gained relevance\nin emerging applications such as large-scale language models, video clip\nretrieval, and data labeling. We discuss the corresponding research challenges\nposed by these new problem areas and share insights from our preliminary\ndiscoveries.",
        "translated": ""
    },
    {
        "title": "User-Controllable Recommendation via Counterfactual Retrospective and\n  Prospective Explanations",
        "url": "http://arxiv.org/abs/2308.00894v1",
        "pub_date": "2023-08-02",
        "summary": "Modern recommender systems utilize users' historical behaviors to generate\npersonalized recommendations. However, these systems often lack user\ncontrollability, leading to diminished user satisfaction and trust in the\nsystems. Acknowledging the recent advancements in explainable recommender\nsystems that enhance users' understanding of recommendation mechanisms, we\npropose leveraging these advancements to improve user controllability. In this\npaper, we present a user-controllable recommender system that seamlessly\nintegrates explainability and controllability within a unified framework. By\nproviding both retrospective and prospective explanations through\ncounterfactual reasoning, users can customize their control over the system by\ninteracting with these explanations.\n  Furthermore, we introduce and assess two attributes of controllability in\nrecommendation systems: the complexity of controllability and the accuracy of\ncontrollability. Experimental evaluations on MovieLens and Yelp datasets\nsubstantiate the effectiveness of our proposed framework. Additionally, our\nexperiments demonstrate that offering users control options can potentially\nenhance recommendation accuracy in the future. Source code and data are\navailable at \\url{https://github.com/chrisjtan/ucr}.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Contrastive BERT Fine-tuning for Fusion-based\n  Reviewed-Item Retrieval",
        "url": "http://arxiv.org/abs/2308.00762v1",
        "pub_date": "2023-08-01",
        "summary": "As natural language interfaces enable users to express increasingly complex\nnatural language queries, there is a parallel explosion of user review content\nthat can allow users to better find items such as restaurants, books, or movies\nthat match these expressive queries. While Neural Information Retrieval (IR)\nmethods have provided state-of-the-art results for matching queries to\ndocuments, they have not been extended to the task of Reviewed-Item Retrieval\n(RIR), where query-review scores must be aggregated (or fused) into item-level\nscores for ranking. In the absence of labeled RIR datasets, we extend Neural IR\nmethodology to RIR by leveraging self-supervised methods for contrastive\nlearning of BERT embeddings for both queries and reviews. Specifically,\ncontrastive learning requires a choice of positive and negative samples, where\nthe unique two-level structure of our item-review data combined with meta-data\naffords us a rich structure for the selection of these samples. For contrastive\nlearning in a Late Fusion scenario, we investigate the use of positive review\nsamples from the same item and/or with the same rating, selection of hard\npositive samples by choosing the least similar reviews from the same anchor\nitem, and selection of hard negative samples by choosing the most similar\nreviews from different items. We also explore anchor sub-sampling and\naugmenting with meta-data. For a more end-to-end Early Fusion approach, we\nintroduce contrastive item embedding learning to fuse reviews into single item\nembeddings. Experimental results show that Late Fusion contrastive learning for\nNeural RIR outperforms all other contrastive IR configurations, Neural IR, and\nsparse retrieval baselines, thus demonstrating the power of exploiting the\ntwo-level structure in Neural RIR approaches as well as the importance of\npreserving the nuance of individual review content via Late Fusion methods.",
        "translated": ""
    },
    {
        "title": "A Knowledge-Oriented Approach to Enhance Integration and Communicability\n  in the Polkadot Ecosystem",
        "url": "http://arxiv.org/abs/2308.00735v1",
        "pub_date": "2023-08-01",
        "summary": "The Polkadot ecosystem is a disruptive and highly complex multi-chain\narchitecture that poses challenges in terms of data analysis and\ncommunicability. Currently, there is a lack of standardized and holistic\napproaches to retrieve and analyze data across parachains and applications,\nmaking it difficult for general users and developers to access ecosystem data\nconsistently. This paper proposes a conceptual framework that includes a domain\nontology called POnto (a Polkadot Ontology) to address these challenges. POnto\nprovides a structured representation of the ecosystem's concepts and\nrelationships, enabling a formal understanding of the platform. The proposed\nknowledge-oriented approach enhances integration and communicability, enabling\na wider range of users to participate in the ecosystem and facilitating the\ndevelopment of AI-based applications. The paper presents a case study\nmethodology to validate the proposed framework, which includes expert feedback\nand insights from the Polkadot community. The POnto ontology and the roadmap\nfor a query engine based on a Controlled Natural Language using the ontology,\nprovide valuable contributions to the growth and adoption of the Polkadot\necosystem in heterogeneous socio-technical environments.",
        "translated": ""
    },
    {
        "title": "Adaptive Collaborative Filtering with Personalized Time Decay Functions\n  for Financial Product Recommendation",
        "url": "http://arxiv.org/abs/2308.01208v1",
        "pub_date": "2023-08-01",
        "summary": "Classical recommender systems often assume that historical data are\nstationary and fail to account for the dynamic nature of user preferences,\nlimiting their ability to provide reliable recommendations in time-sensitive\nsettings. This assumption is particularly problematic in finance, where\nfinancial products exhibit continuous changes in valuations, leading to\nfrequent shifts in client interests. These evolving interests, summarized in\nthe past client-product interactions, see their utility fade over time with a\ndegree that might differ from one client to another. To address this challenge,\nwe propose a time-dependent collaborative filtering algorithm that can\nadaptively discount distant client-product interactions using personalized\ndecay functions. Our approach is designed to handle the non-stationarity of\nfinancial data and produce reliable recommendations by modeling the dynamic\ncollaborative signals between clients and products. We evaluate our method\nusing a proprietary dataset from BNP Paribas and demonstrate significant\nimprovements over state-of-the-art benchmarks from relevant literature. Our\nfindings emphasize the importance of incorporating time explicitly in the model\nto enhance the accuracy of financial product recommendation.",
        "translated": ""
    },
    {
        "title": "MAP: A Model-agnostic Pretraining Framework for Click-through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2308.01737v1",
        "pub_date": "2023-08-03",
        "summary": "With the widespread application of personalized online services,\nclick-through rate (CTR) prediction has received more and more attention and\nresearch. The most prominent features of CTR prediction are its multi-field\ncategorical data format, and vast and daily-growing data volume. The large\ncapacity of neural models helps digest such massive amounts of data under the\nsupervised learning paradigm, yet they fail to utilize the substantial data to\nits full potential, since the 1-bit click signal is not sufficient to guide the\nmodel to learn capable representations of features and instances. The\nself-supervised learning paradigm provides a more promising pretrain-finetune\nsolution to better exploit the large amount of user click logs, and learn more\ngeneralized and effective representations. However, self-supervised learning\nfor CTR prediction is still an open question, since current works on this line\nare only preliminary and rudimentary. To this end, we propose a Model-agnostic\npretraining (MAP) framework that applies feature corruption and recovery on\nmulti-field categorical data, and more specifically, we derive two practical\nalgorithms: masked feature prediction (MFP) and replaced feature detection\n(RFD). MFP digs into feature interactions within each instance through masking\nand predicting a small portion of input features, and introduces noise\ncontrastive estimation (NCE) to handle large feature spaces. RFD further turns\nMFP into a binary classification mode through replacing and detecting changes\nin input features, making it even simpler and more effective for CTR\npretraining. Our extensive experiments on two real-world large-scale datasets\n(i.e., Avazu, Criteo) demonstrate the advantages of these two methods on\nseveral strong backbones (e.g., DCNv2, DeepFM), and achieve new\nstate-of-the-art performance in terms of both effectiveness and efficiency for\nCTR prediction.",
        "translated": ""
    },
    {
        "title": "Evaluating ChatGPT text-mining of clinical records for obesity\n  monitoring",
        "url": "http://arxiv.org/abs/2308.01666v1",
        "pub_date": "2023-08-03",
        "summary": "Background: Veterinary clinical narratives remain a largely untapped resource\nfor addressing complex diseases. Here we compare the ability of a large\nlanguage model (ChatGPT) and a previously developed regular expression (RegexT)\nto identify overweight body condition scores (BCS) in veterinary narratives.\nMethods: BCS values were extracted from 4,415 anonymised clinical narratives\nusing either RegexT or by appending the narrative to a prompt sent to ChatGPT\ncoercing the model to return the BCS information. Data were manually reviewed\nfor comparison. Results: The precision of RegexT was higher (100%, 95% CI\n94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall\nof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of\nRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is\nneeded to improve ChatGPT output. Conclusions: Large language models create\ndiverse opportunities and, whilst complex, present an intuitive interface to\ninformation but require careful implementation to avoid unpredictable errors.",
        "translated": ""
    },
    {
        "title": "Fast Slate Policy Optimization: Going Beyond Plackett-Luce",
        "url": "http://arxiv.org/abs/2308.01566v1",
        "pub_date": "2023-08-03",
        "summary": "An increasingly important building block of large scale machine learning\nsystems is based on returning slates; an ordered lists of items given a query.\nApplications of this technology include: search, information retrieval and\nrecommender systems. When the action space is large, decision systems are\nrestricted to a particular structure to complete online queries quickly. This\npaper addresses the optimization of these large scale decision systems given an\narbitrary reward function. We cast this learning problem in a policy\noptimization framework and propose a new class of policies, born from a novel\nrelaxation of decision functions. This results in a simple, yet efficient\nlearning algorithm that scales to massive action spaces. We compare our method\nto the commonly adopted Plackett-Luce policy class and demonstrate the\neffectiveness of our approach on problems with action space sizes in the order\nof millions.",
        "translated": ""
    },
    {
        "title": "Density Weighting for Multi-Interest Personalized Recommendation",
        "url": "http://arxiv.org/abs/2308.01563v1",
        "pub_date": "2023-08-03",
        "summary": "Using multiple user representations (MUR) to model user behavior instead of a\nsingle user representation (SUR) has been shown to improve personalization in\nrecommendation systems. However, the performance gains observed with MUR can be\nsensitive to the skewness in the item and/or user interest distribution. When\nthe data distribution is highly skewed, the gains observed by learning multiple\nrepresentations diminish since the model dominates on head items/interests,\nleading to poor performance on tail items. Robustness to data sparsity is\ntherefore essential for MUR-based approaches to achieve good performance for\nrecommendations. Yet, research in MUR and data imbalance have largely been done\nindependently. In this paper, we delve deeper into the shortcomings of MUR\ninferred from imbalanced data distributions. We make several contributions: (1)\nUsing synthetic datasets, we demonstrate the sensitivity of MUR with respect to\ndata imbalance, (2) To improve MUR for tail items, we propose an iterative\ndensity weighting scheme (IDW) with user tower calibration to mitigate the\neffect of training over long-tail distribution on personalization, and (3)\nThrough extensive experiments on three real-world benchmarks, we demonstrate\nIDW outperforms other alternatives that address data imbalance.",
        "translated": ""
    },
    {
        "title": "Adaptive Preferential Attached kNN Graph With Distribution-Awareness",
        "url": "http://arxiv.org/abs/2308.02442v1",
        "pub_date": "2023-08-04",
        "summary": "Graph-based kNN algorithms have garnered widespread popularity for machine\nlearning tasks, due to their simplicity and effectiveness. However, the\nconventional kNN graph's reliance on a fixed value of k can hinder its\nperformance, especially in scenarios involving complex data distributions.\nMoreover, like other classification models, the presence of ambiguous samples\nalong decision boundaries often presents a challenge, as they are more prone to\nincorrect classification. To address these issues, we propose the Preferential\nAttached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with\ndistribution-based graph construction. By incorporating distribution\ninformation, paNNG can significantly improve performance for ambiguous samples\nby \"pulling\" them towards their original classes and hence enable enhanced\noverall accuracy and generalization capability. Through rigorous evaluations on\ndiverse benchmark datasets, paNNG outperforms state-of-the-art algorithms,\nshowcasing its adaptability and efficacy across various real-world scenarios.",
        "translated": ""
    },
    {
        "title": "RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph\n  Classification",
        "url": "http://arxiv.org/abs/2308.02335v1",
        "pub_date": "2023-08-04",
        "summary": "Graph classification is a crucial task in many real-world multimedia\napplications, where graphs can represent various multimedia data types such as\nimages, videos, and social networks. Previous efforts have applied graph neural\nnetworks (GNNs) in balanced situations where the class distribution is\nbalanced. However, real-world data typically exhibit long-tailed class\ndistributions, resulting in a bias towards the head classes when using GNNs and\nlimited generalization ability over the tail classes. Recent approaches mainly\nfocus on re-balancing different classes during model training, which fails to\nexplicitly introduce new knowledge and sacrifices the performance of the head\nclasses. To address these drawbacks, we propose a novel framework called\nRetrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature\nextractor and an unbiased classifier in a decoupled manner. In the feature\nextractor training stage, we develop a graph retrieval module to search for\nrelevant graphs that directly enrich the intra-class diversity for the tail\nclasses. Moreover, we innovatively optimize a category-centered supervised\ncontrastive loss to obtain discriminative representations, which is more\nsuitable for long-tailed scenarios. In the classifier fine-tuning stage, we\nbalance the classifier weights with two weight regularization techniques, i.e.,\nMax-norm and weight decay. Experiments on various popular benchmarks verify the\nsuperiority of the proposed method against state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Learning to Select the Relevant History Turns in Conversational Question\n  Answering",
        "url": "http://arxiv.org/abs/2308.02294v1",
        "pub_date": "2023-08-04",
        "summary": "The increasing demand for the web-based digital assistants has given a rapid\nrise in the interest of the Information Retrieval (IR) community towards the\nfield of conversational question answering (ConvQA). However, one of the\ncritical aspects of ConvQA is the effective selection of conversational history\nturns to answer the question at hand. The dependency between relevant history\nselection and correct answer prediction is an intriguing but under-explored\narea. The selected relevant context can better guide the system so as to where\nexactly in the passage to look for an answer. Irrelevant context, on the other\nhand, brings noise to the system, thereby resulting in a decline in the model's\nperformance. In this paper, we propose a framework, DHS-ConvQA (Dynamic History\nSelection in Conversational Question Answering), that first generates the\ncontext and question entities for all the history turns, which are then pruned\non the basis of similarity they share in common with the question at hand. We\nalso propose an attention-based mechanism to re-rank the pruned terms based on\ntheir calculated weights of how useful they are in answering the question. In\nthe end, we further aid the model by highlighting the terms in the re-ranked\nconversational history using a binary classification task and keeping the\nuseful terms (predicted as 1) and ignoring the irrelevant terms (predicted as\n0). We demonstrate the efficacy of our proposed framework with extensive\nexperimental results on CANARD and QuAC -- the two popularly utilized datasets\nin ConvQA. We demonstrate that selecting relevant turns works better than\nrewriting the original question. We also investigate how adding the irrelevant\nhistory turns negatively impacts the model's performance and discuss the\nresearch challenges that demand more attention from the IR community.",
        "translated": ""
    },
    {
        "title": "Optimally Computing Compressed Indexing Arrays Based on the Compact\n  Directed Acyclic Word Graph",
        "url": "http://arxiv.org/abs/2308.02269v1",
        "pub_date": "2023-08-04",
        "summary": "In this paper, we present the first study of the computational complexity of\nconverting an automata-based text index structure, called the Compact Directed\nAcyclic Word Graph (CDAWG), of size $e$ for a text $T$ of length $n$ into other\ntext indexing structures for the same text, suitable for highly repetitive\ntexts: the run-length BWT of size $r$, the irreducible PLCP array of size $r$,\nand the quasi-irreducible LPF array of size $e$, as well as the lex-parse of\nsize $O(r)$ and the LZ77-parse of size $z$, where $r, z \\le e$. As main\nresults, we showed that the above structures can be optimally computed from\neither the CDAWG for $T$ stored in read-only memory or its self-index version\nof size $e$ without a text in $O(e)$ worst-case time and words of working\nspace. To obtain the above results, we devised techniques for enumerating a\nparticular subset of suffixes in the lexicographic and text orders using the\nforward and backward search on the CDAWG by extending the results by\nBelazzougui et al. in 2015.",
        "translated": ""
    },
    {
        "title": "Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song",
        "url": "http://arxiv.org/abs/2308.02249v1",
        "pub_date": "2023-08-04",
        "summary": "In this paper, we introduce a computational analysis of the field recording\ndataset of approximately 700 hours of Korean folk songs, which were recorded\naround 1980-90s. Because most of the songs were sung by non-expert musicians\nwithout accompaniment, the dataset provides several challenges. To address this\nchallenge, we utilized self-supervised learning with convolutional neural\nnetwork based on pitch contour, then analyzed how the musical concept of tori,\na classification system defined by a specific scale, ornamental notes, and an\nidiomatic melodic contour, is captured by the model. The experimental result\nshows that our approach can better capture the characteristics of tori compared\nto traditional pitch histograms. Using our approaches, we have examined how\nmusical discussions proposed in existing academia manifest in the actual field\nrecordings of Korean folk songs.",
        "translated": ""
    },
    {
        "title": "Towards Personalized Prompt-Model Retrieval for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.02205v1",
        "pub_date": "2023-08-04",
        "summary": "Recommender Systems are built to retrieve relevant items to satisfy users'\ninformation needs. The candidate corpus usually consists of a finite set of\nitems that are ready to be served, such as videos, products, or articles. With\nrecent advances in Generative AI such as GPT and Diffusion models, a new form\nof recommendation task is yet to be explored where items are to be created by\ngenerative models with personalized prompts. Taking image generation as an\nexample, with a single prompt from the user and access to a generative model,\nit is possible to generate hundreds of new images in a few minutes. How shall\nwe attain personalization in the presence of \"infinite\" items? In this\npreliminary study, we propose a two-stage framework, namely Prompt-Model\nRetrieval and Generated Item Ranking, to approach this new task formulation. We\nrelease GEMRec-18K, a prompt-model interaction dataset with 18K images\ngenerated by 200 publicly-available generative models paired with a diverse set\nof 90 textual prompts. Our findings demonstrate the promise of generative model\nrecommendation as a novel personalization problem and the limitations of\nexisting evaluation metrics. We highlight future directions for the RecSys\ncommunity to advance towards generative recommender systems. Our code and\ndataset are available at https://github.com/MAPS-research/GEMRec.",
        "translated": ""
    },
    {
        "title": "Incorporating Recklessness to Collaborative Filtering based Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2308.02058v1",
        "pub_date": "2023-08-03",
        "summary": "Recommender systems that include some reliability measure of their\npredictions tend to be more conservative in forecasting, due to their\nconstraint to preserve reliability. This leads to a significant drop in the\ncoverage and novelty that these systems can provide. In this paper, we propose\nthe inclusion of a new term in the learning process of matrix\nfactorization-based recommender systems, called recklessness, which enables the\ncontrol of the risk level desired when making decisions about the reliability\nof a prediction. Experimental results demonstrate that recklessness not only\nallows for risk regulation but also improves the quantity and quality of\npredictions provided by the recommender system.",
        "translated": ""
    },
    {
        "title": "Seasonality Based Reranking of E-commerce Autocomplete Using Natural\n  Language Queries",
        "url": "http://arxiv.org/abs/2308.02055v1",
        "pub_date": "2023-08-03",
        "summary": "Query autocomplete (QAC) also known as typeahead, suggests list of complete\nqueries as user types prefix in the search box. It is one of the key features\nof modern search engines specially in e-commerce. One of the goals of typeahead\nis to suggest relevant queries to users which are seasonally important. In this\npaper we propose a neural network based natural language processing (NLP)\nalgorithm to incorporate seasonality as a signal and present end to end\nevaluation of the QAC ranking model. Incorporating seasonality into\nautocomplete ranking model can improve autocomplete relevance and business\nmetric.",
        "translated": ""
    },
    {
        "title": "Domain specificity and data efficiency in typo tolerant spell checkers:\n  the case of search in online marketplaces",
        "url": "http://arxiv.org/abs/2308.01976v1",
        "pub_date": "2023-08-03",
        "summary": "Typographical errors are a major source of frustration for visitors of online\nmarketplaces. Because of the domain-specific nature of these marketplaces and\nthe very short queries users tend to search for, traditional spell cheking\nsolutions do not perform well in correcting typos. We present a data\naugmentation method to address the lack of annotated typo data and train a\nrecurrent neural network to learn context-limited domain-specific embeddings.\nThose embeddings are deployed in a real-time inferencing API for the Microsoft\nAppSource marketplace to find the closest match between a misspelled user query\nand the available product names. Our data efficient solution shows that\ncontrolled high quality synthetic data may be a powerful tool especially\nconsidering the current climate of large language models which rely on\nprohibitively huge and often uncontrolled datasets.",
        "translated": ""
    }
]