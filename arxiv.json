[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": ""
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": ""
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": ""
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": ""
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": ""
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": ""
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": ""
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": ""
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": ""
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": ""
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": ""
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": ""
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": ""
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": ""
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": ""
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": ""
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": ""
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": ""
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": ""
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Breaking the Curse of Quality Saturation with User-Centric Ranking",
        "url": "http://arxiv.org/abs/2305.15333v1",
        "pub_date": "2023-05-24",
        "summary": "A key puzzle in search, ads, and recommendation is that the ranking model can\nonly utilize a small portion of the vastly available user interaction data. As\na result, increasing data volume, model size, or computation FLOPs will quickly\nsuffer from diminishing returns. We examined this problem and found that one of\nthe root causes may lie in the so-called ``item-centric'' formulation, which\nhas an unbounded vocabulary and thus uncontrolled model complexity. To mitigate\nquality saturation, we introduce an alternative formulation named\n``user-centric ranking'', which is based on a transposed view of the dyadic\nuser-item interaction data. We show that this formulation has a promising\nscaling property, enabling us to train better-converged models on substantially\nlarger data sets.",
        "translated": ""
    },
    {
        "title": "Neural Summarization of Electronic Health Records",
        "url": "http://arxiv.org/abs/2305.15222v1",
        "pub_date": "2023-05-24",
        "summary": "Hospital discharge documentation is among the most essential, yet\ntime-consuming documents written by medical practitioners. The objective of\nthis study was to automatically generate hospital discharge summaries using\nneural network summarization models. We studied various data preparation and\nneural network training techniques that generate discharge summaries. Using\nnursing notes and discharge summaries from the MIMIC-III dataset, we studied\nthe viability of the automatic generation of various sections of a discharge\nsummary using four state-of-the-art neural network summarization models (BART,\nT5, Longformer and FLAN-T5). Our experiments indicated that training\nenvironments including nursing notes as the source, and discrete sections of\nthe discharge summary as the target output (e.g. \"History of Present Illness\")\nimprove language model efficiency and text quality. According to our findings,\nthe fine-tuned BART model improved its ROUGE F1 score by 43.6% against its\nstandard off-the-shelf version. We also found that fine-tuning the baseline\nBART model with other setups caused different degrees of improvement (up to 80%\nrelative improvement). We also observed that a fine-tuned T5 generally achieves\nhigher ROUGE F1 scores than other fine-tuned models and a fine-tuned FLAN-T5\nachieves the highest ROUGE score overall, i.e., 45.6. For majority of the\nfine-tuned language models, summarizing discharge summary report sections\nseparately outperformed the summarization the entire report quantitatively. On\nthe other hand, fine-tuning language models that were previously instruction\nfine-tuned showed better performance in summarizing entire reports. This study\nconcludes that a focused dataset designed for the automatic generation of\ndischarge summaries by a language model can produce coherent Discharge Summary\nsections.",
        "translated": ""
    },
    {
        "title": "Collaborative Recommendation Model Based on Multi-modal Multi-view\n  Attention Network: Movie and literature cases",
        "url": "http://arxiv.org/abs/2305.15159v1",
        "pub_date": "2023-05-24",
        "summary": "The existing collaborative recommendation models that use multi-modal\ninformation emphasize the representation of users' preferences but easily\nignore the representation of users' dislikes. Nevertheless, modelling users'\ndislikes facilitates comprehensively characterizing user profiles. Thus, the\nrepresentation of users' dislikes should be integrated into the user modelling\nwhen we construct a collaborative recommendation model. In this paper, we\npropose a novel Collaborative Recommendation Model based on Multi-modal\nmulti-view Attention Network (CRMMAN), in which the users are represented from\nboth preference and dislike views. Specifically, the users' historical\ninteractions are divided into positive and negative interactions, used to model\nthe user's preference and dislike views, respectively. Furthermore, the\nsemantic and structural information extracted from the scene is employed to\nenrich the item representation. We validate CRMMAN by designing contrast\nexperiments based on two benchmark MovieLens-1M and Book-Crossing datasets.\nMovielens-1m has about a million ratings, and Book-Crossing has about 300,000\nratings. Compared with the state-of-the-art knowledge-graph-based and\nmulti-modal recommendation methods, the AUC, NDCG@5 and NDCG@10 are improved by\n2.08%, 2.20% and 2.26% on average of two datasets. We also conduct controlled\nexperiments to explore the effects of multi-modal information and multi-view\nmechanism. The experimental results show that both of them enhance the model's\nperformance.",
        "translated": ""
    },
    {
        "title": "Bert4CMR: Cross-Market Recommendation with Bidirectional Encoder\n  Representations from Transformer",
        "url": "http://arxiv.org/abs/2305.15145v1",
        "pub_date": "2023-05-24",
        "summary": "Real-world multinational e-commerce companies, such as Amazon and eBay, serve\nin multiple countries and regions. Obviously, these markets have similar goods\nbut different users. Some markets are data-scarce, while others are data-rich.\nIn recent years, cross-market recommendation (CMR) has been proposed to enhance\ndata-scarce markets by leveraging auxiliary information from data-rich markets.\nPrevious works fine-tune the pre-trained model on the local market after\nfreezing part of the parameters or introducing inter-market similarity into the\nlocal market to improve the performance of CMR. However, they generally do not\nconsider eliminating the mutual interference between markets. Therefore, the\nexisting methods are neither unable to learn unbiased general knowledge nor\nefficient transfer reusable information across markets. In this paper, we\npropose a novel attention-based model called Bert4CMR to simultaneously improve\nall markets' recommendation performance. Specifically, we employ the attention\nmechanism to capture user interests by modelling user behavioural sequences. We\npre-train the proposed model on global data to learn the general knowledge of\nitems. Then we fine-tune specific target markets to perform local\nrecommendations. We propose market embedding to model the bias of each market\nand reduce the mutual inference between the parallel markets. Extensive\nexperiments conducted on seven markets show that our model is state-of-the-art.\nOur model outperforms the suboptimal model by 4.82%, 4.73%, 7.66% and 6.49% on\naverage of seven datasets in terms of four metrics, respectively. We conduct\nablation experiments to analyse the effectiveness of the proposed components.\nExperimental results indicate that our model is able to learn general knowledge\nthrough global data and shield the mutual interference between markets.",
        "translated": ""
    },
    {
        "title": "Semantic-Enhanced Differentiable Search Index Inspired by Learning\n  Strategies",
        "url": "http://arxiv.org/abs/2305.15115v1",
        "pub_date": "2023-05-24",
        "summary": "Recently, a new paradigm called Differentiable Search Index (DSI) has been\nproposed for document retrieval, wherein a sequence-to-sequence model is\nlearned to directly map queries to relevant document identifiers. The key idea\nbehind DSI is to fully parameterize traditional ``index-retrieve'' pipelines\nwithin a single neural model, by encoding all documents in the corpus into the\nmodel parameters. In essence, DSI needs to resolve two major questions: (1) how\nto assign an identifier to each document, and (2) how to learn the associations\nbetween a document and its identifier. In this work, we propose a\nSemantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the\narea of Cognitive Psychology. Our approach advances original DSI in two ways:\n(1) For the document identifier, we take inspiration from Elaboration\nStrategies in human learning. Specifically, we assign each document an\nElaborative Description based on the query generation technique, which is more\nmeaningful than a string of integers in the original DSI; and (2) For the\nassociations between a document and its identifier, we take inspiration from\nRehearsal Strategies in human learning. Specifically, we select fine-grained\nsemantic features from a document as Rehearsal Contents to improve document\nmemorization. Both the offline and online experiments show improved retrieval\nperformance over prevailing baselines.",
        "translated": ""
    },
    {
        "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
        "url": "http://arxiv.org/abs/2305.15053v1",
        "pub_date": "2023-05-24",
        "summary": "When re-finding items, users who forget or are uncertain about identifying\ndetails often rely on creative strategies for expressing their information\nneeds -- complex queries that describe content elements (e.g., book characters\nor events), information beyond the document text (e.g., descriptions of book\ncovers), or personal context (e.g., when they read a book). This retrieval\nsetting, called tip of the tongue (TOT), is especially challenging for models\nheavily reliant on lexical and semantic overlap between query and document\ntext. In this work, we introduce a simple yet effective framework for handling\nsuch complex queries by decomposing the query into individual clues, routing\nthose as sub-queries to specialized retrievers, and ensembling the results.\nThis approach allows us to take advantage of off-the-shelf retrievers (e.g.,\nCLIP for retrieving images of book covers) or incorporate retriever-specific\nlogic (e.g., date constraints). We show that our framework incorportating query\ndecompositions into retrievers can improve gold book recall up to 7% relative\nagain for Recall@5 on a new collection of 14,441 real-world query-book pairs\nfrom an online community for resolving TOT inquiries.",
        "translated": ""
    },
    {
        "title": "Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation",
        "url": "http://arxiv.org/abs/2305.15048v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of\neffect-size-based meta-analysis for multi-task evaluation in NLP and IR. We\nobserved that our communities often face the challenge of aggregating results\nover incomparable metrics and scenarios, which makes conclusions and take-away\nmessages less reliable. With Ranger, we aim to address this issue by providing\na task-agnostic toolkit that combines the effect of a treatment on multiple\ntasks into one statistical evaluation, allowing for comparison of metrics and\ncomputation of an overall summary effect. Our toolkit produces\npublication-ready forest plots that enable clear communication of evaluation\nresults over multiple tasks. Our goal with the ready-to-use Ranger toolkit is\nto promote robust, effect-size-based evaluation and improve evaluation\nstandards in the community. We provide two case studies for common IR and NLP\nsettings to highlight Ranger's benefits.",
        "translated": ""
    },
    {
        "title": "Exploring Adapter-based Transfer Learning for Recommender Systems:\n  Empirical Studies and Practical Insights",
        "url": "http://arxiv.org/abs/2305.15036v1",
        "pub_date": "2023-05-24",
        "summary": "Adapters, a plug-in neural network module with some tunable parameters, have\nemerged as a parameter-efficient transfer learning technique for adapting\npre-trained models to downstream tasks, especially for natural language\nprocessing (NLP) and computer vision (CV) fields. Meanwhile, learning\nrecommendation models directly from raw item modality features -- e.g., texts\nof NLP and images of CV -- can enable effective and transferable recommender\nsystems (called TransRec). In view of this, a natural question arises: can\nadapter-based learning techniques achieve parameter-efficient TransRec with\ngood performance?\n  To this end, we perform empirical studies to address several key\nsub-questions. First, we ask whether the adapter-based TransRec performs\ncomparably to TransRec based on standard full-parameter fine-tuning? does it\nhold for recommendation with different item modalities, e.g., textual RS and\nvisual RS. If yes, we benchmark these existing adapters, which have been shown\nto be effective in NLP and CV tasks, in the item recommendation settings.\nThird, we carefully study several key factors for the adapter-based TransRec in\nterms of where and how to insert these adapters? Finally, we look at the\neffects of adapter-based TransRec by either scaling up its source training data\nor scaling down its target training data. Our paper provides key insights and\npractical guidance on unified &amp; transferable recommendation -- a less studied\nrecommendation scenario. We promise to release all code &amp; datasets for future\nresearch.",
        "translated": ""
    },
    {
        "title": "How Graph Convolutions Amplify Popularity Bias for Recommendation?",
        "url": "http://arxiv.org/abs/2305.14886v1",
        "pub_date": "2023-05-24",
        "summary": "Graph convolutional networks (GCNs) have become prevalent in recommender\nsystem (RS) due to their superiority in modeling collaborative patterns.\nAlthough improving the overall accuracy, GCNs unfortunately amplify popularity\nbias -- tail items are less likely to be recommended. This effect prevents the\nGCN-based RS from making precise and fair recommendations, decreasing the\neffectiveness of recommender systems in the long run.\n  In this paper, we investigate how graph convolutions amplify the popularity\nbias in RS. Through theoretical analyses, we identify two fundamental factors:\n(1) with graph convolution (\\textit{i.e.,} neighborhood aggregation), popular\nitems exert larger influence than tail items on neighbor users, making the\nusers move towards popular items in the representation space; (2) after\nmultiple times of graph convolution, popular items would affect more high-order\nneighbors and become more influential. The two points make popular items get\ncloser to almost users and thus being recommended more frequently. To rectify\nthis, we propose to estimate the amplified effect of popular nodes on each\nnode's representation, and intervene the effect after each graph convolution.\nSpecifically, we adopt clustering to discover highly-influential nodes and\nestimate the amplification effect of each node, then remove the effect from the\nnode embeddings at each graph convolution layer. Our method is simple and\ngeneric -- it can be used in the inference stage to correct existing models\nrather than training a new model from scratch, and can be applied to various\nGCN models. We demonstrate our method on two representative GCN backbones\nLightGCN and UltraGCN, verifying its ability in improving the recommendations\nof tail items without sacrificing the performance of popular items. Codes are\nopen-sourced \\footnote{https://github.com/MEICRS/DAP}.",
        "translated": ""
    },
    {
        "title": "Machine Reading Comprehension using Case-based Reasoning",
        "url": "http://arxiv.org/abs/2305.14815v1",
        "pub_date": "2023-05-24",
        "summary": "We present an accurate and interpretable method for answer extraction in\nmachine reading comprehension that is reminiscent of case-based reasoning (CBR)\nfrom classical AI. Our method (CBR-MRC) builds on the hypothesis that\ncontextualized answers to similar questions share semantic similarities with\neach other. Given a target question, CBR-MRC retrieves a set of similar\nquestions from a memory of observed cases and predicts an answer by selecting\nthe span in the target context that is most similar to the contextualized\nrepresentations of answers in the retrieved cases. The semi-parametric nature\nof our approach allows CBR-MRC to attribute a prediction to the specific set of\ncases used during inference, making it a desirable choice for building reliable\nand debuggable QA systems. We show that CBR-MRC achieves high test accuracy\ncomparable with large reader models, outperforming baselines by 11.5 and 8.4 EM\non NaturalQuestions and NewsQA, respectively. Further, we also demonstrate the\nability of CBR-MRC in identifying not just the correct answer tokens but also\nthe span with the most relevant supporting evidence. Lastly, we observe that\ncontexts for certain question types show higher lexical diversity than others\nand find CBR-MRC to be robust to these variations while performance using\nfully-parametric methods drops.",
        "translated": ""
    },
    {
        "title": "Candidate Set Re-ranking for Composed Image Retrieval with Dual\n  Multi-modal Encoder",
        "url": "http://arxiv.org/abs/2305.16304v1",
        "pub_date": "2023-05-25",
        "summary": "Composed image retrieval aims to find an image that best matches a given\nmulti-modal user query consisting of a reference image and text pair. Existing\nmethods commonly pre-compute image embeddings over the entire corpus and\ncompare these to a reference image embedding modified by the query text at test\ntime. Such a pipeline is very efficient at test time since fast vector\ndistances can be used to evaluate candidates, but modifying the reference image\nembedding guided only by a short textual description can be difficult,\nespecially independent of potential candidates. An alternative approach is to\nallow interactions between the query and every possible candidate, i.e.,\nreference-text-candidate triplets, and pick the best from the entire set.\nThough this approach is more discriminative, for large-scale datasets the\ncomputational cost is prohibitive since pre-computation of candidate embeddings\nis no longer possible. We propose to combine the merits of both schemes using a\ntwo-stage model. Our first stage adopts the conventional vector distancing\nmetric and performs a fast pruning among candidates. Meanwhile, our second\nstage employs a dual-encoder architecture, which effectively attends to the\ninput triplet of reference-text-candidate and re-ranks the candidates. Both\nstages utilize a vision-and-language pre-trained network, which has proven\nbeneficial for various downstream tasks. Our method consistently outperforms\nstate-of-the-art approaches on standard benchmarks for the task.",
        "translated": ""
    },
    {
        "title": "A Survey on Asking Clarification Questions Datasets in Conversational\n  Systems",
        "url": "http://arxiv.org/abs/2305.15933v1",
        "pub_date": "2023-05-25",
        "summary": "The ability to understand a user's underlying needs is critical for\nconversational systems, especially with limited input from users in a\nconversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to\nreveal users' true intent from their queries or utterances arise as an\nessential task. However, it is noticeable that a key limitation of the existing\nACQs studies is their incomparability, from inconsistent use of data, distinct\nexperimental setups and evaluation strategies. Therefore, in this paper, to\nassist the development of ACQs techniques, we comprehensively analyse the\ncurrent ACQs research status, which offers a detailed comparison of publicly\navailable datasets, and discusses the applied evaluation metrics, joined with\nbenchmarks for multiple ACQs-related tasks. In particular, given a thorough\nanalysis of the ACQs task, we discuss a number of corresponding research\ndirections for the investigation of ACQs as well as the development of\nconversational systems.",
        "translated": ""
    },
    {
        "title": "Enhancing the Ranking Context of Dense Retrieval Methods through\n  Reciprocal Nearest Neighbors",
        "url": "http://arxiv.org/abs/2305.15720v1",
        "pub_date": "2023-05-25",
        "summary": "Sparse annotation poses persistent challenges to training dense retrieval\nmodels, such as the problem of false negatives, i.e. unlabeled relevant\ndocuments that are spuriously used as negatives in contrastive learning,\ndistorting the training signal. To alleviate this problem, we introduce\nevidence-based label smoothing, a computationally efficient method that\nprevents penalizing the model for assigning high relevance to false negatives.\nTo compute the target relevance distribution over candidate documents within\nthe ranking context of a given query, candidates most similar to the ground\ntruth are assigned a non-zero relevance probability based on the degree of\ntheir similarity to the ground-truth document(s). As a relevance estimate we\nleverage an improved similarity metric based on reciprocal nearest neighbors,\nwhich can also be used independently to rerank candidates in post-processing.\nThrough extensive experiments on two large-scale ad hoc text retrieval datasets\nwe demonstrate that both methods can improve the ranking effectiveness of dense\nretrieval models.",
        "translated": ""
    },
    {
        "title": "BookGPT: A General Framework for Book Recommendation Empowered by Large\n  Language Model",
        "url": "http://arxiv.org/abs/2305.15673v1",
        "pub_date": "2023-05-25",
        "summary": "With the continuous development and change exhibited by large language model\n(LLM) technology, represented by generative pretrained transformers (GPTs),\nmany classic scenarios in various fields have re-emerged with new\nopportunities. This paper takes ChatGPT as the modeling object, incorporates\nLLM technology into the typical book resource understanding and recommendation\nscenario for the first time, and puts it into practice. By building a\nChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT,\nthis paper attempts to apply ChatGPT to recommendation modeling for three\ntypical tasks, book rating recommendation, user rating recommendation, and book\nsummary recommendation, and explores the feasibility of LLM technology in book\nrecommendation scenarios. At the same time, based on different evaluation\nschemes for book recommendation tasks and the existing classic recommendation\nmodels, this paper discusses the advantages and disadvantages of the BookGPT in\nbook recommendation scenarios and analyzes the opportunities and improvement\ndirections for subsequent LLMs in these scenarios.",
        "translated": ""
    },
    {
        "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
        "url": "http://arxiv.org/abs/2305.15645v1",
        "pub_date": "2023-05-25",
        "summary": "In conversational search, the user's real search intent for the current turn\nis dependent on the previous conversation history. It is challenging to\ndetermine a good search query from the whole conversation context. To avoid the\nexpensive re-training of the query encoder, most existing methods try to learn\na rewriting model to de-contextualize the current query by mimicking the manual\nquery rewriting. However, manually rewritten queries are not always the best\nsearch queries. Training a rewriting model on them would limit the model's\nability to produce good search queries. Another useful hint is the potential\nanswer to the question. In this paper, we propose ConvGQR, a new framework to\nreformulate conversational queries based on generative pre-trained language\nmodels (PLMs), one for query rewriting and another for generating potential\nanswers. By combining both, ConvGQR can produce better search queries. In\naddition, to relate query reformulation to retrieval performance, we propose a\nknowledge infusion mechanism to optimize both query reformulation and\nretrieval. Extensive experiments on four conversational search datasets\ndemonstrate the effectiveness of ConvGQR.",
        "translated": ""
    },
    {
        "title": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language\n  Models",
        "url": "http://arxiv.org/abs/2305.15597v1",
        "pub_date": "2023-05-24",
        "summary": "The mission of open knowledge graph (KG) completion is to draw new findings\nfrom known facts. Existing works that augment KG completion require either (1)\nfactual triples to enlarge the graph reasoning space or (2) manually designed\nprompts to extract knowledge from a pre-trained language model (PLM),\nexhibiting limited performance and requiring expensive efforts from experts. To\nthis end, we propose TAGREAL that automatically generates quality query prompts\nand retrieves support information from large text corpora to probe knowledge\nfrom PLM for KG completion. The results show that TAGREAL achieves\nstate-of-the-art performance on two benchmark datasets. We find that TAGREAL\nhas superb performance even with limited training data, outperforming existing\nembedding-based, graph-based, and PLM-based methods.",
        "translated": ""
    },
    {
        "title": "Representation Online Matters: Practical End-to-End Diversification in\n  Search and Recommender Systems",
        "url": "http://arxiv.org/abs/2305.15534v1",
        "pub_date": "2023-05-24",
        "summary": "As the use of online platforms continues to grow across all demographics,\nusers often express a desire to feel represented in the content. To improve\nrepresentation in search results and recommendations, we introduce end-to-end\ndiversification, ensuring that diverse content flows throughout the various\nstages of these systems, from retrieval to ranking. We develop, experiment, and\ndeploy scalable diversification mechanisms in multiple production surfaces on\nthe Pinterest platform, including Search, Related Products, and New User\nHomefeed, to improve the representation of different skin tones in beauty and\nfashion content. Diversification in production systems includes three\ncomponents: identifying requests that will trigger diversification, ensuring\ndiverse content is retrieved from the large content corpus during the retrieval\nstage, and finally, balancing the diversity-utility trade-off in a\nself-adjusting manner in the ranking stage. Our approaches, which evolved from\nusing Strong-OR logical operator to bucketized retrieval at the retrieval stage\nand from greedy re-rankers to multi-objective optimization using determinantal\npoint processes for the ranking stage, balances diversity and utility while\nenabling fast iterations and scalable expansion to diversification over\nmultiple dimensions. Our experiments indicate that these approaches\nsignificantly improve diversity metrics, with a neutral to a positive impact on\nutility metrics and improved user satisfaction, both qualitatively and\nquantitatively, in production.",
        "translated": ""
    },
    {
        "title": "Large Language Models for User Interest Journeys",
        "url": "http://arxiv.org/abs/2305.15498v1",
        "pub_date": "2023-05-24",
        "summary": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
        "translated": ""
    },
    {
        "title": "Adversarial Attacks on Online Learning to Rank with Click Feedback",
        "url": "http://arxiv.org/abs/2305.17071v1",
        "pub_date": "2023-05-26",
        "summary": "Online learning to rank (OLTR) is a sequential decision-making problem where\na learning agent selects an ordered list of items and receives feedback through\nuser clicks. Although potential attacks against OLTR algorithms may cause\nserious losses in real-world applications, little is known about adversarial\nattacks on OLTR. This paper studies attack strategies against multiple variants\nof OLTR. Our first result provides an attack strategy against the UCB algorithm\non classical stochastic bandits with binary feedback, which solves the key\nissues caused by bounded and discrete feedback that previous works can not\nhandle. Building on this result, we design attack algorithms against UCB-based\nOLTR algorithms in position-based and cascade models. Finally, we propose a\ngeneral attack strategy against any algorithm under the general click model.\nEach attack algorithm manipulates the learning agent into choosing the target\nattack item $T-o(T)$ times, incurring a cumulative cost of $o(T)$. Experiments\non synthetic and real data further validate the effectiveness of our proposed\nattack algorithms.",
        "translated": ""
    },
    {
        "title": "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System",
        "url": "http://arxiv.org/abs/2305.17034v1",
        "pub_date": "2023-05-26",
        "summary": "Significant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type.",
        "translated": ""
    },
    {
        "title": "Is googling risky? A study on risk perception and experiences of adverse\n  consequences in web search",
        "url": "http://arxiv.org/abs/2305.16990v1",
        "pub_date": "2023-05-26",
        "summary": "Search engines, such as Google, have a considerable impact on society.\nTherefore, undesirable consequences, such as retrieving incorrect search\nresults, pose a risk to users. Although previous research has reported the\nadverse outcomes of web search, little is known about how search engine users\nevaluate those outcomes. In this study, we show which aspects of web search are\nperceived as risky using a sample (N = 3,884) representative of the German\nInternet population. We found that many participants are often concerned with\nadverse consequences immediately appearing on the search engine result page.\nMoreover, participants' experiences with adverse consequences are directly\nrelated to their risk perception. Our results demonstrate that people perceive\nrisks related to web search. In addition to our study, there is a need for more\nindependent research on the possible detrimental outcomes of web search to\nmonitor and mitigate risks. Apart from risks for individuals, search engines\nwith a massive number of users have an extraordinary impact on society;\ntherefore, the acceptable risks of web search should be discussed.",
        "translated": ""
    },
    {
        "title": "Efficient Decoding of Compositional Structure in Holistic\n  Representations",
        "url": "http://arxiv.org/abs/2305.16873v1",
        "pub_date": "2023-05-26",
        "summary": "We investigate the task of retrieving information from compositional\ndistributed representations formed by Hyperdimensional Computing/Vector\nSymbolic Architectures and present novel techniques which achieve new\ninformation rate bounds. First, we provide an overview of the decoding\ntechniques that can be used to approach the retrieval task. The techniques are\ncategorized into four groups. We then evaluate the considered techniques in\nseveral settings that involve, e.g., inclusion of external noise and storage\nelements with reduced precision. In particular, we find that the decoding\ntechniques from the sparse coding and compressed sensing literature (rarely\nused for Hyperdimensional Computing/Vector Symbolic Architectures) are also\nwell-suited for decoding information from the compositional distributed\nrepresentations. Combining these decoding techniques with interference\ncancellation ideas from communications improves previously reported bounds\n(Hersche et al., 2021) of the information rate of the distributed\nrepresentations from 1.20 to 1.40 bits per dimension for smaller codebooks and\nfrom 0.60 to 1.26 bits per dimension for larger codebooks.",
        "translated": ""
    },
    {
        "title": "Automating the Analysis of Institutional Design in International\n  Agreements",
        "url": "http://arxiv.org/abs/2305.16750v1",
        "pub_date": "2023-05-26",
        "summary": "This paper explores the automatic knowledge extraction of formal\ninstitutional design - norms, rules, and actors - from international\nagreements. The focus was to analyze the relationship between the visibility\nand centrality of actors in the formal institutional design in regulating\ncritical aspects of cultural heritage relations. The developed tool utilizes\ntechniques such as collecting legal documents, annotating them with\nInstitutional Grammar, and using graph analysis to explore the formal\ninstitutional design. The system was tested against the 2003 UNESCO Convention\nfor the Safeguarding of the Intangible Cultural Heritage.",
        "translated": ""
    },
    {
        "title": "The Search for Stability: Learning Dynamics of Strategic Publishers with\n  Initial Documents",
        "url": "http://arxiv.org/abs/2305.16695v1",
        "pub_date": "2023-05-26",
        "summary": "We study a game-theoretic model of information retrieval, in which strategic\npublishers aim to maximize their chances of being ranked first by the search\nengine, while maintaining the integrity of their original documents. We show\nthat the commonly used PRP ranking scheme results in an unstable environment\nwhere games often fail to reach pure Nash equilibrium. We propose the Relative\nRanking Principle (RRP) as an alternative ranking principle, and introduce two\nranking functions that are instances of the RRP. We provide both theoretical\nand empirical evidence that these methods lead to a stable search ecosystem, by\nproviding positive results on the learning dynamics convergence. We also define\nthe publishers' and users' welfare, and demonstrate a possible publisher-user\ntrade-off, which highlights the complexity of determining which ranking\nfunction should be selected by the search engine designer.",
        "translated": ""
    },
    {
        "title": "Multiview Identifiers Enhanced Generative Retrieval",
        "url": "http://arxiv.org/abs/2305.16675v1",
        "pub_date": "2023-05-26",
        "summary": "Instead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
        "translated": ""
    },
    {
        "title": "FARA: Future-aware Ranking Algorithm for Fairness Optimization",
        "url": "http://arxiv.org/abs/2305.16637v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking systems are the key components of modern Information Retrieval (IR)\napplications, such as search engines and recommender systems. Besides the\nranking relevance to users, the exposure fairness to item providers has also\nbeen considered an important factor in ranking optimization. Many fair ranking\nalgorithms have been proposed to jointly optimize both ranking relevance and\nfairness. However, we find that most existing fair ranking methods adopt greedy\nalgorithms that only optimize rankings for the next immediate session or\nrequest. As shown in this paper, such a myopic paradigm could limit the upper\nbound of ranking optimization and lead to suboptimal performance in the long\nterm. To this end, we propose FARA, a novel Future-Aware Ranking Algorithm for\nranking relevance and fairness optimization. Instead of greedily optimizing\nrankings for the next immediate session, FARA plans ahead by jointly optimizing\nmultiple ranklists together and saving them for future sessions. Particularly,\nFARA first uses the Taylor expansion to investigate how future ranklists will\ninfluence the overall fairness of the system. Then, based on the analysis of\nthe Taylor expansion, FARA adopts a two-phase optimization algorithm where we\nfirst solve an optimal future exposure planning problem and then construct the\noptimal ranklists according to the optimal future exposure planning.\nTheoretically, we show that FARA is optimal for ranking relevance and fairness\njoint optimization. Empirically, our extensive experiments on three\nsemi-synthesized datasets show that FARA is efficient, effective, and can\ndeliver significantly better ranking performance compared to state-of-the-art\nfair ranking methods.",
        "translated": ""
    },
    {
        "title": "DataFinder: Scientific Dataset Recommendation from Natural Language\n  Descriptions",
        "url": "http://arxiv.org/abs/2305.16636v1",
        "pub_date": "2023-05-26",
        "summary": "Modern machine learning relies on datasets to develop and validate research\nideas. Given the growth of publicly available data, finding the right dataset\nto use is increasingly difficult. Any research question imposes explicit and\nimplicit constraints on how well a given dataset will enable researchers to\nanswer this question, such as dataset size, modality, and domain. We introduce\na new task of recommending relevant datasets given a short natural language\ndescription of a research idea, to help people find relevant datasets for their\nneeds. Dataset recommendation poses unique challenges as an information\nretrieval problem; datasets are hard to directly index for search and there are\nno corpora readily available for this task. To operationalize this task, we\nbuild the DataFinder Dataset which consists of a larger\nautomatically-constructed training set (17.5K queries) and a smaller\nexpert-annotated evaluation set (392 queries). Using this data, we compare\nvarious information retrieval algorithms on our test set and present the\nfirst-ever published system for text-based dataset recommendation using machine\nlearning techniques. This system, trained on the DataFinder Dataset, finds more\nrelevant search results than existing third-party dataset search engines. To\nencourage progress on dataset recommendation, we release our dataset and models\nto the public.",
        "translated": ""
    },
    {
        "title": "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach",
        "url": "http://arxiv.org/abs/2305.16606v1",
        "pub_date": "2023-05-26",
        "summary": "Ranking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
        "translated": ""
    },
    {
        "title": "Large Language Models are not Fair Evaluators",
        "url": "http://arxiv.org/abs/2305.17926v1",
        "pub_date": "2023-05-29",
        "summary": "We uncover a systematic bias in the evaluation paradigm of adopting large\nlanguage models~(LLMs), e.g., GPT-4, as a referee to score the quality of\nresponses generated by candidate models. We find that the quality ranking of\ncandidate responses can be easily hacked by simply altering their order of\nappearance in the context. This manipulation allows us to skew the evaluation\nresult, making one model appear considerably superior to the other, e.g.,\nvicuna could beat ChatGPT on 66 over 80 tested queries. To address this issue,\nwe propose two simple yet effective calibration strategies: 1) Multiple\nEvidence Calibration, which requires the evaluator model to generate multiple\ndetailed pieces of evidence before assigning ratings; 2) Balanced Position\nCalibration, which aggregates results across various orders to determine the\nfinal score. Extensive experiments demonstrate that our approach successfully\nmitigates evaluation bias, resulting in closer alignment with human judgments.\nTo facilitate future research on more robust large language model comparison,\nwe integrate the techniques in the paper into an easy-to-use toolkit\n\\emph{FairEval}, along with the human\nannotations.\\footnote{\\url{https://github.com/i-Eval/FairEval}}",
        "translated": ""
    },
    {
        "title": "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation",
        "url": "http://arxiv.org/abs/2305.17866v1",
        "pub_date": "2023-05-29",
        "summary": "Traditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "HyperFormer: Learning Expressive Sparse Feature Representations via\n  Hypergraph Transformer",
        "url": "http://arxiv.org/abs/2305.17386v1",
        "pub_date": "2023-05-27",
        "summary": "Learning expressive representations for high-dimensional yet sparse features\nhas been a longstanding problem in information retrieval. Though recent deep\nlearning methods can partially solve the problem, they often fail to handle the\nnumerous sparse features, particularly those tail feature values with\ninfrequent occurrences in the training data. Worse still, existing methods\ncannot explicitly leverage the correlations among different instances to help\nfurther improve the representation learning on sparse features since such\nrelational prior knowledge is not provided. To address these challenges, in\nthis paper, we tackle the problem of representation learning on feature-sparse\ndata from a graph learning perspective. Specifically, we propose to model the\nsparse features of different instances using hypergraphs where each node\nrepresents a data instance and each hyperedge denotes a distinct feature value.\nBy passing messages on the constructed hypergraphs based on our Hypergraph\nTransformer (HyperFormer), the learned feature representations capture not only\nthe correlations among different instances but also the correlations among\nfeatures. Our experiments demonstrate that the proposed approach can\neffectively improve feature representation learning on sparse features.",
        "translated": ""
    },
    {
        "title": "Counterfactual Evaluation of Peer-Review Assignment Policies",
        "url": "http://arxiv.org/abs/2305.17339v1",
        "pub_date": "2023-05-27",
        "summary": "Peer review assignment algorithms aim to match research papers to suitable\nexpert reviewers, working to maximize the quality of the resulting reviews. A\nkey challenge in designing effective assignment policies is evaluating how\nchanges to the assignment algorithm map to changes in review quality. In this\nwork, we leverage recently proposed policies that introduce randomness in\npeer-review assignment--in order to mitigate fraud--as a valuable opportunity\nto evaluate counterfactual assignment policies. Specifically, we exploit how\nsuch randomized assignments provide a positive probability of observing the\nreviews of many assignment policies of interest. To address challenges in\napplying standard off-policy evaluation methods, such as violations of\npositivity, we introduce novel methods for partial identification based on\nmonotonicity and Lipschitz smoothness assumptions for the mapping between\nreviewer-paper covariates and outcomes. We apply our methods to peer-review\ndata from two computer science venues: the TPDP'21 workshop (95 papers and 35\nreviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We\nconsider estimates of (i) the effect on review quality when changing weights in\nthe assignment algorithm, e.g., weighting reviewers' bids vs. textual\nsimilarity (between the review's past papers and the submission), and (ii) the\n\"cost of randomization\", capturing the difference in expected quality between\nthe perturbed and unperturbed optimal match. We find that placing higher weight\non text similarity results in higher review quality and that introducing\nrandomization in the reviewer-paper assignment only marginally reduces the\nreview quality. Our methods for partial identification may be of independent\ninterest, while our off-policy approach can likely find use evaluating a broad\nclass of algorithmic matching systems.",
        "translated": ""
    },
    {
        "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and\n  Document Deduplication",
        "url": "http://arxiv.org/abs/2305.17310v1",
        "pub_date": "2023-05-27",
        "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To\nremove duplicate results in a Web search, for example, a common approach looks\nat the Jaccard index between all pairs of pages. In social network analysis, a\nmuch-celebrated metric is the Adamic-Adar index, widely used to compare node\nneighborhood sets in the important problem of predicting links. However, with\nthe increasing amount of data to be processed, calculating the exact similarity\nbetween all pairs can be intractable. The challenge of working at this scale\nhas motivated research into efficient estimators for set similarity metrics.\nThe two most popular estimators, MinHash and SimHash, are indeed used in\napplications such as document deduplication and recommender systems where large\nvolumes of data need to be processed. Given the importance of these tasks, the\ndemand for advancing estimators is evident. We propose DotHash, an unbiased\nestimator for the intersection size of two sets. DotHash can be used to\nestimate the Jaccard index and, to the best of our knowledge, is the first\nmethod that can also estimate the Adamic-Adar index and a family of related\nmetrics. We formally define this family of metrics, provide theoretical bounds\non the probability of estimate errors, and analyze its empirical performance.\nOur experimental results indicate that DotHash is more accurate than the other\nestimators in link prediction and detecting duplicate documents with the same\ncomplexity and similar comparison time.",
        "translated": ""
    },
    {
        "title": "Table Detection for Visually Rich Document Images",
        "url": "http://arxiv.org/abs/2305.19181v1",
        "pub_date": "2023-05-30",
        "summary": "Table Detection (TD) is a fundamental task towards visually rich document\nunderstanding. Current studies usually formulate the TD problem as an object\ndetection problem, then leverage Intersection over Union (IoU) based metrics to\nevaluate the model performance and IoU-based loss functions to optimize the\nmodel. TD applications usually require the prediction results to cover all the\ntable contents and avoid information loss. However, IoU and IoU-based loss\nfunctions cannot directly reflect the degree of information loss for the\nprediction results. Therefore, we propose to decouple IoU into a ground truth\ncoverage term and a prediction coverage term, in which the former can be used\nto measure the information loss of the prediction results.\n  Besides, tables in the documents are usually large, sparsely distributed, and\nhave no overlaps because they are designed to summarize essential information\nto make it easy to read and interpret for human readers. Therefore, in this\nstudy, we use SparseR-CNN as the base model, and further improve the model by\nusing Gaussian Noise Augmented Image Size region proposals and many-to-one\nlabel assignments.\n  To demonstrate the effectiveness of proposed method and compare with\nstate-of-the-art methods fairly, we conduct experiments and use IoU-based\nevaluation metrics to evaluate the model performance. The experimental results\nshow that the proposed method can consistently outperform state-of-the-art\nmethods under different IoU-based metric on a variety of datasets. We conduct\nfurther experiments to show the superiority of the proposed decoupled IoU for\nthe TD applications by replacing the IoU-based loss functions and evaluation\nmetrics with proposed decoupled IoU counterparts. The experimental results show\nthat our proposed decoupled IoU loss can encourage the model to alleviate\ninformation loss.",
        "translated": ""
    },
    {
        "title": "Event-Centric Query Expansion in Web Search",
        "url": "http://arxiv.org/abs/2305.19019v1",
        "pub_date": "2023-05-30",
        "summary": "In search engines, query expansion (QE) is a crucial technique to improve\nsearch experience. Previous studies often rely on long-term search log mining,\nwhich leads to slow updates and is sub-optimal for time-sensitive news\nsearches. In this work, we present Event-Centric Query Expansion (EQE), a novel\nQE system that addresses these issues by mining the best expansion from a\nsignificant amount of potential events rapidly and accurately. This system\nconsists of four stages, i.e., event collection, event reformulation, semantic\nretrieval and online ranking. Specifically, we first collect and filter news\nheadlines from websites. Then we propose a generation model that incorporates\ncontrastive learning and prompt-tuning techniques to reformulate these\nheadlines to concise candidates. Additionally, we fine-tune a dual-tower\nsemantic model to function as an encoder for event retrieval and explore a\ntwo-stage contrastive training approach to enhance the accuracy of event\nretrieval. Finally, we rank the retrieved events and select the optimal one as\nQE, which is then used to improve the retrieval of event-related documents.\nThrough offline analysis and online A/B testing, we observe that the EQE system\nsignificantly improves many metrics compared to the baseline. The system has\nbeen deployed in Tencent QQ Browser Search and served hundreds of millions of\nusers. The dataset and baseline codes are available at\nhttps://open-event-hub.github.io/eqe .",
        "translated": ""
    },
    {
        "title": "A Recipe for Efficient SBIR Models: Combining Relative Triplet Loss with\n  Batch Normalization and Knowledge Distillation",
        "url": "http://arxiv.org/abs/2305.18988v1",
        "pub_date": "2023-05-30",
        "summary": "Sketch-Based Image Retrieval (SBIR) is a crucial task in multimedia\nretrieval, where the goal is to retrieve a set of images that match a given\nsketch query. Researchers have already proposed several well-performing\nsolutions for this task, but most focus on enhancing embedding through\ndifferent approaches such as triplet loss, quadruplet loss, adding data\naugmentation, and using edge extraction. In this work, we tackle the problem\nfrom various angles. We start by examining the training data quality and show\nsome of its limitations. Then, we introduce a Relative Triplet Loss (RTL), an\nadapted triplet loss to overcome those limitations through loss weighting based\non anchors similarity. Through a series of experiments, we demonstrate that\nreplacing a triplet loss with RTL outperforms previous state-of-the-art without\nthe need for any data augmentation. In addition, we demonstrate why batch\nnormalization is more suited for SBIR embeddings than l2-normalization and show\nthat it improves significantly the performance of our models. We further\ninvestigate the capacity of models required for the photo and sketch domains\nand demonstrate that the photo encoder requires a higher capacity than the\nsketch encoder, which validates the hypothesis formulated in [34]. Then, we\npropose a straightforward approach to train small models, such as ShuffleNetv2\n[22] efficiently with a marginal loss of accuracy through knowledge\ndistillation. The same approach used with larger models enabled us to\noutperform previous state-of-the-art results and achieve a recall of 62.38% at\nk = 1 on The Sketchy Database [30].",
        "translated": ""
    },
    {
        "title": "The Information Retrieval Experiment Platform",
        "url": "http://arxiv.org/abs/2305.18932v1",
        "pub_date": "2023-05-30",
        "summary": "We integrate ir_datasets, ir_measures, and PyTerrier with TIRA in the\nInformation Retrieval Experiment Platform (TIREx) to promote more standardized,\nreproducible, scalable, and even blinded retrieval experiments. Standardization\nis achieved when a retrieval approach implements PyTerrier's interfaces and the\ninput and output of an experiment are compatible with ir_datasets and\nir_measures. However, none of this is a must for reproducibility and\nscalability, as TIRA can run any dockerized software locally or remotely in a\ncloud-native execution environment. Version control and caching ensure\nefficient (re)execution. TIRA allows for blind evaluation when an experiment\nruns on a remote server or cloud not under the control of the experimenter. The\ntest data and ground truth are then hidden from public access, and the\nretrieval software has to process them in a sandbox that prevents data leaks.\n  We currently host an instance of TIREx with 15 corpora (1.9 billion\ndocuments) on which 32 shared retrieval tasks are based. Using Docker images of\n50 standard retrieval approaches, we automatically evaluated all approaches on\nall tasks (50 $\\cdot$ 32 = 1,600~runs) in less than a week on a midsize cluster\n(1,620 CPU cores and 24 GPUs). This instance of TIREx is open for submissions\nand will be integrated with the IR Anthology, as well as released open source.",
        "translated": ""
    },
    {
        "title": "Criteria Tell You More than Ratings: Criteria Preference-Aware Light\n  Graph Convolution for Effective Multi-Criteria Recommendation",
        "url": "http://arxiv.org/abs/2305.18885v1",
        "pub_date": "2023-05-30",
        "summary": "The multi-criteria (MC) recommender system, which leverages MC rating\ninformation in a wide range of e-commerce areas, is ubiquitous nowadays.\nSurprisingly, although graph neural networks (GNNs) have been widely applied to\ndevelop various recommender systems due to GNN's high expressive capability in\nlearning graph representations, it has been still unexplored how to design MC\nrecommender systems with GNNs. In light of this, we make the first attempt\ntowards designing a GNN-aided MC recommender system. Specifically, rather than\nstraightforwardly adopting existing GNN-based recommendation methods, we devise\na novel criteria preference-aware light graph convolution CPA-LGC method, which\nis capable of precisely capturing the criteria preference of users as well as\nthe collaborative signal in complex high-order connectivities. To this end, we\nfirst construct an MC expansion graph that transforms user--item MC ratings\ninto an expanded bipartite graph to potentially learn from the collaborative\nsignal in MC ratings. Next, to strengthen the capability of criteria preference\nawareness, CPA-LGC incorporates newly characterized embeddings, including\nuser-specific criteria-preference embeddings and item-specific criterion\nembeddings, into our graph convolution model. Through comprehensive evaluations\nusing four real-world datasets, we demonstrate (a) the superiority over\nbenchmark MC recommendation methods and benchmark recommendation methods using\nGNNs with tremendous gains, (b) the effectiveness of core components in\nCPA-LGC, and (c) the computational efficiency.",
        "translated": ""
    },
    {
        "title": "Robust Reinforcement Learning Objectives for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.18820v1",
        "pub_date": "2023-05-30",
        "summary": "Attention-based sequential recommendation methods have demonstrated promising\nresults by accurately capturing users' dynamic interests from historical\ninteractions. In addition to generating superior user representations, recent\nstudies have begun integrating reinforcement learning (RL) into these models.\nFraming sequential recommendation as an RL problem with reward signals, unlocks\ndeveloping recommender systems (RS) that consider a vital aspect-incorporating\ndirect user feedback in the form of rewards to deliver a more personalized\nexperience. Nonetheless, employing RL algorithms presents challenges, including\noff-policy training, expansive combinatorial action spaces, and the scarcity of\ndatasets with sufficient reward signals. Contemporary approaches have attempted\nto combine RL and sequential modeling, incorporating contrastive-based\nobjectives and negative sampling strategies for training the RL component. In\nthis study, we further emphasize the efficacy of contrastive-based objectives\npaired with augmentation to address datasets with extended horizons.\nAdditionally, we recognize the potential instability issues that may arise\nduring the application of negative sampling. These challenges primarily stem\nfrom the data imbalance prevalent in real-world datasets, which is a common\nissue in offline RL contexts. While our established baselines attempt to\nmitigate this through various techniques, instability remains an issue.\nTherefore, we introduce an enhanced methodology aimed at providing a more\neffective solution to these challenges.",
        "translated": ""
    },
    {
        "title": "Who Would be Interested in Services? An Entity Graph Learning System for\n  User Targeting",
        "url": "http://arxiv.org/abs/2305.18780v1",
        "pub_date": "2023-05-30",
        "summary": "With the growing popularity of various mobile devices, user targeting has\nreceived a growing amount of attention, which aims at effectively and\nefficiently locating target users that are interested in specific services.\nMost pioneering works for user targeting tasks commonly perform\nsimilarity-based expansion with a few active users as seeds, suffering from the\nfollowing major issues: the unavailability of seed users for newcoming services\nand the unfriendliness of black-box procedures towards marketers. In this\npaper, we design an Entity Graph Learning (EGL) system to provide explainable\nuser targeting ability meanwhile applicable to addressing the cold-start issue.\nEGL System follows the hybrid online-offline architecture to satisfy the\nrequirements of scalability and timeliness. Specifically, in the offline stage,\nthe system focuses on the heavyweight entity graph construction and user entity\npreference learning, in which we propose a Three-stage Relation Mining\nProcedure (TRMP), breaking loose from the expensive seed users. At the online\nstage, the system offers the ability of user targeting in real-time based on\nthe entity graph from the offline stage. Since the user targeting process is\nbased on graph reasoning, the whole process is transparent and\noperation-friendly to marketers. Finally, extensive offline experiments and\nonline A/B testing demonstrate the superior performance of the proposed EGL\nSystem.",
        "translated": ""
    },
    {
        "title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts",
        "url": "http://arxiv.org/abs/2305.18727v1",
        "pub_date": "2023-05-30",
        "summary": "With a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
        "translated": ""
    },
    {
        "title": "Known by the Company it Keeps: Proximity-Based Indexing for Physical\n  Content in Archival Repositories",
        "url": "http://arxiv.org/abs/2305.18683v1",
        "pub_date": "2023-05-30",
        "summary": "Despite the plethora of born-digital content, vast troves of important\ncontent remain accessible only on physical media such as paper or microfilm.\nThe traditional approach to indexing undigitized content is using manually\ncreated metadata that describes content at some level of aggregation (e.g.,\nfolder, box, or collection). Searchers led in this way to some subset of the\ncontent often must then manually examine substantial quantities of physical\nmedia to find what they are looking for. This paper proposes a complementary\napproach, in which selective digitization of a small portion of the content is\nused as a basis for proximity-based indexing as a way of bringing the user\ncloser to the specific content for which they are looking. Experiments with 35\nboxes of partially digitized US State Department records indicate that\nbox-level indexes built in this way can provide a useful basis for search.",
        "translated": ""
    },
    {
        "title": "Improving Generalization for Multimodal Fake News Detection",
        "url": "http://arxiv.org/abs/2305.18599v1",
        "pub_date": "2023-05-29",
        "summary": "The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for fake news\ndetection. However, state-of-the-art approaches are usually trained on datasets\nof smaller size or with a limited set of specific topics. As a consequence,\nthese models lack generalization capabilities and are not applicable to\nreal-world data. In this paper, we propose three models that adopt and\nfine-tune state-of-the-art multimodal transformers for multimodal fake news\ndetection. We conduct an in-depth analysis by manipulating the input data aimed\nto explore models performance in realistic use cases on social media. Our study\nacross multiple models demonstrates that these systems suffer significant\nperformance drops against manipulated data. To reduce the bias and improve\nmodel generalization, we suggest training data augmentation to conduct more\nmeaningful experiments for fake news detection on social media. The proposed\ndata augmentation techniques enable models to generalize better and yield\nimproved state-of-the-art results.",
        "translated": ""
    },
    {
        "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on\n  Structured Data",
        "url": "http://arxiv.org/abs/2305.19912v1",
        "pub_date": "2023-05-31",
        "summary": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which\nencodes user queries and structured data in one universal embedding space for\nretrieving structured data. SANTA proposes two pretraining methods to make\nlanguage models structure-aware and learn effective representations for\nstructured data: 1) Structured Data Alignment, which utilizes the natural\nalignment relations between structured data and unstructured data for\nstructure-aware pretraining. It contrastively trains language models to\nrepresent multi-modal text data and teaches models to distinguish matched\nstructured data for unstructured texts. 2) Masked Entity Prediction, which\ndesigns an entity-oriented mask strategy and asks language models to fill in\nthe masked entities. Our experiments show that SANTA achieves state-of-the-art\non code search and product search and conducts convincing results in the\nzero-shot setting. SANTA learns tailored representations for multi-modal text\ndata by aligning structured and unstructured data pairs and capturing\nstructural semantics by masking and predicting entities in the structured data.\nAll codes are available at https://github.com/OpenMatch/OpenMatch.",
        "translated": ""
    },
    {
        "title": "Web scraping: a promising tool for geographic data acquisition",
        "url": "http://arxiv.org/abs/2305.19893v1",
        "pub_date": "2023-05-31",
        "summary": "With much of our lives taking place online, researchers are increasingly\nturning to information from the World Wide Web to gain insights into geographic\npatterns and processes. Web scraping as an online data acquisition technique\nallows us to gather intelligence especially on social and economic actions for\nwhich the Web serves as a platform. Specific opportunities relate to\nnear-real-time access to object-level geolocated data, which can be captured in\na cost-effective way. The studied geographic phenomena include, but are not\nlimited to, the rental market and associated processes such as gentrification,\nentrepreneurial ecosystems, or spatial planning processes. Since the\ninformation retrieved from the Web is not made available for that purpose, Web\nscraping faces several unique challenges, several of which relate to location.\nEthical and legal issues mainly relate to intellectual property rights,\ninformed consent and (geo-) privacy, and website integrity and contract. These\nissues also effect the practice of open science. In addition, there are\ntechnical and statistical challenges that relate to dependability and\nincompleteness, data inconsistencies and bias, as well as the limited\nhistorical coverage. Geospatial analyses furthermore usually require the\nautomated extraction and subsequent resolution of toponyms or addresses\n(geoparsing, geocoding). A study on apartment rent in Leipzig, Germany is used\nto illustrate the use of Web scraping and its challenges. We conclude that\ngeographic researchers should embrace Web scraping as a powerful and affordable\ndigital fieldwork tool while paying special attention to its legal, ethical,\nand methodological challenges.",
        "translated": ""
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.19860v1",
        "pub_date": "2023-05-31",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in the field of\nNatural Language Processing (NLP) and have recently gained significant\nattention in the domain of Recommendation Systems (RS). These models, trained\non massive amounts of data using self-supervised learning, have demonstrated\nremarkable success in learning universal representations and have the potential\nto enhance various aspects of recommendation systems by some effective transfer\ntechniques such as fine-tuning and prompt tuning, and so on. The crucial aspect\nof harnessing the power of language models in enhancing recommendation quality\nis the utilization of their high-quality representations of textual features\nand their extensive coverage of external knowledge to establish correlations\nbetween items and users. To provide a comprehensive understanding of the\nexisting LLM-based recommendation systems, this survey presents a taxonomy that\ncategorizes these models into two major paradigms, respectively Discriminative\nLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation\n(GLLM4Rec), with the latter being systematically sorted out for the first time.\nFurthermore, we systematically review and analyze existing LLM-based\nrecommendation systems within each paradigm, providing insights into their\nmethodologies, techniques, and performance. Additionally, we identify key\nchallenges and several valuable findings to provide researchers and\npractitioners with inspiration.",
        "translated": ""
    },
    {
        "title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish\n  Language",
        "url": "http://arxiv.org/abs/2305.19840v1",
        "pub_date": "2023-05-31",
        "summary": "The BEIR dataset is a large, heterogeneous benchmark for Information\nRetrieval (IR) in zero-shot settings, garnering considerable attention within\nthe research community. However, BEIR and analogous datasets are predominantly\nrestricted to the English language. Our objective is to establish extensive\nlarge-scale resources for IR in the Polish language, thereby advancing the\nresearch in this NLP area. In this work, inspired by mMARCO and Mr.~TyDi\ndatasets, we translated all accessible open IR datasets into Polish, and we\nintroduced the BEIR-PL benchmark -- a new benchmark which comprises 13\ndatasets, facilitating further development, training and evaluation of modern\nPolish language models for IR tasks. We executed an evaluation and comparison\nof numerous IR models on the newly introduced BEIR-PL benchmark. Furthermore,\nwe publish pre-trained open IR models for Polish language,d marking a\npioneering development in this field. Additionally, the evaluation revealed\nthat BM25 achieved significantly lower scores for Polish than for English,\nwhich can be attributed to high inflection and intricate morphological\nstructure of the Polish language. Finally, we trained various re-ranking models\nto enhance the BM25 retrieval, and we compared their performance to identify\ntheir unique characteristic features. To ensure accurate model comparisons, it\nis necessary to scrutinise individual results rather than to average across the\nentire benchmark. Thus, we thoroughly analysed the outcomes of IR models in\nrelation to each individual data subset encompassed by the BEIR benchmark. The\nbenchmark data is available at URL {\\bf https://huggingface.co/clarin-knext}.",
        "translated": ""
    },
    {
        "title": "Medication Recommendation via Domain Knowledge Informed Deep Learning",
        "url": "http://arxiv.org/abs/2305.19604v1",
        "pub_date": "2023-05-31",
        "summary": "Medication recommendation is a fundamental yet crucial branch of healthcare,\nwhich provides opportunities to support clinical physicians with more accurate\nmedication prescriptions for patients with complex health conditions. Learning\nfrom electronic health records (EHR) to recommend medications is the most\ncommon way in previous studies. However, most of them neglect incorporating\ndomain knowledge according to the clinical manifestations in the EHR of the\npatient. To address these issues, we propose a novel \\textbf{D}omain\n\\textbf{K}nowledge \\textbf{I}nformed \\textbf{Net}work (DKINet) to integrate\ndomain knowledge with observable clinical manifestations of the patient, which\nis the first dynamic domain knowledge informed framework toward medication\nrecommendation. In particular, we first design a knowledge-driven encoder to\ncapture the domain information and then develop a data-driven encoder to\nintegrate domain knowledge into the observable EHR. To endow the model with the\ncapability of temporal decision, we design an explicit medication encoder for\nlearning the longitudinal dependence of the patient. Extensive experiments on\nthree publicly available datasets verify the superiority of our method. The\ncode will be public upon acceptance.",
        "translated": ""
    },
    {
        "title": "Towards Semi-supervised Universal Graph Classification",
        "url": "http://arxiv.org/abs/2305.19598v1",
        "pub_date": "2023-05-31",
        "summary": "Graph neural networks have pushed state-of-the-arts in graph classifications\nrecently. Typically, these methods are studied within the context of supervised\nend-to-end training, which necessities copious task-specific labels. However,\nin real-world circumstances, labeled data could be limited, and there could be\na massive corpus of unlabeled data, even from unknown classes as a\ncomplementary. Towards this end, we study the problem of semi-supervised\nuniversal graph classification, which not only identifies graph samples which\ndo not belong to known classes, but also classifies the remaining samples into\ntheir respective classes. This problem is challenging due to a severe lack of\nlabels and potential class shifts. In this paper, we propose a novel graph\nneural network framework named UGNN, which makes the best of unlabeled data\nfrom the subgraph perspective. To tackle class shifts, we estimate the\ncertainty of unlabeled graphs using multiple subgraphs, which facilities the\ndiscovery of unlabeled data from unknown categories. Moreover, we construct\nsemantic prototypes in the embedding space for both known and unknown\ncategories and utilize posterior prototype assignments inferred from the\nSinkhorn-Knopp algorithm to learn from abundant unlabeled graphs across\ndifferent subgraph views. Extensive experiments on six datasets verify the\neffectiveness of UGNN in different settings.",
        "translated": ""
    },
    {
        "title": "Multi-Epoch Learning for Deep Click-Through Rate Prediction Models",
        "url": "http://arxiv.org/abs/2305.19531v1",
        "pub_date": "2023-05-31",
        "summary": "The one-epoch overfitting phenomenon has been widely observed in industrial\nClick-Through Rate (CTR) applications, where the model performance experiences\na significant degradation at the beginning of the second epoch. Recent advances\ntry to understand the underlying factors behind this phenomenon through\nextensive experiments. However, it is still unknown whether a multi-epoch\ntraining paradigm could achieve better results, as the best performance is\nusually achieved by one-epoch training. In this paper, we hypothesize that the\nemergence of this phenomenon may be attributed to the susceptibility of the\nembedding layer to overfitting, which can stem from the high-dimensional\nsparsity of data. To maintain feature sparsity while simultaneously avoiding\noverfitting of embeddings, we propose a novel Multi-Epoch learning with Data\nAugmentation (MEDA), which can be directly applied to most deep CTR models.\nMEDA achieves data augmentation by reinitializing the embedding layer in each\nepoch, thereby avoiding embedding overfitting and simultaneously improving\nconvergence. To our best knowledge, MEDA is the first multi-epoch training\nparadigm designed for deep CTR prediction models. We conduct extensive\nexperiments on several public datasets, and the effectiveness of our proposed\nMEDA is fully verified. Notably, the results show that MEDA can significantly\noutperform the conventional one-epoch training. Besides, MEDA has exhibited\nsignificant benefits in a real-world scene on Kuaishou.",
        "translated": ""
    },
    {
        "title": "AdANNS: A Framework for Adaptive Semantic Search",
        "url": "http://arxiv.org/abs/2305.19435v1",
        "pub_date": "2023-05-30",
        "summary": "Web-scale search systems learn an encoder to embed a given query which is\nthen hooked into an approximate nearest neighbor search (ANNS) pipeline to\nretrieve similar data points. To accurately capture tail queries and data\npoints, learned representations typically are rigid, high-dimensional vectors\nthat are generally used as-is in the entire ANNS pipeline and can lead to\ncomputationally expensive retrieval. In this paper, we argue that instead of\nrigid representations, different stages of ANNS can leverage adaptive\nrepresentations of varying capacities to achieve significantly better\naccuracy-compute trade-offs, i.e., stages of ANNS that can get away with more\napproximate computation should use a lower-capacity representation of the same\ndata point. To this end, we introduce AdANNS, a novel ANNS design framework\nthat explicitly leverages the flexibility of Matryoshka Representations. We\ndemonstrate state-of-the-art accuracy-compute trade-offs using novel\nAdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF)\nand quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is\nup to 1.5% more accurate than the rigid representations-based IVF at the same\ncompute budget; and matches accuracy while being up to 90x faster in wall-clock\ntime. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the\n64-byte OPQ baseline constructed using rigid representations -- same accuracy\nat half the cost! We further show that the gains from AdANNS translate to\nmodern-day composite ANNS indices that combine search structures and\nquantization. Finally, we demonstrate that AdANNS can enable inference-time\nadaptivity for compute-aware search on ANNS indices built non-adaptively on\nmatryoshka representations. Code is open-sourced at\nhttps://github.com/RAIVNLab/AdANNS.",
        "translated": ""
    },
    {
        "title": "DuoSearch: A Novel Search Engine for Bulgarian Historical Documents",
        "url": "http://arxiv.org/abs/2305.19392v1",
        "pub_date": "2023-05-30",
        "summary": "Search in collections of digitised historical documents is hindered by a\ntwo-prong problem, orthographic variety and optical character recognition (OCR)\nmistakes. We present a new search engine for historical documents, DuoSearch,\nwhich uses ElasticSearch and machine learning methods based on deep neural\nnetworks to offer a solution to this problem. It was tested on a collection of\nhistorical newspapers in Bulgarian from the mid-19th to the mid-20th century.\nThe system provides an interactive and intuitive interface for the end-users\nallowing them to enter search terms in modern Bulgarian and search across\nhistorical spellings. This is the first solution facilitating the use of\ndigitised historical documents in Bulgarian.",
        "translated": ""
    },
    {
        "title": "AMR4NLI: Interpretable and robust NLI measures from semantic graphs",
        "url": "http://arxiv.org/abs/2306.00936v1",
        "pub_date": "2023-06-01",
        "summary": "The task of natural language inference (NLI) asks whether a given premise\n(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human\nratings of entailment, but the meaning relationships driving these ratings are\nnot formalized. Can the underlying sentence pair relationships be made more\nexplicit in an interpretable yet robust fashion? We compare semantic structures\nto represent premise and hypothesis, including sets of contextualized\nembeddings and semantic graphs (Abstract Meaning Representations), and measure\nwhether the hypothesis is a semantic substructure of the premise, utilizing\ninterpretable metrics. Our evaluation on three English benchmarks finds value\nin both contextualized embeddings and semantic graphs; moreover, they provide\ncomplementary signals, and can be leveraged together in a hybrid model.",
        "translated": ""
    },
    {
        "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach\n  for Low-Resource Complex NER",
        "url": "http://arxiv.org/abs/2306.00928v1",
        "pub_date": "2023-06-01",
        "summary": "Complex Named Entity Recognition (NER) is the task of detecting\nlinguistically complex named entities in low-context text. In this paper, we\npresent ACLM Attention-map aware keyword selection for Conditional Language\nModel fine-tuning), a novel data augmentation approach based on conditional\ngeneration to address the data scarcity problem in low-resource complex NER.\nACLM alleviates the context-entity mismatch issue, a problem existing NER data\naugmentation techniques suffer from and often generates incoherent\naugmentations by placing complex named entities in the wrong context. ACLM\nbuilds on BART and is optimized on a novel text reconstruction or denoising\ntask - we use selective masking (aided by attention maps) to retain the named\nentities and certain keywords in the input sentence that provide contextually\nrelevant additional knowledge or hints about the named entities. Compared with\nother data augmentation strategies, ACLM can generate more diverse and coherent\naugmentations preserving the true word sense of complex entities in the\nsentence. We demonstrate the effectiveness of ACLM both qualitatively and\nquantitatively on monolingual, cross-lingual, and multilingual complex NER\nacross various low-resource settings. ACLM outperforms all our neural baselines\nby a significant margin (1%-36%). In addition, we demonstrate the application\nof ACLM to other domains that suffer from data scarcity (e.g., biomedical). In\npractice, ACLM generates more effective and factual augmentations for these\ndomains than prior methods. Code: https://github.com/Sreyan88/ACLM",
        "translated": ""
    },
    {
        "title": "SpotTarget: Rethinking the Effect of Target Edges for Link Prediction in\n  Graph Neural Networks",
        "url": "http://arxiv.org/abs/2306.00899v1",
        "pub_date": "2023-06-01",
        "summary": "Graph Neural Networks (GNNs) have demonstrated promising outcomes across\nvarious tasks, including node classification and link prediction. Despite their\nremarkable success in various high-impact applications, we have identified\nthree common pitfalls in message passing for link prediction. Particularly, in\nprevalent GNN frameworks (e.g., DGL and PyTorch-Geometric), the target edges\n(i.e., the edges being predicted) consistently exist as message passing edges\nin the graph during training. Consequently, this results in overfitting and\ndistribution shift, both of which adversely impact the generalizability to test\nthe target edges. Additionally, during test time, the failure to exclude the\ntest target edges leads to implicit test leakage caused by neighborhood\naggregation. In this paper, we analyze these three pitfalls and investigate the\nimpact of including or excluding target edges on the performance of nodes with\nvarying degrees during training and test phases. Our theoretical and empirical\nanalysis demonstrates that low-degree nodes are more susceptible to these\npitfalls. These pitfalls can have detrimental consequences when GNNs are\nimplemented in production systems. To systematically address these pitfalls, we\npropose SpotTarget, an effective and efficient GNN training framework. During\ntraining, SpotTarget leverages our insight regarding low-degree nodes and\nexcludes train target edges connected to at least one low-degree node. During\ntest time, it emulates real-world scenarios of GNN usage in production and\nexcludes all test target edges. Our experiments conducted on diverse real-world\ndatasets, demonstrate that SpotTarget significantly enhances GNNs, achieving up\nto a 15x increase in accuracy in sparse graphs. Furthermore, SpotTarget\nconsistently and dramatically improves the performance for low-degree nodes in\ndense graphs.",
        "translated": ""
    },
    {
        "title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection",
        "url": "http://arxiv.org/abs/2306.00765v1",
        "pub_date": "2023-06-01",
        "summary": "Stance Detection is concerned with identifying the attitudes expressed by an\nauthor towards a target of interest. This task spans a variety of domains\nranging from social media opinion identification to detecting the stance for a\nlegal claim. However, the framing of the task varies within these domains, in\nterms of the data collection protocol, the label dictionary and the number of\navailable annotations. Furthermore, these stance annotations are significantly\nimbalanced on a per-topic and inter-topic basis. These make multi-domain stance\ndetection a challenging task, requiring standardization and domain adaptation.\nTo overcome this challenge, we propose $\\textbf{T}$opic $\\textbf{E}$fficient\n$\\textbf{St}$anc$\\textbf{E}$ $\\textbf{D}$etection (TESTED), consisting of a\ntopic-guided diversity sampling technique and a contrastive objective that is\nused for fine-tuning a stance classifier. We evaluate the method on an existing\nbenchmark of $16$ datasets with in-domain, i.e. all topics seen and\nout-of-domain, i.e. unseen topics, experiments. The results show that our\nmethod outperforms the state-of-the-art with an average of $3.5$ F1 points\nincrease in-domain, and is more generalizable with an averaged increase of\n$10.2$ F1 on out-of-domain evaluation while using $\\leq10\\%$ of the training\ndata. We show that our sampling technique mitigates both inter- and per-topic\nclass imbalances. Finally, our analysis demonstrates that the contrastive\nlearning objective allows the model a more pronounced segmentation of samples\nwith varying labels.",
        "translated": ""
    },
    {
        "title": "End-to-End Document Classification and Key Information Extraction using\n  Assignment Optimization",
        "url": "http://arxiv.org/abs/2306.00750v1",
        "pub_date": "2023-06-01",
        "summary": "We propose end-to-end document classification and key information extraction\n(KIE) for automating document processing in forms. Through accurate document\nclassification we harness known information from templates to enhance KIE from\nforms. We use text and layout encoding with a cosine similarity measure to\nclassify visually-similar documents. We then demonstrate a novel application of\nmixed integer programming by using assignment optimization to extract key\ninformation from documents. Our approach is validated on an in-house dataset of\nnoisy scanned forms. The best performing document classification approach\nachieved 0.97 f1 score. A mean f1 score of 0.94 for the KIE task suggests there\nis significant potential in applying optimization techniques. Abation results\nshow that the method relies on document preprocessing techniques to mitigate\nType II errors and achieve optimal performance.",
        "translated": ""
    },
    {
        "title": "Class Anchor Margin Loss for Content-Based Image Retrieval",
        "url": "http://arxiv.org/abs/2306.00630v1",
        "pub_date": "2023-06-01",
        "summary": "The performance of neural networks in content-based image retrieval (CBIR) is\nhighly influenced by the chosen loss (objective) function. The majority of\nobjective functions for neural models can be divided into metric learning and\nstatistical learning. Metric learning approaches require a pair mining strategy\nthat often lacks efficiency, while statistical learning approaches are not\ngenerating highly compact features due to their indirect feature optimization.\nTo this end, we propose a novel repeller-attractor loss that falls in the\nmetric learning paradigm, yet directly optimizes for the L2 metric without the\nneed of generating pairs. Our loss is formed of three components. One leading\nobjective ensures that the learned features are attracted to each designated\nlearnable class anchor. The second loss component regulates the anchors and\nforces them to be separable by a margin, while the third objective ensures that\nthe anchors do not collapse to zero. Furthermore, we develop a more efficient\ntwo-stage retrieval system by harnessing the learned class anchors during the\nfirst stage of the retrieval process, eliminating the need of comparing the\nquery with every image in the database. We establish a set of four datasets\n(CIFAR-100, Food-101, SVHN, and Tiny ImageNet) and evaluate the proposed\nobjective in the context of few-shot and full-set training on the CBIR task, by\nusing both convolutional and transformer architectures. Compared to existing\nobjective functions, our empirical evidence shows that the proposed objective\nis generating superior and more consistent results.",
        "translated": ""
    },
    {
        "title": "End-to-end Knowledge Retrieval with Multi-modal Queries",
        "url": "http://arxiv.org/abs/2306.00424v1",
        "pub_date": "2023-06-01",
        "summary": "We investigate knowledge retrieval with multi-modal queries, i.e. queries\ncontaining information split across image and text inputs, a challenging task\nthat differs from previous work on cross-modal retrieval. We curate a new\ndataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a\nsystem to retrieve knowledge from a large corpus by integrating contents from\nboth text and image queries. We introduce a retriever model ``ReViz'' that can\ndirectly process input text and images to retrieve relevant knowledge in an\nend-to-end fashion without being dependent on intermediate modules such as\nobject detectors or caption generators. We introduce a new pretraining task\nthat is effective for learning knowledge retrieval with multimodal queries and\nalso improves performance on downstream tasks. We demonstrate superior\nperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot\nsettings as well as further improvements when finetuned on these datasets.",
        "translated": ""
    },
    {
        "title": "A Survey on Fairness-aware Recommender Systems",
        "url": "http://arxiv.org/abs/2306.00403v1",
        "pub_date": "2023-06-01",
        "summary": "As information filtering services, recommender systems have extremely\nenriched our daily life by providing personalized suggestions and facilitating\npeople in decision-making, which makes them vital and indispensable to human\nsociety in the information era. However, as people become more dependent on\nthem, recent studies show that recommender systems potentially own\nunintentional impacts on society and individuals because of their unfairness\n(e.g., gender discrimination in job recommendations). To develop trustworthy\nservices, it is crucial to devise fairness-aware recommender systems that can\nmitigate these bias issues. In this survey, we summarise existing methodologies\nand practices of fairness in recommender systems. Firstly, we present concepts\nof fairness in different recommendation scenarios, comprehensively categorize\ncurrent advances, and introduce typical methods to promote fairness in\ndifferent stages of recommender systems. Next, after introducing datasets and\nevaluation metrics applied to assess the fairness of recommender systems, we\nwill delve into the significant influence that fairness-aware recommender\nsystems exert on real-world industrial applications. Subsequently, we highlight\nthe connection between fairness and other principles of trustworthy recommender\nsystems, aiming to consider trustworthiness principles holistically while\nadvocating for fairness. Finally, we summarize this review, spotlighting\npromising opportunities in comprehending concepts, frameworks, the balance\nbetween accuracy and fairness, and the ties with trustworthiness, with the\nultimate goal of fostering the development of fairness-aware recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "Explicit Feature Interaction-aware Uplift Network for Online Marketing",
        "url": "http://arxiv.org/abs/2306.00315v1",
        "pub_date": "2023-06-01",
        "summary": "As a key component in online marketing, uplift modeling aims to accurately\ncapture the degree to which different treatments motivate different users, such\nas coupons or discounts, also known as the estimation of individual treatment\neffect (ITE). In an actual business scenario, the options for treatment may be\nnumerous and complex, and there may be correlations between different\ntreatments. In addition, each marketing instance may also have rich user and\ncontextual features. However, existing methods still fall short in both fully\nexploiting treatment information and mining features that are sensitive to a\nparticular treatment. In this paper, we propose an explicit feature\ninteraction-aware uplift network (EFIN) to address these two problems. Our EFIN\nincludes four customized modules: 1) a feature encoding module encodes not only\nthe user and contextual features, but also the treatment features; 2) a\nself-interaction module aims to accurately model the user's natural response\nwith all but the treatment features; 3) a treatment-aware interaction module\naccurately models the degree to which a particular treatment motivates a user\nthrough interactions between the treatment features and other features, i.e.,\nITE; and 4) an intervention constraint module is used to balance the ITE\ndistribution of users between the control and treatment groups so that the\nmodel would still achieve a accurate uplift ranking on data collected from a\nnon-random intervention marketing scenario. We conduct extensive experiments on\ntwo public datasets and one product dataset to verify the effectiveness of our\nEFIN. In addition, our EFIN has been deployed in a credit card bill payment\nscenario of a large online financial platform with a significant improvement.",
        "translated": ""
    },
    {
        "title": "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest",
        "url": "http://arxiv.org/abs/2306.00248v1",
        "pub_date": "2023-05-31",
        "summary": "Sequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
        "translated": ""
    },
    {
        "title": "Fresh Content Needs More Attention: Multi-funnel Fresh Content\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.01720v1",
        "pub_date": "2023-06-02",
        "summary": "Recommendation system serves as a conduit connecting users to an incredibly\nlarge, diverse and ever growing collection of contents. In practice, missing\ninformation on fresh (and tail) contents needs to be filled in order for them\nto be exposed and discovered by their audience. We here share our success\nstories in building a dedicated fresh content recommendation stack on a large\ncommercial platform. To nominate fresh contents, we built a multi-funnel\nnomination system that combines (i) a two-tower model with strong\ngeneralization power for coverage, and (ii) a sequence model with near\nreal-time update on user feedback for relevance. The multi-funnel setup\neffectively balances between coverage and relevance. An in-depth study uncovers\nthe relationship between user activity level and their proximity toward fresh\ncontents, which further motivates a contextual multi-funnel setup. Nominated\nfresh candidates are then scored and ranked by systems considering prediction\nuncertainty to further bootstrap content with less exposure. We evaluate the\nbenefits of the dedicated fresh content recommendation stack, and the\nmulti-funnel nomination system in particular, through user corpus co-diverted\nlive experiments. We conduct multiple rounds of live experiments on a\ncommercial platform serving billion of users demonstrating efficacy of our\nproposed methods.",
        "translated": ""
    },
    {
        "title": "Pretrained Language Model based Web Search Ranking: From Relevance to\n  Satisfaction",
        "url": "http://arxiv.org/abs/2306.01599v1",
        "pub_date": "2023-06-02",
        "summary": "Search engine plays a crucial role in satisfying users' diverse information\nneeds. Recently, Pretrained Language Models (PLMs) based text ranking models\nhave achieved huge success in web search. However, many state-of-the-art text\nranking approaches only focus on core relevance while ignoring other dimensions\nthat contribute to user satisfaction, e.g., document quality, recency,\nauthority, etc. In this work, we focus on ranking user satisfaction rather than\nrelevance in web search, and propose a PLM-based framework, namely SAT-Ranker,\nwhich comprehensively models different dimensions of user satisfaction in a\nunified manner. In particular, we leverage the capacities of PLMs on both\ntextual and numerical inputs, and apply a multi-field input that modularizes\neach dimension of user satisfaction as an input field. Overall, SAT-Ranker is\nan effective, extensible, and data-centric framework that has huge potential\nfor industrial applications. On rigorous offline and online experiments,\nSAT-Ranker obtains remarkable gains on various evaluation sets targeting\ndifferent dimensions of user satisfaction. It is now fully deployed online to\nimprove the usability of our search engine.",
        "translated": ""
    },
    {
        "title": "Influence Maximization with Fairness at Scale (Extended Version)",
        "url": "http://arxiv.org/abs/2306.01587v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we revisit the problem of influence maximization with\nfairness, which aims to select k influential nodes to maximise the spread of\ninformation in a network, while ensuring that selected sensitive user\nattributes are fairly affected, i.e., are proportionally similar between the\noriginal network and the affected users. Recent studies on this problem focused\nonly on extremely small networks, hence the challenge remains on how to achieve\na scalable solution, applicable to networks with millions or billions of nodes.\nWe propose an approach that is based on learning node representations for fair\nspread from diffusion cascades, instead of the social connectivity s.t. we can\ndeal with very large graphs. We propose two data-driven approaches: (a)\nfairness-based participant sampling (FPS), and (b) fairness as context (FAC).\nSpread related user features, such as the probability of diffusing information\nto others, are derived from the historical information cascades, using a deep\nneural network. The extracted features are then used in selecting influencers\nthat maximize the influence spread, while being also fair with respect to the\nchosen sensitive attributes. In FPS, fairness and cascade length information\nare considered independently in the decision-making process, while FAC\nconsiders these information facets jointly and considers correlations between\nthem. The proposed algorithms are generic and represent the first policy-driven\nsolutions that can be applied to arbitrary sets of sensitive attributes at\nscale. We evaluate the performance of our solutions on a real-world public\ndataset (Sina Weibo) and on a hybrid real-synthethic dataset (Digg), which\nexhibit all the facets that we exploit, namely diffusion network, diffusion\ntraces, and user profiles. These experiments show that our methods outperform\nthe state-the-art solutions in terms of spread, fairness, and scalability.",
        "translated": ""
    },
    {
        "title": "Système de recommandations basé sur les contraintes pour les\n  simulations de gestion de crise",
        "url": "http://arxiv.org/abs/2306.01504v1",
        "pub_date": "2023-06-02",
        "summary": "In the context of the evacuation of populations, some citizens/volunteers may\nwant and be able to participate in the evacuation of populations in difficulty\nby coming to lend a hand to emergency/evacuation vehicles with their own\nvehicles. One way of framing these impulses of solidarity would be to be able\nto list in real-time the citizens/volunteers available with their vehicles\n(land, sea, air, etc.), to be able to geolocate them according to the risk\nareas to be evacuated, and adding them to the evacuation/rescue vehicles.\nBecause it is difficult to propose an effective real-time operational system on\nthe field in a real crisis situation, in this work, we propose to add a module\nfor recommending driver/vehicle pairs (with their specificities) to a system of\ncrisis management simulation. To do that, we chose to model and develop an\nontology-supported constraint-based recommender system for crisis management\nsimulations.",
        "translated": ""
    },
    {
        "title": "Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking\n  Intent in Recommender Systems",
        "url": "http://arxiv.org/abs/2306.01476v1",
        "pub_date": "2023-06-02",
        "summary": "Recommending novel content, which expands user horizons by introducing them\nto new interests, has been shown to improve users' long-term experience on\nrecommendation platforms \\cite{chen2021values}. Users however are not\nconstantly looking to explore novel content. It is therefore crucial to\nunderstand their novelty-seeking intent and adjust the recommendation policy\naccordingly. Most existing literature models a user's propensity to choose\nnovel content or to prefer a more diverse set of recommendations at individual\ninteractions. Hierarchical structure, on the other hand, exists in a user's\nnovelty-seeking intent, which is manifested as a static and intrinsic user\npreference for seeking novelty along with a dynamic session-based propensity.\nTo this end, we propose a novel hierarchical reinforcement learning-based\nmethod to model the hierarchical user novelty-seeking intent, and to adapt the\nrecommendation policy accordingly based on the extracted user novelty-seeking\npropensity. We further incorporate diversity and novelty-related measurement in\nthe reward function of the hierarchical RL (HRL) agent to encourage user\nexploration \\cite{chen2021values}. We demonstrate the benefits of explicitly\nmodeling hierarchical user novelty-seeking intent in recommendations through\nextensive experiments on simulated and real-world datasets. In particular, we\ndemonstrate that the effectiveness of our proposed hierarchical RL-based method\nlies in its ability to capture such hierarchically-structured intent. As a\nresult, the proposed HRL model achieves superior performance on several public\ndatasets, compared with state-of-art baselines.",
        "translated": ""
    },
    {
        "title": "Prompt Tuning Large Language Models on Personalized Aspect Extraction\n  for Recommendations",
        "url": "http://arxiv.org/abs/2306.01475v1",
        "pub_date": "2023-06-02",
        "summary": "Existing aspect extraction methods mostly rely on explicit or ground truth\naspect information, or using data mining or machine learning approaches to\nextract aspects from implicit user feedback such as user reviews. It however\nremains under-explored how the extracted aspects can help generate more\nmeaningful recommendations to the users. Meanwhile, existing research on\naspect-based recommendations often relies on separate aspect extraction models\nor assumes the aspects are given, without accounting for the fact the optimal\nset of aspects could be dependent on the recommendation task at hand.\n  In this work, we propose to combine aspect extraction together with\naspect-based recommendations in an end-to-end manner, achieving the two goals\ntogether in a single framework. For the aspect extraction component, we\nleverage the recent advances in large language models and design a new prompt\nlearning mechanism to generate aspects for the end recommendation task. For the\naspect-based recommendation component, the extracted aspects are concatenated\nwith the usual user and item features used by the recommendation model. The\nrecommendation task mediates the learning of the user embeddings and item\nembeddings, which are used as soft prompts to generate aspects. Therefore, the\nextracted aspects are personalized and contextualized by the recommendation\ntask. We showcase the effectiveness of our proposed method through extensive\nexperiments on three industrial datasets, where our proposed framework\nsignificantly outperforms state-of-the-art baselines in both the personalized\naspect extraction and aspect-based recommendation tasks. In particular, we\ndemonstrate that it is necessary and beneficial to combine the learning of\naspect extraction and aspect-based recommendation together. We also conduct\nextensive ablation studies to understand the contribution of each design\ncomponent in our framework.",
        "translated": ""
    },
    {
        "title": "An OPC UA-based industrial Big Data architecture",
        "url": "http://arxiv.org/abs/2306.01418v1",
        "pub_date": "2023-06-02",
        "summary": "Industry 4.0 factories are complex and data-driven. Data is yielded from many\nsources, including sensors, PLCs, and other devices, but also from IT, like ERP\nor CRM systems. We ask how to collect and process this data in a way, such that\nit includes metadata and can be used for industrial analytics or to derive\nintelligent support systems. This paper describes a new, query model based\napproach, which uses a big data architecture to capture data from various\nsources using OPC UA as a foundation. It buffers and preprocesses the\ninformation for the purpose of harmonizing and providing a holistic state space\nof a factory, as well as mappings to the current state of a production site.\nThat information can be made available to multiple processing sinks, decoupled\nfrom the data sources, which enables them to work with the information without\ninterfering with devices of the production, disturbing the network devices they\nare working in, or influencing the production process negatively. Metadata and\nconnected semantic information is kept throughout the process, allowing to feed\nalgorithms with meaningful data, so that it can be accessed in its entirety to\nperform time series analysis, machine learning or similar evaluations as well\nas replaying the data from the buffer for repeatable simulations.",
        "translated": ""
    },
    {
        "title": "DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG\n  2000 Compressed Documents",
        "url": "http://arxiv.org/abs/2306.01359v1",
        "pub_date": "2023-06-02",
        "summary": "For any digital application with document images such as retrieval, the\nclassification of document images becomes an essential stage. Conventionally\nfor the purpose, the full versions of the documents, that is the uncompressed\ndocument images make the input dataset, which poses a threat due to the big\nvolume required to accommodate the full versions of the documents. Therefore,\nit would be novel, if the same classification task could be accomplished\ndirectly (with some partial decompression) with the compressed representation\nof documents in order to make the whole process computationally more efficient.\nIn this research work, a novel deep learning model, DWT CompCNN is proposed for\nclassification of documents that are compressed using High Throughput JPEG 2000\n(HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional\nlayers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each\nincreasing layer to improve learning from the wavelet coefficients extracted\nfrom the compressed images. Experiments are performed on two benchmark\ndatasets- Tobacco-3482 and RVL-CDIP, which demonstrate that the proposed model\nis time and space efficient, and also achieves a better classification accuracy\nin compressed domain.",
        "translated": ""
    },
    {
        "title": "Reducing Popularity Bias in Recommender Systems through AUC-Optimal\n  Negative Sampling",
        "url": "http://arxiv.org/abs/2306.01348v1",
        "pub_date": "2023-06-02",
        "summary": "Popularity bias is a persistent issue associated with recommendation systems,\nposing challenges to both fairness and efficiency. Existing literature widely\nacknowledges that reducing popularity bias often requires sacrificing\nrecommendation accuracy. In this paper, we challenge this commonly held belief.\nOur analysis under general bias-variance decomposition framework shows that\nreducing bias can actually lead to improved model performance under certain\nconditions. To achieve this win-win situation, we propose to intervene in model\ntraining through negative sampling thereby modifying model predictions.\nSpecifically, we provide an optimal negative sampling rule that maximizes\npartial AUC to preserve the accuracy of any given model, while correcting\nsample information and prior information to reduce popularity bias in a\nflexible and principled way. Our experimental results on real-world datasets\ndemonstrate the superiority of our approach in improving recommendation\nperformance and reducing popularity bias.",
        "translated": ""
    },
    {
        "title": "LyricSIM: A novel Dataset and Benchmark for Similarity Detection in\n  Spanish Song LyricS",
        "url": "http://arxiv.org/abs/2306.01325v1",
        "pub_date": "2023-06-02",
        "summary": "In this paper, we present a new dataset and benchmark tailored to the task of\nsemantic similarity in song lyrics. Our dataset, originally consisting of 2775\npairs of Spanish songs, was annotated in a collective annotation experiment by\n63 native annotators. After collecting and refining the data to ensure a high\ndegree of consensus and data integrity, we obtained 676 high-quality annotated\npairs that were used to evaluate the performance of various state-of-the-art\nmonolingual and multilingual language models. Consequently, we established\nbaseline results that we hope will be useful to the community in all future\nacademic and industrial applications conducted in this context.",
        "translated": ""
    },
    {
        "title": "Learning Similarity among Users for Personalized Session-Based\n  Recommendation from hierarchical structure of User-Session-Item",
        "url": "http://arxiv.org/abs/2306.03040v1",
        "pub_date": "2023-06-05",
        "summary": "The task of the session-based recommendation is to predict the next\ninteraction of the user based on the anonymized user's behavior pattern. And\npersonalized version of this system is a promising research field due to its\navailability to deal with user information. However, there's a problem that the\nuser's preferences and historical sessions were not considered in the typical\nsession-based recommendation since it concentrates only on user-item\ninteraction. In addition, the existing personalized session-based\nrecommendation model has a limited capability in that it only considers the\npreference of the current user without considering those of similar users. It\nmeans there can be the loss of information included within the hierarchical\ndata structure of the user-session-item. To tackle with this problem, we\npropose USP-SBR(abbr. of User Similarity Powered - Session Based Recommender).\nTo model global historical sessions of users, we propose UserGraph that has two\ntypes of nodes - ItemNode and UserNode. We then connect the nodes with three\ntypes of edges. The first type of edges connects ItemNode as chronological\norder, and the second connects ItemNode to UserNode, and the last connects\nUserNode to ItemNode. With these user embeddings, we propose additional\ncontrastive loss, that makes users with similar intention be close to each\nother in the vector space. we apply graph neural network on these UserGraph and\nupdate nodes. Experimental results on two real-world datasets demonstrate that\nour method outperforms some state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Gen-IR @ SIGIR 2023: The First Workshop on Generative Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2306.02887v1",
        "pub_date": "2023-06-05",
        "summary": "Generative information retrieval (IR) has experienced substantial growth\nacross multiple research communities (e.g., information retrieval, computer\nvision, natural language processing, and machine learning), and has been highly\nvisible in the popular press. Theoretical, empirical, and actual user-facing\nproducts have been released that retrieve documents (via generation) or\ndirectly generate answers given an input request. We would like to investigate\nwhether end-to-end generative models are just another trend or, as some claim,\na paradigm change for IR. This necessitates new metrics, theoretical grounding,\nevaluation methods, task definitions, models, user interfaces, etc. The goal of\nthis workshop (https://coda.io/@sigir/gen-ir) is to focus on previously\nexplored Generative IR techniques like document retrieval and direct Grounded\nAnswer Generation, while also offering a venue for the discussion and\nexploration of how Generative IR can be applied to new domains like\nrecommendation systems, summarization, etc. The format of the workshop is\ninteractive, including roundtable and keynote sessions and tends to avoid the\none-sided dialogue of a mini-conference.",
        "translated": ""
    },
    {
        "title": "Benchmarking Middle-Trained Language Models for Neural Search",
        "url": "http://arxiv.org/abs/2306.02867v1",
        "pub_date": "2023-06-05",
        "summary": "Middle training methods aim to bridge the gap between the Masked Language\nModel (MLM) pre-training and the final finetuning for retrieval. Recent models\nsuch as CoCondenser, RetroMAE, and LexMAE argue that the MLM task is not\nsufficient enough to pre-train a transformer network for retrieval and hence\npropose various tasks to do so. Intrigued by those novel methods, we noticed\nthat all these models used different finetuning protocols, making it hard to\nassess the benefits of middle training. We propose in this paper a benchmark of\nCoCondenser, RetroMAE, and LexMAE, under the same finetuning conditions. We\ncompare both dense and sparse approaches under various finetuning protocols and\nmiddle training on different collections (MS MARCO, Wikipedia or Tripclick). We\nuse additional middle training baselines, such as a standard MLM finetuning on\nthe retrieval collection, optionally augmented by a CLS predicting the passage\nterm frequency. For the sparse approach, our study reveals that there is almost\nno statistical difference between those methods: the more effective the\nfinetuning procedure is, the less difference there is between those models. For\nthe dense approach, RetroMAE using MS MARCO as middle-training collection shows\nexcellent results in almost all the settings. Finally, we show that middle\ntraining on the retrieval collection, thus adapting the language model to it,\nis a critical factor. Overall, a better experimental setup should be adopted to\nevaluate middle training methods. Code available at\nhttps://github.com/naver/splade/tree/benchmarch-SIGIR23",
        "translated": ""
    },
    {
        "title": "CTRL: Connect Tabular and Language Model for CTR Prediction",
        "url": "http://arxiv.org/abs/2306.02841v1",
        "pub_date": "2023-06-05",
        "summary": "Traditional click-through rate (CTR) prediction models convert the tabular\ndata into one-hot vectors and leverage the collaborative relations among\nfeatures for inferring user's preference over items. This modeling paradigm\ndiscards the essential semantic information. Though some recent works like P5\nand M6-Rec have explored the potential of using Pre-trained Language Models\n(PLMs) to extract semantic signals for CTR prediction, they are computationally\nexpensive and suffer from low efficiency. Besides, the beneficial collaborative\nrelations are not considered, hindering the recommendation performance. To\nsolve these problems, in this paper, we propose a novel framework\n\\textbf{CTRL}, which is industrial friendly and model-agnostic with high\ntraining and inference efficiency. Specifically, the original tabular data is\nfirst converted into textual data. Both tabular data and converted textual data\nare regarded as two different modalities and are separately fed into the\ncollaborative CTR model and pre-trained language model. A cross-modal knowledge\nalignment procedure is performed to fine-grained align and integrate the\ncollaborative and semantic signals, and the lightweight collaborative model can\nbe deployed online for efficient serving after fine-tuned with supervised\nsignals. Experimental results on three public datasets show that CTRL\noutperforms the SOTA CTR models significantly. Moreover, we further verify its\neffectiveness on a large-scale industrial recommender system.",
        "translated": ""
    },
    {
        "title": "Path-Specific Counterfactual Fairness for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02615v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender systems (RSs) have become an indispensable part of online\nplatforms. With the growing concerns of algorithmic fairness, RSs are not only\nexpected to deliver high-quality personalized content, but are also demanded\nnot to discriminate against users based on their demographic information.\nHowever, existing RSs could capture undesirable correlations between sensitive\nfeatures and observed user behaviors, leading to biased recommendations. Most\nfair RSs tackle this problem by completely blocking the influences of sensitive\nfeatures on recommendations. But since sensitive features may also affect user\ninterests in a fair manner (e.g., race on culture-based preferences),\nindiscriminately eliminating all the influences of sensitive features\ninevitably degenerate the recommendations quality and necessary diversities. To\naddress this challenge, we propose a path-specific fair RS (PSF-RS) for\nrecommendations. Specifically, we summarize all fair and unfair correlations\nbetween sensitive features and observed ratings into two latent proxy\nmediators, where the concept of path-specific bias (PS-Bias) is defined based\non path-specific counterfactual inference. Inspired by Pearl's minimal change\nprinciple, we address the PS-Bias by minimally transforming the biased factual\nworld into a hypothetically fair world, where a fair RS model can be learned\naccordingly by solving a constrained optimization problem. For the technical\npart, we propose a feasible implementation of PSF-RS, i.e., PSF-VAE, with\nweakly-supervised variational inference, which robustly infers the latent\nmediators such that unfairness can be mitigated while necessary recommendation\ndiversities can be maximally preserved simultaneously. Experiments conducted on\nsemi-simulated and real-world datasets demonstrate the effectiveness of PSF-RS.",
        "translated": ""
    },
    {
        "title": "Learning to Relate to Previous Turns in Conversational Search",
        "url": "http://arxiv.org/abs/2306.02553v1",
        "pub_date": "2023-06-05",
        "summary": "Conversational search allows a user to interact with a search system in\nmultiple turns. A query is strongly dependent on the conversation context. An\neffective way to improve retrieval effectiveness is to expand the current query\nwith historical queries. However, not all the previous queries are related to,\nand useful for expanding the current query. In this paper, we propose a new\nmethod to select relevant historical queries that are useful for the current\nquery. To cope with the lack of labeled training data, we use a pseudo-labeling\napproach to annotate useful historical queries based on their impact on the\nretrieval results. The pseudo-labeled data are used to train a selection model.\nWe further propose a multi-task learning framework to jointly train the\nselector and the retriever during fine-tuning, allowing us to mitigate the\npossible inconsistency between the pseudo labels and the changed retriever.\nExtensive experiments on four conversational search datasets demonstrate the\neffectiveness and broad applicability of our method compared with several\nstrong baselines.",
        "translated": ""
    },
    {
        "title": "RecAgent: A Novel Simulation Paradigm for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.02552v1",
        "pub_date": "2023-06-05",
        "summary": "Recommender system has deeply revolutionized people's daily life and\nproduction, bringing a large amount of business value. In the recommendation\ndomain, simulation and real data-based studies are two typical research\nparadigms, with each having different advantages. Previously, real data-based\nstudies occupy more important positions, since accurately simulating the user\npreference is quite difficult. Recently, large language models (LLM) have shown\ngreat potential to achieve human-like intelligence, which provides new\nopportunities to overcome the shortcomings of simulation-based studies and thus\nhighlight their advantages, such as much more application scenarios and cheaper\ndata acquisition strategies. To shed lights on this direction, in this paper,\nwe introduce an LLM-based recommender simulator called RecAgent. Our simulator\nis composed of two modules: (1) the user module and (2) the recommender module.\nThe user module can browse the recommendation website, communicate with other\nusers and broadcast messages on the social media. The recommender module is\ndesigned to provide search or recommendation lists to the users, and one can\ndesign different models to implement the recommender. All the users take\nactions based on LLMs, and can freely evolve like in the real world. We present\nseveral case studies to demonstrate that the users in our simulator can indeed\nbehave in a reasonable manner as expected. Our project has been released at\nhttps://github.com/RUC-GSAI/YuLan-Rec.",
        "translated": ""
    },
    {
        "title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions",
        "url": "http://arxiv.org/abs/2306.02549v1",
        "pub_date": "2023-06-05",
        "summary": "This paper investigates the use of artificial intelligence chatbots for\npatient-specific question answering (QA) from clinical notes using several\nlarge language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google\nBard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and\ncoherence of the answers generated by each model using a 5-point Likert scale\non a set of patient-specific questions.",
        "translated": ""
    },
    {
        "title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models\n  with Same Tower Negatives",
        "url": "http://arxiv.org/abs/2306.02516v1",
        "pub_date": "2023-06-05",
        "summary": "Dual encoders have been used for retrieval tasks and representation learning\nwith good results. A standard way to train dual encoders is using a contrastive\nloss with in-batch negatives. In this work, we propose an improved contrastive\nlearning objective by adding queries or documents from the same encoder towers\nto the negatives, for which we name it as \"contrastive loss with SAMe TOwer\nNEgatives\" (SamToNe). By evaluating on question answering retrieval benchmarks\nfrom MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval\nbenchmarks (BEIR), we demonstrate that SamToNe can effectively improve the\nretrieval quality for both symmetric and asymmetric dual encoders. By directly\nprobing the embedding spaces of the two encoding towers via the t-SNE algorithm\n(van der Maaten and Hinton, 2008), we observe that SamToNe ensures the\nalignment between the embedding spaces from the two encoder towers. Based on\nthe analysis of the embedding distance distributions of the top-$1$ retrieved\nresults, we further explain the efficacy of the method from the perspective of\nregularisation.",
        "translated": ""
    },
    {
        "title": "I^3 Retriever: Incorporating Implicit Interaction in Pre-trained\n  Language Models for Passage Retrieval",
        "url": "http://arxiv.org/abs/2306.02371v1",
        "pub_date": "2023-06-04",
        "summary": "Passage retrieval is a fundamental task in many information systems, such as\nweb search and question answering, where both efficiency and effectiveness are\ncritical concerns. In recent years, neural retrievers based on pre-trained\nlanguage models (PLM), such as dual-encoders, have achieved huge success. Yet,\nstudies have found that the performance of dual-encoders are often limited due\nto the neglecting of the interaction information between queries and candidate\npassages. Therefore, various interaction paradigms have been proposed to\nimprove the performance of vanilla dual-encoders. Particularly, recent\nstate-of-the-art methods often introduce late-interaction during the model\ninference process. However, such late-interaction based methods usually bring\nextensive computation and storage cost on large corpus. Despite their\neffectiveness, the concern of efficiency and space footprint is still an\nimportant factor that limits the application of interaction-based neural\nretrieval models. To tackle this issue, we incorporate implicit interaction\ninto dual-encoders, and propose I^3 retriever. In particular, our implicit\ninteraction paradigm leverages generated pseudo-queries to simulate\nquery-passage interaction, which jointly optimizes with query and passage\nencoders in an end-to-end manner. It can be fully pre-computed and cached, and\nits inference process only involves simple dot product operation of the query\nvector and passage vector, which makes it as efficient as the vanilla dual\nencoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep\nLearning Datasets, demonstrating the I^3 retriever's superiority in terms of\nboth effectiveness and efficiency. Moreover, the proposed implicit interaction\nis compatible with special pre-training and knowledge distillation for passage\nretrieval, which brings a new state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "On Manipulating Signals of User-Item Graph: A Jacobi Polynomial-based\n  Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2306.03624v1",
        "pub_date": "2023-06-06",
        "summary": "Collaborative filtering (CF) is an important research direction in\nrecommender systems that aims to make recommendations given the information on\nuser-item interactions. Graph CF has attracted more and more attention in\nrecent years due to its effectiveness in leveraging high-order information in\nthe user-item bipartite graph for better recommendations. Specifically, recent\nstudies show the success of graph neural networks (GNN) for CF is attributed to\nits low-pass filtering effects. However, current researches lack a study of how\ndifferent signal components contributes to recommendations, and how to design\nstrategies to properly use them well. To this end, from the view of spectral\ntransformation, we analyze the important factors that a graph filter should\nconsider to achieve better performance. Based on the discoveries, we design\nJGCF, an efficient and effective method for CF based on Jacobi polynomial bases\nand frequency decomposition strategies. Extensive experiments on four widely\nused public datasets show the effectiveness and efficiency of the proposed\nmethods, which brings at most 27.06% performance gain on Alibaba-iFashion.\nBesides, the experimental results also show that JGCF is better at handling\nsparse datasets, which shows potential in making recommendations for cold-start\nusers.",
        "translated": ""
    },
    {
        "title": "Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR\n  Prediction in Taobao",
        "url": "http://arxiv.org/abs/2306.03527v1",
        "pub_date": "2023-06-06",
        "summary": "Click-Through Rate (CTR) prediction serves as a fundamental component in\nonline advertising. A common practice is to train a CTR model on advertisement\n(ad) impressions with user feedback. Since ad impressions are purposely\nselected by the model itself, their distribution differs from the inference\ndistribution and thus exhibits sample selection bias (SSB) that affects model\nperformance. Existing studies on SSB mainly employ sample re-weighting\ntechniques which suffer from high variance and poor model calibration. Another\nline of work relies on costly uniform data that is inadequate to train\nindustrial models. Thus mitigating SSB in industrial models with a\nuniform-data-free framework is worth exploring. Fortunately, many platforms\ndisplay mixed results of organic items (i.e., recommendations) and sponsored\nitems (i.e., ads) to users, where impressions of ads and recommendations are\nselected by different systems but share the same user decision rationales.\nBased on the above characteristics, we propose to leverage recommendations\nsamples as a free lunch to mitigate SSB for ads CTR model (Rec4Ad). After\nelaborating data augmentation, Rec4Ad learns disentangled representations with\nalignment and decorrelation modules for enhancement. When deployed in Taobao\ndisplay advertising system, Rec4Ad achieves substantial gains in key business\nmetrics, with a lift of up to +6.6\\% CTR and +2.9\\% RPM.",
        "translated": ""
    },
    {
        "title": "COPR: Consistency-Oriented Pre-Ranking for Online Advertising",
        "url": "http://arxiv.org/abs/2306.03516v1",
        "pub_date": "2023-06-06",
        "summary": "Cascading architecture has been widely adopted in large-scale advertising\nsystems to balance efficiency and effectiveness. In this architecture, the\npre-ranking model is expected to be a lightweight approximation of the ranking\nmodel, which handles more candidates with strict latency requirements. Due to\nthe gap in model capacity, the pre-ranking and ranking models usually generate\ninconsistent ranked results, thus hurting the overall system effectiveness. The\nparadigm of score alignment is proposed to regularize their raw scores to be\nconsistent. However, it suffers from inevitable alignment errors and error\namplification by bids when applied in online advertising. To this end, we\nintroduce a consistency-oriented pre-ranking framework for online advertising,\nwhich employs a chunk-based sampling module and a plug-and-play rank alignment\nmodule to explicitly optimize consistency of ECPM-ranked results. A $\\Delta\nNDCG$-based weighting mechanism is adopted to better distinguish the importance\nof inter-chunk samples in optimization. Both online and offline experiments\nhave validated the superiority of our framework. When deployed in Taobao\ndisplay advertising system, it achieves an improvement of up to +12.3\\% CTR and\n+5.6\\% RPM.",
        "translated": ""
    },
    {
        "title": "Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search",
        "url": "http://arxiv.org/abs/2306.03411v1",
        "pub_date": "2023-06-06",
        "summary": "Customers interacting with product search engines are increasingly\nformulating information-seeking queries. Frequently Asked Question (FAQ)\nretrieval aims to retrieve common question-answer pairs for a user query with\nquestion intent. Integrating FAQ retrieval in product search can not only\nempower users to make more informed purchase decisions, but also enhance user\nretention through efficient post-purchase support. Determining when an FAQ\nentry can satisfy a user's information need within product search, without\ndisrupting their shopping experience, represents an important challenge. We\npropose an intent-aware FAQ retrieval system consisting of (1) an intent\nclassifier that predicts when a user's information need can be answered by an\nFAQ; (2) a reformulation model that rewrites a query into a natural question.\nOffline evaluation demonstrates that our approach improves Hit@1 by 13% on\nretrieving ground-truth FAQs, while reducing latency by 95% compared to\nbaseline systems. These improvements are further validated by real user\nfeedback, where 71% of displayed FAQs on top of product search results received\nexplicit positive user feedback. Overall, our findings show promising\ndirections for integrating FAQ retrieval into product search at scale.",
        "translated": ""
    },
    {
        "title": "Computational Technologies for Fashion Recommendation: A Survey",
        "url": "http://arxiv.org/abs/2306.03395v1",
        "pub_date": "2023-06-06",
        "summary": "Fashion recommendation is a key research field in computational fashion\nresearch and has attracted considerable interest in the computer vision,\nmultimedia, and information retrieval communities in recent years. Due to the\ngreat demand for applications, various fashion recommendation tasks, such as\npersonalized fashion product recommendation, complementary (mix-and-match)\nrecommendation, and outfit recommendation, have been posed and explored in the\nliterature. The continuing research attention and advances impel us to look\nback and in-depth into the field for a better understanding. In this paper, we\ncomprehensively review recent research efforts on fashion recommendation from a\ntechnological perspective. We first introduce fashion recommendation at a macro\nlevel and analyse its characteristics and differences with general\nrecommendation tasks. We then clearly categorize different fashion\nrecommendation efforts into several sub-tasks and focus on each sub-task in\nterms of its problem formulation, research focus, state-of-the-art methods, and\nlimitations. We also summarize the datasets proposed in the literature for use\nin fashion recommendation studies to give readers a brief illustration.\nFinally, we discuss several promising directions for future research in this\nfield. Overall, this survey systematically reviews the development of fashion\nrecommendation research. It also discusses the current limitations and gaps\nbetween academic research and the real needs of the fashion industry. In the\nprocess, we offer a deep insight into how the fashion industry could benefit\nfrom fashion recommendation technologies. the computational technologies of\nfashion recommendation.",
        "translated": ""
    },
    {
        "title": "Tree based Progressive Regression Model for Watch-Time Prediction in\n  Short-video Recommendation",
        "url": "http://arxiv.org/abs/2306.03392v1",
        "pub_date": "2023-06-06",
        "summary": "An accurate prediction of watch time has been of vital importance to enhance\nuser engagement in video recommender systems. To achieve this, there are four\nproperties that a watch time prediction framework should satisfy: first,\ndespite its continuous value, watch time is also an ordinal variable and the\nrelative ordering between its values reflects the differences in user\npreferences. Therefore the ordinal relations should be reflected in watch time\npredictions. Second, the conditional dependence between the video-watching\nbehaviors should be captured in the model. For instance, one has to watch half\nof the video before he/she finishes watching the whole video. Third, modeling\nwatch time with a point estimation ignores the fact that models might give\nresults with high uncertainty and this could cause bad cases in recommender\nsystems. Therefore the framework should be aware of prediction uncertainty.\nForth, the real-life recommender systems suffer from severe bias amplifications\nthus an estimation without bias amplification is expected. Therefore we propose\nTPM for watch time prediction. Specifically, the ordinal ranks of watch time\nare introduced into TPM and the problem is decomposed into a series of\nconditional dependent classification tasks which are organized into a tree\nstructure. The expectation of watch time can be generated by traversing the\ntree and the variance of watch time predictions is explicitly introduced into\nthe objective function as a measurement for uncertainty. Moreover, we\nillustrate that backdoor adjustment can be seamlessly incorporated into TPM,\nwhich alleviates bias amplifications. Extensive offline evaluations have been\nconducted in public datasets and TPM have been deployed in a real-world video\napp Kuaishou with over 300 million DAUs. The results indicate that TPM\noutperforms state-of-the-art approaches and indeed improves video consumption\nsignificantly.",
        "translated": ""
    },
    {
        "title": "Towards Alleviating the Object Bias in Prompt Tuning-based Factual\n  Knowledge Extraction",
        "url": "http://arxiv.org/abs/2306.03378v1",
        "pub_date": "2023-06-06",
        "summary": "Many works employed prompt tuning methods to automatically optimize prompt\nqueries and extract the factual knowledge stored in Pretrained Language Models.\nIn this paper, we observe that the optimized prompts, including discrete\nprompts and continuous prompts, exhibit undesirable object bias. To handle this\nproblem, we propose a novel prompt tuning method called MeCoD. consisting of\nthree modules: Prompt Encoder, Object Equalization and Biased Object\nObstruction. Experimental results show that MeCoD can significantly reduce the\nobject bias and at the same time improve accuracy of factual knowledge\nextraction.",
        "translated": ""
    },
    {
        "title": "Construction d'un système de recommandation basé sur des contraintes\n  via des graphes de connaissances",
        "url": "http://arxiv.org/abs/2306.03247v1",
        "pub_date": "2023-06-05",
        "summary": "Knowledge graphs in RDF model entities and their relations using ontologies,\nand have gained popularity for information modeling. In recommender systems,\nknowledge graphs help represent more links and relationships between users and\nitems. Constraint-based recommender systems leverage deep recommendation\nknowledge to identify relevant suggestions. When combined with knowledge\ngraphs, they offer benefits in constraint sets. This paper explores a\nconstraint-based recommender system using RDF knowledge graphs for the vehicle\npurchase/sale domain. Our experiments demonstrate that the proposed approach\nefficiently identifies recommendations based on user preferences.",
        "translated": ""
    },
    {
        "title": "Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time\n  Light Patterns",
        "url": "http://arxiv.org/abs/2306.03195v1",
        "pub_date": "2023-06-05",
        "summary": "We introduce NightPulse, an interactive tool for Night-time light (NTL) data\nvisualization and analytics, which enables researchers and stakeholders to\nexplore and analyze NTL data with a user-friendly platform. Powered by\nefficient system architecture, NightPulse supports image segmentation,\nclustering, and change pattern detection to identify urban development and\nsprawl patterns. It captures temporal trends of NTL and semantics of cities,\nanswering questions about demographic factors, city boundaries, and unusual\ndifferences.",
        "translated": ""
    },
    {
        "title": "Personalized Federated Domain Adaptation for Item-to-Item Recommendation",
        "url": "http://arxiv.org/abs/2306.03191v1",
        "pub_date": "2023-06-05",
        "summary": "Item-to-Item (I2I) recommendation is an important function in most\nrecommendation systems, which generates replacement or complement suggestions\nfor a particular item based on its semantic similarities to other cataloged\nitems. Given that subsets of items in a recommendation system might be\nco-interacted with by the same set of customers, graph-based models, such as\ngraph neural networks (GNNs), provide a natural framework to combine, ingest\nand extract valuable insights from such high-order relational interactions\nbetween cataloged items, as well as their metadata features, as has been shown\nin many recent studies. However, learning GNNs effectively for I2I requires\ningesting a large amount of relational data, which might not always be\navailable, especially in new, emerging market segments. To mitigate this data\nbottleneck, we postulate that recommendation patterns learned from existing\nmature market segments (with private data) could be adapted to build effective\nwarm-start models for emerging ones. To achieve this, we propose and\ninvestigate a personalized federated modeling framework based on GNNs to\nsummarize, assemble and adapt recommendation patterns across market segments\nwith heterogeneous customer behaviors into effective local models. Our key\ncontribution is a personalized graph adaptation model that bridges the gap\nbetween recent literature on federated GNNs and (non-graph) personalized\nfederated learning, which either does not optimize for the adaptability of the\nfederated model or is restricted to local models with homogeneous\nparameterization, excluding GNNs with heterogeneous local graphs.",
        "translated": ""
    },
    {
        "title": "MarineVRS: Marine Video Retrieval System with Explainability via\n  Semantic Understanding",
        "url": "http://arxiv.org/abs/2306.04593v1",
        "pub_date": "2023-06-07",
        "summary": "Building a video retrieval system that is robust and reliable, especially for\nthe marine environment, is a challenging task due to several factors such as\ndealing with massive amounts of dense and repetitive data, occlusion,\nblurriness, low lighting conditions, and abstract queries. To address these\nchallenges, we present MarineVRS, a novel and flexible video retrieval system\ndesigned explicitly for the marine domain. MarineVRS integrates\nstate-of-the-art methods for visual and linguistic object representation to\nenable efficient and accurate search and analysis of vast volumes of underwater\nvideo data. In addition, unlike the conventional video retrieval system, which\nonly permits users to index a collection of images or videos and search using a\nfree-form natural language sentence, our retrieval system includes an\nadditional Explainability module that outputs the segmentation masks of the\nobjects that the input query referred to. This feature allows users to identify\nand isolate specific objects in the video footage, leading to more detailed\nanalysis and understanding of their behavior and movements. Finally, with its\nadaptability, explainability, accuracy, and scalability, MarineVRS is a\npowerful tool for marine researchers and scientists to efficiently and\naccurately process vast amounts of data and gain deeper insights into the\nbehavior and movements of marine species.",
        "translated": ""
    },
    {
        "title": "Constraint-based recommender system for crisis management simulations",
        "url": "http://arxiv.org/abs/2306.04553v1",
        "pub_date": "2023-06-07",
        "summary": "In the context of the evacuation of populations, some citizens/volunteers may\nwant and be able to participate in the evacuation of populations in difficulty\nby coming to lend a hand to emergency/evacuation vehicles with their own\nvehicles. One way of framing these impulses of solidarity would be to be able\nto list in real-time the citizens/volunteers available with their vehicles\n(land, sea, air, etc.), to be able to geolocate them according to the risk\nareas to be evacuated, and adding them to the evacuation/rescue vehicles.\nBecause it is difficult to propose an effective real-time operational system on\nthe field in a real crisis situation, in this work, we propose to add a module\nfor recommending driver/vehicle pairs (with their specificities) to a system of\ncrisis management simulation. To do that, we chose to model and develop an\nontology-supported constraint-based recommender system for crisis management\nsimulations.",
        "translated": ""
    },
    {
        "title": "Embracing Uncertainty: Adaptive Vague Preference Policy Learning for\n  Multi-round Conversational Recommendation",
        "url": "http://arxiv.org/abs/2306.04487v1",
        "pub_date": "2023-06-07",
        "summary": "Conversational recommendation systems (CRS) effectively address information\nasymmetry by dynamically eliciting user preferences through multi-turn\ninteractions. Existing CRS widely assumes that users have clear preferences.\nUnder this assumption, the agent will completely trust the user feedback and\ntreat the accepted or rejected signals as strong indicators to filter items and\nreduce the candidate space, which may lead to the problem of over-filtering.\nHowever, in reality, users' preferences are often vague and volatile, with\nuncertainty about their desires and changing decisions during interactions.\n  To address this issue, we introduce a novel scenario called Vague Preference\nMulti-round Conversational Recommendation (VPMCR), which considers users' vague\nand volatile preferences in CRS.VPMCR employs a soft estimation mechanism to\nassign a non-zero confidence score for all candidate items to be displayed,\nnaturally avoiding the over-filtering problem. In the VPMCR setting, we\nintroduce an solution called Adaptive Vague Preference Policy Learning (AVPPL),\nwhich consists of two main components: Uncertainty-aware Soft Estimation (USE)\nand Uncertainty-aware Policy Learning (UPL). USE estimates the uncertainty of\nusers' vague feedback and captures their dynamic preferences using a\nchoice-based preferences extraction module and a time-aware decaying strategy.\nUPL leverages the preference distribution estimated by USE to guide the\nconversation and adapt to changes in users' preferences to make recommendations\nor ask for attributes.\n  Our extensive experiments demonstrate the effectiveness of our method in the\nVPMCR scenario, highlighting its potential for practical applications and\nimproving the overall performance and applicability of CRS in real-world\nsettings, particularly for users with vague or dynamic preferences.",
        "translated": ""
    },
    {
        "title": "RD-Suite: A Benchmark for Ranking Distillation",
        "url": "http://arxiv.org/abs/2306.04455v1",
        "pub_date": "2023-06-07",
        "summary": "The distillation of ranking models has become an important topic in both\nacademia and industry. In recent years, several advanced methods have been\nproposed to tackle this problem, often leveraging ranking information from\nteacher rankers that is absent in traditional classification settings. To date,\nthere is no well-established consensus on how to evaluate this class of models.\nMoreover, inconsistent benchmarking on a wide range of tasks and datasets make\nit difficult to assess or invigorate advances in this field. This paper first\nexamines representative prior arts on ranking distillation, and raises three\nquestions to be answered around methodology and reproducibility. To that end,\nwe propose a systematic and unified benchmark, Ranking Distillation Suite\n(RD-Suite), which is a suite of tasks with 4 large real-world datasets,\nencompassing two major modalities (textual and numeric) and two applications\n(standard distillation and distillation transfer). RD-Suite consists of\nbenchmark results that challenge some of the common wisdom in the field, and\nthe release of datasets with teacher scores and evaluation scripts for future\nresearch. RD-Suite paves the way towards better understanding of ranking\ndistillation, facilities more research in this direction, and presents new\nchallenges.",
        "translated": ""
    },
    {
        "title": "Modeling Dual Period-Varying Preferences for Takeaway Recommendation",
        "url": "http://arxiv.org/abs/2306.04370v1",
        "pub_date": "2023-06-07",
        "summary": "Takeaway recommender systems, which aim to accurately provide stores that\noffer foods meeting users' interests, have served billions of users in our\ndaily life. Different from traditional recommendation, takeaway recommendation\nfaces two main challenges: (1) Dual Interaction-Aware Preference Modeling.\nTraditional recommendation commonly focuses on users' single preferences for\nitems while takeaway recommendation needs to comprehensively consider users'\ndual preferences for stores and foods. (2) Period-Varying Preference Modeling.\nConventional recommendation generally models continuous changes in users'\npreferences from a session-level or day-level perspective. However, in\npractical takeaway systems, users' preferences vary significantly during the\nmorning, noon, night, and late night periods of the day. To address these\nchallenges, we propose a Dual Period-Varying Preference modeling (DPVP) for\ntakeaway recommendation. Specifically, we design a dual interaction-aware\nmodule, aiming to capture users' dual preferences based on their interactions\nwith stores and foods. Moreover, to model various preferences in different time\nperiods of the day, we propose a time-based decomposition module as well as a\ntime-aware gating mechanism. Extensive offline and online experiments\ndemonstrate that our model outperforms state-of-the-art methods on real-world\ndatasets and it is capable of modeling the dual period-varying preferences.\nMoreover, our model has been deployed online on Meituan Takeaway platform,\nleading to an average improvement in GMV (Gross Merchandise Value) of 0.70%.",
        "translated": ""
    },
    {
        "title": "An Overview of Challenges in Egocentric Text-Video Retrieval",
        "url": "http://arxiv.org/abs/2306.04345v1",
        "pub_date": "2023-06-07",
        "summary": "Text-video retrieval contains various challenges, including biases coming\nfrom diverse sources. We highlight some of them supported by illustrations to\nopen a discussion. Besides, we address one of the biases, frame length bias,\nwith a simple method which brings a very incremental but promising increase. We\nconclude with future directions.",
        "translated": ""
    },
    {
        "title": "Phrase Retrieval for Open-Domain Conversational Question Answering with\n  Conversational Dependency Modeling via Contrastive Learning",
        "url": "http://arxiv.org/abs/2306.04293v1",
        "pub_date": "2023-06-07",
        "summary": "Open-Domain Conversational Question Answering (ODConvQA) aims at answering\nquestions through a multi-turn conversation based on a retriever-reader\npipeline, which retrieves passages and then predicts answers with them.\nHowever, such a pipeline approach not only makes the reader vulnerable to the\nerrors propagated from the retriever, but also demands additional effort to\ndevelop both the retriever and the reader, which further makes it slower since\nthey are not runnable in parallel. In this work, we propose a method to\ndirectly predict answers with a phrase retrieval scheme for a sequence of\nwords, reducing the conventional two distinct subtasks into a single one. Also,\nfor the first time, we study its capability for ODConvQA tasks. However, simply\nadopting it is largely problematic, due to the dependencies between previous\nand current turns in a conversation. To address this problem, we further\nintroduce a novel contrastive learning strategy, making sure to reflect\nprevious turns when retrieving the phrase for the current context, by\nmaximizing representational similarities of consecutive turns in a conversation\nwhile minimizing irrelevant conversational contexts. We validate our model on\ntwo ODConvQA datasets, whose experimental results show that it substantially\noutperforms the relevant baselines with the retriever-reader. Code is available\nat: https://github.com/starsuzi/PRO-ConvQA.",
        "translated": ""
    },
    {
        "title": "Set-to-Sequence Ranking-based Concept-aware Learning Path Recommendation",
        "url": "http://arxiv.org/abs/2306.04234v1",
        "pub_date": "2023-06-07",
        "summary": "With the development of the online education system, personalized education\nrecommendation has played an essential role. In this paper, we focus on\ndeveloping path recommendation systems that aim to generating and recommending\nan entire learning path to the given user in each session. Noticing that\nexisting approaches fail to consider the correlations of concepts in the path,\nwe propose a novel framework named Set-to-Sequence Ranking-based Concept-aware\nLearning Path Recommendation (SRC), which formulates the recommendation task\nunder a set-to-sequence paradigm. Specifically, we first design a concept-aware\nencoder module which can capture the correlations among the input learning\nconcepts. The outputs are then fed into a decoder module that sequentially\ngenerates a path through an attention mechanism that handles correlations\nbetween the learning and target concepts. Our recommendation policy is\noptimized by policy gradient. In addition, we also introduce an auxiliary\nmodule based on knowledge tracing to enhance the model's stability by\nevaluating students' learning effects on learning concepts. We conduct\nextensive experiments on two real-world public datasets and one industrial\ndataset, and the experimental results demonstrate the superiority and\neffectiveness of SRC. Code will be available at\nhttps://gitee.com/mindspore/models/tree/master/research/recommend/SRC.",
        "translated": ""
    },
    {
        "title": "SANGEET: A XML based Open Dataset for Research in Hindustani Sangeet",
        "url": "http://arxiv.org/abs/2306.04148v1",
        "pub_date": "2023-06-07",
        "summary": "It is very important to access a rich music dataset that is useful in a wide\nvariety of applications. Currently, available datasets are mostly focused on\nstoring vocal or instrumental recording data and ignoring the requirement of\nits visual representation and retrieval. This paper attempts to build an\nXML-based public dataset, called SANGEET, that stores comprehensive information\nof Hindustani Sangeet (North Indian Classical Music) compositions written by\nfamous musicologist Pt. Vishnu Narayan Bhatkhande. SANGEET preserves all the\nrequired information of any given composition including metadata, structural,\nnotational, rhythmic, and melodic information in a standardized way for easy\nand efficient storage and extraction of musical information. The dataset is\nintended to provide the ground truth information for music information research\ntasks, thereby supporting several data-driven analysis from a machine learning\nperspective. We present the usefulness of the dataset by demonstrating its\napplication on music information retrieval using XQuery, visualization through\nOmenad rendering system. Finally, we propose approaches to transform the\ndataset for performing statistical and machine learning tasks for a better\nunderstanding of Hindustani Sangeet. The dataset can be found at\nhttps://github.com/cmisra/Sangeet.",
        "translated": ""
    },
    {
        "title": "Answering Compositional Queries with Set-Theoretic Embeddings",
        "url": "http://arxiv.org/abs/2306.04133v1",
        "pub_date": "2023-06-07",
        "summary": "The need to compactly and robustly represent item-attribute relations arises\nin many important tasks, such as faceted browsing and recommendation systems. A\npopular machine learning approach for this task denotes that an item has an\nattribute by a high dot-product between vectors for the item and attribute -- a\nrepresentation that is not only dense, but also tends to correct noisy and\nincomplete data. While this method works well for queries retrieving items by a\nsingle attribute (such as \\emph{movies that are comedies}), we find that vector\nembeddings do not so accurately support compositional queries (such as movies\nthat are comedies and British but not romances). To address these set-theoretic\ncompositions, this paper proposes to replace vectors with box embeddings, a\nregion-based representation that can be thought of as learnable Venn diagrams.\nWe introduce a new benchmark dataset for compositional queries, and present\nexperiments and analysis providing insights into the behavior of both. We find\nthat, while vector and box embeddings are equally suited to single attribute\nqueries, for compositional queries box embeddings provide substantial\nadvantages over vectors, particularly at the moderate and larger retrieval set\nsizes that are most useful for users' search and browsing.",
        "translated": ""
    },
    {
        "title": "Safe Collaborative Filtering",
        "url": "http://arxiv.org/abs/2306.05292v1",
        "pub_date": "2023-06-08",
        "summary": "Excellent tail performance is crucial for modern machine learning tasks, such\nas algorithmic fairness, class imbalance, and risk-sensitive decision making,\nas it ensures the effective handling of challenging samples within a dataset.\nTail performance is also a vital determinant of success for personalised\nrecommender systems to reduce the risk of losing users with low satisfaction.\nThis study introduces a \"safe\" collaborative filtering method that prioritises\nrecommendation quality for less-satisfied users rather than focusing on the\naverage performance. Our approach minimises the conditional value at risk\n(CVaR), which represents the average risk over the tails of users' loss. To\novercome computational challenges for web-scale recommender systems, we develop\na robust yet practical algorithm that extends the most scalable method,\nimplicit alternating least squares (iALS). Empirical evaluation on real-world\ndatasets demonstrates the excellent tail performance of our approach while\nmaintaining competitive computational efficiency.",
        "translated": ""
    },
    {
        "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
        "url": "http://arxiv.org/abs/2306.05212v1",
        "pub_date": "2023-06-08",
        "summary": "Although Large Language Models (LLMs) have demonstrated extraordinary\ncapabilities in many domains, they still have a tendency to hallucinate and\ngenerate fictitious responses to user requests. This problem can be alleviated\nby augmenting LLMs with information retrieval (IR) systems (also known as\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\nfactual texts in response to user input according to the relevant content\nretrieved by IR systems from external corpora as references. In addition, by\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\nquestions that cannot be answered by solely relying on the world knowledge\nstored in parameters. To support research in this area and facilitate the\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\nto help researchers and users build their customized in-domain LLM-based\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\nprovides more plug-and-play modules to support better interaction between IR\nsystems and LLMs, including {request rewriting, document retrieval, passage\nextraction, answer generation, and fact checking} modules. Our toolkit is\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",
        "translated": ""
    },
    {
        "title": "Controllable Multi-Objective Re-ranking with Policy Hypernetworks",
        "url": "http://arxiv.org/abs/2306.05118v1",
        "pub_date": "2023-06-08",
        "summary": "Multi-stage ranking pipelines have become widely used strategies in modern\nrecommender systems, where the final stage aims to return a ranked list of\nitems that balances a number of requirements such as user preference,\ndiversity, novelty etc. Linear scalarization is arguably the most widely used\ntechnique to merge multiple requirements into one optimization objective, by\nsumming up the requirements with certain preference weights. Existing\nfinal-stage ranking methods often adopt a static model where the preference\nweights are determined during offline training and kept unchanged during online\nserving. Whenever a modification of the preference weights is needed, the model\nhas to be re-trained, which is time and resources inefficient. Meanwhile, the\nmost appropriate weights may vary greatly for different groups of targeting\nusers or at different time periods (e.g., during holiday promotions). In this\npaper, we propose a framework called controllable multi-objective re-ranking\n(CMR) which incorporates a hypernetwork to generate parameters for a re-ranking\nmodel according to different preference weights. In this way, CMR is enabled to\nadapt the preference weights according to the environment changes in an online\nmanner, without retraining the models. Moreover, we classify practical\nbusiness-oriented tasks into four main categories and seamlessly incorporate\nthem in a new proposed re-ranking model based on an Actor-Evaluator framework,\nwhich serves as a reliable real-world testbed for CMR. Offline experiments\nbased on the dataset collected from Taobao App showed that CMR improved several\npopular re-ranking models by using them as underlying models. Online A/B tests\nalso demonstrated the effectiveness and trustworthiness of CMR.",
        "translated": ""
    },
    {
        "title": "Attention Weighted Mixture of Experts with Contrastive Learning for\n  Personalized Ranking in E-commerce",
        "url": "http://arxiv.org/abs/2306.05011v1",
        "pub_date": "2023-06-08",
        "summary": "Ranking model plays an essential role in e-commerce search and\nrecommendation. An effective ranking model should give a personalized ranking\nlist for each user according to the user preference. Existing algorithms\nusually extract a user representation vector from the user behavior sequence,\nthen feed the vector into a feed-forward network (FFN) together with other\nfeatures for feature interactions, and finally produce a personalized ranking\nscore. Despite tremendous progress in the past, there is still room for\nimprovement. Firstly, the personalized patterns of feature interactions for\ndifferent users are not explicitly modeled. Secondly, most of existing\nalgorithms have poor personalized ranking results for long-tail users with few\nhistorical behaviors due to the data sparsity. To overcome the two challenges,\nwe propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive\nlearning for personalized ranking. Firstly, AW-MoE leverages the MoE framework\nto capture personalized feature interactions for different users. To model the\nuser preference, the user behavior sequence is simultaneously fed into expert\nnetworks and the gate network. Within the gate network, one gate unit and one\nactivation unit are designed to adaptively learn the fine-grained activation\nvector for experts using an attention mechanism. Secondly, a random masking\nstrategy is applied to the user behavior sequence to simulate long-tail users,\nand an auxiliary contrastive loss is imposed to the output of the gate network\nto improve the model generalization for these users. This is validated by a\nhigher performance gain on the long-tail user test set. Experiment results on a\nJD real production dataset and a public dataset demonstrate the effectiveness\nof AW-MoE, which significantly outperforms state-of-art methods. Notably,\nAW-MoE has been successfully deployed in the JD e-commerce search engine, ...",
        "translated": ""
    },
    {
        "title": "Unified Embedding Based Personalized Retrieval in Etsy Search",
        "url": "http://arxiv.org/abs/2306.04833v1",
        "pub_date": "2023-06-07",
        "summary": "Embedding-based neural retrieval is a prevalent approach to address the\nsemantic gap problem which often arises in product search on tail queries. In\ncontrast, popular queries typically lack context and have a broad intent where\nadditional context from users historical interaction can be helpful. In this\npaper, we share our novel approach to address both: the semantic gap problem\nfollowed by an end to end trained model for personalized semantic retrieval. We\npropose learning a unified embedding model incorporating graph, transformer and\nterm-based embeddings end to end and share our design choices for optimal\ntradeoff between performance and efficiency. We share our learnings in feature\nengineering, hard negative sampling strategy, and application of transformer\nmodel, including a novel pre-training strategy and other tricks for improving\nsearch relevance and deploying such a model at industry scale. Our personalized\nretrieval model significantly improves the overall search experience, as\nmeasured by a 5.58% increase in search purchase rate and a 2.63% increase in\nsite-wide conversion rate, aggregated across multiple A/B tests - on live\ntraffic.",
        "translated": ""
    },
    {
        "title": "SKG: A Versatile Information Retrieval and Analysis Framework for\n  Academic Papers with Semantic Knowledge Graphs",
        "url": "http://arxiv.org/abs/2306.04758v1",
        "pub_date": "2023-06-07",
        "summary": "The number of published research papers has experienced exponential growth in\nrecent years, which makes it crucial to develop new methods for efficient and\nversatile information extraction and knowledge discovery. To address this need,\nwe propose a Semantic Knowledge Graph (SKG) that integrates semantic concepts\nfrom abstracts and other meta-information to represent the corpus. The SKG can\nsupport various semantic queries in academic literature thanks to the high\ndiversity and rich information content stored within. To extract knowledge from\nunstructured text, we develop a Knowledge Extraction Module that includes a\nsemi-supervised pipeline for entity extraction and entity normalization. We\nalso create an ontology to integrate the concepts with other meta information,\nenabling us to build the SKG. Furthermore, we design and develop a dataflow\nsystem that demonstrates how to conduct various semantic queries flexibly and\ninteractively over the SKG. To demonstrate the effectiveness of our approach,\nwe conduct the research based on the visualization literature and provide\nreal-world use cases to show the usefulness of the SKG.\n  The dataset and codes for this work are available at\nhttps://osf.io/aqv8p/?view_only=2c26b36e3e3941ce999df47e4616207f.",
        "translated": ""
    },
    {
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey",
        "url": "http://arxiv.org/abs/2306.05817v1",
        "pub_date": "2023-06-09",
        "summary": "Recommender systems (RS) play important roles to match users' information\nneeds for Internet applications. In natural language processing (NLP) domains,\nlarge language model (LLM) has shown astonishing emergent abilities (e.g.,\ninstruction following, reasoning), thus giving rise to the promising research\ndirection of adapting LLM to RS for performance enhancements and user\nexperience improvements. In this paper, we conduct a comprehensive survey on\nthis research direction from an application-oriented view. We first summarize\nexisting research works from two orthogonal perspectives: where and how to\nadapt LLM to RS. For the \"WHERE\" question, we discuss the roles that LLM could\nplay in different stages of the recommendation pipeline, i.e., feature\nengineering, feature encoder, scoring/ranking function, and pipeline\ncontroller. For the \"HOW\" question, we investigate the training and inference\nstrategies, resulting in two fine-grained taxonomy criteria, i.e., whether to\ntune LLMs or not, and whether to involve conventional recommendation model\n(CRM) for inference. Detailed analysis and general development trajectories are\nprovided for both questions, respectively. Then, we highlight key challenges in\nadapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and\nethics. Finally, we summarize the survey and discuss the future prospects. We\nalso actively maintain a GitHub repository for papers and other related\nresources in this rising direction:\n$\\href{https://github.com/CHIANGEL/Awesome-LLM-for-RecSys}{[GitHub\\;Link]}$.",
        "translated": ""
    },
    {
        "title": "Interactive Explanation with Varying Level of Details in an Explainable\n  Scientific Literature Recommender System",
        "url": "http://arxiv.org/abs/2306.05809v1",
        "pub_date": "2023-06-09",
        "summary": "Explainable recommender systems (RS) have traditionally followed a\none-size-fits-all approach, delivering the same explanation level of detail to\neach user, without considering their individual needs and goals. Further,\nexplanations in RS have so far been presented mostly in a static and\nnon-interactive manner. To fill these research gaps, we aim in this paper to\nadopt a user-centered, interactive explanation model that provides explanations\nwith different levels of detail and empowers users to interact with, control,\nand personalize the explanations based on their needs and preferences. We\nfollowed a user-centered approach to design interactive explanations with three\nlevels of detail (basic, intermediate, and advanced) and implemented them in\nthe transparent Recommendation and Interest Modeling Application (RIMA). We\nconducted a qualitative user study (N=14) to investigate the impact of\nproviding interactive explanations with varying level of details on the users'\nperception of the explainable RS. Our study showed qualitative evidence that\nfostering interaction and giving users control in deciding which explanation\nthey would like to see can meet the demands of users with different needs,\npreferences, and goals, and consequently can have positive effects on different\ncrucial aspects in explainable recommendation, including transparency, trust,\nsatisfaction, and user experience.",
        "translated": ""
    },
    {
        "title": "RankFormer: Listwise Learning-to-Rank Using Listwide Labels",
        "url": "http://arxiv.org/abs/2306.05808v1",
        "pub_date": "2023-06-09",
        "summary": "Web applications where users are presented with a limited selection of items\nhave long employed ranking models to put the most relevant results first. Any\nfeedback received from users is typically assumed to reflect a relative\njudgement on the utility of items, e.g. a user clicking on an item only implies\nit is better than items not clicked in the same ranked list. Hence, the\nobjectives optimized in Learning-to-Rank (LTR) tend to be pairwise or listwise.\n  Yet, by only viewing feedback as relative, we neglect the user's absolute\nfeedback on the list's overall quality, e.g. when no items in the selection are\nclicked. We thus reconsider the standard LTR paradigm and argue the benefits of\nlearning from this listwide signal. To this end, we propose the RankFormer as\nan architecture that, with a Transformer at its core, can jointly optimize a\nnovel listwide assessment objective and a traditional listwise LTR objective.\n  We simulate implicit feedback on public datasets and observe that the\nRankFormer succeeds in benefitting from listwide signals. Additionally, we\nconduct experiments in e-commerce on Amazon Search data and find the RankFormer\nto be superior to all baselines offline. An online experiment shows that\nknowledge distillation can be used to find immediate practical use for the\nRankFormer.",
        "translated": ""
    },
    {
        "title": "Customizing General-Purpose Foundation Models for Medical Report\n  Generation",
        "url": "http://arxiv.org/abs/2306.05642v1",
        "pub_date": "2023-06-09",
        "summary": "Medical caption prediction which can be regarded as a task of medical report\ngeneration (MRG), requires the automatic generation of coherent and accurate\ncaptions for the given medical images. However, the scarcity of labelled\nmedical image-report pairs presents great challenges in the development of deep\nand large-scale neural networks capable of harnessing the potential artificial\ngeneral intelligence power like large language models (LLMs). In this work, we\npropose customizing off-the-shelf general-purpose large-scale pre-trained\nmodels, i.e., foundation models (FMs), in computer vision and natural language\nprocessing with a specific focus on medical report generation. Specifically,\nfollowing BLIP-2, a state-of-the-art vision-language pre-training approach, we\nintroduce our encoder-decoder-based MRG model. This model utilizes a\nlightweight query Transformer to connect two FMs: the giant vision Transformer\nEVA-ViT-g and a bilingual LLM trained to align with human intentions (referred\nto as ChatGLM-6B). Furthermore, we conduct ablative experiments on the\ntrainable components of the model to identify the crucial factors for effective\ntransfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn\nmedical image representations, followed by parameter-efficient training of\nChatGLM-6B to capture the writing styles of medical reports, is essential for\nachieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and\nthe 2nd, respectively, out of 13 participating teams, based on the BERTScore\nand ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction\nTask competition.",
        "translated": ""
    },
    {
        "title": "Bayesian Knowledge-driven Critiquing with Indirect Evidence",
        "url": "http://arxiv.org/abs/2306.05636v1",
        "pub_date": "2023-06-09",
        "summary": "Conversational recommender systems (CRS) enhance the expressivity and\npersonalization of recommendations through multiple turns of user-system\ninteraction. Critiquing is a well-known paradigm for CRS that allows users to\niteratively refine recommendations by providing feedback about attributes of\nrecommended items. While existing critiquing methodologies utilize direct\nattributes of items to address user requests such as 'I prefer Western movies',\nthe opportunity of incorporating richer contextual and side information about\nitems stored in Knowledge Graphs (KG) into the critiquing paradigm has been\noverlooked. Employing this substantial knowledge together with a\nwell-established reasoning methodology paves the way for critique-based\nrecommenders to allow for complex knowledge-based feedback (e.g., 'I like\nmovies featuring war side effects on veterans') which may arise in natural\nuser-system conversations. In this work, we aim to increase the flexibility of\ncritique-based recommendation by integrating KGs and propose a novel Bayesian\ninference framework that enables reasoning with relational knowledge-based\nfeedback. We study and formulate the framework considering a Gaussian\nlikelihood and evaluate it on two well-known recommendation datasets with KGs.\nOur evaluations demonstrate the effectiveness of our framework in leveraging\nindirect KG-based feedback (i.e., preferred relational properties of items\nrather than preferred items themselves), often improving personalized\nrecommendations over a one-shot recommender by more than 15%. This work enables\na new paradigm for using rich knowledge content and reasoning over indirect\nevidence as a mechanism for critiquing interactions with CRS.",
        "translated": ""
    },
    {
        "title": "CLC: Cluster Assignment via Contrastive Representation Learning",
        "url": "http://arxiv.org/abs/2306.05439v1",
        "pub_date": "2023-06-08",
        "summary": "Clustering remains an important and challenging task of grouping samples into\nclusters without manual annotations. Recent works have achieved excellent\nresults on small datasets by performing clustering on feature representations\nlearned from self-supervised learning. However, for datasets with a large\nnumber of clusters, such as ImageNet, current methods still can not achieve\nhigh clustering performance. In this paper, we propose Contrastive\nLearning-based Clustering (CLC), which uses contrastive learning to directly\nlearn cluster assignment. We decompose the representation into two parts: one\nencodes the categorical information under an equipartition constraint, and the\nother captures the instance-wise factors. We propose a contrastive loss using\nboth parts of the representation. We theoretically analyze the proposed\ncontrastive loss and reveal that CLC sets different weights for the negative\nsamples while learning cluster assignments. Further gradient analysis shows\nthat the larger weights tend to focus more on the hard negative samples.\nTherefore, the proposed loss has high expressiveness that enables us to\nefficiently learn cluster assignments. Experimental evaluation shows that CLC\nachieves overall state-of-the-art or highly competitive clustering performance\non multiple benchmark datasets. In particular, we achieve 53.4% accuracy on the\nfull ImageNet dataset and outperform existing methods by large margins (+\n10.2%).",
        "translated": ""
    },
    {
        "title": "Weakly-Supervised Scientific Document Classification via\n  Retrieval-Augmented Multi-Stage Training",
        "url": "http://arxiv.org/abs/2306.07193v1",
        "pub_date": "2023-06-12",
        "summary": "Scientific document classification is a critical task for a wide range of\napplications, but the cost of obtaining massive amounts of human-labeled data\ncan be prohibitive. To address this challenge, we propose a weakly-supervised\napproach for scientific document classification using label names only. In\nscientific domains, label names often include domain-specific concepts that may\nnot appear in the document corpus, making it difficult to match labels and\ndocuments precisely. To tackle this issue, we propose WANDER, which leverages\ndense retrieval to perform matching in the embedding space to capture the\nsemantics of label names. We further design the label name expansion module to\nenrich the label name representations. Lastly, a self-training step is used to\nrefine the predictions. The experiments on three datasets show that WANDER\noutperforms the best baseline by 11.9% on average. Our code will be published\nat https://github.com/ritaranx/wander.",
        "translated": ""
    },
    {
        "title": "Fair Learning to Rank with Distribution-free Risk Control",
        "url": "http://arxiv.org/abs/2306.07188v1",
        "pub_date": "2023-06-12",
        "summary": "Learning to Rank (LTR) methods are vital in online economies, affecting users\nand item providers. Fairness in LTR models is crucial to allocate exposure\nproportionally to item relevance. The deterministic ranking model can lead to\nunfair exposure distribution when items with the same relevance receive\nslightly different scores. Stochastic LTR models, incorporating the\nPlackett-Luce (PL) model, address fairness issues but have limitations in\ncomputational cost and performance guarantees. To overcome these limitations,\nwe propose FairLTR-RC, a novel post-hoc model-agnostic method. FairLTR-RC\nleverages a pretrained scoring function to create a stochastic LTR model,\neliminating the need for expensive training. Furthermore, FairLTR-RC provides\nfinite-sample guarantees on a user-specified utility using distribution-free\nrisk control framework. By additionally incorporating the Thresholded PL (TPL)\nmodel, we are able to achieve an effective trade-off between utility and\nfairness. Experimental results on several benchmark datasets demonstrate that\nFairLTR-RC significantly improves fairness in widely-used deterministic LTR\nmodels while guaranteeing a specified level of utility.",
        "translated": ""
    },
    {
        "title": "Video-to-Music Recommendation using Temporal Alignment of Segments",
        "url": "http://arxiv.org/abs/2306.07187v1",
        "pub_date": "2023-06-12",
        "summary": "We study cross-modal recommendation of music tracks to be used as soundtracks\nfor videos. This problem is known as the music supervision task. We build on a\nself-supervised system that learns a content association between music and\nvideo. In addition to the adequacy of content, adequacy of structure is crucial\nin music supervision to obtain relevant recommendations. We propose a novel\napproach to significantly improve the system's performance using\nstructure-aware recommendation. The core idea is to consider not only the full\naudio-video clips, but rather shorter segments for training and inference. We\nfind that using semantic segments and ranking the tracks according to sequence\nalignment costs significantly improves the results. We investigate the impact\nof different ranking metrics and segmentation methods.",
        "translated": ""
    },
    {
        "title": "Adversarial Constrained Bidding via Minimax Regret Optimization with\n  Causality-Aware Reinforcement Learning",
        "url": "http://arxiv.org/abs/2306.07106v1",
        "pub_date": "2023-06-12",
        "summary": "The proliferation of the Internet has led to the emergence of online\nadvertising, driven by the mechanics of online auctions. In these repeated\nauctions, software agents participate on behalf of aggregated advertisers to\noptimize for their long-term utility. To fulfill the diverse demands, bidding\nstrategies are employed to optimize advertising objectives subject to different\nspending constraints. Existing approaches on constrained bidding typically rely\non i.i.d. train and test conditions, which contradicts the adversarial nature\nof online ad markets where different parties possess potentially conflicting\nobjectives. In this regard, we explore the problem of constrained bidding in\nadversarial bidding environments, which assumes no knowledge about the\nadversarial factors. Instead of relying on the i.i.d. assumption, our insight\nis to align the train distribution of environments with the potential test\ndistribution meanwhile minimizing policy regret. Based on this insight, we\npropose a practical Minimax Regret Optimization (MiRO) approach that\ninterleaves between a teacher finding adversarial environments for tutoring and\na learner meta-learning its policy over the given distribution of environments.\nIn addition, we pioneer to incorporate expert demonstrations for learning\nbidding strategies. Through a causality-aware policy design, we improve upon\nMiRO by distilling knowledge from the experts. Extensive experiments on both\nindustrial data and synthetic data show that our method, MiRO with\nCausality-aware reinforcement Learning (MiROCL), outperforms prior methods by\nover 30%.",
        "translated": ""
    },
    {
        "title": "Imbalanced Multi-label Classification for Business-related Text with\n  Moderately Large Label Spaces",
        "url": "http://arxiv.org/abs/2306.07046v1",
        "pub_date": "2023-06-12",
        "summary": "In this study, we compared the performance of four different methods for\nmulti label text classification using a specific imbalanced business dataset.\nThe four methods we evaluated were fine tuned BERT, Binary Relevance,\nClassifier Chains, and Label Powerset. The results show that fine tuned BERT\noutperforms the other three methods by a significant margin, achieving high\nvalues of accuracy, F1 Score, Precision, and Recall. Binary Relevance also\nperforms well on this dataset, while Classifier Chains and Label Powerset\ndemonstrate relatively poor performance. These findings highlight the\neffectiveness of fine tuned BERT for multi label text classification tasks, and\nsuggest that it may be a useful tool for businesses seeking to analyze complex\nand multifaceted texts.",
        "translated": ""
    },
    {
        "title": "Skellam Rank: Fair Learning to Rank Algorithm Based on Poisson Process\n  and Skellam Distribution for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.06607v1",
        "pub_date": "2023-06-11",
        "summary": "Recommender system is a widely adopted technology in a diversified class of\nproduct lines. Modern day recommender system approaches include matrix\nfactorization, learning to rank and deep learning paradigms, etc. Unlike many\nother approaches, learning to rank builds recommendation results based on\nmaximization of the probability of ranking orders. There are intrinsic issues\nrelated to recommender systems such as selection bias, exposure bias and\npopularity bias. In this paper, we propose a fair recommender system algorithm\nthat uses Poisson process and Skellam distribution. We demonstrate in our\nexperiments that our algorithm is competitive in accuracy metrics and far more\nsuperior than other modern algorithms in fairness metrics.",
        "translated": ""
    },
    {
        "title": "Mean-Variance Efficient Collaborative Filtering for Stock Recommendation",
        "url": "http://arxiv.org/abs/2306.06590v1",
        "pub_date": "2023-06-11",
        "summary": "The rise of FinTech has transformed financial services onto online platforms,\nyet stock investment recommender systems have received limited attention\ncompared to other industries. Personalized stock recommendations can\nsignificantly impact customer engagement and satisfaction within the industry.\nHowever, traditional investment recommendations focus on high-return stocks or\nhighly diversified portfolios based on the modern portfolio theory, often\nneglecting user preferences. On the other hand, collaborative filtering (CF)\nmethods also may not be directly applicable to stock recommendations, because\nit is inappropriate to just recommend stocks that users like. The key is to\noptimally blend users preference with the portfolio theory. However, research\non stock recommendations within the recommender system domain remains\ncomparatively limited, and no existing model considers both the preference of\nusers and the risk-return characteristics of stocks. In this regard, we propose\na mean-variance efficient collaborative filtering (MVECF) model for stock\nrecommendations that consider both aspects. Our model is specifically designed\nto improve the pareto optimality (mean-variance efficiency) in a trade-off\nbetween the risk (variance of return) and return (mean return) by systemically\nhandling uncertainties in stock prices. Such improvements are incorporated into\nthe MVECF model using regularization, and the model is restructured to fit into\nthe ordinary matrix factorization scheme to boost computational efficiency.\nExperiments on real-world fund holdings data show that our model can increase\nthe mean-variance efficiency of suggested portfolios while sacrificing just a\nsmall amount of mean average precision and recall. Finally, we further show\nMVECF is easily applicable to the state-of-the-art graph-based ranking models.",
        "translated": ""
    },
    {
        "title": "GuP: Fast Subgraph Matching by Guard-based Pruning",
        "url": "http://arxiv.org/abs/2306.06557v1",
        "pub_date": "2023-06-11",
        "summary": "Subgraph matching, which finds subgraphs isomorphic to a query, is the key to\ninformation retrieval from data represented as a graph. To avoid redundant\nexploration in the data, existing methods restrict the search space by\nextracting candidate vertices and candidate edges that may constitute\nisomorphic subgraphs. However, it still requires expensive computation because\ncandidate vertices induce many subgraphs that are not isomorphic to the query.\nIn this paper, we propose GuP, a subgraph matching algorithm with pruning based\non guards. Guards are a pattern of intermediate search states that never find\nisomorphic subgraphs. GuP attaches a guard on each candidate vertex and edge\nand filters out them adaptively to the search state. The experimental results\nshowed that GuP can efficiently solve various queries, including those that the\nstate-of-the-art methods could not solve in practical time.",
        "translated": ""
    },
    {
        "title": "Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain\n  Recommendation in an AI Assistant Application",
        "url": "http://arxiv.org/abs/2306.06302v1",
        "pub_date": "2023-06-09",
        "summary": "Recommender systems have found significant commercial success but still\nstruggle with integrating new users. Since users often interact with content in\ndifferent domains, it is possible to leverage a user's interactions in previous\ndomains to improve that user's recommendations in a new one (multi-domain\nrecommendation). A separate research thread on knowledge graph enhancement uses\nexternal knowledge graphs to improve single domain recommendations (knowledge\ngraph enhancement). Both research threads incorporate related information to\nimprove predictions in a new domain. We propose in this work to unify these\napproaches: Using information from interactions in other domains as well as\nexternal knowledge graphs to make predictions in a new domain that would be\nimpossible with either information source alone. We apply these ideas to a\ndataset derived from millions of users' requests for content across three\ndomains (videos, music, and books) in a live virtual assistant application. We\ndemonstrate the advantage of combining knowledge graph enhancement with\nprevious multi-domain recommendation techniques to provide better overall\nrecommendations as well as for better recommendations on new users of a domain.",
        "translated": ""
    },
    {
        "title": "Open Data on GitHub: Unlocking the Potential of AI",
        "url": "http://arxiv.org/abs/2306.06191v1",
        "pub_date": "2023-06-09",
        "summary": "GitHub is the world's largest platform for collaborative software\ndevelopment, with over 100 million users. GitHub is also used extensively for\nopen data collaboration, hosting more than 800 million open data files,\ntotaling 142 terabytes of data. This study highlights the potential of open\ndata on GitHub and demonstrates how it can accelerate AI research. We analyze\nthe existing landscape of open data on GitHub and the patterns of how users\nshare datasets. Our findings show that GitHub is one of the largest hosts of\nopen data in the world and has experienced an accelerated growth of open data\nassets over the past four years. By examining the open data landscape on\nGitHub, we aim to empower users and organizations to leverage existing open\ndatasets and improve their discoverability -- ultimately contributing to the\nongoing AI revolution to help address complex societal issues. We release the\nthree datasets that we have collected to support this analysis as open datasets\nat https://github.com/github/open-data-on-github.",
        "translated": ""
    },
    {
        "title": "Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal\n  Rank with Lexicographic Precision",
        "url": "http://arxiv.org/abs/2306.07908v1",
        "pub_date": "2023-06-13",
        "summary": "Across a variety of ranking tasks, researchers use reciprocal rank to measure\nthe effectiveness for users interested in exactly one relevant item. Despite\nits widespread use, evidence suggests that reciprocal rank is brittle when\ndiscriminating between systems. This brittleness, in turn, is compounded in\nmodern evaluation settings where current, high-precision systems may be\ndifficult to distinguish. We address the lack of sensitivity of reciprocal rank\nby introducing and connecting it to the concept of best-case retrieval, an\nevaluation method focusing on assessing the quality of a ranking for the most\nsatisfied possible user across possible recall requirements. This perspective\nallows us to generalize reciprocal rank and define a new preference-based\nevaluation we call lexicographic precision or lexiprecision. By mathematical\nconstruction, we ensure that lexiprecision preserves differences detected by\nreciprocal rank, while empirically improving sensitivity and robustness across\na broad set of retrieval and recommendation tasks.",
        "translated": ""
    },
    {
        "title": "ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support\n  Lateral Reading",
        "url": "http://arxiv.org/abs/2306.07875v1",
        "pub_date": "2023-06-13",
        "summary": "With the rapid growth and spread of online misinformation, people need tools\nto help them evaluate the credibility and accuracy of online information.\nLateral reading, a strategy that involves cross-referencing information with\nmultiple sources, may be an effective approach to achieving this goal. In this\npaper, we present ReadProbe, a tool to support lateral reading, powered by\ngenerative large language models from OpenAI and the Bing search engine. Our\ntool is able to generate useful questions for lateral reading, scour the web\nfor relevant documents, and generate well-attributed answers to help people\nbetter evaluate online information. We made a web-based application to\ndemonstrate how ReadProbe can help reduce the risk of being misled by false\ninformation. The code is available at\nhttps://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won\nthe first prize in a national AI misinformation hackathon.",
        "translated": ""
    },
    {
        "title": "KuaiSAR: A Unified Search And Recommendation Dataset",
        "url": "http://arxiv.org/abs/2306.07705v1",
        "pub_date": "2023-06-13",
        "summary": "The confluence of Search and Recommendation services is a vital aspect of\nonline content platforms like Kuaishou and TikTok. The integration of S&amp;R\nmodeling is a highly intuitive approach adopted by industry practitioners.\nHowever, there is a noticeable lack of research conducted in this area within\nthe academia, primarily due to the absence of publicly available datasets.\nConsequently, a substantial gap has emerged between academia and industry\nregarding research endeavors in this field. To bridge this gap, we introduce\nthe first large-scale, real-world dataset KuaiSAR of integrated Search And\nRecommendation behaviors collected from Kuaishou, a leading short-video app in\nChina with over 300 million daily active users. Previous research in this field\nhas predominantly employed publicly available datasets that are semi-synthetic\nand simulated, with artificially fabricated search behaviors. Distinct from\nprevious datasets, KuaiSAR records genuine user behaviors, the occurrence of\neach interaction within either search or recommendation service, and the users'\ntransitions between the two services. This work aids in joint modeling of S&amp;R,\nand the utilization of search data for recommenders (and recommendation data\nfor search engines). Additionally, due to the diverse feedback labels of\nuser-video interactions, KuaiSAR also supports a wide range of other tasks,\nincluding intent recommendation, multi-task learning, and long sequential\nmulti-behavior modeling etc. We believe this dataset will facilitate innovative\nresearch and enrich our understanding of S&amp;R services integration in real-world\napplications.",
        "translated": ""
    },
    {
        "title": "Practice with Graph-based ANN Algorithms on Sparse Data: Chi-square\n  Two-tower model, HNSW, Sign Cauchy Projections",
        "url": "http://arxiv.org/abs/2306.07607v1",
        "pub_date": "2023-06-13",
        "summary": "Sparse data are common. The traditional ``handcrafted'' features are often\nsparse. Embedding vectors from trained models can also be very sparse, for\nexample, embeddings trained via the ``ReLu'' activation function. In this\npaper, we report our exploration of efficient search in sparse data with\ngraph-based ANN algorithms (e.g., HNSW, or SONG which is the GPU version of\nHNSW), which are popular in industrial practice, e.g., search and ads\n(advertising).\n  We experiment with the proprietary ads targeting application, as well as\nbenchmark public datasets. For ads targeting, we train embeddings with the\nstandard ``cosine two-tower'' model and we also develop the ``chi-square\ntwo-tower'' model. Both models produce (highly) sparse embeddings when they are\nintegrated with the ``ReLu'' activation function. In EBR (embedding-based\nretrieval) applications, after we the embeddings are trained, the next crucial\ntask is the approximate near neighbor (ANN) search for serving. While there are\nmany ANN algorithms we can choose from, in this study, we focus on the\ngraph-based ANN algorithm (e.g., HNSW-type).\n  Sparse embeddings should help improve the efficiency of EBR. One benefit is\nthe reduced memory cost for the embeddings. The other obvious benefit is the\nreduced computational time for evaluating similarities, because, for\ngraph-based ANN algorithms such as HNSW, computing similarities is often the\ndominating cost. In addition to the effort on leveraging data sparsity for\nstorage and computation, we also integrate ``sign cauchy random projections''\n(SignCRP) to hash vectors to bits, to further reduce the memory cost and speed\nup the ANN search. In NIPS'13, SignCRP was proposed to hash the chi-square\nsimilarity, which is a well-adopted nonlinear kernel in NLP and computer\nvision. Therefore, the chi-square two-tower model, SignCRP, and HNSW are now\ntightly integrated.",
        "translated": ""
    },
    {
        "title": "Unified Off-Policy Learning to Rank: a Reinforcement Learning\n  Perspective",
        "url": "http://arxiv.org/abs/2306.07528v1",
        "pub_date": "2023-06-13",
        "summary": "Off-policy Learning to Rank (LTR) aims to optimize a ranker from data\ncollected by a deployed logging policy. However, existing off-policy learning\nto rank methods often make strong assumptions about how users generate the\nclick data, i.e., the click model, and hence need to tailor their methods\nspecifically under different click models. In this paper, we unified the\nranking process under general stochastic click models as a Markov Decision\nProcess (MDP), and the optimal ranking could be learned with offline\nreinforcement learning (RL) directly. Building upon this, we leverage offline\nRL techniques for off-policy LTR and propose the Click Model-Agnostic Unified\nOff-policy Learning to Rank (CUOLR) method, which could be easily applied to a\nwide range of click models. Through a dedicated formulation of the MDP, we show\nthat offline RL algorithms can adapt to various click models without complex\ndebiasing techniques and prior knowledge of the model. Results on various\nlarge-scale datasets demonstrate that CUOLR consistently outperforms the\nstate-of-the-art off-policy learning to rank algorithms while maintaining\nconsistency and robustness under different click models.",
        "translated": ""
    },
    {
        "title": "Topic-Centric Explanations for News Recommendation",
        "url": "http://arxiv.org/abs/2306.07506v1",
        "pub_date": "2023-06-13",
        "summary": "News recommender systems (NRS) have been widely applied for online news\nwebsites to help users find relevant articles based on their interests. Recent\nmethods have demonstrated considerable success in terms of recommendation\nperformance. However, the lack of explanation for these recommendations can\nlead to mistrust among users and lack of acceptance of recommendations. To\naddress this issue, we propose a new explainable news model to construct a\ntopic-aware explainable recommendation approach that can both accurately\nidentify relevant articles and explain why they have been recommended, using\ninformation from associated topics. Additionally, our model incorporates two\ncoherence metrics applied to assess topic quality, providing measure of the\ninterpretability of these explanations. The results of our experiments on the\nMIND dataset indicate that the proposed explainable NRS outperforms several\nother baseline systems, while it is also capable of producing interpretable\ntopics compared to those generated by a classical LDA topic model. Furthermore,\nwe present a case study through a real-world example showcasing the usefulness\nof our NRS for generating explanations.",
        "translated": ""
    },
    {
        "title": "Incentivizing High-Quality Content in Online Recommender Systems",
        "url": "http://arxiv.org/abs/2306.07479v1",
        "pub_date": "2023-06-13",
        "summary": "For content recommender systems such as TikTok and YouTube, the platform's\ndecision algorithm shapes the incentives of content producers, including how\nmuch effort the content producers invest in the quality of their content. Many\nplatforms employ online learning, which creates intertemporal incentives, since\ncontent produced today affects recommendations of future content. In this\npaper, we study the incentives arising from online learning, analyzing the\nquality of content produced at a Nash equilibrium. We show that classical\nonline learning algorithms, such as Hedge and EXP3, unfortunately incentivize\nproducers to create low-quality content. In particular, the quality of content\nis upper bounded in terms of the learning rate and approaches zero for typical\nlearning rate schedules. Motivated by this negative result, we design a\ndifferent learning algorithm -- based on punishing producers who create\nlow-quality content -- that correctly incentivizes producers to create\nhigh-quality content. At a conceptual level, our work illustrates the\nunintended impact that a platform's learning algorithm can have on content\nquality and opens the door towards designing platform learning algorithms that\nincentivize the creation of high-quality content.",
        "translated": ""
    },
    {
        "title": "Resources for Brewing BEIR: Reproducible Reference Models and an\n  Official Leaderboard",
        "url": "http://arxiv.org/abs/2306.07471v1",
        "pub_date": "2023-06-13",
        "summary": "BEIR is a benchmark dataset for zero-shot evaluation of information retrieval\nmodels across 18 different domain/task combinations. In recent years, we have\nwitnessed the growing popularity of a representation learning approach to\nbuilding retrieval models, typically using pretrained transformers in a\nsupervised setting. This naturally begs the question: How effective are these\nmodels when presented with queries and documents that differ from the training\ndata? Examples include searching in different domains (e.g., medical or legal\ntext) and with different types of queries (e.g., keywords vs. well-formed\nquestions). While BEIR was designed to answer these questions, our work\naddresses two shortcomings that prevent the benchmark from achieving its full\npotential: First, the sophistication of modern neural methods and the\ncomplexity of current software infrastructure create barriers to entry for\nnewcomers. To this end, we provide reproducible reference implementations that\ncover the two main classes of approaches: learned dense and sparse models.\nSecond, there does not exist a single authoritative nexus for reporting the\neffectiveness of different models on BEIR, which has led to difficulty in\ncomparing different methods. To remedy this, we present an official\nself-service BEIR leaderboard that provides fair and consistent comparisons of\nretrieval models. By addressing both shortcomings, our work facilitates future\nexplorations in a range of interesting research questions that BEIR enables.",
        "translated": ""
    },
    {
        "title": "Web of Things and Trends in Agriculture: A Systematic Literature Review",
        "url": "http://arxiv.org/abs/2306.09079v1",
        "pub_date": "2023-06-15",
        "summary": "In the past few years, the Web of Things (WOT) became a beneficial\ngame-changing technology within the Agriculture domain as it introduces\ninnovative and promising solutions to the Internet of Things (IoT) agricultural\napplications problems by providing its services. WOT provides the support for\nintegration, interoperability for heterogeneous devices, infrastructures,\nplatforms, and the emergence of various other technologies. The main aim of\nthis study is about understanding and providing a growing and existing research\ncontent, issues, and directions for the future regarding WOT-based agriculture.\nTherefore, a systematic literature review (SLR) of research articles is\npresented by categorizing the selected studies published between 2010 and 2020\ninto the following categories: research type, approaches, and their application\ndomains. Apart from reviewing the state-of-the-art articles on WOT solutions\nfor the agriculture field, a taxonomy of WOT-base agriculture application\ndomains has also been presented in this study. A model has also presented to\nshow the picture of WOT based Smart Agriculture. Lastly, the findings of this\nSLR and the research gaps in terms of open issues have been presented to\nprovide suggestions on possible future directions for the researchers for\nfuture research.",
        "translated": ""
    },
    {
        "title": "Fast and Examination-agnostic Reciprocal Recommendation in Matching\n  Markets",
        "url": "http://arxiv.org/abs/2306.09060v1",
        "pub_date": "2023-06-15",
        "summary": "In matching markets such as job posting and online dating platforms, the\nrecommender system plays a critical role in the success of the platform. Unlike\nstandard recommender systems that suggest items to users, reciprocal\nrecommender systems (RRSs) that suggest other users must take into account the\nmutual interests of users. In addition, ensuring that recommendation\nopportunities do not disproportionately favor popular users is essential for\nthe total number of matches and for fairness among users. Existing\nrecommendation methods in matching markets, however, face computational\nchallenges on large-scale platforms and depend on specific examination\nfunctions in the position-based model (PBM). In this paper, we introduce the\nreciprocal recommendation method based on the matching with transferable\nutility (TU matching) model in the context of ranking recommendations in\nmatching markets and propose a fast and examination-model-free algorithm.\nFurthermore, we evaluate our approach on experiments with synthetic data and\nreal-world data from an online dating platform in Japan. Our method performs\nbetter than or as well as existing methods in terms of the total number of\nmatches and works well even in a large-scale dataset for which one existing\nmethod does not work.",
        "translated": ""
    },
    {
        "title": "Mapping Researcher Activity based on Publication Data by means of\n  Transformers",
        "url": "http://arxiv.org/abs/2306.09049v1",
        "pub_date": "2023-06-15",
        "summary": "Modern performance on several natural language processing (NLP) tasks has\nbeen enhanced thanks to the Transformer-based pre-trained language model BERT.\nWe employ this concept to investigate a local publication database. Research\npapers are encoded and clustered to form a landscape view of the scientific\ntopics, in which research is active. Authors working on similar topics can be\nidentified by calculating the similarity between their papers. Based on this,\nwe define a similarity metric between authors. Additionally we introduce the\nconcept of self-similarity to indicate the topical variety of authors.",
        "translated": ""
    },
    {
        "title": "RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation",
        "url": "http://arxiv.org/abs/2306.08947v1",
        "pub_date": "2023-06-15",
        "summary": "In this paper we propose RecFusion, which comprise a set of diffusion models\nfor recommendation. Unlike image data which contain spatial correlations, a\nuser-item interaction matrix, commonly utilized in recommendation, lacks\nspatial relationships between users and items. We formulate diffusion on a 1D\nvector and propose binomial diffusion, which explicitly models binary user-item\ninteractions with a Bernoulli process. We show that RecFusion approaches the\nperformance of complex VAE baselines on the core recommendation setting (top-n\nrecommendation for binary non-sequential feedback) and the most common datasets\n(MovieLens and Netflix). Our proposed diffusion models that are specialized for\n1D and/or binary setups have implications beyond recommendation systems, such\nas in the medical domain with MRI and CT scans.",
        "translated": ""
    },
    {
        "title": "Document Entity Retrieval with Massive and Noisy Pre-training",
        "url": "http://arxiv.org/abs/2306.08937v1",
        "pub_date": "2023-06-15",
        "summary": "Visually-Rich Document Entity Retrieval (VDER) is a type of machine learning\ntask that aims at recovering text spans in the documents for each of the\nentities in question. VDER has gained significant attention in recent years\nthanks to its broad applications in enterprise AI. Unfortunately, as document\nimages often contain personally identifiable information (PII), publicly\navailable data have been scarce, not only because of privacy constraints but\nalso the costs of acquiring annotations. To make things worse, each dataset\nwould often define its own sets of entities, and the non-overlapping entity\nspaces between datasets make it difficult to transfer knowledge between\ndocuments. In this paper, we propose a method to collect massive-scale, noisy,\nand weakly labeled data from the web to benefit the training of VDER models.\nSuch a method will generate a huge amount of document image data to compensate\nfor the lack of training data in many VDER settings. Moreover, the collected\ndataset named DocuNet would not need to be dependent on specific document types\nor entity sets, making it universally applicable to all VDER tasks. Empowered\nby DocuNet, we present a lightweight multimodal architecture named UniFormer,\nwhich can learn a unified representation from text, layout, and image crops\nwithout needing extra visual pertaining. We experiment with our methods on\npopular VDER models in various settings and show the improvements when this\nmassive dataset is incorporated with UniFormer on both classic entity retrieval\nand few-shot learning settings.",
        "translated": ""
    },
    {
        "title": "Community Detection Attack against Collaborative Learning-based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2306.08929v1",
        "pub_date": "2023-06-15",
        "summary": "Collaborative-learning based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while keeping their history of consumed items on their\ndevices. While these solutions seemed appealing for preserving the privacy of\nthe participants at a first glance, recent studies have shown that\ncollaborative learning can be vulnerable to a variety of privacy attacks. In\nthis paper we propose a novel privacy attack called Community Detection Attack\n(CDA), which allows an adversary to discover the members of a community based\non a set of items of her choice (e.g., discovering users interested in LGBT\ncontent). Through experiments on three real recommendation datasets and by\nusing two state-of-the-art recommendation models, we assess the sensitivity of\nan FL-based recommender system as well as two flavors of Gossip Learning-based\nrecommender systems to CDA. Results show that on all models and all datasets,\nthe FL setting is more vulnerable to CDA than Gossip settings. We further\nevaluated two off-the-shelf mitigation strategies, namely differential privacy\n(DP) and a share less policy, which consists in sharing a subset of model\nparameters. Results show a better privacy-utility trade-off for the share less\npolicy compared to DP especially in the Gossip setting.",
        "translated": ""
    },
    {
        "title": "Prompt Performance Prediction for Generative IR",
        "url": "http://arxiv.org/abs/2306.08915v1",
        "pub_date": "2023-06-15",
        "summary": "The ability to predict the performance of a query in Information Retrieval\n(IR) systems has been a longstanding challenge. In this paper, we introduce a\nnovel task called \"Prompt Performance Prediction\" that aims to predict the\nperformance of a query, referred to as a prompt, before obtaining the actual\nsearch results. The context of our task leverages a generative model as an IR\nengine to evaluate the prompts' performance on image retrieval tasks. We\ndemonstrate the plausibility of our task by measuring the correlation\ncoefficient between predicted and actual performance scores across three\ndatasets containing pairs of prompts and generated images. Our results show\npromising performance prediction capabilities, suggesting potential\napplications for optimizing generative IR systems.",
        "translated": ""
    },
    {
        "title": "Description-Enhanced Label Embedding Contrastive Learning for Text\n  Classification",
        "url": "http://arxiv.org/abs/2306.08817v1",
        "pub_date": "2023-06-15",
        "summary": "Text Classification is one of the fundamental tasks in natural language\nprocessing, which requires an agent to determine the most appropriate category\nfor input sentences. Recently, deep neural networks have achieved impressive\nperformance in this area, especially Pre-trained Language Models (PLMs).\nUsually, these methods concentrate on input sentences and corresponding\nsemantic embedding generation. However, for another essential component:\nlabels, most existing works either treat them as meaningless one-hot vectors or\nuse vanilla embedding methods to learn label representations along with model\ntraining, underestimating the semantic information and guidance that these\nlabels reveal. To alleviate this problem and better exploit label information,\nin this paper, we employ Self-Supervised Learning (SSL) in model learning\nprocess and design a novel self-supervised Relation of Relation (R2)\nclassification task for label utilization from a one-hot manner perspective.\nThen, we propose a novel Relation of Relation Learning Network (R2-Net) for\ntext classification, in which text classification and R2 classification are\ntreated as optimization targets. Meanwhile, triplet loss is employed to enhance\nthe analysis of differences and connections among labels. Moreover, considering\nthat one-hot usage is still short of exploiting label information, we\nincorporate external knowledge from WordNet to obtain multi-aspect descriptions\nfor label semantic learning and extend R2-Net to a novel Description-Enhanced\nLabel Embedding network (DELE) from a label embedding perspective. ...",
        "translated": ""
    },
    {
        "title": "ReLoop2: Building Self-Adaptive Recommendation Models via Responsive\n  Error Compensation Loop",
        "url": "http://arxiv.org/abs/2306.08808v1",
        "pub_date": "2023-06-15",
        "summary": "Industrial recommender systems face the challenge of operating in\nnon-stationary environments, where data distribution shifts arise from evolving\nuser behaviors over time. To tackle this challenge, a common approach is to\nperiodically re-train or incrementally update deployed deep models with newly\nobserved data, resulting in a continual training process. However, the\nconventional learning paradigm of neural networks relies on iterative\ngradient-based updates with a small learning rate, making it slow for large\nrecommendation models to adapt. In this paper, we introduce ReLoop2, a\nself-correcting learning loop that facilitates fast model adaptation in online\nrecommender systems through responsive error compensation. Inspired by the\nslow-fast complementary learning system observed in human brains, we propose an\nerror memory module that directly stores error samples from incoming data\nstreams. These stored samples are subsequently leveraged to compensate for\nmodel prediction errors during testing, particularly under distribution shifts.\nThe error memory module is designed with fast access capabilities and undergoes\ncontinual refreshing with newly observed data samples during the model serving\nphase to support fast model adaptation. We evaluate the effectiveness of\nReLoop2 on three open benchmark datasets as well as a real-world production\ndataset. The results demonstrate the potential of ReLoop2 in enhancing the\nresponsiveness and adaptiveness of recommender systems operating in\nnon-stationary environments.",
        "translated": ""
    },
    {
        "title": "Learning to Rank when Grades Matter",
        "url": "http://arxiv.org/abs/2306.08650v1",
        "pub_date": "2023-06-14",
        "summary": "Graded labels are ubiquitous in real-world learning-to-rank applications,\nespecially in human rated relevance data. Traditional learning-to-rank\ntechniques aim to optimize the ranked order of documents. They typically,\nhowever, ignore predicting actual grades. This prevents them from being adopted\nin applications where grades matter, such as filtering out ``poor'' documents.\nAchieving both good ranking performance and good grade prediction performance\nis still an under-explored problem. Existing research either focuses only on\nranking performance by not calibrating model outputs, or treats grades as\nnumerical values, assuming labels are on a linear scale and failing to leverage\nthe ordinal grade information. In this paper, we conduct a rigorous study of\nlearning to rank with grades, where both ranking performance and grade\nprediction performance are important. We provide a formal discussion on how to\nperform ranking with non-scalar predictions for grades, and propose a\nmultiobjective formulation to jointly optimize both ranking and grade\npredictions. In experiments, we verify on several public datasets that our\nmethods are able to push the Pareto frontier of the tradeoff between ranking\nand grade prediction performance, showing the benefit of leveraging ordinal\ngrade information.",
        "translated": ""
    },
    {
        "title": "GRM: Generative Relevance Modeling Using Relevance-Aware Sample\n  Estimation for Document Retrieval",
        "url": "http://arxiv.org/abs/2306.09938v1",
        "pub_date": "2023-06-16",
        "summary": "Recent studies show that Generative Relevance Feedback (GRF), using text\ngenerated by Large Language Models (LLMs), can enhance the effectiveness of\nquery expansion. However, LLMs can generate irrelevant information that harms\nretrieval effectiveness. To address this, we propose Generative Relevance\nModeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more\naccurate weighting of expansion terms. Specifically, we identify similar real\ndocuments for each generated document and use a neural re-ranker to estimate\ntheir relevance. Experiments on three standard document ranking benchmarks show\nthat GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.",
        "translated": ""
    },
    {
        "title": "Smart Sentiment Analysis-based Search Engine Classification Intelligence",
        "url": "http://arxiv.org/abs/2306.09777v1",
        "pub_date": "2023-06-16",
        "summary": "Search engines are widely used for finding information on the internet.\nHowever, there are limitations in the current search approach, such as\nproviding popular but not necessarily relevant results. This research addresses\nthe issue of polysemy in search results by implementing a search function that\ndetermines the sentimentality of the retrieved information. The study utilizes\na web crawler to collect data from the British Broadcasting Corporation (BBC)\nnews site, and the sentimentality of the news articles is determined using the\nSentistrength program. The results demonstrate that the proposed search\nfunction improves recall value while accurately retrieving nonpolysemous news.\nFurthermore, Sentistrength outperforms deep learning and clustering methods in\nclassifying search results. The methodology presented in this article can be\napplied to analyze the sentimentality and reputation of entities on the\ninternet.",
        "translated": ""
    },
    {
        "title": "Online Distillation for Pseudo-Relevance Feedback",
        "url": "http://arxiv.org/abs/2306.09657v1",
        "pub_date": "2023-06-16",
        "summary": "Model distillation has emerged as a prominent technique to improve neural\nsearch models. To date, distillation taken an offline approach, wherein a new\nneural model is trained to predict relevance scores between arbitrary queries\nand documents. In this paper, we explore a departure from this offline\ndistillation strategy by investigating whether a model for a specific query can\nbe effectively distilled from neural re-ranking results (i.e., distilling in an\nonline setting). Indeed, we find that a lexical model distilled online can\nreasonably replicate the re-ranking of a neural model. More importantly, these\nmodels can be used as queries that execute efficiently on indexes. This second\nretrieval stage can enrich the pool of documents for re-ranking by identifying\ndocuments that were missed in the first retrieval stage. Empirically, we show\nthat this approach performs favourably when compared with established pseudo\nrelevance feedback techniques, dense retrieval methods, and sparse-dense\nensemble \"hybrid\" approaches.",
        "translated": ""
    },
    {
        "title": "I Want This, Not That: Personalized Summarization of Scientific\n  Scholarly Texts",
        "url": "http://arxiv.org/abs/2306.09604v1",
        "pub_date": "2023-06-16",
        "summary": "In this paper, we present a proposal for an unsupervised algorithm, P-Summ,\nthat generates an extractive summary of scientific scholarly text to meet the\npersonal knowledge needs of the user. The method delves into the latent\nsemantic space of the document exposed by Weighted Non-negative Matrix\nFactorization, and scores sentences in consonance with the knowledge needs of\nthe user. The novelty of the algorithm lies in its ability to include desired\nknowledge and eliminate unwanted knowledge in the personal summary.\n  We also propose a multi-granular evaluation framework, which assesses the\nquality of generated personal summaries at three levels of granularity -\nsentence, terms and semantic. The framework uses system generated generic\nsummary instead of human generated summary as gold standard for evaluating the\nquality of personal summary generated by the algorithm. The effectiveness of\nthe algorithm at the semantic level is evaluated by taking into account the\nreference summary and the knowledge signals. We evaluate the performance of\nP-Summ algorithm over four data-sets consisting of scientific articles. Our\nempirical investigations reveal that the proposed method has the capability to\nmeet negative (or positive) knowledge preferences of the user.",
        "translated": ""
    },
    {
        "title": "Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive\n  Volume Lines",
        "url": "http://arxiv.org/abs/2306.11612v1",
        "pub_date": "2023-06-20",
        "summary": "To visually compare ensembles of volumes, dynamic volume lines (DVLs)\nrepresent each ensemble member as a 1D polyline. To compute these, the volume\ncells are sorted on a space-filling curve and scaled by the ensemble's local\nvariation. The resulting 1D plot can augment or serve as an alternative to a 3D\nvolume visualization free of visual clutter and occlusion. Interactively\ncomputing DVLs is challenging when the data is large, and the volume grid is\nnot structured/regular, as is often the case with computational fluid dynamics\nsimulations. We extend DVLs to support large-scale, multi-field adaptive mesh\nrefinement (AMR) data that can be explored interactively. Our GPU-based system\nupdates the DVL representation whenever the data or the alpha transfer function\nchanges. We demonstrate and evaluate our interactive prototype using large AMR\nvolumes from astrophysics simulations.",
        "translated": ""
    },
    {
        "title": "Mining Interest Trends and Adaptively Assigning SampleWeight for\n  Session-based Recommendation",
        "url": "http://arxiv.org/abs/2306.11610v1",
        "pub_date": "2023-06-20",
        "summary": "Session-based Recommendation (SR) aims to predict users' next click based on\ntheir behavior within a short period, which is crucial for online platforms.\nHowever, most existing SR methods somewhat ignore the fact that user preference\nis not necessarily strongly related to the order of interactions. Moreover,\nthey ignore the differences in importance between different samples, which\nlimits the model-fitting performance. To tackle these issues, we put forward\nthe method, Mining Interest Trends and Adaptively Assigning Sample Weight,\nabbreviated as MTAW. Specifically, we model users' instant interest based on\ntheir present behavior and all their previous behaviors. Meanwhile, we\ndiscriminatively integrate instant interests to capture the changing trend of\nuser interest to make more personalized recommendations. Furthermore, we devise\na novel loss function that dynamically weights the samples according to their\nprediction difficulty in the current epoch. Extensive experimental results on\ntwo benchmark datasets demonstrate the effectiveness and superiority of our\nmethod.",
        "translated": ""
    },
    {
        "title": "Polytope: An Algorithm for Efficient Feature Extraction on Hypercubes",
        "url": "http://arxiv.org/abs/2306.11553v1",
        "pub_date": "2023-06-20",
        "summary": "Data extraction algorithms on data hypercubes, or datacubes, are\ntraditionally only capable of cutting boxes of data along the datacube axes.\nFor many use cases however, this is not a sufficient approach and returns more\ndata than users might actually need. This not only forces users to apply\npost-processing after extraction, but more importantly this consumes more I/O\nresources than is necessary. When considering very large datacubes from which\nusers only want to extract small non-rectangular subsets, the box approach does\nnot scale well. Indeed, with this traditional approach, I/O systems quickly\nreach capacity, trying to read and return unwanted data to users. In this\npaper, we propose a novel technique, based on computational geometry concepts,\nwhich instead carefully pre-selects the precise bytes of data which the user\nneeds in order to then only read those from the datacube. As we discuss later\non, this novel extraction method will considerably help scale access to large\npetabyte size data hypercubes in a variety of scientific fields.",
        "translated": ""
    },
    {
        "title": "Generative Retrieval as Dense Retrieval",
        "url": "http://arxiv.org/abs/2306.11397v1",
        "pub_date": "2023-06-20",
        "summary": "Generative retrieval is a promising new neural retrieval paradigm that aims\nto optimize the retrieval pipeline by performing both indexing and retrieval\nwith a single transformer model. However, this new paradigm faces challenges\nwith updating the index and scaling to large collections. In this paper, we\nanalyze two prominent variants of generative retrieval and show that they can\nbe conceptually viewed as bi-encoders for dense retrieval. Specifically, we\nanalytically demonstrate that the generative retrieval process can be\ndecomposed into dot products between query and document vectors, similar to\ndense retrieval. This analysis leads us to propose a new variant of generative\nretrieval, called Tied-Atomic, which addresses the updating and scaling issues\nby incorporating techniques from dense retrieval. In experiments on two\ndatasets, NQ320k and the full MSMARCO, we confirm that this approach does not\nreduce retrieval effectiveness while enabling the model to scale to large\ncollections.",
        "translated": ""
    },
    {
        "title": "CAPRI: Context-Aware Interpretable Point-of-Interest Recommendation\n  Framework",
        "url": "http://arxiv.org/abs/2306.11395v1",
        "pub_date": "2023-06-20",
        "summary": "Point-of-Interest (POI ) recommendation systems have gained popularity for\ntheir unique ability to suggest geographical destinations with the\nincorporation of contextual information such as time, location, and user-item\ninteraction. Existing recommendation frameworks lack the contextual fusion\nrequired for POI systems. This paper presents CAPRI, a novel POI recommendation\nframework that effectively integrates context-aware models, such as GeoSoCa,\nLORE, and USG, and introduces a novel strategy for the efficient merging of\ncontextual information. CAPRI integrates an evaluation module that expands the\nevaluation scope beyond accuracy to include novelty, personalization,\ndiversity, and fairness. With an aim to establish a new industry standard for\nreproducible results in the realm of POI recommendation systems, we have made\nCAPRI openly accessible on GitHub, facilitating easy access and contribution to\nthe continued development and refinement of this innovative framework.",
        "translated": ""
    },
    {
        "title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF\n  Synthesis",
        "url": "http://arxiv.org/abs/2306.11296v1",
        "pub_date": "2023-06-20",
        "summary": "We use prompt engineering to guide ChatGPT in the automation of text mining\nof metal-organic frameworks (MOFs) synthesis conditions from diverse formats\nand styles of the scientific literature. This effectively mitigates ChatGPT's\ntendency to hallucinate information -- an issue that previously made the use of\nLarge Language Models (LLMs) in scientific fields challenging. Our approach\ninvolves the development of a workflow implementing three different processes\nfor text mining, programmed by ChatGPT itself. All of them enable parsing,\nsearching, filtering, classification, summarization, and data unification with\ndifferent tradeoffs between labor, speed, and accuracy. We deploy this system\nto extract 26,257 distinct synthesis parameters pertaining to approximately 800\nMOFs sourced from peer-reviewed research articles. This process incorporates\nour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,\nresulting in impressive precision, recall, and F1 scores of 90-99%.\nFurthermore, with the dataset built by text mining, we constructed a\nmachine-learning model with over 86% accuracy in predicting MOF experimental\ncrystallization outcomes and preliminarily identifying important factors in MOF\ncrystallization. We also developed a reliable data-grounded MOF chatbot to\nanswer questions on chemical reactions and synthesis procedures. Given that the\nprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesis\ninformation in a unified format, while using only narrative language requiring\nno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be\nvery useful across various other chemistry sub-disciplines.",
        "translated": ""
    },
    {
        "title": "Representation Sparsification with Hybrid Thresholding for Fast\n  SPLADE-based Document Retrieval",
        "url": "http://arxiv.org/abs/2306.11293v1",
        "pub_date": "2023-06-20",
        "summary": "Learned sparse document representations using a transformer-based neural\nmodel has been found to be attractive in both relevance effectiveness and time\nefficiency. This paper describes a representation sparsification scheme based\non hard and soft thresholding with an inverted index approximation for faster\nSPLADE-based document retrieval. It provides analytical and experimental\nresults on the impact of this learnable hybrid thresholding scheme.",
        "translated": ""
    },
    {
        "title": "Less Can Be More: Exploring Population Rating Dispositions with\n  Partitioned Models in Recommender Systems",
        "url": "http://arxiv.org/abs/2306.11279v1",
        "pub_date": "2023-06-20",
        "summary": "In this study, we partition users by rating disposition - looking first at\ntheir percentage of negative ratings, and then at the general use of the rating\nscale. We hypothesize that users with different rating dispositions may use the\nrecommender system differently and therefore the agreement with their past\nratings may be less predictive of the future agreement.\n  We use data from a large movie rating website to explore whether users should\nbe grouped by disposition, focusing on identifying their various rating\ndistributions that may hurt recommender effectiveness. We find that such\npartitioning not only improves computational efficiency but also improves top-k\nperformance and predictive accuracy. Though such effects are largest for the\nuser-based KNN CF, smaller for item-based KNN CF, and smallest for latent\nfactor algorithms such as SVD.",
        "translated": ""
    },
    {
        "title": "Hybrid Multi-Criteria Preference Ranking by Subsorting",
        "url": "http://arxiv.org/abs/2306.11233v1",
        "pub_date": "2023-06-20",
        "summary": "Multi-criteria recommender systems can improve the quality of recommendations\nby considering user preferences on multiple criteria. One promising approach\nproposed recently is multi-criteria ranking, which uses Pareto ranking to\nassign a ranking score based on the dominance relationship between predicted\nratings across criteria. However, applying Pareto ranking to all criteria may\nresult in non-differentiable ranking scores. To alleviate this issue, we\nproposed a hybrid multi-criteria ranking method by using subsorting. More\nspecifically, we utilize one ranking method as the major sorting approach,\nwhile we apply another preference ordering method as subsorting. Our\nexperimental results on the OpenTable and Yahoo!Movies data present the\nadvantages of this hybrid ranking approach. In addition, the experiments also\nreveal more insights about the sustainability of the multi-criteria ranking for\ntop-N item recommendations.",
        "translated": ""
    },
    {
        "title": "Co-design Hardware and Algorithm for Vector Search",
        "url": "http://arxiv.org/abs/2306.11182v1",
        "pub_date": "2023-06-19",
        "summary": "Vector search has emerged as the foundation for large-scale information\nretrieval and machine learning systems, with search engines like Google and\nBing processing tens of thousands of queries per second on petabyte-scale\ndocument datasets by evaluating vector similarities between encoded query texts\nand web documents. As performance demands for vector search systems surge,\naccelerated hardware offers a promising solution in the post-Moore's Law era.\nWe introduce \\textit{FANNS}, an end-to-end and scalable vector search framework\non FPGAs. Given a user-provided recall requirement on a dataset and a hardware\nresource budget, \\textit{FANNS} automatically co-designs hardware and\nalgorithm, subsequently generating the corresponding accelerator. The framework\nalso supports scale-out by incorporating a hardware TCP/IP stack in the\naccelerator. \\textit{FANNS} attains up to 23.0$\\times$ and 37.2$\\times$ speedup\ncompared to FPGA and CPU baselines, respectively, and demonstrates superior\nscalability to GPUs, achieving 5.5$\\times$ and 7.6$\\times$ speedup in median\nand 95\\textsuperscript{th} percentile (P95) latency within an eight-accelerator\nconfiguration. The remarkable performance of \\textit{FANNS} lays a robust\ngroundwork for future FPGA integration in data centers and AI supercomputers.",
        "translated": ""
    },
    {
        "title": "Knowledge-based Multimodal Music Similarity",
        "url": "http://arxiv.org/abs/2306.12249v1",
        "pub_date": "2023-06-21",
        "summary": "Music similarity is an essential aspect of music retrieval, recommendation\nsystems, and music analysis. Moreover, similarity is of vital interest for\nmusic experts, as it allows studying analogies and influences among composers\nand historical periods. Current approaches to musical similarity rely mainly on\nsymbolic content, which can be expensive to produce and is not always readily\navailable. Conversely, approaches using audio signals typically fail to provide\nany insight about the reasons behind the observed similarity. This research\naddresses the limitations of current approaches by focusing on the study of\nmusical similarity using both symbolic and audio content. The aim of this\nresearch is to develop a fully explainable and interpretable system that can\nprovide end-users with more control and understanding of music similarity and\nclassification systems.",
        "translated": ""
    },
    {
        "title": "CompMix: A Benchmark for Heterogeneous Question Answering",
        "url": "http://arxiv.org/abs/2306.12235v1",
        "pub_date": "2023-06-21",
        "summary": "Fact-centric question answering (QA) often requires access to multiple,\nheterogeneous, information sources. By jointly considering several sources like\na knowledge base (KB), a text collection, and tables from the web, QA systems\ncan enhance their answer coverage and confidence. However, existing QA\nbenchmarks are mostly constructed with a single source of knowledge in mind.\nThis limits capabilities of these benchmarks to fairly evaluate QA systems that\ncan tap into more than one information repository. To bridge this gap, we\nrelease CompMix, a crowdsourced QA benchmark which naturally demands the\nintegration of a mixture of input sources. CompMix has a total of 9,410\nquestions, and features several complex intents like joins and temporal\nconditions. Evaluation of a range of QA systems on CompMix highlights the need\nfor further research on leveraging information from heterogeneous sources.",
        "translated": ""
    },
    {
        "title": "STAN: Stage-Adaptive Network for Multi-Task Recommendation by Learning\n  User Lifecycle-Based Representation",
        "url": "http://arxiv.org/abs/2306.12232v1",
        "pub_date": "2023-06-21",
        "summary": "Recommendation systems play a vital role in many online platforms, with their\nprimary objective being to satisfy and retain users. As directly optimizing\nuser retention is challenging, multiple evaluation metrics are often employed.\nExisting methods generally formulate the optimization of these evaluation\nmetrics as a multitask learning problem, but often overlook the fact that user\npreferences for different tasks are personalized and change over time.\nIdentifying and tracking the evolution of user preferences can lead to better\nuser retention. To address this issue, we introduce the concept of \"user\nlifecycle\", consisting of multiple stages characterized by users' varying\npreferences for different tasks. We propose a novel Stage-Adaptive Network\n(STAN) framework for modeling user lifecycle stages. STAN first identifies\nlatent user lifecycle stages based on learned user preferences, and then\nemploys the stage representation to enhance multi-task learning performance.\nOur experimental results using both public and industrial datasets demonstrate\nthat the proposed model significantly improves multi-task prediction\nperformance compared to state-of-the-art methods, highlighting the importance\nof considering user lifecycle stages in recommendation systems. Furthermore,\nonline A/B testing reveals that our model outperforms the existing model,\nachieving a significant improvement of 3.05% in staytime per user and 0.88% in\nCVR. These results indicate that our approach effectively improves the overall\nefficiency of the multi-task recommendation system.",
        "translated": ""
    },
    {
        "title": "Post-hoc Selection of Pareto-Optimal Solutions in Search and\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.12165v1",
        "pub_date": "2023-06-21",
        "summary": "Information Retrieval (IR) and Recommender Systems (RS) tasks are moving from\ncomputing a ranking of final results based on a single metric to\nmulti-objective problems. Solving these problems leads to a set of\nPareto-optimal solutions, known as Pareto frontier, in which no objective can\nbe further improved without hurting the others. In principle, all the points on\nthe Pareto frontier are potential candidates to represent the best model\nselected with respect to the combination of two, or more, metrics. To our\nknowledge, there are no well-recognized strategies to decide which point should\nbe selected on the frontier. In this paper, we propose a novel, post-hoc,\ntheoretically-justified technique, named \"Population Distance from Utopia\"\n(PDU), to identify and select the one-best Pareto-optimal solution from the\nfrontier. In detail, PDU analyzes the distribution of the points by\ninvestigating how far each point is from its utopia point (the ideal\nperformance for the objectives). The possibility of considering fine-grained\nutopia points allows PDU to select solutions tailored to individual user\npreferences, a novel feature we call \"calibration\". We compare PDU against\nexisting state-of-the-art strategies through extensive experiments on tasks\nfrom both IR and RS. Experimental results show that PDU and combined with\ncalibration notably impact the solution selection. Furthermore, the results\nshow that the proposed framework selects a solution in a principled way,\nirrespective of its position on the frontier, thus overcoming the limits of\nother strategies.",
        "translated": ""
    },
    {
        "title": "Visualizing Relation Between (De)Motivating Topics and Public Stance\n  toward COVID-19 Vaccine",
        "url": "http://arxiv.org/abs/2306.12118v1",
        "pub_date": "2023-06-21",
        "summary": "While social media plays a vital role in communication nowadays,\nmisinformation and trolls can easily take over the conversation and steer\npublic opinion on these platforms. We saw the effect of misinformation during\nthe {COVID-19} pandemic when public health officials faced significant\npush-back while trying to motivate the public to vaccinate. To tackle the\ncurrent and any future threats in emergencies and motivate the public towards a\ncommon goal, it is essential to understand how public motivation shifts and\nwhich topics resonate among the general population. In this study, we proposed\nan interactive visualization tool to inspect and analyze the topics that\nresonated among Twitter-sphere during the {COVID-19} pandemic and understand\nthe key factors that shifted public stance for vaccination. This tool can\neasily be generalized for any scenario for visual analysis and to increase the\ntransparency of social media data for researchers and the general population\nalike.",
        "translated": ""
    },
    {
        "title": "Comparative analysis of various web crawler algorithms",
        "url": "http://arxiv.org/abs/2306.12027v1",
        "pub_date": "2023-06-21",
        "summary": "This presentation focuses on the importance of web crawling and page ranking\nalgorithms in dealing with the massive amount of data present on the World Wide\nWeb. As the web continues to grow exponentially, efficient search and retrieval\nmethods become crucial. Web crawling is a process that converts unstructured\ndata into structured data, enabling effective information retrieval.\nAdditionally, page ranking algorithms play a significant role in assessing the\nquality and popularity of web pages. The presentation explores the background\nof these algorithms and evaluates five different crawling algorithms: Shark\nSearch, Priority-Based Queue, Naive Bayes, Breadth-First, and Depth-First. The\ngoal is to identify the most effective algorithm for crawling web pages. By\nunderstanding these algorithms, we can enhance our ability to navigate the web\nand extract valuable information efficiently.",
        "translated": ""
    },
    {
        "title": "Addressing the Rank Degeneration in Sequential Recommendation via\n  Singular Spectrum Smoothing",
        "url": "http://arxiv.org/abs/2306.11986v1",
        "pub_date": "2023-06-21",
        "summary": "Sequential recommendation (SR) investigates the dynamic user preferences\nmodeling and generates the next-item prediction. The next item preference is\ntypically generated by the affinity between the sequence and item\nrepresentations. However, both sequence and item representations suffer from\nthe rank degeneration issue due to the data sparsity problem. The rank\ndegeneration issue significantly impairs the representations for SR. This\nmotivates us to measure how severe is the rank degeneration issue and alleviate\nthe sequence and item representation rank degeneration issues simultaneously\nfor SR.\n  In this work, we theoretically connect the sequence representation\ndegeneration issue with the item rank degeneration, particularly for short\nsequences and cold items. We also identify the connection between the fast\nsingular value decay phenomenon and the rank collapse issue in transformer\nsequence output and item embeddings. We propose the area under the singular\nvalue curve metric to evaluate the severity of the singular value decay\nphenomenon and use it as an indicator of rank degeneration. We further\nintroduce a novel singular spectrum smoothing regularization to alleviate the\nrank degeneration on both sequence and item sides, which is the Singular\nsPectrum sMoothing for sequential Recommendation (SPMRec). We also establish a\ncorrelation between the ranks of sequence and item embeddings and the rank of\nthe user-item preference prediction matrix, which can affect recommendation\ndiversity. We conduct experiments on four benchmark datasets to demonstrate the\nsuperiority of SPMRec over the state-of-the-art recommendation methods,\nespecially in short sequences. The experiments also demonstrate a strong\nconnection between our proposed singular spectrum smoothing and recommendation\ndiversity.",
        "translated": ""
    },
    {
        "title": "Sampling Individually-Fair Rankings that are Always Group Fair",
        "url": "http://arxiv.org/abs/2306.11964v1",
        "pub_date": "2023-06-21",
        "summary": "Rankings on online platforms help their end-users find the relevant\ninformation -- people, news, media, and products -- quickly. Fair ranking\ntasks, which ask to rank a set of items to maximize utility subject to\nsatisfying group-fairness constraints, have gained significant interest in the\nAlgorithmic Fairness, Information Retrieval, and Machine Learning literature.\nRecent works, however, identify uncertainty in the utilities of items as a\nprimary cause of unfairness and propose introducing randomness in the output.\nThis randomness is carefully chosen to guarantee an adequate representation of\neach item (while accounting for the uncertainty). However, due to this\nrandomness, the output rankings may violate group fairness constraints. We give\nan efficient algorithm that samples rankings from an individually-fair\ndistribution while ensuring that every output ranking is group fair. The\nexpected utility of the output ranking is at least $\\alpha$ times the utility\nof the optimal fair solution. Here, $\\alpha$ depends on the utilities,\nposition-discounts, and constraints -- it approaches 1 as the range of\nutilities or the position-discounts shrinks, or when utilities satisfy\ndistributional assumptions. Empirically, we observe that our algorithm achieves\nindividual and group fairness and that Pareto dominates the state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "Multimodality Fusion for Smart Healthcare: a Journey from Data,\n  Information, Knowledge to Wisdom",
        "url": "http://arxiv.org/abs/2306.11963v1",
        "pub_date": "2023-06-21",
        "summary": "Multimodal medical data fusion has emerged as a transformative approach in\nsmart healthcare, enabling a comprehensive understanding of patient health and\npersonalized treatment plans. In this paper, a journey from data, information,\nand knowledge to wisdom (DIKW) is explored through multimodal fusion for smart\nhealthcare. A comprehensive review of multimodal medical data fusion focuses on\nthe integration of various data modalities are presented. It explores different\napproaches such as Feature selection, Rule-based systems, Machine learning,\nDeep learning, and Natural Language Processing for fusing and analyzing\nmultimodal data. The paper also highlights the challenges associated with\nmultimodal fusion in healthcare. By synthesizing the reviewed frameworks and\ninsights, a generic framework for multimodal medical data fusion is proposed\nwhile aligning with the DIKW mechanism. Moreover, it discusses future\ndirections aligned with the four pillars of healthcare: Predictive, Preventive,\nPersonalized, and Participatory approaches based on the DIKW and the generic\nframework. The components from this comprehensive survey form the foundation\nfor the successful implementation of multimodal fusion in smart healthcare. The\nfindings of this survey can guide researchers and practitioners in leveraging\nthe power of multimodal fusion with the approaches to revolutionize healthcare\nand improve patient outcomes.",
        "translated": ""
    },
    {
        "title": "Retrieval-Based Transformer for Table Augmentation",
        "url": "http://arxiv.org/abs/2306.11843v1",
        "pub_date": "2023-06-20",
        "summary": "Data preparation, also called data wrangling, is considered one of the most\nexpensive and time-consuming steps when performing analytics or building\nmachine learning models. Preparing data typically involves collecting and\nmerging data from complex heterogeneous, and often large-scale data sources,\nsuch as data lakes. In this paper, we introduce a novel approach toward\nautomatic data wrangling in an attempt to alleviate the effort of end-users,\ne.g. data analysts, in structuring dynamic views from data lakes in the form of\ntabular data. We aim to address table augmentation tasks, including row/column\npopulation and data imputation. Given a corpus of tables, we propose a\nretrieval augmented self-trained transformer model. Our self-learning strategy\nconsists in randomly ablating tables from the corpus and training the\nretrieval-based model to reconstruct the original values or headers given the\npartial tables as input. We adopt this strategy to first train the dense neural\nretrieval model encoding table-parts to vectors, and then the end-to-end model\ntrained to perform table augmentation tasks. We test on EntiTables, the\nstandard benchmark for table augmentation, as well as introduce a new benchmark\nto advance further research: WebTables. Our model consistently and\nsubstantially outperforms both supervised statistical methods and the current\nstate-of-the-art transformer-based models.",
        "translated": ""
    },
    {
        "title": "Data augmentation for recommender system: A semi-supervised approach\n  using maximum margin matrix factorization",
        "url": "http://arxiv.org/abs/2306.13050v1",
        "pub_date": "2023-06-22",
        "summary": "Collaborative filtering (CF) has become a popular method for developing\nrecommender systems (RS) where ratings of a user for new items is predicted\nbased on her past preferences and available preference information of other\nusers. Despite the popularity of CF-based methods, their performance is often\ngreatly limited by the sparsity of observed entries. In this study, we explore\nthe data augmentation and refinement aspects of Maximum Margin Matrix\nFactorization (MMMF), a widely accepted CF technique for the rating\npredictions, which have not been investigated before. We exploit the inherent\ncharacteristics of CF algorithms to assess the confidence level of individual\nratings and propose a semi-supervised approach for rating augmentation based on\nself-training. We hypothesize that any CF algorithm's predictions with low\nconfidence are due to some deficiency in the training data and hence, the\nperformance of the algorithm can be improved by adopting a systematic data\naugmentation strategy. We iteratively use some of the ratings predicted with\nhigh confidence to augment the training data and remove low-confidence entries\nthrough a refinement process. By repeating this process, the system learns to\nimprove prediction accuracy. Our method is experimentally evaluated on several\nstate-of-the-art CF algorithms and leads to informative rating augmentation,\nimproving the performance of the baseline approaches.",
        "translated": ""
    },
    {
        "title": "Efficient Partitioning Method of Large-Scale Public Safety\n  Spatio-Temporal Data based on Information Loss Constraints",
        "url": "http://arxiv.org/abs/2306.12857v1",
        "pub_date": "2023-06-22",
        "summary": "The storage, management, and application of massive spatio-temporal data are\nwidely applied in various practical scenarios, including public safety.\nHowever, due to the unique spatio-temporal distribution characteristics of\nre-al-world data, most existing methods have limitations in terms of the\nspatio-temporal proximity of data and load balancing in distributed storage.\nThere-fore, this paper proposes an efficient partitioning method of large-scale\npublic safety spatio-temporal data based on information loss constraints\n(IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal\npoint da-ta by combining the spatio-temporal partitioning module (STPM) with\nthe graph partitioning module (GPM). This approach can significantly reduce the\nscale of data while maintaining the model's accuracy, in order to improve the\npartitioning efficiency. It can also ensure the load balancing of distributed\nstorage while maintaining spatio-temporal proximity of the data partitioning\nresults. This method provides a new solution for distributed storage of\nmas-sive spatio-temporal data. The experimental results on multiple real-world\nda-tasets demonstrate the effectiveness and superiority of IFL-LSTP.",
        "translated": ""
    },
    {
        "title": "HypeRS: Building a Hypergraph-driven ensemble Recommender System",
        "url": "http://arxiv.org/abs/2306.12800v1",
        "pub_date": "2023-06-22",
        "summary": "Recommender systems are designed to predict user preferences over collections\nof items. These systems process users' previous interactions to decide which\nitems should be ranked higher to satisfy their desires. An ensemble recommender\nsystem can achieve great recommendation performance by effectively combining\nthe decisions generated by individual models. In this paper, we propose a novel\nensemble recommender system that combines predictions made by different models\ninto a unified hypergraph ranking framework. This is the first time that\nhypergraph ranking has been employed to model an ensemble of recommender\nsystems. Hypergraphs are generalizations of graphs where multiple vertices can\nbe connected via hyperedges, efficiently modeling high-order relations. We\ndifferentiate real and predicted connections between users and items by\nassigning different hyperedge weights to individual recommender systems. We\nperform experiments using four datasets from the fields of movie, music and\nnews media recommendation. The obtained results show that the ensemble\nhypergraph ranking method generates more accurate recommendations compared to\nthe individual models and a weighted hybrid approach. The assignment of\ndifferent hyperedge weights to the ensemble hypergraph further improves the\nperformance compared to a setting with identical hyperedge weights.",
        "translated": ""
    },
    {
        "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution\n  Perspective",
        "url": "http://arxiv.org/abs/2306.12756v1",
        "pub_date": "2023-06-22",
        "summary": "Recently, we have witnessed generative retrieval increasingly gaining\nattention in the information retrieval (IR) field, which retrieves documents by\ndirectly generating their identifiers. So far, much effort has been devoted to\ndeveloping effective generative retrieval models. There has been less attention\npaid to the robustness perspective. When a new retrieval paradigm enters into\nthe real-world application, it is also critical to measure the\nout-of-distribution (OOD) generalization, i.e., how would generative retrieval\nmodels generalize to new distributions. To answer this question, firstly, we\ndefine OOD robustness from three perspectives in retrieval problems: 1) The\nquery variations; 2) The unforeseen query types; and 3) The unforeseen tasks.\nBased on this taxonomy, we conduct empirical studies to analyze the OOD\nrobustness of several representative generative retrieval models against dense\nretrieval models. The empirical results indicate that the OOD robustness of\ngenerative retrieval models requires enhancement. We hope studying the OOD\nrobustness of generative retrieval models would be advantageous to the IR\ncommunity.",
        "translated": ""
    },
    {
        "title": "Vec2Vec: A Compact Neural Network Approach for Transforming Text\n  Embeddings with High Fidelity",
        "url": "http://arxiv.org/abs/2306.12689v1",
        "pub_date": "2023-06-22",
        "summary": "Vector embeddings have become ubiquitous tools for many language-related\ntasks. A leading embedding model is OpenAI's text-ada-002 which can embed\napproximately 6,000 words into a 1,536-dimensional vector. While powerful,\ntext-ada-002 is not open source and is only available via API. We trained a\nsimple neural network to convert open-source 768-dimensional MPNet embeddings\ninto text-ada-002 embeddings. We compiled a subset of 50,000 online food\nreviews. We calculated MPNet and text-ada-002 embeddings for each review and\ntrained a simple neural network to for 75 epochs. The neural network was\ndesigned to predict the corresponding text-ada-002 embedding for a given MPNET\nembedding. Our model achieved an average cosine similarity of 0.932 on 10,000\nunseen reviews in our held-out test dataset. We manually assessed the quality\nof our predicted embeddings for vector search over text-ada-002-embedded\nreviews. While not as good as real text-ada-002 embeddings, predicted\nembeddings were able to retrieve highly relevant reviews. Our final model,\nVec2Vec, is lightweight (&lt;80 MB) and fast. Future steps include training a\nneural network with a more sophisticated architecture and a larger dataset of\npaired embeddings to achieve greater performance. The ability to convert\nbetween and align embedding spaces may be helpful for interoperability,\nlimiting dependence on proprietary models, protecting data privacy, reducing\ncosts, and offline operations.",
        "translated": ""
    },
    {
        "title": "Recent Developments in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2306.12680v1",
        "pub_date": "2023-06-22",
        "summary": "In this technical survey, we comprehensively summarize the latest\nadvancements in the field of recommender systems. The objective of this study\nis to provide an overview of the current state-of-the-art in the field and\nhighlight the latest trends in the development of recommender systems. The\nstudy starts with a comprehensive summary of the main taxonomy of recommender\nsystems, including personalized and group recommender systems, and then delves\ninto the category of knowledge-based recommender systems. In addition, the\nsurvey analyzes the robustness, data bias, and fairness issues in recommender\nsystems, summarizing the evaluation metrics used to assess the performance of\nthese systems. Finally, the study provides insights into the latest trends in\nthe development of recommender systems and highlights the new directions for\nfuture research in the field.",
        "translated": ""
    },
    {
        "title": "Resources and Evaluations for Multi-Distribution Dense Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2306.12601v1",
        "pub_date": "2023-06-21",
        "summary": "We introduce and define the novel problem of multi-distribution information\nretrieval (IR) where given a query, systems need to retrieve passages from\nwithin multiple collections, each drawn from a different distribution. Some of\nthese collections and distributions might not be available at training time. To\nevaluate methods for multi-distribution retrieval, we design three benchmarks\nfor this task from existing single-distribution datasets, namely, a dataset\nbased on question answering and two based on entity matching. We propose simple\nmethods for this task which allocate the fixed retrieval budget (top-k\npassages) strategically across domains to prevent the known domains from\nconsuming most of the budget. We show that our methods lead to an average of\n3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and\nthat improvements are consistent when fine-tuning different base retrieval\nmodels. Our benchmarks are made publicly available.",
        "translated": ""
    },
    {
        "title": "Fuzzification-based Feature Selection for Enhanced Website Content\n  Encryption",
        "url": "http://arxiv.org/abs/2306.13548v1",
        "pub_date": "2023-06-23",
        "summary": "We propose a novel approach that utilizes fuzzification theory to perform\nfeature selection on website content for encryption purposes. Our objective is\nto identify and select the most relevant features from the website by\nharnessing the principles of fuzzy logic. Fuzzification allows us to transform\nthe crisp website content into fuzzy representations, enabling a more nuanced\nanalysis of their characteristics. By considering the degree of membership of\neach feature in different fuzzy categories, we can evaluate their importance\nand relevance for encryption. This approach enables us to prioritize and focus\non the features that exhibit higher membership degrees, indicating their\nsignificance in the encryption process. By employing fuzzification-based\nfeature selection, we aim to enhance the effectiveness and efficiency of\nwebsite content encryption, ultimately improving the overall internet security.",
        "translated": ""
    },
    {
        "title": "OptMSM: Optimizing Multi-Scenario Modeling for Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2306.13382v1",
        "pub_date": "2023-06-23",
        "summary": "A large-scale industrial recommendation platform typically consists of\nmultiple associated scenarios, requiring a unified click-through rate (CTR)\nprediction model to serve them simultaneously. Existing approaches for\nmulti-scenario CTR prediction generally consist of two main modules: i) a\nscenario-aware learning module that learns a set of multi-functional\nrepresentations with scenario-shared and scenario-specific information from\ninput features, and ii) a scenario-specific prediction module that serves each\nscenario based on these representations. However, most of these approaches\nprimarily focus on improving the former module and neglect the latter module.\nThis can result in challenges such as increased model parameter size, training\ndifficulty, and performance bottlenecks for each scenario. To address these\nissues, we propose a novel framework called OptMSM (\\textbf{Opt}imizing\n\\textbf{M}ulti-\\textbf{S}cenario \\textbf{M}odeling). First, we introduce a\nsimplified yet effective scenario-enhanced learning module to alleviate the\naforementioned challenges. Specifically, we partition the input features into\nscenario-specific and scenario-shared features, which are mapped to specific\ninformation embedding encodings and a set of shared information embeddings,\nrespectively. By imposing an orthogonality constraint on the shared information\nembeddings to facilitate the disentanglement of shared information\ncorresponding to each scenario, we combine them with the specific information\nembeddings to obtain multi-functional representations. Second, we introduce a\nscenario-specific hypernetwork in the scenario-specific prediction module to\ncapture interactions within each scenario more effectively, thereby alleviating\nthe performance bottlenecks. Finally, we conduct extensive offline experiments\nand an online A/B test to demonstrate the effectiveness of OptMSM.",
        "translated": ""
    },
    {
        "title": "Human Activity Behavioural Pattern Recognition in Smarthome with\n  Long-hour Data Collection",
        "url": "http://arxiv.org/abs/2306.13374v1",
        "pub_date": "2023-06-23",
        "summary": "The research on human activity recognition has provided novel solutions to\nmany applications like healthcare, sports, and user profiling. Considering the\ncomplex nature of human activities, it is still challenging even after\neffective and efficient sensors are available. The existing works on human\nactivity recognition using smartphone sensors focus on recognizing basic human\nactivities like sitting, sleeping, standing, stair up and down and running.\nHowever, more than these basic activities is needed to analyze human\nbehavioural pattern. The proposed framework recognizes basic human activities\nusing deep learning models. Also, ambient sensors like PIR, pressure sensors,\nand smartphone-based sensors like accelerometers and gyroscopes are combined to\nmake it hybrid-sensor-based human activity recognition. The hybrid approach\nhelped derive more activities than the basic ones, which also helped derive\nhuman activity patterns or user profiling. User profiling provides sufficient\ninformation to identify daily living activity patterns and predict whether any\nanomaly exists. The framework provides the base for applications such as\nelderly monitoring when they are alone at home. The GRU model's accuracy of\n95\\% is observed to recognize the basic activities. Finally, Human activity\npatterns over time are recognized based on the duration and frequency of the\nactivities. It is observed that human activity pattern, like, morning walking\nduration, varies depending on the day of the week.",
        "translated": ""
    },
    {
        "title": "A Decade of Scholarly Research on Open Knowledge Graphs",
        "url": "http://arxiv.org/abs/2306.13186v1",
        "pub_date": "2023-06-22",
        "summary": "The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.",
        "translated": ""
    },
    {
        "title": "An overview on the evaluated video retrieval tasks at TRECVID 2022",
        "url": "http://arxiv.org/abs/2306.13118v1",
        "pub_date": "2023-06-22",
        "summary": "The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis\nand retrieval evaluation with the goal of promoting progress in research and\ndevelopment of content-based exploitation and retrieval of information from\ndigital video via open, tasks-based evaluation supported by metrology. Over the\nlast twenty-one years this effort has yielded a better understanding of how\nsystems can effectively accomplish such processing and how one can reliably\nbenchmark their performance. TRECVID has been funded by NIST (National\nInstitute of Standards and Technology) and other US government agencies. In\naddition, many organizations and individuals worldwide contribute significant\ntime and effort. TRECVID 2022 planned for the following six tasks: Ad-hoc video\nsearch, Video to text captioning, Disaster scene description and indexing,\nActivity in extended videos, deep video understanding, and movie summarization.\nIn total, 35 teams from various research organizations worldwide signed up to\njoin the evaluation campaign this year. This paper introduces the tasks,\ndatasets used, evaluation frameworks and metrics, as well as a high-level\nresults overview.",
        "translated": ""
    },
    {
        "title": "Scalable Neural Contextual Bandit for Recommender Systems",
        "url": "http://arxiv.org/abs/2306.14834v1",
        "pub_date": "2023-06-26",
        "summary": "High-quality recommender systems ought to deliver both innovative and\nrelevant content through effective and exploratory interactions with users.\nYet, supervised learning-based neural networks, which form the backbone of many\nexisting recommender systems, only leverage recognized user interests, falling\nshort when it comes to efficiently uncovering unknown user preferences. While\nthere has been some progress with neural contextual bandit algorithms towards\nenabling online exploration through neural networks, their onerous\ncomputational demands hinder widespread adoption in real-world recommender\nsystems. In this work, we propose a scalable sample-efficient neural contextual\nbandit algorithm for recommender systems. To do this, we design an epistemic\nneural network architecture, Epistemic Neural Recommendation (ENR), that\nenables Thompson sampling at a large scale. In two distinct large-scale\nexperiments with real-world tasks, ENR significantly boosts click-through rates\nand user ratings by at least 9% and 6% respectively compared to\nstate-of-the-art neural contextual bandit algorithms. Furthermore, it achieves\nequivalent performance with at least 29% fewer user interactions compared to\nthe best-performing baseline algorithm. Remarkably, while accomplishing these\nimprovements, ENR demands orders of magnitude fewer computational resources\nthan neural contextual bandit baseline algorithms.",
        "translated": ""
    },
    {
        "title": "Reciprocal Sequential Recommendation",
        "url": "http://arxiv.org/abs/2306.14712v1",
        "pub_date": "2023-06-26",
        "summary": "Reciprocal recommender system (RRS), considering a two-way matching between\ntwo parties, has been widely applied in online platforms like online dating and\nrecruitment. Existing RRS models mainly capture static user preferences, which\nhave neglected the evolving user tastes and the dynamic matching relation\nbetween the two parties. Although dynamic user modeling has been well-studied\nin sequential recommender systems, existing solutions are developed in a\nuser-oriented manner. Therefore, it is non-trivial to adapt sequential\nrecommendation algorithms to reciprocal recommendation. In this paper, we\nformulate RRS as a distinctive sequence matching task, and further propose a\nnew approach ReSeq for RRS, which is short for Reciprocal Sequential\nrecommendation. To capture dual-perspective matching, we propose to learn\nfine-grained sequence similarities by co-attention mechanism across different\ntime steps. Further, to improve the inference efficiency, we introduce the\nself-distillation technique to distill knowledge from the fine-grained matching\nmodule into the more efficient student module. In the deployment stage, only\nthe efficient student module is used, greatly speeding up the similarity\ncomputation. Extensive experiments on five real-world datasets from two\nscenarios demonstrate the effectiveness and efficiency of the proposed method.\nOur code is available at https://github.com/RUCAIBox/ReSeq/.",
        "translated": ""
    },
    {
        "title": "PTVD: A Large-Scale Plot-Oriented Multimodal Dataset Based on Television\n  Dramas",
        "url": "http://arxiv.org/abs/2306.14644v1",
        "pub_date": "2023-06-26",
        "summary": "Art forms such as movies and television (TV) dramas are reflections of the\nreal world, which have attracted much attention from the multimodal learning\ncommunity recently. However, existing corpora in this domain share three\nlimitations: (1) annotated in a scene-oriented fashion, they ignore the\ncoherence within plots; (2) their text lacks empathy and seldom mentions\nsituational context; (3) their video clips fail to cover long-form relationship\ndue to short duration. To address these fundamental issues, using 1,106 TV\ndrama episodes and 24,875 informative plot-focused sentences written by\nprofessionals, with the help of 449 human annotators, we constructed PTVD, the\nfirst plot-oriented multimodal dataset in the TV domain. It is also the first\nnon-English dataset of its kind. Additionally, PTVD contains more than 26\nmillion bullet screen comments (BSCs), powering large-scale pre-training. Next,\naiming to open-source a strong baseline for follow-up works, we developed the\nmultimodal algorithm that attacks different cinema/TV modelling problems with a\nunified architecture. Extensive experiments on three cognitive-inspired tasks\nyielded a number of novel observations (some of them being quite\ncounter-intuition), further validating the value of PTVD in promoting\nmultimodal research. The dataset and codes are released at\n\\url{https://ptvd.github.io/}.",
        "translated": ""
    },
    {
        "title": "Multi-task Item-attribute Graph Pre-training for Strict Cold-start Item\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.14462v1",
        "pub_date": "2023-06-26",
        "summary": "Recommendation systems suffer in the strict cold-start (SCS) scenario, where\nthe user-item interactions are entirely unavailable. The ID-based approaches\ncompletely fail to work. Cold-start recommenders, on the other hand, leverage\nitem contents to map the new items to the existing ones. However, the existing\nSCS recommenders explore item contents in coarse-grained manners that introduce\nnoise or information loss. Moreover, informative data sources other than item\ncontents, such as users' purchase sequences and review texts, are ignored. We\nexplore the role of the fine-grained item attributes in bridging the gaps\nbetween the existing and the SCS items and pre-train a knowledgeable\nitem-attribute graph for SCS item recommendation. Our proposed framework,\nColdGPT, models item-attribute correlations into an item-attribute graph by\nextracting fine-grained attributes from item contents. ColdGPT then transfers\nknowledge into the item-attribute graph from various available data sources,\ni.e., item contents, historical purchase sequences, and review texts of the\nexisting items, via multi-task learning. To facilitate the positive transfer,\nColdGPT designs submodules according to the natural forms of the data sources\nand coordinates the multiple pre-training tasks via unified\nalignment-and-uniformity losses. Our pre-trained item-attribute graph acts as\nan implicit, extendable item embedding matrix, which enables the SCS item\nembeddings to be easily acquired by inserting these items and propagating their\nattributes' embeddings. We carefully process three public datasets, i.e., Yelp,\nAmazon-home, and Amazon-sports, to guarantee the SCS setting for evaluation.\nExtensive experiments show that ColdGPT consistently outperforms the existing\nSCS recommenders by large margins and even surpasses models that are\npre-trained on 75-224 times more, cross-domain data on two out of four\ndatasets.",
        "translated": ""
    },
    {
        "title": "Contrastive Multi-view Framework for Customer Lifetime Value Prediction",
        "url": "http://arxiv.org/abs/2306.14400v1",
        "pub_date": "2023-06-26",
        "summary": "Accurate customer lifetime value (LTV) prediction can help service providers\noptimize their marketing policies in customer-centric applications. However,\nthe heavy sparsity of consumption events and the interference of data variance\nand noise obstruct LTV estimation. Many existing LTV prediction methods\ndirectly train a single-view LTV predictor on consumption samples, which may\nyield inaccurate and even biased knowledge extraction. In this paper, we\npropose a contrastive multi-view framework for LTV prediction, which is a\nplug-and-play solution compatible with various backbone models. It synthesizes\nmultiple heterogeneous LTV regressors with complementary knowledge to improve\nmodel robustness and captures sample relatedness via contrastive learning to\nmitigate the dependency on data abundance. Concretely, we use a decomposed\nscheme that converts the LTV prediction problem into a combination of\nestimating consumption probability and payment amount. To alleviate the impact\nof noisy data on model learning, we propose a multi-view framework that jointly\noptimizes multiple types of regressors with diverse characteristics and\nadvantages to encode and fuse comprehensive knowledge. To fully exploit the\npotential of limited training samples, we propose a hybrid contrastive learning\nmethod to help capture the relatedness between samples in both classification\nand regression tasks. We conduct extensive experiments on a real-world game LTV\nprediction dataset and the results validate the effectiveness of our method. We\nhave deployed our solution online in Huawei's mobile game center and achieved\n32.26% of total payment amount gains.",
        "translated": ""
    },
    {
        "title": "G-STO: Sequential Main Shopping Intention Detection via\n  Graph-Regularized Stochastic Transformer",
        "url": "http://arxiv.org/abs/2306.14314v1",
        "pub_date": "2023-06-25",
        "summary": "Sequential recommendation requires understanding the dynamic patterns of\nusers' behaviors, contexts, and preferences from their historical interactions.\nMost existing works focus on modeling user-item interactions only from the item\nlevel, ignoring that they are driven by latent shopping intentions (e.g.,\nballpoint pens, miniatures, etc). The detection of the underlying shopping\nintentions of users based on their historical interactions is a crucial aspect\nfor e-commerce platforms, such as Amazon, to enhance the convenience and\nefficiency of their customers' shopping experiences. Despite its significance,\nthe area of main shopping intention detection remains under-investigated in the\nacademic literature. To fill this gap, we propose a graph-regularized\nstochastic Transformer method, G-STO. By considering intentions as sets of\nproducts and user preferences as compositions of intentions, we model both of\nthem as stochastic Gaussian embeddings in the latent representation space.\nInstead of training the stochastic representations from scratch, we develop a\nglobal intention relational graph as prior knowledge for regularization,\nallowing relevant shopping intentions to be distributionally close. Finally, we\nfeed the newly regularized stochastic embeddings into Transformer-based models\nto encode sequential information from the intention transitions. We evaluate\nour main shopping intention identification model on three different real-world\ndatasets, where G-STO achieves significantly superior performances to the\nbaselines by 18.08% in Hit@1, 7.01% in Hit@10, and 6.11% in NDCG@10 on average.",
        "translated": ""
    },
    {
        "title": "RecBaselines2023: a new dataset for choosing baselines for recommender\n  models",
        "url": "http://arxiv.org/abs/2306.14292v1",
        "pub_date": "2023-06-25",
        "summary": "The number of proposed recommender algorithms continues to grow. The authors\npropose new approaches and compare them with existing models, called baselines.\nDue to the large number of recommender models, it is difficult to estimate\nwhich algorithms to choose in the article. To solve this problem, we have\ncollected and published a dataset containing information about the recommender\nmodels used in 903 papers, both as baselines and as proposed approaches. This\ndataset can be seen as a typical dataset with interactions between papers and\npreviously proposed models. In addition, we provide a descriptive analysis of\nthe dataset and highlight possible challenges to be investigated with the data.\nFurthermore, we have conducted extensive experiments using a well-established\nmethodology to build a good recommender algorithm under the dataset. Our\nexperiments show that the selection of the best baselines for proposing new\nrecommender approaches can be considered and successfully solved by existing\nstate-of-the-art collaborative filtering models. Finally, we discuss\nlimitations and future work.",
        "translated": ""
    },
    {
        "title": "Mining Stable Preferences: Adaptive Modality Decorrelation for\n  Multimedia Recommendation",
        "url": "http://arxiv.org/abs/2306.14179v1",
        "pub_date": "2023-06-25",
        "summary": "Multimedia content is of predominance in the modern Web era. In real\nscenarios, multiple modalities reveal different aspects of item attributes and\nusually possess different importance to user purchase decisions. However, it is\ndifficult for models to figure out users' true preference towards different\nmodalities since there exists strong statistical correlation between\nmodalities. Even worse, the strong statistical correlation might mislead models\nto learn the spurious preference towards inconsequential modalities. As a\nresult, when data (modal features) distribution shifts, the learned spurious\npreference might not guarantee to be as effective on the inference set as on\nthe training set. We propose a novel MOdality DEcorrelating STable learning\nframework, MODEST for brevity, to learn users' stable preference. Inspired by\nsample re-weighting techniques, the proposed method aims to estimate a weight\nfor each item, such that the features from different modalities in the weighted\ndistribution are decorrelated. We adopt Hilbert Schmidt Independence Criterion\n(HSIC) as independence testing measure which is a kernel-based method capable\nof evaluating the correlation degree between two multi-dimensional and\nnon-linear variables. Our method could be served as a play-and-plug module for\nexisting multimedia recommendation backbones. Extensive experiments on four\npublic datasets and four state-of-the-art multimedia recommendation backbones\nunequivocally show that our proposed method can improve the performances by a\nlarge margin.",
        "translated": ""
    },
    {
        "title": "Enhancing Dynamic Image Advertising with Vision-Language Pre-training",
        "url": "http://arxiv.org/abs/2306.14112v1",
        "pub_date": "2023-06-25",
        "summary": "In the multimedia era, image is an effective medium in search advertising.\nDynamic Image Advertising (DIA), a system that matches queries with ad images\nand generates multimodal ads, is introduced to improve user experience and ad\nrevenue. The core of DIA is a query-image matching module performing ad image\nretrieval and relevance modeling. Current query-image matching suffers from\nlimited and inconsistent data, and insufficient cross-modal interaction. Also,\nthe separate optimization of retrieval and relevance models affects overall\nperformance. To address this issue, we propose a vision-language framework\nconsisting of two parts. First, we train a base model on large-scale image-text\npairs to learn general multimodal representation. Then, we fine-tune the base\nmodel on advertising business data, unifying relevance modeling and retrieval\nthrough multi-objective learning. Our framework has been implemented in Baidu\nsearch advertising system \"Phoneix Nest\". Online evaluation shows that it\nimproves cost per mille (CPM) and click-through rate (CTR) by 1.04% and 1.865%.",
        "translated": ""
    },
    {
        "title": "Cross-domain Recommender Systems via Multimodal Domain Adaptation",
        "url": "http://arxiv.org/abs/2306.13887v1",
        "pub_date": "2023-06-24",
        "summary": "Collaborative Filtering (CF) has emerged as one of the most prominent\nimplementation strategies for building recommender systems. The key idea is to\nexploit the usage patterns of individuals to generate personalized\nrecommendations. CF techniques, especially for newly launched platforms, often\nface a critical issue known as the data sparsity problem, which greatly limits\ntheir performance. Several approaches have been proposed in the literature to\ntackle the problem of data sparsity, among which cross-domain collaborative\nfiltering (CDCF) has gained significant attention in the recent past. In order\nto compensate for the scarcity of available feedback in a target domain, the\nCDCF approach makes use of information available in other auxiliary domains.\nMost of the traditional CDCF approach aim is to find a common set of entities\n(users or items) across the domains and then use them as a bridge for knowledge\ntransfer. However, most real-world datasets are collected from different\ndomains, so they often lack information about anchor points or reference\ninformation for entity alignment. In this paper, we propose a domain adaptation\ntechnique to align the embeddings of users and items across the two domains.\nOur approach first exploits the available textual and visual information to\nindependently learn a multi-view latent representation for each user and item\nin the auxiliary and target domains. The different representations of a user or\nitem are then fused to generate the corresponding unified representation. A\ndomain classifier is then trained to learn the embedding for the domain\nalignment by fixing the unified features as the anchor points. Experiments on\ntwo publicly benchmark datasets indicate the effectiveness of our proposed\napproach.",
        "translated": ""
    },
    {
        "title": "Unleashing the Power of User Reviews: Exploring Airline Choices at\n  Catania Airport, Italy",
        "url": "http://arxiv.org/abs/2306.15541v1",
        "pub_date": "2023-06-27",
        "summary": "This study aims to investigate the possible relationship between the\nmechanisms of social influence and the choice of airline, through the use of\nnew tools, with the aim of understanding whether they can contribute to a\nbetter understanding of the factors influencing the decisions of consumers in\nthe aviation sector. We have chosen to extract user reviews from well-known\nplatforms: Trustpilot, Google, and Twitter. By combining web scraping\ntechniques, we have been able to collect a comprehensive dataset comprising a\nwide range of user opinions, feedback, and ratings. We then refined the BERT\nmodel to focus on insightful sentiment in the context of airline reviews.\nThrough our analysis, we observed an intriguing trend of average negative\nsentiment scores across various airlines, giving us deeper insight into the\ndynamics between airlines and helping us identify key partnerships, popular\nroutes, and airlines that play a central role in the aeronautical ecosystem of\nCatania airport during the specified period. Our investigation led us to find\nthat, despite an airline having received prestigious awards as a low-cost\nleader in Europe for two consecutive years 2021 and 2022, the \"Catanese\" user\ntends to suffer the dominant position of other companies. Understanding the\nimpact of positive reviews and leveraging sentiment analysis can help airlines\nimprove their reputation, attract more customers, and ultimately gain a\ncompetitive edge in the marketplace.",
        "translated": ""
    },
    {
        "title": "Learning to Rank in Generative Retrieval",
        "url": "http://arxiv.org/abs/2306.15222v1",
        "pub_date": "2023-06-27",
        "summary": "Generative retrieval is a promising new paradigm in text retrieval that\ngenerates identifier strings of relevant passages as the retrieval target. This\nparadigm leverages powerful generation models and represents a new paradigm\ndistinct from traditional learning-to-rank methods. However, despite its rapid\ndevelopment, current generative retrieval methods are still limited. They\ntypically rely on a heuristic function to transform predicted identifiers into\na passage rank list, which creates a gap between the learning objective of\ngenerative retrieval and the desired passage ranking target. Moreover, the\ninherent exposure bias problem of text generation also persists in generative\nretrieval. To address these issues, we propose a novel framework, called LTRGR,\nthat combines generative retrieval with the classical learning-to-rank\nparadigm. Our approach involves training an autoregressive model using a\npassage rank loss, which directly optimizes the autoregressive model toward the\noptimal passage ranking. This framework only requires an additional training\nstep to enhance current generative retrieval systems and does not add any\nburden to the inference stage. We conducted experiments on three public\ndatasets, and our results demonstrate that LTRGR achieves state-of-the-art\nperformance among generative retrieval methods, indicating its effectiveness\nand robustness.",
        "translated": ""
    },
    {
        "title": "Off-Policy Evaluation of Ranking Policies under Diverse User Behavior",
        "url": "http://arxiv.org/abs/2306.15098v1",
        "pub_date": "2023-06-26",
        "summary": "Ranking interfaces are everywhere in online platforms. There is thus an ever\ngrowing interest in their Off-Policy Evaluation (OPE), aiming towards an\naccurate performance evaluation of ranking policies using logged data. A\nde-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides\nan unbiased and consistent value estimate. However, it becomes extremely\ninaccurate in the ranking setup due to its high variance under large action\nspaces. To deal with this problem, previous studies assume either independent\nor cascade user behavior, resulting in some ranking versions of IPS. While\nthese estimators are somewhat effective in reducing the variance, all existing\nestimators apply a single universal assumption to every user, causing excessive\nbias and variance. Therefore, this work explores a far more general formulation\nwhere user behavior is diverse and can vary depending on the user context. We\nshow that the resulting estimator, which we call Adaptive IPS (AIPS), can be\nunbiased under any complex user behavior. Moreover, AIPS achieves the minimum\nvariance among all unbiased estimators based on IPS. We further develop a\nprocedure to identify the appropriate user behavior model to minimize the mean\nsquared error (MSE) of AIPS in a data-driven fashion. Extensive experiments\ndemonstrate that the empirical accuracy improvement can be significant,\nenabling effective OPE of ranking systems even under diverse user behavior.",
        "translated": ""
    },
    {
        "title": "Efficient High-Resolution Template Matching with Vector Quantized\n  Nearest Neighbour Fields",
        "url": "http://arxiv.org/abs/2306.15010v1",
        "pub_date": "2023-06-26",
        "summary": "Template matching is a fundamental problem in computer vision and has\napplications in various fields, such as object detection, image registration,\nand object tracking. The current state-of-the-art methods rely on\nnearest-neighbour (NN) matching in which the query feature space is converted\nto NN space by representing each query pixel with its NN in the template\npixels. The NN-based methods have been shown to perform better in occlusions,\nchanges in appearance, illumination variations, and non-rigid transformations.\nHowever, NN matching scales poorly with high-resolution data and high feature\ndimensions. In this work, we present an NN-based template-matching method which\nefficiently reduces the NN computations and introduces filtering in the NN\nfields to consider deformations. A vector quantization step first represents\nthe template with $k$ features, then filtering compares the template and query\ndistributions over the $k$ features. We show that state-of-the-art performance\nwas achieved in low-resolution data, and our method outperforms previous\nmethods at higher resolution showing the robustness and scalability of the\napproach.",
        "translated": ""
    },
    {
        "title": "SE-PQA: Personalized Community Question Answering",
        "url": "http://arxiv.org/abs/2306.16261v1",
        "pub_date": "2023-06-28",
        "summary": "Personalization in Information Retrieval is a topic studied for a long time.\nNevertheless, there is still a lack of high-quality, real-world datasets to\nconduct large-scale experiments and evaluate models for personalized search.\nThis paper contributes to filling this gap by introducing SE-PQA (StackExchange\n- Personalized Question Answering), a new curated resource to design and\nevaluate personalized models related to the task of community Question\nAnswering (cQA). The contributed dataset includes more than 1 million queries\nand 2 million answers, annotated with a rich set of features modeling the\nsocial interactions among the users of a popular cQA platform. We describe the\ncharacteristics of SE-PQA and detail the features associated with questions and\nanswers. We also provide reproducible baseline methods for the cQA task based\non the resource, including deep learning models and personalization approaches.\nThe results of the preliminary experiments conducted show the appropriateness\nof SE-PQA to train effective cQA models; they also show that personalization\nremarkably improves the effectiveness of all the methods tested. Furthermore,\nwe show the benefits in terms of robustness and generalization of combining\ndata from multiple communities for personalization purposes.",
        "translated": ""
    },
    {
        "title": "Query Understanding in the Age of Large Language Models",
        "url": "http://arxiv.org/abs/2306.16004v1",
        "pub_date": "2023-06-28",
        "summary": "Querying, conversing, and controlling search and information-seeking\ninterfaces using natural language are fast becoming ubiquitous with the rise\nand adoption of large-language models (LLM). In this position paper, we\ndescribe a generic framework for interactive query-rewriting using LLMs. Our\nproposal aims to unfold new opportunities for improved and transparent intent\nunderstanding while building high-performance retrieval systems using LLMs. A\nkey aspect of our framework is the ability of the rewriter to fully specify the\nmachine intent by the search engine in natural language that can be further\nrefined, controlled, and edited before the final retrieval phase. The ability\nto present, interact, and reason over the underlying machine intent in natural\nlanguage has profound implications on transparency, ranking performance, and a\ndeparture from the traditional way in which supervised signals were collected\nfor understanding intents. We detail the concept, backed by initial\nexperiments, along with open questions for this interactive query understanding\nframework.",
        "translated": ""
    },
    {
        "title": "Streamlining Social Media Information Retrieval for Public Health\n  Research with Deep Learning",
        "url": "http://arxiv.org/abs/2306.16001v1",
        "pub_date": "2023-06-28",
        "summary": "The utilization of social media in epidemic surveillance has been well\nestablished. Nonetheless, bias is often introduced when pre-defined lexicons\nare used to retrieve relevant corpus. This study introduces a framework aimed\nat curating extensive dictionaries of medical colloquialisms and Unified\nMedical Language System (UMLS) concepts. The framework comprises three modules:\na BERT-based Named Entity Recognition (NER) model that identifies medical\nentities from social media content, a deep-learning powered normalization\nmodule that standardizes the extracted entities, and a semi-supervised\nclustering module that assigns the most probable UMLS concept to each\nstandardized entity. We applied this framework to COVID-19-related tweets from\nFebruary 1, 2020, to April 30, 2022, generating a symptom dictionary (available\nat https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249\nstandardized entities mapped to 876 UMLS concepts and 38,175 colloquial\nexpressions. This framework demonstrates encouraging potential in addressing\nthe constraints of keyword matching information retrieval in social media-based\npublic health research.",
        "translated": ""
    },
    {
        "title": "Disentangled Variational Auto-encoder Enhanced by Counterfactual Data\n  for Debiasing Recommendation",
        "url": "http://arxiv.org/abs/2306.15961v1",
        "pub_date": "2023-06-28",
        "summary": "Recommender system always suffers from various recommendation biases,\nseriously hindering its development. In this light, a series of debias methods\nhave been proposed in the recommender system, especially for two most common\nbiases, i.e., popularity bias and amplified subjective bias. However, exsisting\ndebias methods usually concentrate on correcting a single bias. Such\nsingle-functionality debiases neglect the bias-coupling issue in which the\nrecommended items are collectively attributed to multiple biases. Besides,\nprevious work cannot tackle the lacking supervised signals brought by sparse\ndata, yet which has become a commonplace in the recommender system. In this\nwork, we introduce a disentangled debias variational auto-encoder\nframework(DB-VAE) to address the single-functionality issue as well as a\ncounterfactual data enhancement method to mitigate the adverse effect due to\nthe data sparsity. In specific, DB-VAE first extracts two types of extreme\nitems only affected by a single bias based on the collier theory, which are\nrespectively employed to learn the latent representation of corresponding\nbiases, thereby realizing the bias decoupling. In this way, the exact unbiased\nuser representation can be learned by these decoupled bias representations.\nFurthermore, the data generation module employs Pearl's framework to produce\nmassive counterfactual data, making up the lacking supervised signals due to\nthe sparse data. Extensive experiments on three real-world datasets demonstrate\nthe effectiveness of our proposed model. Besides, the counterfactual data can\nfurther improve DB-VAE, especially on the dataset with low sparsity.",
        "translated": ""
    },
    {
        "title": "Pb-Hash: Partitioned b-bit Hashing",
        "url": "http://arxiv.org/abs/2306.15944v1",
        "pub_date": "2023-06-28",
        "summary": "Many hashing algorithms including minwise hashing (MinHash), one permutation\nhashing (OPH), and consistent weighted sampling (CWS) generate integers of $B$\nbits. With $k$ hashes for each data vector, the storage would be $B\\times k$\nbits; and when used for large-scale learning, the model size would be\n$2^B\\times k$, which can be expensive. A standard strategy is to use only the\nlowest $b$ bits out of the $B$ bits and somewhat increase $k$, the number of\nhashes. In this study, we propose to re-use the hashes by partitioning the $B$\nbits into $m$ chunks, e.g., $b\\times m =B$. Correspondingly, the model size\nbecomes $m\\times 2^b \\times k$, which can be substantially smaller than the\noriginal $2^B\\times k$.\n  Our theoretical analysis reveals that by partitioning the hash values into\n$m$ chunks, the accuracy would drop. In other words, using $m$ chunks of $B/m$\nbits would not be as accurate as directly using $B$ bits. This is due to the\ncorrelation from re-using the same hash. On the other hand, our analysis also\nshows that the accuracy would not drop much for (e.g.,) $m=2\\sim 4$. In some\nregions, Pb-Hash still works well even for $m$ much larger than 4. We expect\nPb-Hash would be a good addition to the family of hashing methods/applications\nand benefit industrial practitioners.\n  We verify the effectiveness of Pb-Hash in machine learning tasks, for linear\nSVM models as well as deep learning models. Since the hashed data are\nessentially categorical (ID) features, we follow the standard practice of using\nembedding tables for each hash. With Pb-Hash, we need to design an effective\nstrategy to combine $m$ embeddings. Our study provides an empirical evaluation\non four pooling schemes: concatenation, max pooling, mean pooling, and product\npooling. There is no definite answer which pooling would be always better and\nwe leave that for future study.",
        "translated": ""
    },
    {
        "title": "Confidence-Calibrated Ensemble Dense Phrase Retrieval",
        "url": "http://arxiv.org/abs/2306.15917v1",
        "pub_date": "2023-06-28",
        "summary": "In this paper, we consider the extent to which the transformer-based Dense\nPassage Retrieval (DPR) algorithm, developed by (Karpukhin et. al. 2020), can\nbe optimized without further pre-training. Our method involves two particular\ninsights: we apply the DPR context encoder at various phrase lengths (e.g.\none-sentence versus five-sentence segments), and we take a\nconfidence-calibrated ensemble prediction over all of these different\nsegmentations. This somewhat exhaustive approach achieves start-of-the-art\nresults on benchmark datasets such as Google NQ and SQuAD. We also apply our\nmethod to domain-specific datasets, and the results suggest how different\ngranularities are optimal for different domains",
        "translated": ""
    },
    {
        "title": "Dimension Independent Mixup for Hard Negative Sample in Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2306.15905v1",
        "pub_date": "2023-06-28",
        "summary": "Collaborative filtering (CF) is a widely employed technique that predicts\nuser preferences based on past interactions. Negative sampling plays a vital\nrole in training CF-based models with implicit feedback. In this paper, we\npropose a novel perspective based on the sampling area to revisit existing\nsampling methods. We point out that current sampling methods mainly focus on\nPoint-wise or Line-wise sampling, lacking flexibility and leaving a significant\nportion of the hard sampling area un-explored. To address this limitation, we\npropose Dimension Independent Mixup for Hard Negative Sampling (DINS), which is\nthe first Area-wise sampling method for training CF-based models. DINS\ncomprises three modules: Hard Boundary Definition, Dimension Independent Mixup,\nand Multi-hop Pooling. Experiments with real-world datasets on both matrix\nfactorization and graph-based models demonstrate that DINS outperforms other\nnegative sampling methods, establishing its effectiveness and superiority. Our\nwork contributes a new perspective, introduces Area-wise sampling, and presents\nDINS as a novel approach that achieves state-of-the-art performance for\nnegative sampling. Our implementations are available in PyTorch.",
        "translated": ""
    },
    {
        "title": "Blockwise Feature Interaction in Recommendation Systems",
        "url": "http://arxiv.org/abs/2306.15881v1",
        "pub_date": "2023-06-28",
        "summary": "Feature interactions can play a crucial role in recommendation systems as\nthey capture complex relationships between user preferences and item\ncharacteristics. Existing methods such as Deep &amp; Cross Network (DCNv2) may\nsuffer from high computational requirements due to their cross-layer\noperations. In this paper, we propose a novel approach called blockwise feature\ninteraction (BFI) to help alleviate this issue. By partitioning the feature\ninteraction process into smaller blocks, we can significantly reduce both the\nmemory footprint and the computational burden. Four variants (denoted by P, Q,\nT, S, respectively) of BFI have been developed and empirically compared. Our\nexperimental results demonstrate that the proposed algorithms achieves close\naccuracy compared to the standard DCNv2, while greatly reducing the\ncomputational overhead and the number of parameters. This paper contributes to\nthe development of efficient recommendation systems by providing a practical\nsolution for improving feature interaction efficiency.",
        "translated": ""
    },
    {
        "title": "Ducho: A Unified Framework for the Extraction of Multimodal Features in\n  Recommendation",
        "url": "http://arxiv.org/abs/2306.17125v1",
        "pub_date": "2023-06-29",
        "summary": "In multimodal-aware recommendation, the extraction of meaningful multimodal\nfeatures is at the basis of high-quality recommendations. Generally, each\nrecommendation framework implements its multimodal extraction procedures with\nspecific strategies and tools. This is limiting for two reasons: (i) different\nextraction strategies do not ease the interdependence among multimodal\nrecommendation frameworks; thus, they cannot be efficiently and fairly\ncompared; (ii) given the large plethora of pre-trained deep learning models\nmade available by different open source tools, model designers do not have\naccess to shared interfaces to extract features. Motivated by the outlined\naspects, we propose Ducho, a unified framework for the extraction of multimodal\nfeatures in recommendation. By integrating three widely-adopted deep learning\nlibraries as backends, namely, TensorFlow, PyTorch, and Transformers, we\nprovide a shared interface to extract and process features where each backend's\nspecific methods are abstracted to the end user. Noteworthy, the extraction\npipeline is easily configurable with a YAML-based file where the user can\nspecify, for each modality, the list of models (and their specific\nbackends/parameters) to perform the extraction. Finally, to make Ducho\naccessible to the community, we build a public Docker image equipped with a\nready-to-use CUDA environment and propose three demos to test its\nfunctionalities for different scenarios and tasks. The GitHub repository and\nthe documentation is accessible at this link:\nhttps://github.com/sisinflab/Ducho.",
        "translated": ""
    },
    {
        "title": "Re-Rank - Expand - Repeat: Adaptive Query Expansion for Document\n  Retrieval Using Words and Entities",
        "url": "http://arxiv.org/abs/2306.17082v1",
        "pub_date": "2023-06-29",
        "summary": "Sparse and dense pseudo-relevance feedback (PRF) approaches perform poorly on\nchallenging queries due to low precision in first-pass retrieval. However,\nrecent advances in neural language models (NLMs) can re-rank relevant documents\nto top ranks, even when few are in the re-ranking pool. This paper first\naddresses the problem of poor pseudo-relevance feedback by simply applying\nre-ranking prior to query expansion and re-executing this query. We find that\nthis change alone can improve the retrieval effectiveness of sparse and dense\nPRF approaches by 5-8%. Going further, we propose a new expansion model, Latent\nEntity Expansion (LEE), a fine-grained word and entity-based relevance\nmodelling incorporating localized features. Finally, we include an \"adaptive\"\ncomponent to the retrieval process, which iteratively refines the re-ranking\npool during scoring using the expansion model, i.e. we \"re-rank - expand -\nrepeat\". Using LEE, we achieve (to our knowledge) the best NDCG, MAP and R@1000\nresults on the TREC Robust 2004 and CODEC adhoc document datasets,\ndemonstrating a significant advancement in expansion effectiveness.",
        "translated": ""
    },
    {
        "title": "Harnessing the Power of Hugging Face Transformers for Predicting Mental\n  Health Disorders in Social Networks",
        "url": "http://arxiv.org/abs/2306.16891v1",
        "pub_date": "2023-06-29",
        "summary": "Early diagnosis of mental disorders and intervention can facilitate the\nprevention of severe injuries and the improvement of treatment results. Using\nsocial media and pre-trained language models, this study explores how\nuser-generated data can be used to predict mental disorder symptoms. Our study\ncompares four different BERT models of Hugging Face with standard machine\nlearning techniques used in automatic depression diagnosis in recent\nliterature. The results show that new models outperform the previous approach\nwith an accuracy rate of up to 97%. Analyzing the results while complementing\npast findings, we find that even tiny amounts of data (like users' bio\ndescriptions) have the potential to predict mental disorders. We conclude that\nsocial media data is an excellent source of mental health screening, and\npre-trained models can effectively automate this critical task.",
        "translated": ""
    },
    {
        "title": "Computing all-vs-all MEMs in grammar-compressed text",
        "url": "http://arxiv.org/abs/2306.16815v1",
        "pub_date": "2023-06-29",
        "summary": "We describe a compression-aware method to compute all-vs-all maximal exact\nmatches (MEM) among strings of a repetitive collection $\\mathcal{T}$. The key\nconcept in our work is the construction of a fully-balanced grammar\n$\\mathcal{G}$ from $\\mathcal{T}$ that meets a property that we call\n\\emph{fix-free}: the expansions of the nonterminals that have the same height\nin the parse tree form a fix-free set (i.e., prefix-free and suffix-free). The\nfix-free property allows us to compute the MEMs of $\\mathcal{T}$ incrementally\nover $\\mathcal{G}$ using a standard suffix-tree-based MEM algorithm, which runs\non a subset of grammar rules at a time and does not decompress nonterminals. By\nmodifying the locally-consistent grammar of Christiansen et al 2020., we show\nhow we can build $\\mathcal{G}$ from $\\mathcal{T}$ in linear time and space. We\nalso demonstrate that our MEM algorithm runs on top of $\\mathcal{G}$ in $O(G\n+occ)$ time and uses $O(\\log G(G+occ))$ bits, where $G$ is the grammar size,\nand $occ$ is the number of MEMs in $\\mathcal{T}$. In the conclusions, we\ndiscuss how our idea can be modified to implement approximate pattern matching\nin compressed space.",
        "translated": ""
    },
    {
        "title": "Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall\n  Classification",
        "url": "http://arxiv.org/abs/2306.16760v1",
        "pub_date": "2023-06-29",
        "summary": "We present working notes on transfer learning with semi-supervised dataset\nannotation for the BirdCLEF 2023 competition, focused on identifying African\nbird species in recorded soundscapes. Our approach utilizes existing\noff-the-shelf models, BirdNET and MixIT, to address representation and labeling\nchallenges in the competition. We explore the embedding space learned by\nBirdNET and propose a process to derive an annotated dataset for supervised\nlearning. Our experiments involve various models and feature engineering\napproaches to maximize performance on the competition leaderboard. The results\ndemonstrate the effectiveness of our approach in classifying bird species and\nhighlight the potential of transfer learning and semi-supervised dataset\nannotation in similar tasks.",
        "translated": ""
    },
    {
        "title": "Multi-Scenario Ranking with Adaptive Feature Learning",
        "url": "http://arxiv.org/abs/2306.16732v1",
        "pub_date": "2023-06-29",
        "summary": "Recently, Multi-Scenario Learning (MSL) is widely used in recommendation and\nretrieval systems in the industry because it facilitates transfer learning from\ndifferent scenarios, mitigating data sparsity and reducing maintenance cost.\nThese efforts produce different MSL paradigms by searching more optimal network\nstructure, such as Auxiliary Network, Expert Network, and Multi-Tower Network.\nIt is intuitive that different scenarios could hold their specific\ncharacteristics, activating the user's intents quite differently. In other\nwords, different kinds of auxiliary features would bear varying importance\nunder different scenarios. With more discriminative feature representations\nrefined in a scenario-aware manner, better ranking performance could be easily\nobtained without expensive search for the optimal network structure.\nUnfortunately, this simple idea is mainly overlooked but much desired in\nreal-world systems.Further analysis also validates the rationality of adaptive\nfeature learning under a multi-scenario scheme. Moreover, our A/B test results\non the Alibaba search advertising platform also demonstrate that Maria is\nsuperior in production environments.",
        "translated": ""
    },
    {
        "title": "Exploring the Representation Power of SPLADE Models",
        "url": "http://arxiv.org/abs/2306.16680v1",
        "pub_date": "2023-06-29",
        "summary": "The SPLADE (SParse Lexical AnD Expansion) model is a highly effective\napproach to learned sparse retrieval, where documents are represented by term\nimpact scores derived from large language models. During training, SPLADE\napplies regularization to ensure postings lists are kept sparse -- with the aim\nof mimicking the properties of natural term distributions -- allowing efficient\nand effective lexical matching and ranking. However, we hypothesize that SPLADE\nmay encode additional signals into common postings lists to further improve\neffectiveness. To explore this idea, we perform a number of empirical analyses\nwhere we re-train SPLADE with different, controlled vocabularies and measure\nhow effective it is at ranking passages. Our findings suggest that SPLADE can\neffectively encode useful ranking signals in documents even when the vocabulary\nis constrained to terms that are not traditionally useful for ranking, such as\nstopwords or even random words.",
        "translated": ""
    },
    {
        "title": "Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of\n  Information Retrieval Models",
        "url": "http://arxiv.org/abs/2306.16668v1",
        "pub_date": "2023-06-29",
        "summary": "As in other fields of artificial intelligence, the information retrieval\ncommunity has grown interested in investigating the power consumption\nassociated with neural models, particularly models of search. This interest has\nbecome particularly relevant as the energy consumption of information retrieval\nmodels has risen with new neural models based on large language models, leading\nto an associated increase of CO2 emissions, albeit relatively low compared to\nfields such as natural language processing.",
        "translated": ""
    },
    {
        "title": "Event Detection from Social Media Stream: Methods, Datasets and\n  Opportunities",
        "url": "http://arxiv.org/abs/2306.16495v1",
        "pub_date": "2023-06-28",
        "summary": "Social media streams contain large and diverse amount of information, ranging\nfrom daily-life stories to the latest global and local events and news.\nTwitter, especially, allows a fast spread of events happening real time, and\nenables individuals and organizations to stay informed of the events happening\nnow. Event detection from social media data poses different challenges from\ntraditional text and is a research area that has attracted much attention in\nrecent years. In this paper, we survey a wide range of event detection methods\nfor Twitter data stream, helping readers understand the recent development in\nthis area. We present the datasets available to the public. Furthermore, a few\nresearch opportunities",
        "translated": ""
    },
    {
        "title": "Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual\n  Question Answering",
        "url": "http://arxiv.org/abs/2306.16478v1",
        "pub_date": "2023-06-28",
        "summary": "This paper studies a category of visual question answering tasks, in which\naccessing external knowledge is necessary for answering the questions. This\ncategory is called outside-knowledge visual question answering (OK-VQA). A\nmajor step in developing OK-VQA systems is to retrieve relevant documents for\nthe given multi-modal query. Current state-of-the-art asymmetric dense\nretrieval model for this task uses an architecture with a multi-modal query\nencoder and a uni-modal document encoder. Such an architecture requires a large\namount of training data for effective performance. We propose an automatic data\ngeneration pipeline for pre-training passage retrieval models for OK-VQA tasks.\nThe proposed approach leads to 26.9% Precision@5 improvements compared to the\ncurrent state-of-the-art asymmetric architecture. Additionally, the proposed\npre-training approach exhibits a good ability in zero-shot retrieval scenarios.",
        "translated": ""
    },
    {
        "title": "Precision Anti-Cancer Drug Selection via Neural Ranking",
        "url": "http://arxiv.org/abs/2306.17771v1",
        "pub_date": "2023-06-30",
        "summary": "Personalized cancer treatment requires a thorough understanding of complex\ninteractions between drugs and cancer cell lines in varying genetic and\nmolecular contexts. To address this, high-throughput screening has been used to\ngenerate large-scale drug response data, facilitating data-driven computational\nmodels. Such models can capture complex drug-cell line interactions across\nvarious contexts in a fully data-driven manner. However, accurately\nprioritizing the most sensitive drugs for each cell line still remains a\nsignificant challenge. To address this, we developed neural ranking approaches\nthat leverage large-scale drug response data across multiple cell lines from\ndiverse cancer types. Unlike existing approaches that primarily utilize\nregression and classification techniques for drug response prediction, we\nformulated the objective of drug selection and prioritization as a drug ranking\nproblem. In this work, we proposed two neural listwise ranking methods that\nlearn latent representations of drugs and cell lines, and then use those\nrepresentations to score drugs in each cell line via a learnable scoring\nfunction. Specifically, we developed a neural listwise ranking method,\nList-One, on top of the existing method ListNet. Additionally, we proposed a\nnovel listwise ranking method, List-All, that focuses on all the sensitive\ndrugs instead of the top sensitive drug, unlike List-One. Our results\ndemonstrate that List-All outperforms the best baseline with significant\nimprovements of as much as 8.6% in hit@20 across 50% test cell lines.\nFurthermore, our analyses suggest that the learned latent spaces from our\nproposed methods demonstrate informative clustering structures and capture\nrelevant underlying biological features. Moreover, our comprehensive empirical\nevaluation provides a thorough and objective comparison of the performance of\ndifferent methods (including our proposed ones).",
        "translated": ""
    },
    {
        "title": "Outcome-based Evaluation of Systematic Review Automation",
        "url": "http://arxiv.org/abs/2306.17614v1",
        "pub_date": "2023-06-30",
        "summary": "Current methods of evaluating search strategies and automated citation\nscreening for systematic literature reviews typically rely on counting the\nnumber of relevant and not relevant publications. This established practice,\nhowever, does not accurately reflect the reality of conducting a systematic\nreview, because not all included publications have the same influence on the\nfinal outcome of the systematic review. More specifically, if an important\npublication gets excluded or included, this might significantly change the\noverall review outcome, while not including or excluding less influential\nstudies may only have a limited impact. However, in terms of evaluation\nmeasures, all inclusion and exclusion decisions are treated equally and,\ntherefore, failing to retrieve publications with little to no impact on the\nreview outcome leads to the same decrease in recall as failing to retrieve\ncrucial publications. We propose a new evaluation framework that takes into\naccount the impact of the reported study on the overall systematic review\noutcome. We demonstrate the framework by extracting review meta-analysis data\nand estimating outcome effects using predictions from ranking runs on\nsystematic reviews of interventions from CLEF TAR 2019 shared task. We further\nmeasure how closely the obtained outcomes are to the outcomes of the original\nreview if the arbitrary rankings were used. We evaluate 74 runs using the\nproposed framework and compare the results with those obtained using standard\nIR measures. We find that accounting for the difference in review outcomes\nleads to a different assessment of the quality of a system than if traditional\nevaluation measures were used. Our analysis provides new insights into the\nevaluation of retrieval results in the context of systematic review automation,\nemphasising the importance of assessing the usefulness of each document beyond\nbinary relevance.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking\n  Prompting",
        "url": "http://arxiv.org/abs/2306.17563v1",
        "pub_date": "2023-06-30",
        "summary": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, there has been limited success so far, as researchers have\nfound it difficult to outperform fine-tuned baseline rankers on benchmark\ndatasets. We analyze pointwise and listwise ranking prompts used by existing\nmethods and argue that off-the-shelf LLMs do not fully understand these ranking\nformulations, possibly due to the nature of how LLMs are trained. In this\npaper, we propose to significantly reduce the burden on LLMs by using a new\ntechnique called Pairwise Ranking Prompting (PRP). Our results are the first in\nthe literature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on\nthe Flan-UL2 model with 20B parameters outperforms the previous best approach\nin the literature, which is based on the blackbox commercial GPT-4 that has 50x\n(estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only\ninferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while\noutperforming other existing solutions, such as InstructGPT which has 175B\nparameters, by over 10% for nearly all ranking metrics. Furthermore, we propose\nseveral variants of PRP to improve efficiency and show that it is possible to\nachieve competitive results even with linear complexity. We also discuss other\nbenefits of PRP, such as supporting both generation and scoring LLM APIs, as\nwell as being insensitive to input ordering.",
        "translated": ""
    },
    {
        "title": "Leveraging Watch-time Feedback for Short-Video Recommendations: A Causal\n  Labeling Framework",
        "url": "http://arxiv.org/abs/2306.17426v1",
        "pub_date": "2023-06-30",
        "summary": "With the proliferation of short video applications, the significance of short\nvideo recommendations has vastly increased. Unlike other recommendation\nscenarios, short video recommendation systems heavily rely on feedback from\nwatch time. Existing approaches simply treat watch time as a direct label,\nfailing to effectively harness its extensive semantics and introduce bias,\nthereby limiting the potential for modeling user interests based on watch time.\nTo overcome this challenge, we propose a framework named Debiasied\nMultiple-semantics-extracting Labeling (DML). DML constructs labels that\nencompass various semantics by utilizing quantiles derived from the\ndistribution of watch time, prioritizing relative order rather than absolute\nlabel values. This approach facilitates easier model learning while aligning\nwith the ranking objective of recommendations. Furthermore, we introduce a\nmethod inspired by causal adjustment to refine label definitions, thereby\nreducing the impact of bias on the label and directly mitigating bias at the\nlabel level. We substantiate the effectiveness of our DML framework through\nboth online and offline experiments. Extensive results demonstrate that our DML\ncould effectively leverage watch time to discover users' real interests,\nenhancing their engagement in our application.",
        "translated": ""
    },
    {
        "title": "Audio Embeddings as Teachers for Music Classification",
        "url": "http://arxiv.org/abs/2306.17424v1",
        "pub_date": "2023-06-30",
        "summary": "Music classification has been one of the most popular tasks in the field of\nmusic information retrieval. With the development of deep learning models, the\nlast decade has seen impressive improvements in a wide range of classification\ntasks. However, the increasing model complexity makes both training and\ninference computationally expensive. In this paper, we integrate the ideas of\ntransfer learning and feature-based knowledge distillation and systematically\ninvestigate using pre-trained audio embeddings as teachers to guide the\ntraining of low-complexity student networks. By regularizing the feature space\nof the student networks with the pre-trained embeddings, the knowledge in the\nteacher embeddings can be transferred to the students. We use various\npre-trained audio embeddings and test the effectiveness of the method on the\ntasks of musical instrument classification and music auto-tagging. Results show\nthat our method significantly improves the results in comparison to the\nidentical model trained without the teacher's knowledge. This technique can\nalso be combined with classical knowledge distillation approaches to further\nimprove the model's performance.",
        "translated": ""
    },
    {
        "title": "DeepTagger: Knowledge Enhanced Named Entity Recognition for Web-Based\n  Ads Queries",
        "url": "http://arxiv.org/abs/2306.17413v1",
        "pub_date": "2023-06-30",
        "summary": "Named entity recognition (NER) is a crucial task for online advertisement.\nState-of-the-art solutions leverage pre-trained language models for this task.\nHowever, three major challenges remain unresolved: web queries differ from\nnatural language, on which pre-trained models are trained; web queries are\nshort and lack contextual information; and labeled data for NER is scarce. We\npropose DeepTagger, a knowledge-enhanced NER model for web-based ads queries.\nThe proposed knowledge enhancement framework leverages both model-free and\nmodel-based approaches. For model-free enhancement, we collect unlabeled web\nqueries to augment domain knowledge; and we collect web search results to\nenrich the information of ads queries. We further leverage effective prompting\nmethods to automatically generate labels using large language models such as\nChatGPT. Additionally, we adopt a model-based knowledge enhancement method\nbased on adversarial data augmentation. We employ a three-stage training\nframework to train DeepTagger models. Empirical results in various NER tasks\ndemonstrate the effectiveness of the proposed framework.",
        "translated": ""
    },
    {
        "title": "Towards Personalized Cold-Start Recommendation with Prompts",
        "url": "http://arxiv.org/abs/2306.17256v1",
        "pub_date": "2023-06-29",
        "summary": "Recommender systems play a crucial role in helping users discover information\nthat aligns with their interests based on their past behaviors. However,\ndeveloping personalized recommendation systems becomes challenging when\nhistorical records of user-item interactions are unavailable, leading to what\nis known as the system cold-start recommendation problem. This issue is\nparticularly prominent in start-up businesses or platforms with insufficient\nuser engagement history. Previous studies focus on user or item cold-start\nscenarios, where systems could make recommendations for new users or items but\nare still trained with historical user-item interactions in the same domain,\nwhich cannot solve our problem. To bridge the gap, our research introduces an\ninnovative and effective approach, capitalizing on the capabilities of\npre-trained language models. We transform the recommendation process into\nsentiment analysis of natural languages containing information of user profiles\nand item attributes, where the sentiment polarity is predicted with prompt\nlearning. By harnessing the extensive knowledge housed within language models,\nthe prediction can be made without historical user-item interaction records. A\nbenchmark is also introduced to evaluate the proposed method under the\ncold-start setting, and the results demonstrate the effectiveness of our\nmethod. To the best of our knowledge, this is the first study to tackle the\nsystem cold-start recommendation problem. The benchmark and implementation of\nthe method are available at https://github.com/JacksonWuxs/PromptRec.",
        "translated": ""
    },
    {
        "title": "ChatGPT vs. Google: A Comparative Study of Search Performance and User\n  Experience",
        "url": "http://arxiv.org/abs/2307.01135v1",
        "pub_date": "2023-07-03",
        "summary": "The advent of ChatGPT, a large language model-powered chatbot, has prompted\nquestions about its potential implications for traditional search engines. In\nthis study, we investigate the differences in user behavior when employing\nsearch engines and chatbot tools for information-seeking tasks. We carry out a\nrandomized online experiment, dividing participants into two groups: one using\na ChatGPT-like tool and the other using a Google Search-like tool. Our findings\nreveal that the ChatGPT group consistently spends less time on all tasks, with\nno significant difference in overall task performance between the groups.\nNotably, ChatGPT levels user search performance across different education\nlevels and excels in answering straightforward questions and providing general\nsolutions but falls short in fact-checking tasks. Users perceive ChatGPT's\nresponses as having higher information quality compared to Google Search,\ndespite displaying a similar level of trust in both tools. Furthermore,\nparticipants using ChatGPT report significantly better user experiences in\nterms of usefulness, enjoyment, and satisfaction, while perceived ease of use\nremains comparable between the two tools. However, ChatGPT may also lead to\noverreliance and generate or replicate misinformation, yielding inconsistent\nresults. Our study offers valuable insights for search engine management and\nhighlights opportunities for integrating chatbot technologies into search\nengine designs.",
        "translated": ""
    },
    {
        "title": "OpenSiteRec: An Open Dataset for Site Recommendation",
        "url": "http://arxiv.org/abs/2307.00856v1",
        "pub_date": "2023-07-03",
        "summary": "As a representative information retrieval task, site recommendation, which\naims at predicting the optimal sites for a brand or an institution to open new\nbranches in an automatic data-driven way, is beneficial and crucial for brand\ndevelopment in modern business. However, there is no publicly available dataset\nso far and most existing approaches are limited to an extremely small scope of\nbrands, which seriously hinders the research on site recommendation. Therefore,\nwe collect, construct and release an open comprehensive dataset, namely\nOpenSiteRec, to facilitate and promote the research on site recommendation.\nSpecifically, OpenSiteRec leverages a heterogeneous graph schema to represent\nvarious types of real-world entities and relations in four international\nmetropolises. To evaluate the performance of the existing general methods on\nthe site recommendation task, we conduct benchmarking experiments of several\nrepresentative recommendation models on OpenSiteRec. Furthermore, we also\nhighlight the potential application directions to demonstrate the wide\napplicability of OpenSiteRec. We believe that our OpenSiteRec dataset is\nsignificant and anticipated to encourage the development of advanced methods\nfor site recommendation. OpenSiteRec is available online at\nhttps://OpenSiteRec.github.io/.",
        "translated": ""
    },
    {
        "title": "Looks Can Be Deceiving: Linking User-Item Interactions and User's\n  Propensity Towards Multi-Objective Recommendations",
        "url": "http://arxiv.org/abs/2307.00654v1",
        "pub_date": "2023-07-02",
        "summary": "Multi-objective recommender systems (MORS) provide suggestions to users\naccording to multiple (and possibly conflicting) goals. When a system optimizes\nits results at the individual-user level, it tailors them on a user's\npropensity towards the different objectives. Hence, the capability to\nunderstand users' fine-grained needs towards each goal is crucial. In this\npaper, we present the results of a user study in which we monitored the way\nusers interacted with recommended items, as well as their self-proclaimed\npropensities towards relevance, novelty and diversity objectives. The study was\ndivided into several sessions, where users evaluated recommendation lists\noriginating from a relevance-only single-objective baseline as well as MORS. We\nshow that despite MORS-based recommendations attracted less selections, its\npresence in the early sessions is crucial for users' satisfaction in the later\nstages. Surprisingly, the self-proclaimed willingness of users to interact with\nnovel and diverse items is not always reflected in the recommendations they\naccept. Post-study questionnaires provide insights on how to deal with this\nmatter, suggesting that MORS-based results should be accompanied by elements\nthat allow users to understand the recommendations, so as to facilitate their\nacceptance.",
        "translated": ""
    },
    {
        "title": "BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed\n  Search Logs for Zero-shot Biomedical Information Retrieval",
        "url": "http://arxiv.org/abs/2307.00589v1",
        "pub_date": "2023-07-02",
        "summary": "Information retrieval (IR) is essential in biomedical knowledge acquisition\nand clinical decision support. While recent progress has shown that language\nmodel encoders perform better semantic retrieval, training such models requires\nabundant query-article annotations that are difficult to obtain in biomedicine.\nAs a result, most biomedical IR systems only conduct lexical matching. In\nresponse, we introduce BioCPT, a first-of-its-kind Contrastively Pre-trained\nTransformer model for zero-shot biomedical IR. To train BioCPT, we collected an\nunprecedented scale of 255 million user click logs from PubMed. With such data,\nwe use contrastive learning to train a pair of closely-integrated retriever and\nre-ranker. Experimental results show that BioCPT sets new state-of-the-art\nperformance on five biomedical IR tasks, outperforming various baselines\nincluding much larger models such as GPT-3-sized cpt-text-XL. In addition,\nBioCPT also generates better biomedical article and sentence representations\nfor semantic evaluations. As such, BioCPT can be readily applied to various\nreal-world biomedical IR tasks. BioCPT API and code are publicly available at\nhttps://github.com/ncbi/BioCPT.",
        "translated": ""
    },
    {
        "title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text",
        "url": "http://arxiv.org/abs/2307.00509v1",
        "pub_date": "2023-07-02",
        "summary": "The task of textual geolocation - retrieving the coordinates of a place based\non a free-form language description - calls for not only grounding but also\nnatural language understanding and geospatial reasoning. Even though there are\nquite a few datasets in English used for geolocation, they are currently based\non open-source data (Wikipedia and Twitter), where the location of the\ndescribed place is mostly implicit, such that the location retrieval resolution\nis limited. Furthermore, there are no datasets available for addressing the\nproblem of textual geolocation in morphologically rich and resource-poor\nlanguages, such as Hebrew. In this paper, we present the Hebrew Geo-Location\n(HeGeL) corpus, designed to collect literal place descriptions and analyze\nlingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place\ndescriptions of various place types in three cities in Israel. Qualitative and\nempirical analysis show that the data exhibits abundant use of geospatial\nreasoning and requires a novel environmental representation.",
        "translated": ""
    },
    {
        "title": "Text based Large Language Model for Recommendation",
        "url": "http://arxiv.org/abs/2307.00457v1",
        "pub_date": "2023-07-02",
        "summary": "In recent years, large language models (LLM) have emerged as powerful tools\nfor diverse natural language processing tasks. However, their potential for\nrecommendation systems remains relatively unexplored. This paper presents an\ninnovative approach to recommendation systems using large language models\n(LLMs) based on text data. In this paper, we present a novel text-based large\nlanguage model for recommendation (TBLLMR) that utilized the expressive power\nof LLM to generate personalized recommendation. TBLLMR uses LLM's understanding\nability to interpret context, learn user preferences, and generate relevant\nrecommendation. Our proposed approach leverages the vast knowledge encoded in\nlarge language models to accomplish recommendation tasks. We first we formulate\nspecialized prompts to enhance the ability of LLM to comprehend recommendation\ntasks. Subsequently, we use these prompts to fine-tune the model on a dataset\nof user-item interactions, represented by textual data, to capture user\npreferences and item characteristics. Our research underscores the potential of\ntext-based LLMs in revolutionizing the domain of recommendation systems and\noffers a foundational framework for future explorations in this field. We\nconduct extensive experiments on benchmark datasets, and the experiments shows\nthat our TBLLMR has significant better results on large dataset.",
        "translated": ""
    },
    {
        "title": "One Copy Is All You Need: Resource-Efficient Streaming of Medical\n  Imaging Data at Scale",
        "url": "http://arxiv.org/abs/2307.00438v1",
        "pub_date": "2023-07-01",
        "summary": "Large-scale medical imaging datasets have accelerated development of\nartificial intelligence tools for clinical decision support. However, the large\nsize of these datasets is a bottleneck for users with limited storage and\nbandwidth. Many users may not even require such large datasets as AI models are\noften trained on lower resolution images. If users could directly download at\ntheir desired resolution, storage and bandwidth requirements would\nsignificantly decrease. However, it is impossible to anticipate every users'\nrequirements and impractical to store the data at multiple resolutions. What if\nwe could store images at a single resolution but send them at different ones?\nWe propose MIST, an open-source framework to operationalize progressive\nresolution for streaming medical images at multiple resolutions from a single\nhigh-resolution copy. We demonstrate that MIST can dramatically reduce imaging\ninfrastructure inefficiencies for hosting and streaming medical images by &gt;90%,\nwhile maintaining diagnostic quality for deep learning applications.",
        "translated": ""
    },
    {
        "title": "Effective Matching of Patients to Clinical Trials using Entity\n  Extraction and Neural Re-ranking",
        "url": "http://arxiv.org/abs/2307.00381v1",
        "pub_date": "2023-07-01",
        "summary": "Clinical trials (CTs) often fail due to inadequate patient recruitment. This\npaper tackles the challenges of CT retrieval by presenting an approach that\naddresses the patient-to-trials paradigm. Our approach involves two key\ncomponents in a pipeline-based model: (i) a data enrichment technique for\nenhancing both queries and documents during the first retrieval stage, and (ii)\na novel re-ranking schema that uses a Transformer network in a setup adapted to\nthis task by leveraging the structure of the CT documents. We use named entity\nrecognition and negation detection in both patient description and the\neligibility section of CTs. We further classify patient descriptions and CT\neligibility criteria into current, past, and family medical conditions. This\nextracted information is used to boost the importance of disease and drug\nmentions in both query and index for lexical retrieval. Furthermore, we propose\na two-step training schema for the Transformer network used to re-rank the\nresults from the lexical retrieval. The first step focuses on matching patient\ninformation with the descriptive sections of trials, while the second step aims\nto determine eligibility by matching patient information with the criteria\nsection. Our findings indicate that the inclusion criteria section of the CT\nhas a great influence on the relevance score in lexical models, and that the\nenrichment techniques for queries and documents improve the retrieval of\nrelevant trials. The re-ranking strategy, based on our training schema,\nconsistently enhances CT retrieval and shows improved performance by 15\\% in\nterms of precision at retrieving eligible trials. The results of our\nexperiments suggest the benefit of making use of extracted entities. Moreover,\nour proposed re-ranking schema shows promising effectiveness compared to larger\nneural models, even with limited training data.",
        "translated": ""
    },
    {
        "title": "Improving Text Matching in E-Commerce Search with A Rationalizable,\n  Intervenable and Fast Entity-Based Relevance Model",
        "url": "http://arxiv.org/abs/2307.00370v1",
        "pub_date": "2023-07-01",
        "summary": "Discovering the intended items of user queries from a massive repository of\nitems is one of the main goals of an e-commerce search system. Relevance\nprediction is essential to the search system since it helps improve\nperformance. When online serving a relevance model, the model is required to\nperform fast and accurate inference. Currently, the widely used models such as\nBi-encoder and Cross-encoder have their limitations in accuracy or inference\nspeed respectively. In this work, we propose a novel model called the\nEntity-Based Relevance Model (EBRM). We identify the entities contained in an\nitem and decompose the QI (query-item) relevance problem into multiple QE\n(query-entity) relevance problems; we then aggregate their results to form the\nQI prediction using a soft logic formulation. The decomposition allows us to\nuse a Cross-encoder QE relevance module for high accuracy as well as cache QE\npredictions for fast online inference. Utilizing soft logic makes the\nprediction procedure interpretable and intervenable. We also show that\npretraining the QE module with auto-generated QE data from user logs can\nfurther improve the overall performance. The proposed method is evaluated on\nlabeled data from e-commerce websites. Empirical results show that it achieves\npromising improvements with computation efficiency.",
        "translated": ""
    },
    {
        "title": "Improving Multitask Retrieval by Promoting Task Specialization",
        "url": "http://arxiv.org/abs/2307.00342v1",
        "pub_date": "2023-07-01",
        "summary": "In multitask retrieval, a single retriever is trained to retrieve relevant\ncontexts for multiple tasks. Despite its practical appeal, naive multitask\nretrieval lags behind task-specific retrieval in which a separate retriever is\ntrained for each task. We show that it is possible to train a multitask\nretriever that outperforms task-specific retrievers by promoting task\nspecialization. The main ingredients are: (1) a better choice of pretrained\nmodel (one that is explicitly optimized for multitasking) along with compatible\nprompting, and (2) a novel adaptive learning method that encourages each\nparameter to specialize in a particular task. The resulting multitask retriever\nis highly performant on the KILT benchmark. Upon analysis, we find that the\nmodel indeed learns parameters that are more task-specialized compared to naive\nmultitasking without prompting or adaptive learning.",
        "translated": ""
    },
    {
        "title": "MultiVENT: Multilingual Videos of Events with Aligned Natural Text",
        "url": "http://arxiv.org/abs/2307.03153v1",
        "pub_date": "2023-07-06",
        "summary": "Everyday news coverage has shifted from traditional broadcasts towards a wide\nrange of presentation formats such as first-hand, unedited video footage.\nDatasets that reflect the diverse array of multimodal, multilingual news\nsources available online could be used to teach models to benefit from this\nshift, but existing news video datasets focus on traditional news broadcasts\nproduced for English-speaking audiences. We address this limitation by\nconstructing MultiVENT, a dataset of multilingual, event-centric videos\ngrounded in text documents across five target languages. MultiVENT includes\nboth news broadcast videos and non-professional event footage, which we use to\nanalyze the state of online news videos and how they can be leveraged to build\nrobust, factually accurate models. Finally, we provide a model for complex,\nmultilingual video retrieval to serve as a baseline for information retrieval\nusing MultiVENT.",
        "translated": ""
    },
    {
        "title": "Track Mix Generation on Music Streaming Services using Transformers",
        "url": "http://arxiv.org/abs/2307.03045v1",
        "pub_date": "2023-07-06",
        "summary": "This paper introduces Track Mix, a personalized playlist generation system\nreleased in 2022 on the music streaming service Deezer. Track Mix automatically\ngenerates \"mix\" playlists inspired by initial music tracks, allowing users to\ndiscover music similar to their favorite content. To generate these mixes, we\nconsider a Transformer model trained on millions of track sequences from user\nplaylists. In light of the growing popularity of Transformers in recent years,\nwe analyze the advantages, drawbacks, and technical challenges of using such a\nmodel for mix generation on the service, compared to a more traditional\ncollaborative filtering approach. Since its release, Track Mix has been\ngenerating playlists for millions of users daily, enhancing their music\ndiscovery experience on Deezer.",
        "translated": ""
    },
    {
        "title": "Improving Retrieval-Augmented Large Language Models via Data Importance\n  Learning",
        "url": "http://arxiv.org/abs/2307.03027v1",
        "pub_date": "2023-07-06",
        "summary": "Retrieval augmentation enables large language models to take advantage of\nexternal knowledge, for example on tasks like question answering and data\nimputation. However, the performance of such retrieval-augmented models is\nlimited by the data quality of their underlying retrieval corpus. In this\npaper, we propose an algorithm based on multilinear extension for evaluating\nthe data importance of retrieved data points. There are exponentially many\nterms in the multilinear extension, and one key contribution of this paper is a\npolynomial time algorithm that computes exactly, given a retrieval-augmented\nmodel with an additive utility function and a validation set, the data\nimportance of data points in the retrieval corpus using the multilinear\nextension of the model's utility function. We further proposed an even more\nefficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental\nresults illustrate that we can enhance the performance of large language models\nby only pruning or reweighting the retrieval corpus, without requiring further\ntraining. For some tasks, this even allows a small model (e.g., GPT-JT),\naugmented with a search engine API, to outperform GPT-3.5 (without retrieval\naugmentation). Moreover, we show that weights based on multilinear extension\ncan be computed efficiently in practice (e.g., in less than ten minutes for a\ncorpus with 100 million elements).",
        "translated": ""
    },
    {
        "title": "A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System\n  Ranking Consistency and Discriminative Power",
        "url": "http://arxiv.org/abs/2307.02936v1",
        "pub_date": "2023-07-06",
        "summary": "Recently, Moffat et al. proposed an analytic framework, namely C/W/L/A, for\noffline evaluation metrics. This framework allows information retrieval (IR)\nresearchers to design evaluation metrics through the flexible combination of\nuser browsing models and user gain aggregations. However, the statistical\nstability of C/W/L/A metrics with different aggregations is not yet\ninvestigated. In this study, we investigate the statistical stability of\nC/W/L/A metrics from the perspective of: (1) the system ranking similarity\namong aggregations, (2) the system ranking consistency of aggregations and (3)\nthe discriminative power of aggregations. More specifically, we combined\nvarious aggregation functions with the browsing model of Precision, Discounted\nCumulative Gain (DCG), Rank-Biased Precision (RBP), INST, Average Precision\n(AP) and Expected Reciprocal Rank (ERR), examing their performances in terms of\nsystem ranking similarity, system ranking consistency and discriminative power\non two offline test collections. Our experimental result suggests that, in\nterms of system ranking consistency and discriminative power, the aggregation\nfunction of expected rate of gain (ERG) has an outstanding performance while\nthe aggregation function of maximum relevance usually has an insufficient\nperformance. The result also suggests that Precision, DCG, RBP, INST and AP\nwith their canonical aggregation all have favourable performances in system\nranking consistency and discriminative power; but for ERR, replacing its\ncanonical aggregation with ERG can further strengthen the discriminative power\nwhile obtaining a system ranking list similar to the canonical version at the\nsame time.",
        "translated": ""
    },
    {
        "title": "PLIERS: a Popularity-Based Recommender System for Content Dissemination\n  in Online Social Networks",
        "url": "http://arxiv.org/abs/2307.02865v1",
        "pub_date": "2023-07-06",
        "summary": "In this paper, we propose a novel tag-based recommender system called PLIERS,\nwhich relies on the assumption that users are mainly interested in items and\ntags with similar popularity to those they already own. PLIERS is aimed at\nreaching a good tradeoff between algorithmic complexity and the level of\npersonalization of recommended items. To evaluate PLIERS, we performed a set of\nexperiments on real OSN datasets, demonstrating that it outperforms\nstate-of-the-art solutions in terms of personalization, relevance, and novelty\nof recommendations.",
        "translated": ""
    },
    {
        "title": "BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by\n  Eliminating Ideological Segregation in Knowledge-based Recommendations",
        "url": "http://arxiv.org/abs/2307.02797v1",
        "pub_date": "2023-07-06",
        "summary": "In the realm of personalized recommendation systems, the increasing concern\nis the amplification of belief imbalance and user biases, a phenomenon\nprimarily attributed to the filter bubble. Addressing this critical issue, we\nintroduce an innovative intermediate agency (BHEISR) between users and existing\nrecommendation systems to attenuate the negative repercussions of the filter\nbubble effect in extant recommendation systems. The main objective is to strike\na belief balance for users while minimizing the detrimental influence caused by\nfilter bubbles. The BHEISR model amalgamates principles from nudge theory while\nupholding democratic and transparent principles. It harnesses user-specific\ncategory information to stimulate curiosity, even in areas users might\ninitially deem uninteresting. By progressively stimulating interest in novel\ncategories, the model encourages users to broaden their belief horizons and\nexplore the information they typically overlook. Our model is time-sensitive\nand operates on a user feedback loop. It utilizes the existing recommendation\nalgorithm of the model and incorporates user feedback from the prior time\nframe. This approach endeavors to transcend the constraints of the filter\nbubble, enrich recommendation diversity, and strike a belief balance among\nusers while also catering to user preferences and system-specific business\nrequirements. To validate the effectiveness and reliability of the BHEISR\nmodel, we conducted a series of comprehensive experiments with real-world\ndatasets. These experiments compared the performance of the BHEISR model\nagainst several baseline models using nearly 200 filter bubble-impacted users\nas test subjects. Our experimental results conclusively illustrate the superior\nperformance of the BHEISR model in mitigating filter bubbles and balancing user\nperspectives.",
        "translated": ""
    },
    {
        "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start\n  Recommendation",
        "url": "http://arxiv.org/abs/2307.02761v1",
        "pub_date": "2023-07-06",
        "summary": "Multimedia recommendation aims to fuse the multi-modal information of items\nfor feature enrichment to improve the recommendation performance. However,\nexisting methods typically introduce multi-modal information based on\ncollaborative information to improve the overall recommendation precision,\nwhile failing to explore its cold-start recommendation performance. Meanwhile,\nthese above methods are only applicable when such multi-modal data is\navailable. To address this problem, this paper proposes a recommendation\nframework, named Cross-modal Content Inference and Feature Enrichment\nRecommendation (CIERec), which exploits the multi-modal information to improve\nits cold-start recommendation performance. Specifically, CIERec first\nintroduces image annotation as the privileged information to help guide the\nmapping of unified features from the visual space to the semantic space in the\ntraining phase. And then CIERec enriches the content representation with the\nfusion of collaborative, visual, and cross-modal inferred representations, so\nas to improve its cold-start recommendation performance. Experimental results\non two real-world datasets show that the content representations learned by\nCIERec are able to achieve superior cold-start recommendation performance over\nexisting visually-aware recommendation algorithms. More importantly, CIERec can\nconsistently achieve significant improvements with different conventional\nvisually-aware backbones, which verifies its universality and effectiveness.",
        "translated": ""
    },
    {
        "title": "Knowledge Graph Self-Supervised Rationalization for Recommendation",
        "url": "http://arxiv.org/abs/2307.02759v1",
        "pub_date": "2023-07-06",
        "summary": "In this paper, we introduce a new self-supervised rationalization method,\ncalled KGRec, for knowledge-aware recommender systems. To effectively identify\ninformative knowledge connections, we propose an attentive knowledge\nrationalization mechanism that generates rational scores for knowledge\ntriplets. With these scores, KGRec integrates generative and contrastive\nself-supervised tasks for recommendation through rational masking. To highlight\nrationales in the knowledge graph, we design a novel generative task in the\nform of masking-reconstructing. By masking important knowledge with high\nrational scores, KGRec is trained to rebuild and highlight useful knowledge\nconnections that serve as rationales. To further rationalize the effect of\ncollaborative interactions on knowledge graph learning, we introduce a\ncontrastive learning task that aligns signals from knowledge and user-item\ninteraction views. To ensure noise-resistant contrasting, potential noisy edges\nin both graphs judged by the rational scores are masked. Extensive experiments\non three real-world datasets demonstrate that KGRec outperforms\nstate-of-the-art methods. We also provide the implementation codes for our\napproach at https://github.com/HKUDS/KGRec.",
        "translated": ""
    },
    {
        "title": "Dense Retrieval Adaptation using Target Domain Description",
        "url": "http://arxiv.org/abs/2307.02740v1",
        "pub_date": "2023-07-06",
        "summary": "In information retrieval (IR), domain adaptation is the process of adapting a\nretrieval model to a new domain whose data distribution is different from the\nsource domain. Existing methods in this area focus on unsupervised domain\nadaptation where they have access to the target document collection or\nsupervised (often few-shot) domain adaptation where they additionally have\naccess to (limited) labeled data in the target domain. There also exists\nresearch on improving zero-shot performance of retrieval models with no\nadaptation. This paper introduces a new category of domain adaptation in IR\nthat is as-yet unexplored. Here, similar to the zero-shot setting, we assume\nthe retrieval model does not have access to the target document collection. In\ncontrast, it does have access to a brief textual description that explains the\ntarget domain. We define a taxonomy of domain attributes in retrieval tasks to\nunderstand different properties of a source domain that can be adapted to a\ntarget domain. We introduce a novel automatic data construction pipeline that\nproduces a synthetic document collection, query set, and pseudo relevance\nlabels, given a textual domain description. Extensive experiments on five\ndiverse target domains show that adapting dense retrieval models using the\nconstructed synthetic data leads to effective retrieval performance on the\ntarget domain.",
        "translated": ""
    },
    {
        "title": "Improving Address Matching using Siamese Transformer Networks",
        "url": "http://arxiv.org/abs/2307.02300v1",
        "pub_date": "2023-07-05",
        "summary": "Matching addresses is a critical task for companies and post offices involved\nin the processing and delivery of packages. The ramifications of incorrectly\ndelivering a package to the wrong recipient are numerous, ranging from harm to\nthe company's reputation to economic and environmental costs. This research\nintroduces a deep learning-based model designed to increase the efficiency of\naddress matching for Portuguese addresses. The model comprises two parts: (i) a\nbi-encoder, which is fine-tuned to create meaningful embeddings of Portuguese\npostal addresses, utilized to retrieve the top 10 likely matches of the\nun-normalized target address from a normalized database, and (ii) a\ncross-encoder, which is fine-tuned to accurately rerank the 10 addresses\nobtained by the bi-encoder. The model has been tested on a real-case scenario\nof Portuguese addresses and exhibits a high degree of accuracy, exceeding 95%\nat the door level. When utilized with GPU computations, the inference speed is\nabout 4.5 times quicker than other traditional approaches such as BM25. An\nimplementation of this system in a real-world scenario would substantially\nincrease the effectiveness of the distribution process. Such an implementation\nis currently under investigation.",
        "translated": ""
    },
    {
        "title": "A Network Resource Allocation Recommendation Method with An Improved\n  Similarity Measure",
        "url": "http://arxiv.org/abs/2307.03399v1",
        "pub_date": "2023-07-07",
        "summary": "Recommender systems have been acknowledged as efficacious tools for managing\ninformation overload. Nevertheless, conventional algorithms adopted in such\nsystems primarily emphasize precise recommendations and, consequently, overlook\nother vital aspects like the coverage, diversity, and novelty of items. This\napproach results in less exposure for long-tail items. In this paper, to\npersonalize the recommendations and allocate recommendation resources more\npurposively, a method named PIM+RA is proposed. This method utilizes a\nbipartite network that incorporates self-connecting edges and weights.\nFurthermore, an improved Pearson correlation coefficient is employed for better\nredistribution. The evaluation of PIM+RA demonstrates a significant enhancement\nnot only in accuracy but also in coverage, diversity, and novelty of the\nrecommendation. It leads to a better balance in recommendation frequency by\nproviding effective exposure to long-tail items, while allowing customized\nparameters to adjust the recommendation list bias.",
        "translated": ""
    },
    {
        "title": "InfoSync: Information Synchronization across Multilingual\n  Semi-structured Tables",
        "url": "http://arxiv.org/abs/2307.03313v1",
        "pub_date": "2023-07-06",
        "summary": "Information Synchronization of semi-structured data across languages is\nchallenging. For instance, Wikipedia tables in one language should be\nsynchronized across languages. To address this problem, we introduce a new\ndataset InfoSyncC and a two-step method for tabular synchronization. InfoSync\ncontains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages,\nof which a subset (3.5K pairs) are manually annotated. The proposed method\nincludes 1) Information Alignment to map rows and 2) Information Update for\nupdating missing/outdated information for aligned tables across multilingual\ntables. When evaluated on InfoSync, information alignment achieves an F1 score\nof 87.91 (en &lt;-&gt; non-en). To evaluate information updation, we perform\nhuman-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach\nobtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of\nthe proposed method.",
        "translated": ""
    },
    {
        "title": "Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph\n  Reasoning",
        "url": "http://arxiv.org/abs/2307.03591v1",
        "pub_date": "2023-07-06",
        "summary": "Multimodal knowledge graphs (MKGs), which intuitively organize information in\nvarious modalities, can benefit multiple practical downstream tasks, such as\nrecommendation systems, and visual question answering. However, most MKGs are\nstill far from complete, which motivates the flourishing of MKG reasoning\nmodels. Recently, with the development of general artificial architectures, the\npretrained transformer models have drawn increasing attention, especially for\nmultimodal scenarios. However, the research of multimodal pretrained\ntransformer (MPT) for knowledge graph reasoning (KGR) is still at an early\nstage. As the biggest difference between MKG and other multimodal data, the\nrich structural information underlying the MKG still cannot be fully leveraged\nin existing MPT models. Most of them only utilize the graph structure as a\nretrieval map for matching images and texts connected with the same entity.\nThis manner hinders their reasoning performances. To this end, we propose the\ngraph Structure Guided Multimodal Pretrained Transformer for knowledge graph\nreasoning, termed SGMPT. Specifically, the graph structure encoder is adopted\nfor structural feature encoding. Then, a structure-guided fusion module with\ntwo different strategies, i.e., weighted summation and alignment constraint, is\nfirst designed to inject the structural information into both the textual and\nvisual features. To the best of our knowledge, SGMPT is the first MPT model for\nmultimodal KGR, which mines the structural information underlying the knowledge\ngraph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that\nour SGMPT outperforms existing state-of-the-art models, and prove the\neffectiveness of the designed strategies.",
        "translated": ""
    },
    {
        "title": "Undecimated Wavelet Transform for Word Embedded Semantic Marginal\n  Autoencoder in Security improvement and Denoising different Languages",
        "url": "http://arxiv.org/abs/2307.03679v1",
        "pub_date": "2023-07-06",
        "summary": "By combining the undecimated wavelet transform within a Word Embedded\nSemantic Marginal Autoencoder (WESMA), this research study provides a novel\nstrategy for improving security measures and denoising multiple languages. The\nincorporation of these strategies is intended to address the issues of\nrobustness, privacy, and multilingualism in data processing applications. The\nundecimated wavelet transform is used as a feature extraction tool to identify\nprominent language patterns and structural qualities in the input data. The\nproposed system may successfully capture significant information while\npreserving the temporal and geographical links within the data by employing\nthis transform. This improves security measures by increasing the system's\nability to detect abnormalities, discover hidden patterns, and distinguish\nbetween legitimate content and dangerous threats. The Word Embedded Semantic\nMarginal Autoencoder also functions as an intelligent framework for\ndimensionality and noise reduction. The autoencoder effectively learns the\nunderlying semantics of the data and reduces noise components by exploiting\nword embeddings and semantic context. As a result, data quality and accuracy\nare increased in following processing stages. The suggested methodology is\ntested using a diversified dataset that includes several languages and security\nscenarios. The experimental results show that the proposed approach is\neffective in attaining security enhancement and denoising capabilities across\nmultiple languages. The system is strong in dealing with linguistic variances,\nproducing consistent outcomes regardless of the language used. Furthermore,\nincorporating the undecimated wavelet transform considerably improves the\nsystem's ability to efficiently address complex security concerns",
        "translated": ""
    },
    {
        "title": "Fairness and Diversity in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2307.04644v1",
        "pub_date": "2023-07-10",
        "summary": "Recommender systems are effective tools for mitigating information overload\nand have seen extensive applications across various domains. However, the\nsingle focus on utility goals proves to be inadequate in addressing real-world\nconcerns, leading to increasing attention to fairness-aware and diversity-aware\nrecommender systems. While most existing studies explore fairness and diversity\nindependently, we identify strong connections between these two domains. In\nthis survey, we first discuss each of them individually and then dive into\ntheir connections. Additionally, motivated by the concepts of user-level and\nitem-level fairness, we broaden the understanding of diversity to encompass not\nonly the item level but also the user level. With this expanded perspective on\nuser and item-level diversity, we re-interpret fairness studies from the\nviewpoint of diversity. This fresh perspective enhances our understanding of\nfairness-related work and paves the way for potential future research\ndirections. Papers discussed in this survey along with public code links are\navailable at\nhttps://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems .",
        "translated": ""
    },
    {
        "title": "InPars Toolkit: A Unified and Reproducible Synthetic Data Generation\n  Pipeline for Neural Information Retrieval",
        "url": "http://arxiv.org/abs/2307.04601v1",
        "pub_date": "2023-07-10",
        "summary": "Recent work has explored Large Language Models (LLMs) to overcome the lack of\ntraining data for Information Retrieval (IR) tasks. The generalization\nabilities of these models have enabled the creation of synthetic in-domain data\nby providing instructions and a few examples on a prompt. InPars and\nPromptagator have pioneered this approach and both methods have demonstrated\nthe potential of using LLMs as synthetic data generators for IR tasks. This\nmakes them an attractive solution for IR tasks that suffer from a lack of\nannotated data. However, the reproducibility of these methods was limited,\nbecause InPars' training scripts are based on TPUs -- which are not widely\naccessible -- and because the code for Promptagator was not released and its\nproprietary LLM is not publicly accessible. To fully realize the potential of\nthese methods and make their impact more widespread in the research community,\nthe resources need to be accessible and easy to reproduce by researchers and\npractitioners. Our main contribution is a unified toolkit for end-to-end\nreproducible synthetic data generation research, which includes generation,\nfiltering, training and evaluation. Additionally, we provide an interface to IR\nlibraries widely used by the community and support for GPU. Our toolkit not\nonly reproduces the InPars method and partially reproduces Promptagator, but\nalso provides a plug-and-play functionality allowing the use of different LLMs,\nexploring filtering methods and finetuning various reranker models on the\ngenerated data. We also made available all the synthetic data generated in this\nwork for the 18 different datasets in the BEIR benchmark which took more than\n2,000 GPU hours to be generated as well as the reranker models finetuned on the\nsynthetic data. Code and data are available at\nhttps://github.com/zetaalphavector/InPars",
        "translated": ""
    },
    {
        "title": "A Semi-Automated Solution Approach Selection Tool for Any Use Case via\n  Scopus and OpenAI: a Case Study for AI/ML in Oncology",
        "url": "http://arxiv.org/abs/2307.04573v1",
        "pub_date": "2023-07-10",
        "summary": "In today's vast literature landscape, a manual review is very time-consuming.\nTo address this challenge, this paper proposes a semi-automated tool for\nsolution method review and selection. It caters to researchers, practitioners,\nand decision-makers while serving as a benchmark for future work. The tool\ncomprises three modules: (1) paper selection and scoring, using a keyword\nselection scheme to query Scopus API and compute relevancy; (2) solution method\nextraction in papers utilizing OpenAI API; (3) sensitivity analysis and\npost-analyzes. It reveals trends, relevant papers, and methods. AI in the\noncology case study and several use cases are presented with promising results,\ncomparing the tool to manual ground truth.",
        "translated": ""
    },
    {
        "title": "Alleviating Matthew Effect of Offline Reinforcement Learning in\n  Interactive Recommendation",
        "url": "http://arxiv.org/abs/2307.04571v1",
        "pub_date": "2023-07-10",
        "summary": "Offline reinforcement learning (RL), a technology that offline learns a\npolicy from logged data without the need to interact with online environments,\nhas become a favorable choice in decision-making processes like interactive\nrecommendation. Offline RL faces the value overestimation problem. To address\nit, existing methods employ conservatism, e.g., by constraining the learned\npolicy to be close to behavior policies or punishing the rarely visited\nstate-action pairs. However, when applying such offline RL to recommendation,\nit will cause a severe Matthew effect, i.e., the rich get richer and the poor\nget poorer, by promoting popular items or categories while suppressing the less\npopular ones. It is a notorious issue that needs to be addressed in practical\nrecommender systems.\n  In this paper, we aim to alleviate the Matthew effect in offline RL-based\nrecommendation. Through theoretical analyses, we find that the conservatism of\nexisting methods fails in pursuing users' long-term satisfaction. It inspires\nus to add a penalty term to relax the pessimism on states with high entropy of\nthe logging policy and indirectly penalizes actions leading to less diverse\nstates. This leads to the main technical contribution of the work: Debiased\nmodel-based Offline RL (DORL) method. Experiments show that DORL not only\ncaptures user interests well but also alleviates the Matthew effect. The\nimplementation is available via https://github.com/chongminggao/DORL-codes.",
        "translated": ""
    },
    {
        "title": "Counterfactual Explanation for Fairness in Recommendation",
        "url": "http://arxiv.org/abs/2307.04386v1",
        "pub_date": "2023-07-10",
        "summary": "Fairness-aware recommendation eliminates discrimination issues to build\ntrustworthy recommendation systems.Explaining the causes of unfair\nrecommendations is critical, as it promotes fairness diagnostics, and thus\nsecures users' trust in recommendation models. Existing fairness explanation\nmethods suffer high computation burdens due to the large-scale search space and\nthe greedy nature of the explanation search process. Besides, they perform\nscore-based optimizations with continuous values, which are not applicable to\ndiscrete attributes such as gender and race. In this work, we adopt the novel\nparadigm of counterfactual explanation from causal inference to explore how\nminimal alterations in explanations change model fairness, to abandon the\ngreedy search for explanations. We use real-world attributes from Heterogeneous\nInformation Networks (HINs) to empower counterfactual reasoning on discrete\nattributes. We propose a novel Counterfactual Explanation for Fairness\n(CFairER) that generates attribute-level counterfactual explanations from HINs\nfor recommendation fairness. Our CFairER conducts off-policy reinforcement\nlearning to seek high-quality counterfactual explanations, with an attentive\naction pruning reducing the search space of candidate counterfactuals. The\ncounterfactual explanations help to provide rational and proximate explanations\nfor model fairness, while the attentive action pruning narrows the search space\nof attributes. Extensive experiments demonstrate our proposed model can\ngenerate faithful explanations while maintaining favorable recommendation\nperformance.",
        "translated": ""
    },
    {
        "title": "Causal Neural Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2307.04384v1",
        "pub_date": "2023-07-10",
        "summary": "Graph collaborative filtering (GCF) has gained considerable attention in\nrecommendation systems by leveraging graph learning techniques to enhance\ncollaborative filtering (CF) models. One classical approach in GCF is to learn\nuser and item embeddings by modeling complex graph relations and utilizing\nthese embeddings for CF models. However, the quality of the embeddings\nsignificantly impacts the recommendation performance of GCF models. In this\npaper, we argue that existing graph learning methods are insufficient in\ngenerating satisfactory embeddings for CF models. This is because they\naggregate neighboring node messages directly, which can result in incorrect\nestimations of user-item correlations. To overcome this limitation, we propose\na novel approach that incorporates causal modeling to explicitly encode the\ncausal effects of neighboring nodes on the target node. This approach enables\nus to identify spurious correlations and uncover the root causes of user\npreferences. We introduce Causal Neural Graph Collaborative Filtering (CNGCF),\nthe first causality-aware graph learning framework for CF. CNGCF integrates\ncausal modeling into the graph representation learning process, explicitly\ncoupling causal effects between node pairs into the core message-passing\nprocess of graph learning. As a result, CNGCF yields causality-aware embeddings\nthat promote robust recommendations. Our extensive experiments demonstrate that\nCNGCF provides precise recommendations that align with user preferences.\nTherefore, our proposed framework can address the limitations of existing GCF\nmodels and offer a more effective solution for recommendation systems.",
        "translated": ""
    },
    {
        "title": "Graph Contrastive Learning with Multi-Objective for Personalized Product\n  Retrieval in Taobao Search",
        "url": "http://arxiv.org/abs/2307.04322v1",
        "pub_date": "2023-07-10",
        "summary": "In e-commerce search, personalized retrieval is a crucial technique for\nimproving user shopping experience. Recent works in this domain have achieved\nsignificant improvements by the representation learning paradigm, e.g.,\nembedding-based retrieval (EBR) and collaborative filtering (CF). EBR methods\ndo not sufficiently exploit the useful collaborative signal and are difficult\nto learn the representations of long-tail item well. Graph-based CF methods\nimprove personalization by modeling collaborative signal within the user click\ngraph. However, existing Graph-based methods ignore user's multiple behaviours,\nsuch as click/purchase and the relevance constraint between user behaviours and\nitems.In this paper, we propose a Graph Contrastive Learning with\nMulti-Objective (GCL-MO) collaborative filtering model, which solves the\nproblems of weak relevance and incomplete personalization in e-commerce search.\nSpecifically, GCL-MO builds a homogeneous graph of items and then optimizes a\nmulti-objective function of personalization and relevance. Moreover, we propose\na modified contrastive loss for multi-objectives graph learning, which avoids\nthe mutual suppression among positive samples and thus improves the\ngeneralization and robustness of long-tail item representations. These learned\nitem embeddings are then used for personalized retrieval by constructing an\nefficient offline-to-online inverted table. GCL-MO outperforms the online\ncollaborative filtering baseline in both offline/online experimental metrics\nand shows a significant improvement in the online A/B testing of Taobao search.",
        "translated": ""
    },
    {
        "title": "DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge\n  Graphs",
        "url": "http://arxiv.org/abs/2307.04090v1",
        "pub_date": "2023-07-09",
        "summary": "Recent work within the Argument Mining community has shown the applicability\nof Natural Language Processing systems for solving problems found within\ncompetitive debate. One of the most important tasks within competitive debate\nis for debaters to create high quality debate cases. We show that effective\ndebate cases can be constructed using constrained shortest path traversals on\nArgumentative Semantic Knowledge Graphs. We study this potential in the context\nof a type of American Competitive Debate, called Policy Debate, which already\nhas a large scale dataset targeting it called DebateSum. We significantly\nimprove upon DebateSum by introducing 53180 new examples, as well as further\nuseful metadata for every example, to the dataset. We leverage the txtai\nsemantic search and knowledge graph toolchain to produce and contribute 9\nsemantic knowledge graphs built on this dataset. We create a unique method for\nevaluating which knowledge graphs are better in the context of producing policy\ndebate cases. A demo which automatically generates debate cases, along with all\nother code and the Knowledge Graphs, are open-sourced and made available to the\npublic here: https://github.com/Hellisotherpeople/DebateKG",
        "translated": ""
    },
    {
        "title": "Fairness-Aware Graph Neural Networks: A Survey",
        "url": "http://arxiv.org/abs/2307.03929v1",
        "pub_date": "2023-07-08",
        "summary": "Graph Neural Networks (GNNs) have become increasingly important due to their\nrepresentational power and state-of-the-art predictive performance on many\nfundamental learning tasks. Despite this success, GNNs suffer from fairness\nissues that arise as a result of the underlying graph data and the fundamental\naggregation mechanism that lies at the heart of the large class of GNN models.\nIn this article, we examine and categorize fairness techniques for improving\nthe fairness of GNNs. Previous work on fair GNN models and techniques are\ndiscussed in terms of whether they focus on improving fairness during a\npreprocessing step, during training, or in a post-processing phase.\nFurthermore, we discuss how such techniques can be used together whenever\nappropriate, and highlight the advantages and intuition as well. We also\nintroduce an intuitive taxonomy for fairness evaluation metrics including\ngraph-level fairness, neighborhood-level fairness, embedding-level fairness,\nand prediction-level fairness metrics. In addition, graph datasets that are\nuseful for benchmarking the fairness of GNN models are summarized succinctly.\nFinally, we highlight key open problems and challenges that remain to be\naddressed.",
        "translated": ""
    },
    {
        "title": "Embedding Mental Health Discourse for Community Recommendation",
        "url": "http://arxiv.org/abs/2307.03892v1",
        "pub_date": "2023-07-08",
        "summary": "Our paper investigates the use of discourse embedding techniques to develop a\ncommunity recommendation system that focuses on mental health support groups on\nsocial media. Social media platforms provide a means for users to anonymously\nconnect with communities that cater to their specific interests. However, with\nthe vast number of online communities available, users may face difficulties in\nidentifying relevant groups to address their mental health concerns. To address\nthis challenge, we explore the integration of discourse information from\nvarious subreddit communities using embedding techniques to develop an\neffective recommendation system. Our approach involves the use of content-based\nand collaborative filtering techniques to enhance the performance of the\nrecommendation system. Our findings indicate that the proposed approach\noutperforms the use of each technique separately and provides interpretability\nin the recommendation process.",
        "translated": ""
    },
    {
        "title": "Duncode Characters Shorter",
        "url": "http://arxiv.org/abs/2307.05414v1",
        "pub_date": "2023-07-11",
        "summary": "This paper investigates the employment of various encoders in text\ntransformation, converting characters into bytes. It discusses local encoders\nsuch as ASCII and GB-2312, which encode specific characters into shorter bytes,\nand universal encoders like UTF-8 and UTF-16, which can encode the complete\nUnicode set with greater space requirements and are gaining widespread\nacceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,\nhowever, lack self-synchronizing capabilities. Duncode is introduced as an\ninnovative encoding method that aims to encode the entire Unicode character set\nwith high space efficiency, akin to local encoders. It has the potential to\ncompress multiple characters of a string into a Duncode unit using fewer bytes.\nDespite offering less self-synchronizing identification information, Duncode\nsurpasses UTF8 in terms of space efficiency. The application is available at\n\\url{https://github.com/laohur/duncode}. Additionally, we have developed a\nbenchmark for evaluating character encoders across different languages. It\nencompasses 179 languages and can be accessed at\n\\url{https://github.com/laohur/wiki2txt}.",
        "translated": ""
    },
    {
        "title": "Temporal Graphs Anomaly Emergence Detection: Benchmarking For Social\n  Media Interactions",
        "url": "http://arxiv.org/abs/2307.05268v1",
        "pub_date": "2023-07-11",
        "summary": "Temporal graphs have become an essential tool for analyzing complex dynamic\nsystems with multiple agents. Detecting anomalies in temporal graphs is crucial\nfor various applications, including identifying emerging trends, monitoring\nnetwork security, understanding social dynamics, tracking disease outbreaks,\nand understanding financial dynamics. In this paper, we present a comprehensive\nbenchmarking study that compares 12 data-driven methods for anomaly detection\nin temporal graphs. We conduct experiments on two temporal graphs extracted\nfrom Twitter and Facebook, aiming to identify anomalies in group interactions.\nSurprisingly, our study reveals an unclear pattern regarding the best method\nfor such tasks, highlighting the complexity and challenges involved in anomaly\nemergence detection in large and dynamic systems. The results underscore the\nneed for further research and innovative approaches to effectively detect\nemerging anomalies in dynamic systems represented as temporal graphs.",
        "translated": ""
    },
    {
        "title": "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion",
        "url": "http://arxiv.org/abs/2307.05260v1",
        "pub_date": "2023-07-11",
        "summary": "The task of Prior Case Retrieval (PCR) in the legal domain is about\nautomatically citing relevant (based on facts and precedence) prior legal cases\nin a given query case. To further promote research in PCR, in this paper, we\npropose a new large benchmark (in English) for the PCR task: IL-PCR (Indian\nLegal Prior Case Retrieval) corpus. Given the complex nature of case relevance\nand the long size of legal documents, BM25 remains a strong baseline for\nranking the cited prior documents. In this work, we explore the role of events\nin legal case retrieval and propose an unsupervised retrieval method-based\npipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find\nthat the proposed unsupervised retrieval method significantly increases\nperformance compared to BM25 and makes retrieval faster by a considerable\nmargin, making it applicable to real-time case retrieval systems. Our proposed\nsystem is generic, we show that it generalizes across two different legal\nsystems (Indian and Canadian), and it shows state-of-the-art performance on the\nbenchmarks for both the legal systems (IL-PCR and COLIEE corpora).",
        "translated": ""
    },
    {
        "title": "Generative Contrastive Graph Learning for Recommendation",
        "url": "http://arxiv.org/abs/2307.05100v1",
        "pub_date": "2023-07-11",
        "summary": "By treating users' interactions as a user-item graph, graph learning models\nhave been widely deployed in Collaborative Filtering(CF) based recommendation.\nRecently, researchers have introduced Graph Contrastive Learning(GCL)\ntechniques into CF to alleviate the sparse supervision issue, which first\nconstructs contrastive views by data augmentations and then provides\nself-supervised signals by maximizing the mutual information between\ncontrastive views. Despite the effectiveness, we argue that current GCL-based\nrecommendation models are still limited as current data augmentation\ntechniques, either structure augmentation or feature augmentation. First,\nstructure augmentation randomly dropout nodes or edges, which is easy to\ndestroy the intrinsic nature of the user-item graph. Second, feature\naugmentation imposes the same scale noise augmentation on each node, which\nneglects the unique characteristics of nodes on the graph. To tackle the above\nlimitations, we propose a novel Variational Graph Generative-Contrastive\nLearning(VGCL) framework for recommendation. Specifically, we leverage\nvariational graph reconstruction to estimate a Gaussian distribution of each\nnode, then generate multiple contrastive views through multiple samplings from\nthe estimated distributions, which builds a bridge between generative and\ncontrastive learning. Besides, the estimated variances are tailored to each\nnode, which regulates the scale of contrastive loss for each node on\noptimization. Considering the similarity of the estimated distributions, we\npropose a cluster-aware twofold contrastive learning, a node-level to encourage\nconsistency of a node's contrastive views and a cluster-level to encourage\nconsistency of nodes in a cluster. Finally, extensive experimental results on\nthree public datasets clearly demonstrate the effectiveness of the proposed\nmodel.",
        "translated": ""
    },
    {
        "title": "Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with\n  Sample-aware Prompting and Dynamic Revision Chain",
        "url": "http://arxiv.org/abs/2307.05074v1",
        "pub_date": "2023-07-11",
        "summary": "Text-to-SQL aims at generating SQL queries for the given natural language\nquestions and thus helping users to query databases. Prompt learning with large\nlanguage models (LLMs) has emerged as a recent approach, which designs prompts\nto lead LLMs to understand the input question and generate the corresponding\nSQL. However, it faces challenges with strict SQL syntax requirements. Existing\nwork prompts the LLMs with a list of demonstration examples (i.e. question-SQL\npairs) to generate SQL, but the fixed prompts can hardly handle the scenario\nwhere the semantic gap between the retrieved demonstration and the input\nquestion is large. In this paper, we propose a retrieval-augmented prompting\nmethod for a LLM-based Text-to-SQL framework, involving sample-aware prompting\nand a dynamic revision chain. Our approach incorporates sample-aware\ndemonstrations, which include the composition of SQL operators and fine-grained\ninformation related to the given question. To retrieve questions sharing\nsimilar intents with input questions, we propose two strategies for assisting\nretrieval. Firstly, we leverage LLMs to simplify the original questions,\nunifying the syntax and thereby clarifying the users' intentions. To generate\nexecutable and accurate SQLs without human intervention, we design a dynamic\nrevision chain which iteratively adapts fine-grained feedback from the\npreviously generated SQL. Experimental results on three Text-to-SQL benchmarks\ndemonstrate the superiority of our method over strong baseline models.",
        "translated": ""
    },
    {
        "title": "Mining for Unknown Unknowns",
        "url": "http://arxiv.org/abs/2307.05071v1",
        "pub_date": "2023-07-11",
        "summary": "Unknown unknowns are future relevant contingencies that lack an ex ante\ndescription. While there are numerous retrospective accounts showing that\nsignificant gains or losses might have been achieved or avoided had such\ncontingencies been previously uncovered, getting hold of unknown unknowns still\nremains elusive, both in practice and conceptually. Using Formal Concept\nAnalysis (FCA) - a subfield of lattice theory which is increasingly applied for\nmining and organizing data - this paper introduces a simple framework to\nsystematically think out of the box and direct the search for unknown unknowns.",
        "translated": ""
    },
    {
        "title": "Neural-Symbolic Recommendation with Graph-Enhanced Information",
        "url": "http://arxiv.org/abs/2307.05036v1",
        "pub_date": "2023-07-11",
        "summary": "The recommendation system is not only a problem of inductive statistics from\ndata but also a cognitive task that requires reasoning ability. The most\nadvanced graph neural networks have been widely used in recommendation systems\nbecause they can capture implicit structured information from graph-structured\ndata. However, like most neural network algorithms, they only learn matching\npatterns from a perception perspective. Some researchers use user behavior for\nlogic reasoning to achieve recommendation prediction from the perspective of\ncognitive reasoning, but this kind of reasoning is a local one and ignores\nimplicit information on a global scale. In this work, we combine the advantages\nof graph neural networks and propositional logic operations to construct a\nneuro-symbolic recommendation model with both global implicit reasoning ability\nand local explicit logic reasoning ability. We first build an item-item graph\nbased on the principle of adjacent interaction and use graph neural networks to\ncapture implicit information in global data. Then we transform user behavior\ninto propositional logic expressions to achieve recommendations from the\nperspective of cognitive reasoning. Extensive experiments on five public\ndatasets show that our proposed model outperforms several state-of-the-art\nmethods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].",
        "translated": ""
    },
    {
        "title": "Empowering recommender systems using automatically generated Knowledge\n  Graphs and Reinforcement Learning",
        "url": "http://arxiv.org/abs/2307.04996v1",
        "pub_date": "2023-07-11",
        "summary": "Personalized recommendations have a growing importance in direct marketing,\nwhich motivates research to enhance customer experiences by knowledge graph\n(KG) applications. For example, in financial services, companies may benefit\nfrom providing relevant financial articles to their customers to cultivate\nrelationships, foster client engagement and promote informed financial\ndecisions. While several approaches center on KG-based recommender systems for\nimproved content, in this study we focus on interpretable KG-based recommender\nsystems for decision making.To this end, we present two knowledge graph-based\napproaches for personalized article recommendations for a set of customers of a\nlarge multinational financial services company. The first approach employs\nReinforcement Learning and the second approach uses the XGBoost algorithm for\nrecommending articles to the customers. Both approaches make use of a KG\ngenerated from both structured (tabular data) and unstructured data (a large\nbody of text data).Using the Reinforcement Learning-based recommender system we\ncould leverage the graph traversal path leading to the recommendation as a way\nto generate interpretations (Path Directed Reasoning (PDR)). In the\nXGBoost-based approach, one can also provide explainable results using post-hoc\nmethods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I\nam Five).Importantly, our approach offers explainable results, promoting better\ndecision-making. This study underscores the potential of combining advanced\nmachine learning techniques with KG-driven insights to bolster experience in\ncustomer relationship management.",
        "translated": ""
    },
    {
        "title": "Ranking with Long-Term Constraints",
        "url": "http://arxiv.org/abs/2307.04923v1",
        "pub_date": "2023-07-10",
        "summary": "The feedback that users provide through their choices (e.g., clicks,\npurchases) is one of the most common types of data readily available for\ntraining search and recommendation algorithms. However, myopically training\nsystems based on choice data may only improve short-term engagement, but not\nthe long-term sustainability of the platform and the long-term benefits to its\nusers, content providers, and other stakeholders. In this paper, we thus\ndevelop a new framework in which decision makers (e.g., platform operators,\nregulators, users) can express long-term goals for the behavior of the platform\n(e.g., fairness, revenue distribution, legal requirements). These goals take\nthe form of exposure or impact targets that go well beyond individual sessions,\nand we provide new control-based algorithms to achieve these goals. In\nparticular, the controllers are designed to achieve the stated long-term goals\nwith minimum impact on short-term engagement. Beyond the principled theoretical\nderivation of the controllers, we evaluate the algorithms on both synthetic and\nreal-world data. While all controllers perform well, we find that they provide\ninteresting trade-offs in efficiency, robustness, and the ability to plan\nahead.",
        "translated": ""
    },
    {
        "title": "Testing different Log Bases For Vector Model Weighting Technique",
        "url": "http://arxiv.org/abs/2307.06213v1",
        "pub_date": "2023-07-12",
        "summary": "Information retrieval systems retrieves relevant documents based on a query\nsubmitted by the user. The documents are initially indexed and the words in the\ndocuments are assigned weights using a weighting technique called TFIDF which\nis the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF\nrepresents the number of occurrences of a term in a document. IDF measures\nwhether the term is common or rare across all documents. It is computed by\ndividing the total number of documents in the system by the number of documents\ncontaining the term and then computing the logarithm of the quotient. By\ndefault, we use base 10 to calculate the logarithm. In this paper, we are going\nto test this weighting technique by using a range of log bases from 0.1 to\n100.0 to calculate the IDF. Testing different log bases for vector model\nweighting technique is to highlight the importance of understanding the\nperformance of the system at different weighting values. We use the documents\nof MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled\nexplicitly for experiments in data information retrieval systems.",
        "translated": ""
    },
    {
        "title": "DDNAS: Discretized Differentiable Neural Architecture Search for Text\n  Classification",
        "url": "http://arxiv.org/abs/2307.06005v1",
        "pub_date": "2023-07-12",
        "summary": "Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.",
        "translated": ""
    },
    {
        "title": "Contrastive Learning for Conversion Rate Prediction",
        "url": "http://arxiv.org/abs/2307.05974v1",
        "pub_date": "2023-07-12",
        "summary": "Conversion rate (CVR) prediction plays an important role in advertising\nsystems. Recently, supervised deep neural network-based models have shown\npromising performance in CVR prediction. However, they are data hungry and\nrequire an enormous amount of training data. In online advertising systems,\nalthough there are millions to billions of ads, users tend to click only a\nsmall set of them and to convert on an even smaller set. This data sparsity\nissue restricts the power of these deep models. In this paper, we propose the\nContrastive Learning for CVR prediction (CL4CVR) framework. It associates the\nsupervised CVR prediction task with a contrastive learning task, which can\nlearn better data representations exploiting abundant unlabeled data and\nimprove the CVR prediction performance. To tailor the contrastive learning task\nto the CVR prediction problem, we propose embedding masking (EM), rather than\nfeature masking, to create two views of augmented samples. We also propose a\nfalse negative elimination (FNE) component to eliminate samples with the same\nfeature as the anchor sample, to account for the natural property in user\nbehavior data. We further propose a supervised positive inclusion (SPI)\ncomponent to include additional positive samples for each anchor sample, in\norder to make full use of sparse but precious user conversion events.\nExperimental results on two real-world conversion datasets demonstrate the\nsuperior performance of CL4CVR. The source code is available at\nhttps://github.com/DongRuiHust/CL4CVR.",
        "translated": ""
    },
    {
        "title": "Relational Extraction on Wikipedia Tables using Convolutional and Memory\n  Networks",
        "url": "http://arxiv.org/abs/2307.05827v1",
        "pub_date": "2023-07-11",
        "summary": "Relation extraction (RE) is the task of extracting relations between entities\nin text. Most RE methods extract relations from free-form running text and\nleave out other rich data sources, such as tables. We explore RE from the\nperspective of applying neural methods on tabularly organized data. We\nintroduce a new model consisting of Convolutional Neural Network (CNN) and\nBidirectional-Long Short Term Memory (BiLSTM) network to encode entities and\nlearn dependencies among them, respectively. We evaluate our model on a large\nand recent dataset and compare results with previous neural methods.\nExperimental results show that our model consistently outperforms the previous\nmodel for the task of relation extraction on tabular data. We perform\ncomprehensive error analyses and ablation study to show the contribution of\nvarious components of our model. Finally, we discuss the usefulness and\ntrade-offs of our approach, and provide suggestions for fostering further\nresearch.",
        "translated": ""
    },
    {
        "title": "Parmesan: mathematical concept extraction for education",
        "url": "http://arxiv.org/abs/2307.06699v1",
        "pub_date": "2023-07-13",
        "summary": "Mathematics is a highly specialized domain with its own unique set of\nchallenges that has seen limited study in natural language processing. However,\nmathematics is used in a wide variety of fields and multidisciplinary research\nin many different domains often relies on an understanding of mathematical\nconcepts. To aid researchers coming from other fields, we develop a prototype\nsystem for searching for and defining mathematical concepts in context,\nfocusing on the field of category theory. This system, Parmesan, depends on\nnatural language processing components including concept extraction, relation\nextraction, definition extraction, and entity linking. In developing this\nsystem, we show that existing techniques cannot be applied directly to the\ncategory theory domain, and suggest hybrid techniques that do perform well,\nthough we expect the system to evolve over time. We also provide two cleaned\nmathematical corpora that power the prototype system, which are based on\njournal articles and wiki pages, respectively. The corpora have been annotated\nwith dependency trees, lemmas, and part-of-speech tags.",
        "translated": ""
    },
    {
        "title": "Going Beyond Local: Global Graph-Enhanced Personalized News\n  Recommendations",
        "url": "http://arxiv.org/abs/2307.06576v1",
        "pub_date": "2023-07-13",
        "summary": "Precisely recommending candidate news articles to users has always been a\ncore challenge for personalized news recommendation systems. Most recent works\nprimarily focus on using advanced natural language processing techniques to\nextract semantic information from rich textual data, employing content-based\nmethods derived from local historical news. However, this approach lacks a\nglobal perspective, failing to account for users' hidden motivations and\nbehaviors beyond semantic information. To address this challenge, we propose a\nnovel model called GLORY (Global-LOcal news Recommendation sYstem), which\ncombines global representations learned from other users with local\nrepresentations to enhance personalized recommendation systems. We accomplish\nthis by constructing a Global-aware Historical News Encoder, which includes a\nglobal news graph and employs gated graph neural networks to enrich news\nrepresentations, thereby fusing historical news representations by a historical\nnews aggregator. Similarly, we extend this approach to a Global Candidate News\nEncoder, utilizing a global entity graph and a candidate news aggregator to\nenhance candidate news representation. Evaluation results on two public news\ndatasets demonstrate that our method outperforms existing approaches.\nFurthermore, our model offers more diverse recommendations.",
        "translated": ""
    },
    {
        "title": "Assessing the Ability of ChatGPT to Screen Articles for Systematic\n  Reviews",
        "url": "http://arxiv.org/abs/2307.06464v1",
        "pub_date": "2023-07-12",
        "summary": "By organizing knowledge within a research field, Systematic Reviews (SR)\nprovide valuable leads to steer research. Evidence suggests that SRs have\nbecome first-class artifacts in software engineering. However, the tedious\nmanual effort associated with the screening phase of SRs renders these studies\na costly and error-prone endeavor. While screening has traditionally been\nconsidered not amenable to automation, the advent of generative AI-driven\nchatbots, backed with large language models is set to disrupt the field. In\nthis report, we propose an approach to leverage these novel technological\ndevelopments for automating the screening of SRs. We assess the consistency,\nclassification performance, and generalizability of ChatGPT in screening\narticles for SRs and compare these figures with those of traditional\nclassifiers used in SR automation. Our results indicate that ChatGPT is a\nviable option to automate the SR processes, but requires careful considerations\nfrom developers when integrating ChatGPT into their SR tools.",
        "translated": ""
    },
    {
        "title": "Streaming CTR Prediction: Rethinking Recommendation Task for Real-World\n  Streaming Data",
        "url": "http://arxiv.org/abs/2307.07509v1",
        "pub_date": "2023-07-14",
        "summary": "The Click-Through Rate (CTR) prediction task is critical in industrial\nrecommender systems, where models are usually deployed on dynamic streaming\ndata in practical applications. Such streaming data in real-world recommender\nsystems face many challenges, such as distribution shift, temporal\nnon-stationarity, and systematic biases, which bring difficulties to the\ntraining and utilizing of recommendation models. However, most existing studies\napproach the CTR prediction as a classification task on static datasets,\nassuming that the train and test sets are independent and identically\ndistributed (a.k.a, i.i.d. assumption). To bridge this gap, we formulate the\nCTR prediction problem in streaming scenarios as a Streaming CTR Prediction\ntask. Accordingly, we propose dedicated benchmark settings and metrics to\nevaluate and analyze the performance of the models in streaming data. To better\nunderstand the differences compared to traditional CTR prediction tasks, we\ndelve into the factors that may affect the model performance, such as parameter\nscale, normalization, regularization, etc. The results reveal the existence of\nthe ''streaming learning dilemma'', whereby the same factor may have different\neffects on model performance in the static and streaming scenarios. Based on\nthe findings, we propose two simple but inspiring methods (i.e., tuning key\nparameters and exemplar replay) that significantly improve the effectiveness of\nthe CTR models in the new streaming scenario. We hope our work will inspire\nfurther research on streaming CTR prediction and help improve the robustness\nand adaptability of recommender systems.",
        "translated": ""
    },
    {
        "title": "PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language\n  Pre-training via Prompting",
        "url": "http://arxiv.org/abs/2307.07341v1",
        "pub_date": "2023-07-14",
        "summary": "Vision-language (VL) Pre-training (VLP) has shown to well generalize VL\nmodels over a wide range of VL downstream tasks, especially for cross-modal\nretrieval. However, it hinges on a huge amount of image-text pairs, which\nrequires tedious and costly curation. On the contrary, weakly-supervised VLP\n(W-VLP) explores means with object tags generated by a pre-trained object\ndetector (OD) from images. Yet, they still require paired information, i.e.\nimages and object-level annotations, as supervision to train an OD.\n  To further reduce the amount of supervision, we propose Prompts-in-The-Loop\n(PiTL) that prompts knowledge from large language models (LLMs) to describe\nimages. Concretely, given a category label of an image, e.g. refinery, the\nknowledge, e.g. a refinery could be seen with large storage tanks, pipework,\nand ..., extracted by LLMs is used as the language counterpart. The knowledge\nsupplements, e.g. the common relations among entities most likely appearing in\na scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of\n14K categories from ImageNet21K with PiTL. Empirically, the VL models\npre-trained with PiTL-generated pairs are strongly favored over other W-VLP\nworks on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with less\nsupervision. The results reveal the effectiveness of PiTL-generated pairs for\nVLP.",
        "translated": ""
    },
    {
        "title": "Hybrid moderation in the newsroom: Recommending featured posts to\n  content moderators",
        "url": "http://arxiv.org/abs/2307.07317v1",
        "pub_date": "2023-07-14",
        "summary": "Online news outlets are grappling with the moderation of user-generated\ncontent within their comment section. We present a recommender system based on\nranking class probabilities to support and empower the moderator in choosing\nfeatured posts, a time-consuming task. By combining user and textual content\nfeatures we obtain an optimal classification F1-score of 0.44 on the test set.\nFurthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of\nvalidation articles. As an expert evaluation, content moderators assessed the\noutput of a random selection of articles by choosing comments to feature based\non the recommendations, which resulted in a NDCG score of 0.83. We conclude\nthat first, adding text features yields the best score and second, while\nchoosing featured content remains somewhat subjective, content moderators found\nsuitable comments in all but one evaluated recommendations. We end the paper by\nanalyzing our best-performing model, a step towards transparency and\nexplainability in hybrid content moderation.",
        "translated": ""
    },
    {
        "title": "Learning to Retrieve In-Context Examples for Large Language Models",
        "url": "http://arxiv.org/abs/2307.07164v1",
        "pub_date": "2023-07-14",
        "summary": "Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of 30 tasks demonstrate that our framework significantly\nenhances in-context learning performance. Furthermore, we show the\ngeneralization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes.",
        "translated": ""
    },
    {
        "title": "Digital Health Discussion Through Articles Published Until the Year\n  2021: A Digital Topic Modeling Approach",
        "url": "http://arxiv.org/abs/2307.07130v1",
        "pub_date": "2023-07-14",
        "summary": "The digital health industry has grown in popularity since the 2010s, but\nthere has been limited analysis of the topics discussed in the field across\nacademic disciplines. This study aims to analyze the research trends of digital\nhealth-related articles published on the Web of Science until 2021, in order to\nunderstand the concentration, scope, and characteristics of the research.\n15,950 digital health-related papers from the top 10 academic fields were\nanalyzed using the Web of Science. The papers were grouped into three domains:\npublic health, medicine, and electrical engineering and computer science\n(EECS). Two time periods (2012-2016 and 2017-2021) were compared using Latent\nDirichlet Allocation (LDA) for topic modeling. The number of topics was\ndetermined based on coherence score, and topic compositions were compared using\na homogeneity test. The number of optimal topics varied across domains and time\nperiods. For public health, the first and second halves had 13 and 19 topics,\nrespectively. Medicine had 14 and 25 topics, and EECS had 7 and 21 topics. Text\nanalysis revealed shared topics among the domains, but with variations in\ncomposition. The homogeneity test confirmed significant differences between the\ngroups (p&lt;2.2e-16). Six dominant themes emerged, including journal article\nmethodology, information technology, medical issues, population demographics,\nsocial phenomena, and healthcare. Digital health research is expanding and\nevolving, particularly in relation to Covid-19, where topics such as depression\nand mental disorders, education, and physical activity have gained prominence.\nThere was no bias in topic composition among the three domains, but other\nfields like kinesiology or psychology could contribute to future digital health\nresearch. Exploring expanded topics that reflect people's needs for digital\nhealth over time will be crucial.",
        "translated": ""
    },
    {
        "title": "Making the Most Out of the Limited Context Length: Predictive Power\n  Varies with Clinical Note Type and Note Section",
        "url": "http://arxiv.org/abs/2307.07051v1",
        "pub_date": "2023-07-13",
        "summary": "Recent advances in large language models have led to renewed interest in\nnatural language processing in healthcare using the free text of clinical\nnotes. One distinguishing characteristic of clinical notes is their long time\nspan over multiple long documents. The unique structure of clinical notes\ncreates a new design choice: when the context length for a language model\npredictor is limited, which part of clinical notes should we choose as the\ninput? Existing studies either choose the inputs with domain knowledge or\nsimply truncate them. We propose a framework to analyze the sections with high\npredictive power. Using MIMIC-III, we show that: 1) predictive power\ndistribution is different between nursing notes and discharge notes and 2)\ncombining different types of notes could improve performance when the context\nlength is large. Our findings suggest that a carefully selected sampling\nfunction could enable more efficient information extraction from clinical\nnotes.",
        "translated": ""
    },
    {
        "title": "Towards Populating Generalizable Engineering Design Knowledge",
        "url": "http://arxiv.org/abs/2307.06985v1",
        "pub_date": "2023-07-13",
        "summary": "Aiming to populate generalizable engineering design knowledge, we propose a\nmethod to extract facts of the form head entity :: relationship :: tail entity\nfrom sentences found in patent documents. These facts could be combined within\nand across patent documents to form knowledge graphs that serve as schemes for\nrepresenting as well as storing design knowledge. Existing methods in\nengineering design literature often utilise a set of predefined relationships\nto populate triples that are statistical approximations rather than facts. In\nour method, we train a tagger to identify both entities and relationships from\na sentence. Given a pair of entities thus identified, we train another tagger\nto identify the relationship tokens that specifically denote the relationship\nbetween the pair. For training these taggers, we manually construct a dataset\nof 44,227 sentences and corresponding facts. We also compare the performance of\nthe method against typically recommended approaches, wherein, we predict the\nedges among tokens by pairing the tokens independently and as part of a graph.\nWe apply our method to sentences found in patents related to fan systems and\nbuild a domain knowledge base. Upon providing an overview of the knowledge\nbase, we search for solutions relevant to some key issues prevailing in fan\nsystems. We organize the responses into knowledge graphs and hold a comparative\ndiscussion against the opinions from ChatGPT.",
        "translated": ""
    },
    {
        "title": "NS4AR: A new, focused on sampling areas sampling method in graphical\n  recommendation Systems",
        "url": "http://arxiv.org/abs/2307.07321v1",
        "pub_date": "2023-07-13",
        "summary": "The effectiveness of graphical recommender system depends on the quantity and\nquality of negative sampling. This paper selects some typical recommender\nsystem models, as well as some latest negative sampling strategies on the\nmodels as baseline. Based on typical graphical recommender model, we divide\nsample region into assigned-n areas and use AdaSim to give different weight to\nthese areas to form positive set and negative set. Because of the volume and\nsignificance of negative items, we also proposed a subset selection model to\nnarrow the core negative samples.",
        "translated": ""
    },
    {
        "title": "Leveraging Recommender Systems to Reduce Content Gaps on Peer Production\n  Platforms",
        "url": "http://arxiv.org/abs/2307.08669v1",
        "pub_date": "2023-07-17",
        "summary": "Peer production platforms like Wikipedia commonly suffer from content gaps.\nPrior research suggests recommender systems can help solve this problem, by\nguiding editors towards underrepresented topics. However, it remains unclear\nwhether this approach would result in less relevant recommendations, leading to\nreduced overall engagement with recommended items. To answer this question, we\nfirst conducted offline analyses (Study 1) on SuggestBot, a task-routing\nrecommender system for Wikipedia, then did a three-month controlled experiment\n(Study 2). Our results show that presenting users with articles from\nunderrepresented topics increased the proportion of work done on those articles\nwithout significantly reducing overall recommendation uptake. We discuss the\nimplications of our results, including how ignoring the article discovery\nprocess can artificially narrow recommendations. We draw parallels between this\nphenomenon and the common issue of ``filter bubbles'' to show how any platform\nthat employs recommender systems is susceptible to it.",
        "translated": ""
    },
    {
        "title": "Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2307.08303v1",
        "pub_date": "2023-07-17",
        "summary": "Dense retrieval (DR) converts queries and documents into dense embeddings and\nmeasures the similarity between queries and documents in vector space. One of\nthe challenges in DR is the lack of domain-specific training data. While DR\nmodels can learn from large-scale public datasets like MS MARCO through\ntransfer learning, evidence shows that not all DR models and domains can\nbenefit from transfer learning equally. Recently, some researchers have\nresorted to large language models (LLMs) to improve the zero-shot and few-shot\nDR models. However, the hard prompts or human-written prompts utilized in these\nworks cannot guarantee the good quality of generated weak queries. To tackle\nthis, we propose soft prompt tuning for augmenting DR (SPTAR): For each task,\nwe leverage soft prompt-tuning to optimize a task-specific soft prompt on\nlimited ground truth data and then prompt the LLMs to tag unlabeled documents\nwith weak queries, yielding enough weak document-query pairs to train\ntask-specific dense retrievers. We design a filter to select high-quality\nexample document-query pairs in the prompt to further improve the quality of\nweak tagged queries. To the best of our knowledge, there is no prior work\nutilizing soft prompt tuning to augment DR models. The experiments demonstrate\nthat SPTAR outperforms the unsupervised baselines BM25 and the recently\nproposed LLMs-based augmentation method for DR.",
        "translated": ""
    },
    {
        "title": "Measuring Item Global Residual Value for Fair Recommendation",
        "url": "http://arxiv.org/abs/2307.08259v1",
        "pub_date": "2023-07-17",
        "summary": "In the era of information explosion, numerous items emerge every day,\nespecially in feed scenarios. Due to the limited system display slots and user\nbrowsing attention, various recommendation systems are designed not only to\nsatisfy users' personalized information needs but also to allocate items'\nexposure. However, recent recommendation studies mainly focus on modeling user\npreferences to present satisfying results and maximize user interactions, while\npaying little attention to developing item-side fair exposure mechanisms for\nrational information delivery. This may lead to serious resource allocation\nproblems on the item side, such as the Snowball Effect. Furthermore, unfair\nexposure mechanisms may hurt recommendation performance. In this paper, we call\nfor a shift of attention from modeling user preferences to developing fair\nexposure mechanisms for items. We first conduct empirical analyses of feed\nscenarios to explore exposure problems between items with distinct uploaded\ntimes. This points out that unfair exposure caused by the time factor may be\nthe major cause of the Snowball Effect. Then, we propose to explicitly model\nitem-level customized timeliness distribution, Global Residual Value (GRV), for\nfair resource allocation. This GRV module is introduced into recommendations\nwith the designed Timeliness-aware Fair Recommendation Framework (TaFR).\nExtensive experiments on two datasets demonstrate that TaFR achieves consistent\nimprovements with various backbone recommendation models. By modeling item-side\ncustomized Global Residual Value, we achieve a fairer distribution of resources\nand, at the same time, improve recommendation performance.",
        "translated": ""
    },
    {
        "title": "Data Discovery for the SDGs: A Systematic Rule-based Approach",
        "url": "http://arxiv.org/abs/2307.07983v1",
        "pub_date": "2023-07-16",
        "summary": "In 2015, the United Nations put forward 17 Sustainable Development Goals\n(SDGs) to be achieved by 2030, where data has been promoted as a focus to\ninnovating sustainable development and as a means to measuring progress towards\nachieving the SDGs. In this study, we propose a systematic approach towards\ndiscovering data types and sources that can be used for SDG research. The\nproposed method integrates a systematic mapping approach using manual\nqualitative coding over a corpus of SDG-related research literature followed by\nan automated process that applies rules to perform data entity extraction\ncomputationally. This approach is exemplified by an analysis of literature\nrelating to SDG 7, the results of which are also presented in this paper. The\npaper concludes with a discussion of the approach and suggests future work to\nextend the method with more advance NLP and machine learning techniques.",
        "translated": ""
    },
    {
        "title": "Opinion mining using Double Channel CNN for Recommender System",
        "url": "http://arxiv.org/abs/2307.07798v1",
        "pub_date": "2023-07-15",
        "summary": "Much unstructured data has been produced with the growth of the Internet and\nsocial media. A significant volume of textual data includes users' opinions\nabout products in online stores and social media. By exploring and categorizing\nthem, helpful information can be acquired, including customer satisfaction,\nuser feedback about a particular event, predicting the sale of a specific\nproduct, and other similar cases. In this paper, we present an approach for\nsentiment analysis with a deep learning model and use it to recommend products.\nA two-channel convolutional neural network model has been used for opinion\nmining, which has five layers and extracts essential features from the data. We\nincreased the number of comments by applying the SMOTE algorithm to the initial\ndataset and balanced the data. Then we proceed to cluster the aspects. We also\nassign a weight to each cluster using tensor decomposition algorithms that\nimprove the recommender system's performance. Our proposed method has reached\n91.6% accuracy, significantly improved compared to previous aspect-based\napproaches.",
        "translated": ""
    },
    {
        "title": "Improving Trace Link Recommendation by Using Non-Isotropic Distances and\n  Combinations",
        "url": "http://arxiv.org/abs/2307.07781v1",
        "pub_date": "2023-07-15",
        "summary": "The existence of trace links between artifacts of the software development\nlife cycle can improve the efficiency of many activities during software\ndevelopment, maintenance and operations. Unfortunately, the creation and\nmaintenance of trace links is time-consuming and error-prone. Research efforts\nhave been spent to automatically compute trace links and lately gained\nmomentum, e.g., due to the availability of powerful tools in the area of\nnatural language processing. In this paper, we report on some observations that\nwe made during studying non-linear similarity measures for computing trace\nlinks. We argue, that taking a geometric viewpoint on semantic similarity can\nbe helpful for future traceability research. We evaluated our observations on a\ndataset of four open source projects and two industrial projects. We\nfurthermore point out that our findings are more general and can build the\nbasis for other information retrieval problems as well.",
        "translated": ""
    },
    {
        "title": "Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model",
        "url": "http://arxiv.org/abs/2307.07740v1",
        "pub_date": "2023-07-15",
        "summary": "Sentiment analysis is the process of identifying and categorizing people's\nemotions or opinions regarding various topics. The analysis of Twitter\nsentiment has become an increasingly popular topic in recent years. In this\npaper, we present several machine learning and a deep learning model to\nanalysis sentiment of Persian political tweets. Our analysis was conducted\nusing Bag of Words and ParsBERT for word representation. We applied Gaussian\nNaive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random\nForests, as well as a combination of CNN and LSTM to classify the polarities of\ntweets. The results of this study indicate that deep learning with ParsBERT\nembedding performs better than machine learning. The CNN-LSTM model had the\nhighest classification accuracy with 89 percent on the first dataset with three\nclasses and 71 percent on the second dataset with seven classes. Due to the\ncomplexity of Persian, it was a difficult task to achieve this level of\nefficiency.",
        "translated": ""
    },
    {
        "title": "On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit\n  Mechanisms",
        "url": "http://arxiv.org/abs/2307.07675v1",
        "pub_date": "2023-07-15",
        "summary": "Efficient learning in multi-armed bandit mechanisms such as pay-per-click\n(PPC) auctions typically involves three challenges: 1) inducing truthful\nbidding behavior (incentives), 2) using personalization in the users (context),\nand 3) circumventing manipulations in click patterns (corruptions). Each of\nthese challenges has been studied orthogonally in the literature; incentives\nhave been addressed by a line of work on truthful multi-armed bandit\nmechanisms, context has been extensively tackled by contextual bandit\nalgorithms, while corruptions have been discussed via a recent line of work on\nbandits with adversarial corruptions. Since these challenges co-exist, it is\nimportant to understand the robustness of each of these approaches in\naddressing the other challenges, provide algorithms that can handle all\nsimultaneously, and highlight inherent limitations in this combination. In this\nwork, we show that the most prominent contextual bandit algorithm,\n$\\epsilon$-greedy can be extended to handle the challenges introduced by\nstrategic arms in the contextual multi-arm bandit mechanism setting. We further\nshow that $\\epsilon$-greedy is inherently robust to adversarial data corruption\nattacks and achieves performance that degrades linearly with the amount of\ncorruption.",
        "translated": ""
    },
    {
        "title": "Deep Neural Aggregation for Recommending Items to Group of Users",
        "url": "http://arxiv.org/abs/2307.09447v1",
        "pub_date": "2023-07-18",
        "summary": "Modern society devotes a significant amount of time to digital interaction.\nMany of our daily actions are carried out through digital means. This has led\nto the emergence of numerous Artificial Intelligence tools that assist us in\nvarious aspects of our lives. One key tool for the digital society is\nRecommender Systems, intelligent systems that learn from our past actions to\npropose new ones that align with our interests. Some of these systems have\nspecialized in learning from the behavior of user groups to make\nrecommendations to a group of individuals who want to perform a joint task. In\nthis article, we analyze the current state of Group Recommender Systems and\npropose two new models that use emerging Deep Learning architectures.\nExperimental results demonstrate the improvement achieved by employing the\nproposed models compared to the state-of-the-art models using four different\ndatasets. The source code of the models, as well as that of all the experiments\nconducted, is available in a public repository.",
        "translated": ""
    },
    {
        "title": "Zero-shot Query Reformulation for Conversational Search",
        "url": "http://arxiv.org/abs/2307.09384v1",
        "pub_date": "2023-07-18",
        "summary": "As the popularity of voice assistants continues to surge, conversational\nsearch has gained increased attention in Information Retrieval. However, data\nsparsity issues in conversational search significantly hinder the progress of\nsupervised conversational search methods. Consequently, researchers are\nfocusing more on zero-shot conversational search approaches. Nevertheless,\nexisting zero-shot methods face three primary limitations: they are not\nuniversally applicable to all retrievers, their effectiveness lacks sufficient\nexplainability, and they struggle to resolve common conversational ambiguities\ncaused by omission. To address these limitations, we introduce a novel\nZero-shot Query Reformulation (ZeQR) framework that reformulates queries based\non previous dialogue contexts without requiring supervision from conversational\nsearch data. Specifically, our framework utilizes language models designed for\nmachine reading comprehension tasks to explicitly resolve two common\nambiguities: coreference and omission, in raw queries. In comparison to\nexisting zero-shot methods, our approach is universally applicable to any\nretriever without additional adaptation or indexing. It also provides greater\nexplainability and effectively enhances query intent understanding because\nambiguities are explicitly and proactively resolved. Through extensive\nexperiments on four TREC conversational datasets, we demonstrate the\neffectiveness of our method, which consistently outperforms state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via\n  Parameter Constraint",
        "url": "http://arxiv.org/abs/2307.09193v1",
        "pub_date": "2023-07-18",
        "summary": "Large-scale online recommender system spreads all over the Internet being in\ncharge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion\nRate (CVR) estimations. However, traditional CVR estimators suffer from\nwell-known Sample Selection Bias and Data Sparsity issues. Entire space models\nwere proposed to address the two issues via tracing the decision-making path of\n\"exposure_click_purchase\". Further, some researchers observed that there are\npurchase-related behaviors between click and purchase, which can better draw\nthe user's decision-making intention and improve the recommendation\nperformance. Thus, the decision-making path has been extended to\n\"exposure_click_in-shop action_purchase\" and can be modeled with conditional\nprobability approach. Nevertheless, we observe that the chain rule of\nconditional probability does not always hold. We report Probability Space\nConfusion (PSC) issue and give a derivation of difference between ground-truth\nand estimation mathematically. We propose a novel Entire Space Multi-Task Model\nfor Post-Click Conversion Rate via Parameter Constraint (ESMC) and two\nalternatives: Entire Space Multi-Task Model with Siamese Network (ESMS) and\nEntire Space Multi-Task Model in Global Domain (ESMG) to address the PSC issue.\nSpecifically, we handle \"exposure_click_in-shop action\" and \"in-shop\naction_purchase\" separately in the light of characteristics of in-shop action.\nThe first path is still treated with conditional probability while the second\none is treated with parameter constraint strategy. Experiments on both offline\nand online environments in a large-scale recommendation system illustrate the\nsuperiority of our proposed methods over state-of-the-art models. The\nreal-world datasets will be released.",
        "translated": ""
    },
    {
        "title": "Jean-Luc Picard at Touché 2023: Comparing Image Generation, Stance\n  Detection and Feature Matching for Image Retrieval for Arguments",
        "url": "http://arxiv.org/abs/2307.09172v1",
        "pub_date": "2023-07-18",
        "summary": "Participating in the shared task \"Image Retrieval for arguments\", we used\ndifferent pipelines for image retrieval containing Image Generation, Stance\nDetection, Preselection and Feature Matching. We submitted four different runs\nwith different pipeline layout and compare them to given baseline. Our\npipelines perform similarly to the baseline.",
        "translated": ""
    },
    {
        "title": "Modeling Orders of User Behaviors via Differentiable Sorting: A\n  Multi-task Framework to Predicting User Post-click Conversion",
        "url": "http://arxiv.org/abs/2307.09089v1",
        "pub_date": "2023-07-18",
        "summary": "User post-click conversion prediction is of high interest to researchers and\ndevelopers. Recent studies employ multi-task learning to tackle the selection\nbias and data sparsity problem, two severe challenges in post-click behavior\nprediction, by incorporating click data. However, prior works mainly focused on\npointwise learning and the orders of labels (i.e., click and post-click) are\nnot well explored, which naturally poses a listwise learning problem. Inspired\nby recent advances on differentiable sorting, in this paper, we propose a novel\nmulti-task framework that leverages orders of user behaviors to predict user\npost-click conversion in an end-to-end approach. Specifically, we define an\naggregation operator to combine predicted outputs of different tasks to a\nunified score, then we use the computed scores to model the label relations via\ndifferentiable sorting. Extensive experiments on public and industrial datasets\nshow the superiority of our proposed model against competitive baselines.",
        "translated": ""
    },
    {
        "title": "GraphCL-DTA: a graph contrastive learning with molecular semantics for\n  drug-target binding affinity prediction",
        "url": "http://arxiv.org/abs/2307.08989v1",
        "pub_date": "2023-07-18",
        "summary": "Drug-target binding affinity prediction plays an important role in the early\nstages of drug discovery, which can infer the strength of interactions between\nnew drugs and new targets. However, the performance of previous computational\nmodels is limited by the following drawbacks. The learning of drug\nrepresentation relies only on supervised data, without taking into account the\ninformation contained in the molecular graph itself. Moreover, most previous\nstudies tended to design complicated representation learning module, while\nuniformity, which is used to measure representation quality, is ignored. In\nthis study, we propose GraphCL-DTA, a graph contrastive learning with molecular\nsemantics for drug-target binding affinity prediction. In GraphCL-DTA, we\ndesign a graph contrastive learning framework for molecular graphs to learn\ndrug representations, so that the semantics of molecular graphs are preserved.\nThrough this graph contrastive framework, a more essential and effective drug\nrepresentation can be learned without additional supervised data. Next, we\ndesign a new loss function that can be directly used to smoothly adjust the\nuniformity of drug and target representations. By directly optimizing the\nuniformity of representations, the representation quality of drugs and targets\ncan be improved. The effectiveness of the above innovative elements is verified\non two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA\non the above datasets suggests its superiority to the state-of-the-art model.",
        "translated": ""
    },
    {
        "title": "Sharpness-Aware Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2307.08910v1",
        "pub_date": "2023-07-18",
        "summary": "Graph Neural Networks (GNNs) have achieved impressive performance in\ncollaborative filtering. However, GNNs tend to yield inferior performance when\nthe distributions of training and test data are not aligned well. Also,\ntraining GNNs requires optimizing non-convex neural networks with an abundance\nof local and global minima, which may differ widely in their performance at\ntest time. Thus, it is essential to choose the minima carefully. Here we\npropose an effective training schema, called {gSAM}, under the principle that\nthe \\textit{flatter} minima has a better generalization ability than the\n\\textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of\nthe weight loss landscape by forming a bi-level optimization: the outer problem\nconducts the standard model training while the inner problem helps the model\njump out of the sharp minima. Experimental results show the superiority of our\ngSAM.",
        "translated": ""
    },
    {
        "title": "An Admissible Shift-Consistent Method for Recommender Systems",
        "url": "http://arxiv.org/abs/2307.08857v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper, we propose a new constraint, called shift-consistency, for\nsolving matrix/tensor completion problems in the context of recommender\nsystems. Our method provably guarantees several key mathematical properties:\n(1) satisfies a recently established admissibility criterion for recommender\nsystems; (2) satisfies a definition of fairness that eliminates a specific\nclass of potential opportunities for users to maliciously influence system\nrecommendations; and (3) offers robustness by exploiting provable uniqueness of\nmissing-value imputation. We provide a rigorous mathematical description of the\nmethod, including its generalization from matrix to tensor form to permit\nrepresentation and exploitation of complex structural relationships among sets\nof user and product attributes. We argue that our analysis suggests a\nstructured means for defining latent-space projections that can permit provable\nperformance properties to be established for machine learning methods.",
        "translated": ""
    },
    {
        "title": "An Exploration Study of Mixed-initiative Query Reformulation in\n  Conversational Passage Retrieval",
        "url": "http://arxiv.org/abs/2307.08803v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper, we report our methods and experiments for the TREC\nConversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce\nmulti-stage retrieval pipelines and explore one of the potential benefits of\ninvolving mixed-initiative interaction in conversational passage retrieval\nscenarios: reformulating raw queries. Before the first ranking stage of a\nmulti-stage retrieval pipeline, we propose a mixed-initiative query\nreformulation module, which achieves query reformulation based on the\nmixed-initiative interaction between the users and the system, as the\nreplacement for the neural reformulation method. Specifically, we design an\nalgorithm to generate appropriate questions related to the ambiguities in raw\nqueries, and another algorithm to reformulate raw queries by parsing users'\nfeedback and incorporating it into the raw query. For the first ranking stage\nof our multi-stage pipelines, we adopt a sparse ranking function: BM25, and a\ndense retrieval method: TCT-ColBERT. For the second-ranking step, we adopt a\npointwise reranker: MonoT5, and a pairwise reranker: DuoT5. Experiments on both\nTREC CAsT 2021 and TREC CAsT 2022 datasets show the effectiveness of our\nmixed-initiative-based query reformulation method on improving retrieval\nperformance compared with two popular reformulators: a neural reformulator:\nCANARD-T5 and a rule-based reformulator: historical query reformulator(HQE).",
        "translated": ""
    },
    {
        "title": "Imposing Consistency Properties on Blackbox Systems with Applications to\n  SVD-Based Recommender Systems",
        "url": "http://arxiv.org/abs/2307.08760v1",
        "pub_date": "2023-07-17",
        "summary": "In this paper we discuss pre- and post-processing methods to induce desired\nconsistency and/or invariance properties in blackbox systems, e.g., AI-based.\nWe demonstrate our approach in the context of blackbox SVD-based\nmatrix-completion methods commonly used in recommender system (RS)\napplications. We provide empirical results showing that enforcement of\nunit-consistency and shift-consistency, which have provable RS-relevant\nproperties relating to robustness and fairness, also lead to improved\nperformance according to generic RMSE and MAE performance metrics, irrespective\nof the initial chosen hyperparameter.",
        "translated": ""
    },
    {
        "title": "UniMatch: A Unified User-Item Matching Framework for the Multi-purpose\n  Merchant Marketing",
        "url": "http://arxiv.org/abs/2307.09989v1",
        "pub_date": "2023-07-19",
        "summary": "When doing private domain marketing with cloud services, the merchants\nusually have to purchase different machine learning models for the multiple\nmarketing purposes, leading to a very high cost. We present a unified user-item\nmatching framework to simultaneously conduct item recommendation and user\ntargeting with just one model. We empirically demonstrate that the above\nconcurrent modeling is viable via modeling the user-item interaction matrix\nwith the multinomial distribution, and propose a bidirectional bias-corrected\nNCE loss for the implementation. The proposed loss function guides the model to\nlearn the user-item joint probability $p(u,i)$ instead of the conditional\nprobability $p(i|u)$ or $p(u|i)$ through correcting both the users and items'\nbiases caused by the in-batch negative sampling. In addition, our framework is\nmodel-agnostic enabling a flexible adaptation of different model architectures.\nExtensive experiments demonstrate that our framework results in significant\nperformance gains in comparison with the state-of-the-art methods, with greatly\nreduced cost on computing resources and daily maintenance.",
        "translated": ""
    },
    {
        "title": "Our Model Achieves Excellent Performance on MovieLens: What Does it\n  Mean?",
        "url": "http://arxiv.org/abs/2307.09985v1",
        "pub_date": "2023-07-19",
        "summary": "A typical benchmark dataset for recommender system (RecSys) evaluation\nconsists of user-item interactions generated on a platform within a time\nperiod. The interaction generation mechanism partially explains why a user\ninteracts with (e.g.,like, purchase, rate) an item, and the context of when a\nparticular interaction happened. In this study, we conduct a meticulous\nanalysis on the MovieLens dataset and explain the potential impact on using the\ndataset for evaluating recommendation algorithms. We make a few main findings\nfrom our analysis. First, there are significant differences in user\ninteractions at the different stages when a user interacts with the MovieLens\nplatform. The early interactions largely define the user portrait which affect\nthe subsequent interactions. Second, user interactions are highly affected by\nthe candidate movies that are recommended by the platform's internal\nrecommendation algorithm(s). Removal of interactions that happen nearer to the\nlast few interactions of a user leads to increasing difficulty in learning user\npreference, thus deteriorating recommendation accuracy. Third, changing the\norder of user interactions makes it more difficult for sequential algorithms to\ncapture the progressive interaction process. Based on these findings, we\nfurther discuss the discrepancy between the interaction generation mechanism\nthat is employed by the MovieLens system and that of typical real world\nrecommendation scenarios. In summary, models that achieve excellent\nrecommendation accuracy on the MovieLens dataset may not demonstrate superior\nperformance in practice for at least two kinds of differences: (i) the\ndifferences in the contexts of user-item interaction generation, and (ii) the\ndifferences in user knowledge about the item collections.",
        "translated": ""
    },
    {
        "title": "Who Provides the Largest Megaphone? The Role of Google News in Promoting\n  Russian State-Affiliated News Sources",
        "url": "http://arxiv.org/abs/2307.09834v1",
        "pub_date": "2023-07-19",
        "summary": "The Internet has not only digitized but also democratized information access\nacross the globe. This gradual but path-breaking move to online information\npropagation has resulted in search engines playing an increasingly prominent\nrole in shaping access to human knowledge. When an Internet user enters a\nquery, the search engine sorts through the hundreds of billions of possible\nwebpages to determine what to show. Google dominates the search engine market,\nwith Google Search surpassing 80% market share globally every year of the last\ndecade. Only in Russia and China do Google competitors claim more market share,\nwith approximately 60% of Internet users in Russia preferring Yandex (compared\nto 40% in favor of Google) and more than 80% of China's Internet users\naccessing Baidu as of 2022. Notwithstanding this long-standing regional\nvariation in Internet search providers, there is limited research showing how\nthese providers compare in terms of propagating state-sponsored information.\nOur study fills this research gap by focusing on Russian cyberspace and\nexamining how Google and Yandex's search algorithms rank content from Russian\nstate-controlled media (hereon, RSM) outlets. This question is timely and of\npractical interest given widespread reports indicating that RSM outlets have\nactively engaged in promoting Kremlin propaganda in the lead-up to, and in the\naftermath of, the Russian invasion of Ukraine in February 2022.",
        "translated": ""
    },
    {
        "title": "DisCover: Disentangled Music Representation Learning for Cover Song\n  Identification",
        "url": "http://arxiv.org/abs/2307.09775v1",
        "pub_date": "2023-07-19",
        "summary": "In the field of music information retrieval (MIR), cover song identification\n(CSI) is a challenging task that aims to identify cover versions of a query\nsong from a massive collection. Existing works still suffer from high\nintra-song variances and inter-song correlations, due to the entangled nature\nof version-specific and version-invariant factors in their modeling. In this\nwork, we set the goal of disentangling version-specific and version-invariant\nfactors, which could make it easier for the model to learn invariant music\nrepresentations for unseen query songs. We analyze the CSI task in a\ndisentanglement view with the causal graph technique, and identify the\nintra-version and inter-version effects biasing the invariant learning. To\nblock these effects, we propose the disentangled music representation learning\nframework (DisCover) for CSI. DisCover consists of two critical components: (1)\nKnowledge-guided Disentanglement Module (KDM) and (2) Gradient-based\nAdversarial Disentanglement Module (GADM), which block intra-version and\ninter-version biased effects, respectively. KDM minimizes the mutual\ninformation between the learned representations and version-variant factors\nthat are identified with prior domain knowledge. GADM identifies\nversion-variant factors by simulating the representation transitions between\nintra-song versions, and exploits adversarial distillation for effect blocking.\nExtensive comparisons with best-performing methods and in-depth analysis\ndemonstrate the effectiveness of DisCover and the and necessity of\ndisentanglement for CSI.",
        "translated": ""
    },
    {
        "title": "Information Retrieval Meets Large Language Models: A Strategic Report\n  from Chinese IR Community",
        "url": "http://arxiv.org/abs/2307.09751v1",
        "pub_date": "2023-07-19",
        "summary": "The research field of Information Retrieval (IR) has evolved significantly,\nexpanding beyond traditional search to meet diverse user information needs.\nRecently, Large Language Models (LLMs) have demonstrated exceptional\ncapabilities in text understanding, generation, and knowledge inference,\nopening up exciting avenues for IR research. LLMs not only facilitate\ngenerative retrieval but also offer improved solutions for user understanding,\nmodel evaluation, and user-system interactions. More importantly, the\nsynergistic relationship among IR models, LLMs, and humans forms a new\ntechnical paradigm that is more powerful for information seeking. IR models\nprovide real-time and relevant information, LLMs contribute internal knowledge,\nand humans play a central role of demanders and evaluators to the reliability\nof information services. Nevertheless, significant challenges exist, including\ncomputational costs, credibility concerns, domain-specific limitations, and\nethical considerations. To thoroughly discuss the transformative impact of LLMs\non IR research, the Chinese IR community conducted a strategic workshop in\nApril 2023, yielding valuable insights. This paper provides a summary of the\nworkshop's outcomes, including the rethinking of IR's core values, the mutual\nenhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and\nopen challenges.",
        "translated": ""
    },
    {
        "title": "Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for\n  Recommendation and Text Generation",
        "url": "http://arxiv.org/abs/2307.09688v1",
        "pub_date": "2023-07-19",
        "summary": "Modeling customer shopping intentions is a crucial task for e-commerce, as it\ndirectly impacts user experience and engagement. Thus, accurately understanding\ncustomer preferences is essential for providing personalized recommendations.\nSession-based recommendation, which utilizes customer session data to predict\ntheir next interaction, has become increasingly popular. However, existing\nsession datasets have limitations in terms of item attributes, user diversity,\nand dataset scale. As a result, they cannot comprehensively capture the\nspectrum of user behaviors and preferences. To bridge this gap, we present the\nAmazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It\nis the first multilingual dataset consisting of millions of user sessions from\nsix different locales, where the major languages of products are English,\nGerman, Japanese, French, Italian, and Spanish. Remarkably, the dataset can\nhelp us enhance personalization and understanding of user preferences, which\ncan benefit various existing tasks as well as enable new tasks. To test the\npotential of the dataset, we introduce three tasks in this work: (1)\nnext-product recommendation, (2) next-product recommendation with domain\nshifts, and (3) next-product title generation. With the above tasks, we\nbenchmark a range of algorithms on our proposed dataset, drawing new insights\nfor further research and practice. In addition, based on the proposed dataset\nand tasks, we hosted a competition in the KDD CUP 2023 and have attracted\nthousands of users and submissions. The winning solutions and the associated\nworkshop can be accessed at our website https://kddcup23.github.io/.",
        "translated": ""
    },
    {
        "title": "PubMed and Beyond: Recent Advances and Best Practices in Biomedical\n  Literature Search",
        "url": "http://arxiv.org/abs/2307.09683v1",
        "pub_date": "2023-07-18",
        "summary": "Biomedical research yields a wealth of information, much of which is only\naccessible through the literature. Consequently, literature search is an\nessential tool for building on prior knowledge in clinical and biomedical\nresearch. Although recent improvements in artificial intelligence have expanded\nfunctionality beyond keyword-based search, these advances may be unfamiliar to\nclinicians and researchers. In response, we present a survey of literature\nsearch tools tailored to both general and specific information needs in\nbiomedicine, with the objective of helping readers efficiently fulfill their\ninformation needs. We first examine the widely used PubMed search engine,\ndiscussing recent improvements and continued challenges. We then describe\nliterature search tools catering to five specific information needs: 1.\nIdentifying high-quality clinical research for evidence-based medicine. 2.\nRetrieving gene-related information for precision medicine and genomics. 3.\nSearching by meaning, including natural language questions. 4. Locating related\narticles with literature recommendation. 5. Mining literature to discover\nassociations between concepts such as diseases and genetic variants.\nAdditionally, we cover practical considerations and best practices for choosing\nand using these tools. Finally, we provide a perspective on the future of\nliterature search engines, considering recent breakthroughs in large language\nmodels such as ChatGPT. In summary, our survey provides a comprehensive view of\nbiomedical literature search functionalities with 36 publicly available tools.",
        "translated": ""
    },
    {
        "title": "Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation",
        "url": "http://arxiv.org/abs/2307.11019v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require\na substantial amount of factual knowledge and often rely on external\ninformation for assistance. Recently, large language models (LLMs) (e.g.,\nChatGPT), have demonstrated impressive prowess in solving a wide range of tasks\nwith world knowledge, including knowledge-intensive tasks. However, it remains\nunclear how well LLMs are able to perceive their factual knowledge boundaries,\nparticularly how they behave when incorporating retrieval augmentation. In this\nstudy, we present an initial analysis of the factual knowledge boundaries of\nLLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially,\nwe focus on three primary research questions and analyze them by examining QA\nperformance, priori judgement and posteriori judgement of LLMs. We show\nevidence that LLMs possess unwavering confidence in their capabilities to\nrespond to questions and the accuracy of their responses. Furthermore,\nretrieval augmentation proves to be an effective approach in enhancing LLMs'\nawareness of knowledge boundaries, thereby improving their judgemental\nabilities. Additionally, we also find that LLMs have a propensity to rely on\nthe provided retrieval results when formulating answers, while the quality of\nthese results significantly impacts their reliance. The code to reproduce this\nwork is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
        "translated": ""
    },
    {
        "title": "Enhancing Job Recommendation through LLM-based Generative Adversarial\n  Networks",
        "url": "http://arxiv.org/abs/2307.10747v1",
        "pub_date": "2023-07-20",
        "summary": "Recommending suitable jobs to users is a critical task in online recruitment\nplatforms, as it can enhance users' satisfaction and the platforms'\nprofitability. While existing job recommendation methods encounter challenges\nsuch as the low quality of users' resumes, which hampers their accuracy and\npractical effectiveness. With the rapid development of large language models\n(LLMs), utilizing the rich external knowledge encapsulated within them, as well\nas their powerful capabilities of text processing and reasoning, is a promising\nway to complete users' resumes for more accurate recommendations. However,\ndirectly leveraging LLMs to enhance recommendation results is not a\none-size-fits-all solution, as LLMs may suffer from fabricated generation and\nfew-shot problems, which degrade the quality of resume completion. In this\npaper, we propose a novel LLM-based approach for job recommendation. To\nalleviate the limitation of fabricated generation for LLMs, we extract accurate\nand valuable information beyond users' self-description, which helps the LLMs\nbetter profile users for resume completion. Specifically, we not only extract\nusers' explicit properties (e.g., skills, interests) from their\nself-description but also infer users' implicit characteristics from their\nbehaviors for more accurate and meaningful resume completion. Nevertheless,\nsome users still suffer from few-shot problems, which arise due to scarce\ninteraction records, leading to limited guidance for the models in generating\nhigh-quality resumes. To address this issue, we propose aligning unpaired\nlow-quality with high-quality generated resumes by Generative Adversarial\nNetworks (GANs), which can refine the resume representations for better\nrecommendation results. Extensive experiments on three large real-world\nrecruitment datasets demonstrate the effectiveness of our proposed method.",
        "translated": ""
    },
    {
        "title": "A Constraint-based Recommender System via RDF Knowledge Graphs",
        "url": "http://arxiv.org/abs/2307.10702v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge graphs, represented in RDF, are able to model entities and their\nrelations by means of ontologies. The use of knowledge graphs for information\nmodeling has attracted interest in recent years. In recommender systems, items\nand users can be mapped and integrated into the knowledge graph, which can\nrepresent more links and relationships between users and items.\nConstraint-based recommender systems are based on the idea of explicitly\nexploiting deep recommendation knowledge through constraints to identify\nrelevant recommendations. When combined with knowledge graphs, a\nconstraint-based recommender system gains several benefits in terms of\nconstraint sets. In this paper, we investigate and propose the construction of\na constraint-based recommender system via RDF knowledge graphs applied to the\nvehicle purchase/sale domain. The results of our experiments show that the\nproposed approach is able to efficiently identify recommendations in accordance\nwith user preferences.",
        "translated": ""
    },
    {
        "title": "A Personalized Recommender System Based-on Knowledge Graph Embeddings",
        "url": "http://arxiv.org/abs/2307.10680v1",
        "pub_date": "2023-07-20",
        "summary": "Knowledge graphs have proven to be effective for modeling entities and their\nrelationships through the use of ontologies. The recent emergence in interest\nfor using knowledge graphs as a form of information modeling has led to their\nincreased adoption in recommender systems. By incorporating users and items\ninto the knowledge graph, these systems can better capture the implicit\nconnections between them and provide more accurate recommendations. In this\npaper, we investigate and propose the construction of a personalized\nrecommender system via knowledge graphs embedding applied to the vehicle\npurchase/sale domain. The results of our experimentation demonstrate the\nefficacy of the proposed method in providing relevant recommendations that are\nconsistent with individual users.",
        "translated": ""
    },
    {
        "title": "Language-Enhanced Session-Based Recommendation with Decoupled\n  Contrastive Learning",
        "url": "http://arxiv.org/abs/2307.10650v1",
        "pub_date": "2023-07-20",
        "summary": "Session-based recommendation techniques aim to capture dynamic user behavior\nby analyzing past interactions. However, existing methods heavily rely on\nhistorical item ID sequences to extract user preferences, leading to challenges\nsuch as popular bias and cold-start problems. In this paper, we propose a\nhybrid multimodal approach for session-based recommendation to address these\nchallenges. Our approach combines different modalities, including textual\ncontent and item IDs, leveraging the complementary nature of these modalities\nusing CatBoost. To learn universal item representations, we design a language\nrepresentation-based item retrieval architecture that extracts features from\nthe textual content utilizing pre-trained language models. Furthermore, we\nintroduce a novel Decoupled Contrastive Learning method to enhance the\neffectiveness of the language representation. This technique decouples the\nsequence representation and item representation space, facilitating\nbidirectional alignment through dual-queue contrastive learning.\nSimultaneously, the momentum queue provides a large number of negative samples,\neffectively enhancing the effectiveness of contrastive learning. Our approach\nyielded competitive results, securing a 5th place ranking in KDD CUP 2023 Task\n1. We have released the source code and pre-trained models associated with this\nwork.",
        "translated": ""
    },
    {
        "title": "Improving Semantic Similarity Measure Within a Recommender System\n  Based-on RDF Graphs",
        "url": "http://arxiv.org/abs/2307.10639v1",
        "pub_date": "2023-07-20",
        "summary": "In today's era of information explosion, more users are becoming more reliant\nupon recommender systems to have better advice, suggestions, or inspire them.\nThe measure of the semantic relatedness or likeness between terms, words, or\ntext data plays an important role in different applications dealing with\ntextual data, as in a recommender system. Over the past few years, many\nontologies have been developed and used as a form of structured representation\nof knowledge bases for information systems. The measure of semantic similarity\nfrom ontology has developed by several methods. In this paper, we propose and\ncarry on an approach for the improvement of semantic similarity calculations\nwithin a recommender system based-on RDF graphs.",
        "translated": ""
    },
    {
        "title": "Detecting deceptive reviews using text classification",
        "url": "http://arxiv.org/abs/2307.10617v1",
        "pub_date": "2023-07-20",
        "summary": "In recent years, online reviews play a vital role for promoting any kind of\nproduct or services. Businesses may embed fake reviews in order to attract\ncustomers to purchase their products. They may even highlight the benefits of\ntheir own product or criticize the competition's product. Marketers,\nadvertisers, and other online business users have incentive to create fake\npositive reviews for products which they want to promote or give fake negative\nreviews for products which they really don't like. So now-a-days writing a\ndeceptive review is inevitable thing for promoting their own business or\ndegrading competitor's reputation. Thus, identifying deceptive reviews is an\nintense and on-going research area. This research paper proposes machine\nlearning model approach to identify deceptive reviews. The paper investigates\nthe performance of the several experiments done on a Deceptive Opinion Spam\nCorpus dataset of restaurants reviews. We developed a n-gram model and max\nfeatures to identify deceptive contents with a particular focus on fake\nreviews. Further, we conduct a benchmark study to investigate the performance\nof two different features extraction techniques and apply five machine learning\nclassification techniques. The experimental results show that passive\naggressive classifier outperforms other algorithms, and it reaches the highest\naccuracy not only in text classification but also to fake reviews. We also\nstudy the data augmentation and implement different deep learning techniques.",
        "translated": ""
    },
    {
        "title": "SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot\n  Neural Sparse Retrieval",
        "url": "http://arxiv.org/abs/2307.10488v1",
        "pub_date": "2023-07-19",
        "summary": "Traditionally, sparse retrieval systems relied on lexical representations to\nretrieve documents, such as BM25, dominated information retrieval tasks. With\nthe onset of pre-trained transformer models such as BERT, neural sparse\nretrieval has led to a new paradigm within retrieval. Despite the success,\nthere has been limited software supporting different sparse retrievers running\nin a unified, common environment. This hinders practitioners from fairly\ncomparing different sparse models and obtaining realistic evaluation results.\nAnother missing piece is, that a majority of prior work evaluates sparse\nretrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO.\nHowever, a key requirement in practical retrieval systems requires models that\ncan generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In\nthis work, we provide SPRINT, a unified Python toolkit based on Pyserini and\nLucene, supporting a common interface for evaluating neural sparse retrieval.\nThe toolkit currently includes five built-in models: uniCOIL, DeepImpact,\nSPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by\ndefining their term weighting method. Using our toolkit, we establish strong\nand reproducible zero-shot sparse retrieval baselines across the\nwell-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2\nachieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural\nsparse retrievers. In this work, we further uncover the reasons behind its\nperformance gain. We show that SPLADEv2 produces sparse representations with a\nmajority of tokens outside of the original query and document which is often\ncrucial for its performance gains, i.e. a limitation among its other sparse\ncounterparts. We provide our SPRINT toolkit, models, and data used in our\nexperiments publicly here at https://github.com/thakur-nandan/sprint.",
        "translated": ""
    },
    {
        "title": "Fast Approximate Nearest Neighbor Search with a Dynamic Exploration\n  Graph using Continuous Refinement",
        "url": "http://arxiv.org/abs/2307.10479v1",
        "pub_date": "2023-07-19",
        "summary": "For approximate nearest neighbor search, graph-based algorithms have shown to\noffer the best trade-off between accuracy and search time. We propose the\nDynamic Exploration Graph (DEG) which significantly outperforms existing\nalgorithms in terms of search and exploration efficiency by combining two new\nideas: First, a single undirected even regular graph is incrementally built by\npartially replacing existing edges to integrate new vertices and to update old\nneighborhoods at the same time. Secondly, an edge optimization algorithm is\nused to continuously improve the quality of the graph. Combining this ongoing\nrefinement with the graph construction process leads to a well-organized graph\nstructure at all times, resulting in: (1) increased search efficiency, (2)\npredictable index size, (3) guaranteed connectivity and therefore reachability\nof all vertices, and (4) a dynamic graph structure. In addition we investigate\nhow well existing graph-based search systems can handle indexed queries where\nthe seed vertex of a search is the query itself. Such exploration tasks,\ndespite their good starting point, are not necessarily easy. High efficiency in\napproximate nearest neighbor search (ANNS) does not automatically imply good\nperformance in exploratory search. Extensive experiments show that our new\nDynamic Exploration Graph outperforms existing algorithms significantly for\nindexed and unindexed queries.",
        "translated": ""
    },
    {
        "title": "Classification of Visualization Types and Perspectives in Patents",
        "url": "http://arxiv.org/abs/2307.10471v1",
        "pub_date": "2023-07-19",
        "summary": "Due to the swift growth of patent applications each year, information and\nmultimedia retrieval approaches that facilitate patent exploration and\nretrieval are of utmost importance. Different types of visualizations (e.g.,\ngraphs, technical drawings) and perspectives (e.g., side view, perspective) are\nused to visualize details of innovations in patents. The classification of\nthese images enables a more efficient search and allows for further analysis.\nSo far, datasets for image type classification miss some important\nvisualization types for patents. Furthermore, related work does not make use of\nrecent deep learning approaches including transformers. In this paper, we adopt\nstate-of-the-art deep learning methods for the classification of visualization\ntypes and perspectives in patent images. We extend the CLEF-IP dataset for\nimage type classification in patents to ten classes and provide manual ground\ntruth annotations. In addition, we derive a set of hierarchical classes from a\ndataset that provides weakly-labeled data for image perspectives. Experimental\nresults have demonstrated the feasibility of the proposed approaches. Source\ncode, models, and dataset will be made publicly available.",
        "translated": ""
    },
    {
        "title": "Alleviating the Long-Tail Problem in Conversational Recommender Systems",
        "url": "http://arxiv.org/abs/2307.11650v1",
        "pub_date": "2023-07-21",
        "summary": "Conversational recommender systems (CRS) aim to provide the recommendation\nservice via natural language conversations. To develop an effective CRS,\nhigh-quality CRS datasets are very crucial. However, existing CRS datasets\nsuffer from the long-tail issue, \\ie a large proportion of items are rarely (or\neven never) mentioned in the conversations, which are called long-tail items.\nAs a result, the CRSs trained on these datasets tend to recommend frequent\nitems, and the diversity of the recommended items would be largely reduced,\nmaking users easier to get bored.\n  To address this issue, this paper presents \\textbf{LOT-CRS}, a novel\nframework that focuses on simulating and utilizing a balanced CRS dataset (\\ie\ncovering all the items evenly) for improving \\textbf{LO}ng-\\textbf{T}ail\nrecommendation performance of CRSs. In our approach, we design two pre-training\ntasks to enhance the understanding of simulated conversation for long-tail\nitems, and adopt retrieval-augmented fine-tuning with label smoothness strategy\nto further improve the recommendation of long-tail items. Extensive experiments\non two public CRS datasets have demonstrated the effectiveness and\nextensibility of our approach, especially on long-tail recommendation.",
        "translated": ""
    },
    {
        "title": "Identifying document similarity using a fast estimation of the\n  Levenshtein Distance based on compression and signatures",
        "url": "http://arxiv.org/abs/2307.11496v1",
        "pub_date": "2023-07-21",
        "summary": "Identifying document similarity has many applications, e.g., source code\nanalysis or plagiarism detection. However, identifying similarities is not\ntrivial and can be time complex. For instance, the Levenshtein Distance is a\ncommon metric to define the similarity between two documents but has quadratic\nruntime which makes it impractical for large documents where large starts with\na few hundred kilobytes. In this paper, we present a novel concept that allows\nestimating the Levenshtein Distance: the algorithm first compresses documents\nto signatures (similar to hash values) using a user-defined compression ratio.\nSignatures can then be compared against each other (some constrains apply)\nwhere the outcome is the estimated Levenshtein Distance. Our evaluation shows\npromising results in terms of runtime efficiency and accuracy. In addition, we\nintroduce a significance score allowing examiners to set a threshold and\nidentify related documents.",
        "translated": ""
    },
    {
        "title": "Analysis of Elephant Movement in Sub-Saharan Africa: Ecological,\n  Climatic, and Conservation Perspectives",
        "url": "http://arxiv.org/abs/2307.11325v1",
        "pub_date": "2023-07-21",
        "summary": "The interaction between elephants and their environment has profound\nimplications for both ecology and conservation strategies. This study presents\nan analytical approach to decipher the intricate patterns of elephant movement\nin Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal\nvariations and rainfall patterns. Despite the complexities surrounding these\ninfluential factors, our analysis provides a holistic view of elephant\nmigratory behavior in the context of the dynamic African landscape. Our\ncomprehensive approach enables us to predict the potential impact of these\necological determinants on elephant migration, a critical step in establishing\ninformed conservation strategies. This projection is particularly crucial given\nthe impacts of global climate change on seasonal and rainfall patterns, which\ncould substantially influence elephant movements in the future. The findings of\nour work aim to not only advance the understanding of movement ecology but also\nfoster a sustainable coexistence of humans and elephants in Sub-Saharan Africa.\nBy predicting potential elephant routes, our work can inform strategies to\nminimize human-elephant conflict, effectively manage land use, and enhance\nanti-poaching efforts. This research underscores the importance of integrating\nmovement ecology and climatic variables for effective wildlife management and\nconservation planning.",
        "translated": ""
    },
    {
        "title": "Jina Embeddings: A Novel Set of High-Performance Sentence Embedding\n  Models",
        "url": "http://arxiv.org/abs/2307.11224v1",
        "pub_date": "2023-07-20",
        "summary": "Jina Embeddings constitutes a set of high-performance sentence embedding\nmodels adept at translating various textual inputs into numerical\nrepresentations, thereby capturing the semantic essence of the text. While\nthese models are not exclusively designed for text generation, they excel in\napplications such as dense retrieval and semantic textual similarity. This\npaper details the development of Jina Embeddings, starting with the creation of\na high-quality pairwise and triplet dataset. It underlines the crucial role of\ndata cleaning in dataset preparation, gives in-depth insights into the model\ntraining process, and concludes with a comprehensive performance evaluation\nusing the Massive Textual Embedding Benchmark (MTEB).",
        "translated": ""
    },
    {
        "title": "RCVaR: an Economic Approach to Estimate Cyberattacks Costs using Data\n  from Industry Reports",
        "url": "http://arxiv.org/abs/2307.11140v1",
        "pub_date": "2023-07-20",
        "summary": "Digitization increases business opportunities and the risk of companies being\nvictims of devastating cyberattacks. Therefore, managing risk exposure and\ncybersecurity strategies is essential for digitized companies that want to\nsurvive in competitive markets. However, understanding company-specific risks\nand quantifying their associated costs is not trivial. Current approaches fail\nto provide individualized and quantitative monetary estimations of\ncybersecurity impacts. Due to limited resources and technical expertise, SMEs\nand even large companies are affected and struggle to quantify their\ncyberattack exposure. Therefore, novel approaches must be placed to support the\nunderstanding of the financial loss due to cyberattacks. This article\nintroduces the Real Cyber Value at Risk (RCVaR), an economical approach for\nestimating cybersecurity costs using real-world information from public\ncybersecurity reports. RCVaR identifies the most significant cyber risk factors\nfrom various sources and combines their quantitative results to estimate\nspecific cyberattacks costs for companies. Furthermore, RCVaR extends current\nmethods to achieve cost and risk estimations based on historical real-world\ndata instead of only probability-based simulations. The evaluation of the\napproach on unseen data shows the accuracy and efficiency of the RCVaR in\npredicting and managing cyber risks. Thus, it shows that the RCVaR is a\nvaluable addition to cybersecurity planning and risk management processes.",
        "translated": ""
    },
    {
        "title": "HeteFedRec: Federated Recommender Systems with Model Heterogeneity",
        "url": "http://arxiv.org/abs/2307.12810v1",
        "pub_date": "2023-07-24",
        "summary": "Owing to the nature of privacy protection, federated recommender systems\n(FedRecs) have garnered increasing interest in the realm of on-device\nrecommender systems. However, most existing FedRecs only allow participating\nclients to collaboratively train a recommendation model of the same public\nparameter size. Training a model of the same size for all clients can lead to\nsuboptimal performance since clients possess varying resources. For example,\nclients with limited training data may prefer to train a smaller recommendation\nmodel to avoid excessive data consumption, while clients with sufficient data\nwould benefit from a larger model to achieve higher recommendation accuracy. To\naddress the above challenge, this paper introduces HeteFedRec, a novel FedRec\nframework that enables the assignment of personalized model sizes to\nparticipants. In HeteFedRec, we present a heterogeneous recommendation model\naggregation strategy, including a unified dual-task learning mechanism and a\ndimensional decorrelation regularization, to allow knowledge aggregation among\nrecommender models of different sizes. Additionally, a relation-based ensemble\nknowledge distillation method is proposed to effectively distil knowledge from\nheterogeneous item embeddings. Extensive experiments conducted on three\nreal-world recommendation datasets demonstrate the effectiveness and efficiency\nof HeteFedRec in training federated recommender systems under heterogeneous\nsettings.",
        "translated": ""
    },
    {
        "title": "RRAML: Reinforced Retrieval Augmented Machine Learning",
        "url": "http://arxiv.org/abs/2307.12798v1",
        "pub_date": "2023-07-24",
        "summary": "The emergence of large language models (LLMs) has revolutionized machine\nlearning and related fields, showcasing remarkable abilities in comprehending,\ngenerating, and manipulating human language. However, their conventional usage\nthrough API-based text prompt submissions imposes certain limitations in terms\nof context constraints and external source availability. To address these\nchallenges, we propose a novel framework called Reinforced Retrieval Augmented\nMachine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs\nwith supporting information retrieved by a purpose-built retriever from a vast\nuser-provided database. By leveraging recent advancements in reinforcement\nlearning, our method effectively addresses several critical challenges.\nFirstly, it circumvents the need for accessing LLM gradients. Secondly, our\nmethod alleviates the burden of retraining LLMs for specific tasks, as it is\noften impractical or impossible due to restricted access to the model and the\ncomputational intensity involved. Additionally we seamlessly link the\nretriever's task with the reasoner, mitigating hallucinations and reducing\nirrelevant, and potentially damaging retrieved documents. We believe that the\nresearch agenda outlined in this paper has the potential to profoundly impact\nthe field of AI, democratizing access to and utilization of LLMs for a wide\nrange of entities.",
        "translated": ""
    },
    {
        "title": "Unbiased Delayed Feedback Label Correction for Conversion Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2307.12756v1",
        "pub_date": "2023-07-24",
        "summary": "Conversion rate prediction is critical to many online applications such as\ndigital display advertising. To capture dynamic data distribution, industrial\nsystems often require retraining models on recent data daily or weekly.\nHowever, the delay of conversion behavior usually leads to incorrect labeling,\nwhich is called delayed feedback problem. Existing work may fail to introduce\nthe correct information about false negative samples due to data sparsity and\ndynamic data distribution. To directly introduce the correct feedback label\ninformation, we propose an Unbiased delayed feedback Label Correction framework\n(ULC), which uses an auxiliary model to correct labels for observed negative\nfeedback samples. Firstly, we theoretically prove that the label-corrected loss\nis an unbiased estimate of the oracle loss using true labels. Then, as there\nare no ready training data for label correction, counterfactual labeling is\nused to construct artificial training data. Furthermore, since counterfactual\nlabeling utilizes only partial training data, we design an embedding-based\nalternative training method to enhance performance. Comparative experiments on\nboth public and private datasets and detailed analyses show that our proposed\napproach effectively alleviates the delayed feedback problem and consistently\noutperforms the previous state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Self-refining of Pseudo Labels for Music Source Separation with Noisy\n  Labeled Data",
        "url": "http://arxiv.org/abs/2307.12576v1",
        "pub_date": "2023-07-24",
        "summary": "Music source separation (MSS) faces challenges due to the limited\navailability of correctly-labeled individual instrument tracks. With the push\nto acquire larger datasets to improve MSS performance, the inevitability of\nencountering mislabeled individual instrument tracks becomes a significant\nchallenge to address. This paper introduces an automated technique for refining\nthe labels in a partially mislabeled dataset. Our proposed self-refining\ntechnique, employed with a noisy-labeled dataset, results in only a 1% accuracy\ndegradation in multi-label instrument recognition compared to a classifier\ntrained on a clean-labeled dataset. The study demonstrates the importance of\nrefining noisy-labeled data in MSS model training and shows that utilizing the\nrefined dataset leads to comparable results derived from a clean-labeled\ndataset. Notably, upon only access to a noisy dataset, MSS models trained on a\nself-refined dataset even outperform those trained on a dataset refined with a\nclassifier trained on clean labels.",
        "translated": ""
    },
    {
        "title": "FaFCNN: A General Disease Classification Framework Based on Feature\n  Fusion Neural Networks",
        "url": "http://arxiv.org/abs/2307.12518v1",
        "pub_date": "2023-07-24",
        "summary": "There are two fundamental problems in applying deep learning/machine learning\nmethods to disease classification tasks, one is the insufficient number and\npoor quality of training samples; another one is how to effectively fuse\nmultiple source features and thus train robust classification models. To\naddress these problems, inspired by the process of human learning knowledge, we\npropose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which\nintroduces a feature-aware interaction module and a feature alignment module\nbased on domain adversarial learning. This is a general framework for disease\nclassification, and FaFCNN improves the way existing methods obtain sample\ncorrelation features. The experimental results show that training using\naugmented features obtained by pre-training gradient boosting decision tree\nyields more performance gains than random-forest based methods. On the\nlow-quality dataset with a large amount of missing data in our setup, FaFCNN\nobtains a consistently optimal performance compared to competitive baselines.\nIn addition, extensive experiments demonstrate the robustness of the proposed\nmethod and the effectiveness of each component of the model\\footnote{Accepted\nin IEEE SMC2023}.",
        "translated": ""
    },
    {
        "title": "Interface Design to Mitigate Inflation in Recommender Systems",
        "url": "http://arxiv.org/abs/2307.12424v1",
        "pub_date": "2023-07-23",
        "summary": "Recommendation systems rely on user-provided data to learn about item quality\nand provide personalized recommendations. An implicit assumption when\naggregating ratings into item quality is that ratings are strong indicators of\nitem quality. In this work, we test this assumption using data collected from a\nmusic discovery application. Our study focuses on two factors that cause rating\ninflation: heterogeneous user rating behavior and the dynamics of personalized\nrecommendations. We show that user rating behavior substantially varies by\nuser, leading to item quality estimates that reflect the users who rated an\nitem more than the item quality itself. Additionally, items that are more\nlikely to be shown via personalized recommendations can experience a\nsubstantial increase in their exposure and potential bias toward them. To\nmitigate these effects, we analyze the results of a randomized controlled trial\nin which the rating interface was modified. The test resulted in a substantial\nimprovement in user rating behavior and a reduction in item quality inflation.\nThese findings highlight the importance of carefully considering the\nassumptions underlying recommendation systems and designing interfaces that\nencourage accurate rating behavior.",
        "translated": ""
    },
    {
        "title": "RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC",
        "url": "http://arxiv.org/abs/2307.12301v1",
        "pub_date": "2023-07-23",
        "summary": "Image outlier detection (OD) is crucial for ensuring the quality and accuracy\nof image datasets used in computer vision tasks. The majority of OD algorithms,\nhowever, have not been targeted toward image data. Consequently, the results of\napplying such algorithms to images are often suboptimal. In this work, we\npropose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for\nimages. By comparing images in a RANSAC-based approach, our algorithm\nautomatically predicts the outlier score of each image without additional\ntraining or label information. We evaluate RANSAC-NN against state-of-the-art\nOD algorithms on 15 diverse datasets. Without any hyperparameter tuning,\nRANSAC-NN consistently performs favorably in contrast to other algorithms in\nalmost every dataset category. Furthermore, we provide a detailed analysis to\nunderstand each RANSAC-NN component, and we demonstrate its potential\napplications in image mislabeled detection. Code for RANSAC-NN is provided at\nhttps://github.com/mxtsai/ransac-nn",
        "translated": ""
    },
    {
        "title": "Conformal Group Recommender System",
        "url": "http://arxiv.org/abs/2307.12034v1",
        "pub_date": "2023-07-22",
        "summary": "Group recommender systems (GRS) are critical in discovering relevant items\nfrom a near-infinite inventory based on group preferences rather than\nindividual preferences, like recommending a movie, restaurant, or tourist\ndestination to a group of individuals. The traditional models of group\nrecommendation are designed to act like a black box with a strict focus on\nimproving recommendation accuracy, and most often, they place the onus on the\nusers to interpret recommendations. In recent years, the focus of Recommender\nSystems (RS) research has shifted away from merely improving recommendation\naccuracy towards value additions such as confidence and explanation. In this\nwork, we propose a conformal prediction framework that provides a measure of\nconfidence with prediction in conjunction with a group recommender system to\naugment the system-generated plain recommendations. In the context of group\nrecommender systems, we propose various nonconformity measures that play a\nvital role in the efficiency of the conformal framework. We also show that\ndefined nonconformity satisfies the exchangeability property. Experimental\nresults demonstrate the effectiveness of the proposed approach over several\nbenchmark datasets. Furthermore, our proposed approach also satisfies validity\nand efficiency properties.",
        "translated": ""
    },
    {
        "title": "XWalk: Random Walk Based Candidate Retrieval for Product Search",
        "url": "http://arxiv.org/abs/2307.12019v1",
        "pub_date": "2023-07-22",
        "summary": "In e-commerce, head queries account for the vast majority of gross\nmerchandise sales and improvements to head queries are highly impactful to the\nbusiness. While most supervised approaches to search perform better in head\nqueries vs. tail queries, we propose a method that further improves head query\nperformance dramatically. We propose XWalk, a random-walk based graph approach\nto candidate retrieval for product search that borrows from recommendation\nsystem techniques. XWalk is highly efficient to train and inference in a\nlarge-scale high traffic e-commerce setting, and shows substantial improvements\nin head query performance over state-of-the-art neural retreivers. Ensembling\nXWalk with a neural and/or lexical retriever combines the best of both worlds\nand the resulting retrieval system outperforms all other methods in both\noffline relevance-based evaluation and in online A/B tests.",
        "translated": ""
    },
    {
        "title": "HTP: Exploiting Holistic Temporal Patterns for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2307.11994v1",
        "pub_date": "2023-07-22",
        "summary": "Sequential recommender systems have demonstrated a huge success for next-item\nrecommendation by explicitly exploiting the temporal order of users' historical\ninteractions. In practice, user interactions contain more useful temporal\ninformation beyond order, as shown by some pioneering studies. In this paper,\nwe systematically investigate various temporal information for sequential\nrecommendation and identify three types of advantageous temporal patterns\nbeyond order, including absolute time information, relative item time intervals\nand relative recommendation time intervals. We are the first to explore\nitem-oriented absolute time patterns. While existing models consider only one\nor two of these three patterns, we propose a novel holistic temporal pattern\nbased neural network, named HTP, to fully leverage all these three patterns. In\nparticular, we introduce novel components to address the subtle correlations\nbetween relative item time intervals and relative recommendation time\nintervals, which render a major technical challenge. Extensive experiments on\nthree real-world benchmark datasets show that our HTP model consistently and\nsubstantially outperforms many state-of-the-art models. Our code is publically\navailable at https://github.com/623851394/HTP/tree/main/HTP-main",
        "translated": ""
    },
    {
        "title": "Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning",
        "url": "http://arxiv.org/abs/2307.13632v1",
        "pub_date": "2023-07-25",
        "summary": "Mainstream bias, where some users receive poor recommendations because their\npreferences are uncommon or simply because they are less active, is an\nimportant aspect to consider regarding fairness in recommender systems.\nExisting methods to mitigate mainstream bias do not explicitly model the\nimportance of these non-mainstream users or, when they do, it is in a way that\nis not necessarily compatible with the data and recommendation model at hand.\nIn contrast, we use the recommendation utility as a more generic and implicit\nproxy to quantify mainstreamness, and propose a simple user-weighting approach\nto incorporate it into the training process while taking the cost of potential\nrecommendation errors into account. We provide extensive experimental results\nshowing that quantifying mainstreamness via utility is better able at\nidentifying non-mainstream users, and that they are indeed better served when\ntraining the model in a cost-sensitive way. This is achieved with negligible or\nno loss in overall recommendation accuracy, meaning that the models learn a\nbetter balance across users. In addition, we show that research of this kind,\nwhich evaluates recommendation quality at the individual user level, may not be\nreliable if not using enough interactions when assessing model performance.",
        "translated": ""
    },
    {
        "title": "Gaussian Graph with Prototypical Contrastive Learning in E-Commerce\n  Bundle Recommendation",
        "url": "http://arxiv.org/abs/2307.13468v1",
        "pub_date": "2023-07-25",
        "summary": "Bundle recommendation aims to provide a bundle of items to satisfy the user\npreference on e-commerce platform. Existing successful solutions are based on\nthe contrastive graph learning paradigm where graph neural networks (GNNs) are\nemployed to learn representations from user-level and bundle-level graph views\nwith a contrastive learning module to enhance the cooperative association\nbetween different views. Nevertheless, they ignore the uncertainty issue which\nhas a significant impact in real bundle recommendation scenarios due to the\nlack of discriminative information caused by highly sparsity or diversity. We\nfurther suggest that their instancewise contrastive learning fails to\ndistinguish the semantically similar negatives (i.e., sampling bias issue),\nresulting in performance degradation. In this paper, we propose a novel\nGaussian Graph with Prototypical Contrastive Learning (GPCL) framework to\novercome these challenges. In particular, GPCL embeds each user/bundle/item as\na Gaussian distribution rather than a fixed vector. We further design a\nprototypical contrastive learning module to capture the contextual information\nand mitigate the sampling bias issue. Extensive experiments demonstrate that\nbenefiting from the proposed components, we achieve new state-of-the-art\nperformance compared to previous methods on several public datasets. Moreover,\nGPCL has been deployed on real-world e-commerce platform and achieved\nsubstantial improvements.",
        "translated": ""
    },
    {
        "title": "Comprehensive Review on Semantic Information Retrieval and Ontology\n  Engineering",
        "url": "http://arxiv.org/abs/2307.13427v1",
        "pub_date": "2023-07-25",
        "summary": "Situation awareness is a crucial cognitive skill that enables individuals to\nperceive, comprehend, and project the current state of their environment\naccurately. It involves being conscious of relevant information, understanding\nits meaning, and using that understanding to make well-informed decisions.\nAwareness systems often need to integrate new knowledge and adapt to changing\nenvironments. Ontology reasoning facilitates knowledge integration and\nevolution, allowing for seamless updates and expansions of the ontology. With\nthe consideration of above, we are providing a quick review on semantic\ninformation retrieval and ontology engineering to understand the emerging\nchallenges and future research. In the review we have found that the ontology\nreasoning addresses the limitations of traditional systems by providing a\nformal, flexible, and scalable framework for knowledge representation,\nreasoning, and inference.",
        "translated": ""
    },
    {
        "title": "An End-to-End Workflow using Topic Segmentation and Text Summarisation\n  Methods for Improved Podcast Comprehension",
        "url": "http://arxiv.org/abs/2307.13394v1",
        "pub_date": "2023-07-25",
        "summary": "The consumption of podcast media has been increasing rapidly. Due to the\nlengthy nature of podcast episodes, users often carefully select which ones to\nlisten to. Although episode descriptions aid users by providing a summary of\nthe entire podcast, they do not provide a topic-by-topic breakdown. This study\nexplores the combined application of topic segmentation and text summarisation\nmethods to investigate how podcast episode comprehension can be improved. We\nhave sampled 10 episodes from Spotify's English-Language Podcast Dataset and\nemployed TextTiling and TextSplit to segment them. Moreover, three text\nsummarisation models, namely T5, BART, and Pegasus, were applied to provide a\nvery short title for each segment. The segmentation part was evaluated using\nour annotated sample with the $P_k$ and WindowDiff ($WD$) metrics. A survey was\nalso rolled out ($N=25$) to assess the quality of the generated summaries. The\nTextSplit algorithm achieved the lowest mean for both evaluation metrics\n($\\bar{P_k}=0.41$ and $\\bar{WD}=0.41$), while the T5 model produced the best\nsummaries, achieving a relevancy score only $8\\%$ less to the one achieved by\nthe human-written titles.",
        "translated": ""
    },
    {
        "title": "Embedding Models for Supervised Automatic Extraction and Classification\n  of Named Entities in Scientific Acknowledgements",
        "url": "http://arxiv.org/abs/2307.13377v1",
        "pub_date": "2023-07-25",
        "summary": "Acknowledgments in scientific papers may give an insight into aspects of the\nscientific community, such as reward systems, collaboration patterns, and\nhidden research trends. The aim of the paper is to evaluate the performance of\ndifferent embedding models for the task of automatic extraction and\nclassification of acknowledged entities from the acknowledgment text in\nscientific papers. We trained and implemented a named entity recognition (NER)\ntask using the Flair NLP framework. The training was conducted using three\ndefault Flair NER models with four differently-sized corpora and different\nversions of the Flair NLP framework. The Flair Embeddings model trained on the\nmedium corpus with the latest FLAIR version showed the best accuracy of 0.79.\nExpanding the size of a training corpus from very small to medium size\nmassively increased the accuracy of all training algorithms, but further\nexpansion of the training corpus did not bring further improvement. Moreover,\nthe performance of the model slightly deteriorated. Our model is able to\nrecognize six entity types: funding agency, grant number, individuals,\nuniversity, corporation, and miscellaneous. The model works more precisely for\nsome entity types than for others; thus, individuals and grant numbers showed a\nvery good F1-Score over 0.9. Most of the previous works on acknowledgment\nanalysis were limited by the manual evaluation of data and therefore by the\namount of processed data. This model can be applied for the comprehensive\nanalysis of acknowledgment texts and may potentially make a great contribution\nto the field of automated acknowledgment analysis.",
        "translated": ""
    },
    {
        "title": "An Intent Taxonomy of Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2307.13298v1",
        "pub_date": "2023-07-25",
        "summary": "Legal case retrieval is a special Information Retrieval~(IR) task focusing on\nlegal case documents. Depending on the downstream tasks of the retrieved case\ndocuments, users' information needs in legal case retrieval could be\nsignificantly different from those in Web search and traditional ad-hoc\nretrieval tasks. While there are several studies that retrieve legal cases\nbased on text similarity, the underlying search intents of legal retrieval\nusers, as shown in this paper, are more complicated than that yet mostly\nunexplored. To this end, we present a novel hierarchical intent taxonomy of\nlegal case retrieval. It consists of five intent types categorized by three\ncriteria, i.e., search for Particular Case(s), Characterization, Penalty,\nProcedure, and Interest. The taxonomy was constructed transparently and\nevaluated extensively through interviews, editorial user studies, and query log\nanalysis. Through a laboratory user study, we reveal significant differences in\nuser behavior and satisfaction under different search intents in legal case\nretrieval. Furthermore, we apply the proposed taxonomy to various downstream\nlegal retrieval tasks, e.g., result ranking and satisfaction prediction, and\ndemonstrate its effectiveness. Our work provides important insights into the\nunderstanding of user intents in legal case retrieval and potentially leads to\nbetter retrieval techniques in the legal domain, such as intent-aware ranking\nstrategies and evaluation methodologies.",
        "translated": ""
    },
    {
        "title": "Investigating the Robustness of Sequential Recommender Systems Against\n  Training Data Perturbations: an Empirical Study",
        "url": "http://arxiv.org/abs/2307.13165v1",
        "pub_date": "2023-07-24",
        "summary": "Sequential Recommender Systems (SRSs) have been widely used to model user\nbehavior over time, but their robustness in the face of perturbations to\ntraining data is a critical issue. In this paper, we conduct an empirical study\nto investigate the effects of removing items at different positions within a\ntemporally ordered sequence. We evaluate two different SRS models on multiple\ndatasets, measuring their performance using Normalized Discounted Cumulative\nGain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that\nremoving items at the end of the sequence significantly impacts performance,\nwith NDCG decreasing up to 60\\%, while removing items from the beginning or\nmiddle has no significant effect. These findings highlight the importance of\nconsidering the position of the perturbed items in the training data and shall\ninform the design of more robust SRSs.",
        "translated": ""
    },
    {
        "title": "ChatGPT and Persuasive Technologies for the Management and Delivery of\n  Personalized Recommendations in Hotel Hospitality",
        "url": "http://arxiv.org/abs/2307.14298v1",
        "pub_date": "2023-07-26",
        "summary": "Recommender systems have become indispensable tools in the hotel hospitality\nindustry, enabling personalized and tailored experiences for guests. Recent\nadvancements in large language models (LLMs), such as ChatGPT, and persuasive\ntechnologies, have opened new avenues for enhancing the effectiveness of those\nsystems. This paper explores the potential of integrating ChatGPT and\npersuasive technologies for automating and improving hotel hospitality\nrecommender systems. First, we delve into the capabilities of ChatGPT, which\ncan understand and generate human-like text, enabling more accurate and\ncontext-aware recommendations. We discuss the integration of ChatGPT into\nrecommender systems, highlighting the ability to analyze user preferences,\nextract valuable insights from online reviews, and generate personalized\nrecommendations based on guest profiles. Second, we investigate the role of\npersuasive technology in influencing user behavior and enhancing the persuasive\nimpact of hotel recommendations. By incorporating persuasive techniques, such\nas social proof, scarcity and personalization, recommender systems can\neffectively influence user decision-making and encourage desired actions, such\nas booking a specific hotel or upgrading their room. To investigate the\nefficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment\nwith a case study involving a hotel recommender system. We aim to study the\nimpact of integrating ChatGPT and persua-sive techniques on user engagement,\nsatisfaction, and conversion rates. The preliminary results demonstrate the\npotential of these technologies in enhancing the overall guest experience and\nbusiness performance. Overall, this paper contributes to the field of hotel\nhospitality by exploring the synergistic relationship between LLMs and\npersuasive technology in recommender systems, ultimately influencing guest\nsatisfaction and hotel revenue.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Competitive Near Cold-start Recommenders for\n  Language- and Item-based Preferences",
        "url": "http://arxiv.org/abs/2307.14225v1",
        "pub_date": "2023-07-26",
        "summary": "Traditional recommender systems leverage users' item preference history to\nrecommend novel content that users may like. However, modern dialog interfaces\nthat allow users to express language-based preferences offer a fundamentally\ndifferent modality for preference input. Inspired by recent successes of\nprompting paradigms for large language models (LLMs), we study their use for\nmaking recommendations from both item-based and language-based preferences in\ncomparison to state-of-the-art item-based collaborative filtering (CF) methods.\nTo support this investigation, we collect a new dataset consisting of both\nitem-based and language-based preferences elicited from users along with their\nratings on a variety of (biased) recommended items and (unbiased) random items.\nAmong numerous experimental results, we find that LLMs provide competitive\nrecommendation performance for pure language-based preferences (no item\npreferences) in the near cold-start case in comparison to item-based CF\nmethods, despite having no supervised training for this specific task\n(zero-shot) or only a few labels (few-shot). This is particularly promising as\nlanguage-based preference representations are more explainable and scrutable\nthan item-based or vector-based representations.",
        "translated": ""
    },
    {
        "title": "A Probabilistic Position Bias Model for Short-Video Recommendation Feeds",
        "url": "http://arxiv.org/abs/2307.14059v1",
        "pub_date": "2023-07-26",
        "summary": "Modern web-based platforms show ranked lists of recommendations to users,\nattempting to maximise user satisfaction or business metrics. Typically, the\ngoal of such systems boils down to maximising the exposure probability for\nitems that are deemed \"reward-maximising\" according to a metric of interest.\nThis general framing comprises streaming applications, as well as e-commerce or\njob recommendations, and even web search. Position bias or user models can be\nused to estimate exposure probabilities for each use-case, specifically\ntailored to how users interact with the presented rankings. A unifying factor\nin these diverse problem settings is that typically only one or several items\nwill be engaged with (clicked, streamed,...) before a user leaves the ranked\nlist. Short-video feeds on social media platforms diverge from this general\nframing in several ways, most notably that users do not tend to leave the feed\nafter e.g. liking a post. Indeed, seemingly infinite feeds invite users to\nscroll further down the ranked list. For this reason, existing position bias or\nuser models tend to fall short in such settings, as they do not accurately\ncapture users' interaction modalities.\n  In this work, we propose a novel and probabilistically sound personalised\nposition bias model for feed recommendations. We focus on a 1st-level feed in a\nhierarchical structure, where users may enter a 2nd-level feed via any given\n1st-level item. We posit that users come to the platform with a scrolling\nbudget drawn according to some distribution, and show how the survival function\nof said distribution can be used to obtain closed-form estimates for\npersonalised exposure probabilities. Empirical insights from a large-scale\nsocial media platform show how our probabilistic position bias model more\naccurately captures empirical exposure than existing models, and paves the way\nfor unbiased evaluation and learning-to-rank.",
        "translated": ""
    },
    {
        "title": "Multi-view Hypergraph Contrastive Policy Learning for Conversational\n  Recommendation",
        "url": "http://arxiv.org/abs/2307.14024v1",
        "pub_date": "2023-07-26",
        "summary": "Conversational recommendation systems (CRS) aim to interactively acquire user\npreferences and accordingly recommend items to users. Accurately learning the\ndynamic user preferences is of crucial importance for CRS. Previous works learn\nthe user preferences with pairwise relations from the interactive conversation\nand item knowledge, while largely ignoring the fact that factors for a\nrelationship in CRS are multiplex. Specifically, the user likes/dislikes the\nitems that satisfy some attributes (Like/Dislike view). Moreover social\ninfluence is another important factor that affects user preference towards the\nitem (Social view), while is largely ignored by previous works in CRS. The user\npreferences from these three views are inherently different but also correlated\nas a whole. The user preferences from the same views should be more similar\nthan that from different views. The user preferences from Like View should be\nsimilar to Social View while different from Dislike View. To this end, we\npropose a novel model, namely Multi-view Hypergraph Contrastive Policy Learning\n(MHCPL). Specifically, MHCPL timely chooses useful social information according\nto the interactive history and builds a dynamic hypergraph with three types of\nmultiplex relations from different views. The multiplex relations in each view\nare successively connected according to their generation order.",
        "translated": ""
    },
    {
        "title": "Domain Disentanglement with Interpolative Data Augmentation for\n  Dual-Target Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2307.13910v1",
        "pub_date": "2023-07-26",
        "summary": "The conventional single-target Cross-Domain Recommendation (CDR) aims to\nimprove the recommendation performance on a sparser target domain by\ntransferring the knowledge from a source domain that contains relatively richer\ninformation. By contrast, in recent years, dual-target CDR has been proposed to\nimprove the recommendation performance on both domains simultaneously. However,\nto this end, there are two challenges in dual-target CDR: (1) how to generate\nboth relevant and diverse augmented user representations, and (2) how to\neffectively decouple domain-independent information from domain-specific\ninformation, in addition to domain-shared information, to capture comprehensive\nuser preferences. To address the above two challenges, we propose a\nDisentanglement-based framework with Interpolative Data Augmentation for\ndual-target Cross-Domain Recommendation, called DIDA-CDR. In DIDA-CDR, we first\npropose an interpolative data augmentation approach to generating both relevant\nand diverse augmented user representations to augment sparser domain and\nexplore potential user preferences. We then propose a disentanglement module to\neffectively decouple domain-specific and domain-independent information to\ncapture comprehensive user preferences. Both steps significantly contribute to\ncapturing more comprehensive user preferences, thereby improving the\nrecommendation performance on each domain. Extensive experiments conducted on\nfive real-world datasets show the significant superiority of DIDA-CDR over the\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "ClusterSeq: Enhancing Sequential Recommender Systems with Clustering\n  based Meta-Learning",
        "url": "http://arxiv.org/abs/2307.13766v1",
        "pub_date": "2023-07-25",
        "summary": "In practical scenarios, the effectiveness of sequential recommendation\nsystems is hindered by the user cold-start problem, which arises due to limited\ninteractions for accurately determining user preferences. Previous studies have\nattempted to address this issue by combining meta-learning with user and\nitem-side information. However, these approaches face inherent challenges in\nmodeling user preference dynamics, particularly for \"minor users\" who exhibit\ndistinct preferences compared to more common or \"major users.\" To overcome\nthese limitations, we present a novel approach called ClusterSeq, a\nMeta-Learning Clustering-Based Sequential Recommender System. ClusterSeq\nleverages dynamic information in the user sequence to enhance item prediction\naccuracy, even in the absence of side information. This model preserves the\npreferences of minor users without being overshadowed by major users, and it\ncapitalizes on the collective knowledge of users within the same cluster.\nExtensive experiments conducted on various benchmark datasets validate the\neffectiveness of ClusterSeq. Empirical results consistently demonstrate that\nClusterSeq outperforms several state-of-the-art meta-learning recommenders.\nNotably, compared to existing meta-learning methods, our proposed approach\nachieves a substantial improvement of 16-39% in Mean Reciprocal Rank (MRR).",
        "translated": ""
    },
    {
        "title": "On (Normalised) Discounted Cumulative Gain as an Offline Evaluation\n  Metric for Top-$n$ Recommendation",
        "url": "http://arxiv.org/abs/2307.15053v1",
        "pub_date": "2023-07-27",
        "summary": "Approaches to recommendation are typically evaluated in one of two ways: (1)\nvia a (simulated) online experiment, often seen as the gold standard, or (2)\nvia some offline evaluation procedure, where the goal is to approximate the\noutcome of an online experiment. Several offline evaluation metrics have been\nadopted in the literature, inspired by ranking metrics prevalent in the field\nof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one\nsuch metric that has seen widespread adoption in empirical studies, and higher\n(n)DCG values have been used to present new methods as the state-of-the-art in\ntop-$n$ recommendation for many years.\n  Our work takes a critical look at this approach, and investigates when we can\nexpect such metrics to approximate the gold standard outcome of an online\nexperiment. We formally present the assumptions that are necessary to consider\nDCG an unbiased estimator of online reward and provide a derivation for this\nmetric from first principles, highlighting where we deviate from its\ntraditional uses in IR. Importantly, we show that normalising the metric\nrenders it inconsistent, in that even when DCG is unbiased, ranking competing\nmethods by their normalised DCG can invert their relative order. Through a\ncorrelation analysis between off- and on-line experiments conducted on a\nlarge-scale recommendation platform, we show that our unbiased DCG estimates\nstrongly correlate with online reward, even when some of the metric's inherent\nassumptions are violated. This statement no longer holds for its normalised\nvariant, suggesting that nDCG's practical utility may be limited.",
        "translated": ""
    },
    {
        "title": "The Effect of Third Party Implementations on Reproducibility",
        "url": "http://arxiv.org/abs/2307.14956v1",
        "pub_date": "2023-07-27",
        "summary": "Reproducibility of recommender systems research has come under scrutiny\nduring recent years. Along with works focusing on repeating experiments with\ncertain algorithms, the research community has also started discussing various\naspects of evaluation and how these affect reproducibility. We add a novel\nangle to this discussion by examining how unofficial third-party\nimplementations could benefit or hinder reproducibility. Besides giving a\ngeneral overview, we thoroughly examine six third-party implementations of a\npopular recommender algorithm and compare them to the official version on five\npublic datasets. In the light of our alarming findings we aim to draw the\nattention of the research community to this neglected aspect of\nreproducibility.",
        "translated": ""
    },
    {
        "title": "Widespread Flaws in Offline Evaluation of Recommender Systems",
        "url": "http://arxiv.org/abs/2307.14951v1",
        "pub_date": "2023-07-27",
        "summary": "Even though offline evaluation is just an imperfect proxy of online\nperformance -- due to the interactive nature of recommenders -- it will\nprobably remain the primary way of evaluation in recommender systems research\nfor the foreseeable future, since the proprietary nature of production\nrecommenders prevents independent validation of A/B test setups and\nverification of online results. Therefore, it is imperative that offline\nevaluation setups are as realistic and as flawless as they can be.\nUnfortunately, evaluation flaws are quite common in recommender systems\nresearch nowadays, due to later works copying flawed evaluation setups from\ntheir predecessors without questioning their validity. In the hope of improving\nthe quality of offline evaluation of recommender systems, we discuss four of\nthese widespread flaws and why researchers should avoid them.",
        "translated": ""
    },
    {
        "title": "Scaling Session-Based Transformer Recommendations using Optimized\n  Negative Sampling and Loss Functions",
        "url": "http://arxiv.org/abs/2307.14906v1",
        "pub_date": "2023-07-27",
        "summary": "This work introduces TRON, a scalable session-based Transformer Recommender\nusing Optimized Negative-sampling. Motivated by the scalability and performance\nlimitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates\ntop-k negative sampling and listwise loss functions to enhance its\nrecommendation accuracy. Evaluations on relevant large-scale e-commerce\ndatasets show that TRON improves upon the recommendation quality of current\nmethods while maintaining training speeds similar to SASRec. A live A/B test\nyielded an 18.14% increase in click-through rate over SASRec, highlighting the\npotential of TRON in practical settings. For further research, we provide\naccess to our source code at https://github.com/otto-de/TRON and an anonymized\ndataset at https://github.com/otto-de/recsys-dataset.",
        "translated": ""
    },
    {
        "title": "Integrating Offline Reinforcement Learning with Transformers for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2307.14450v1",
        "pub_date": "2023-07-26",
        "summary": "We consider the problem of sequential recommendation, where the current\nrecommendation is made based on past interactions. This recommendation task\nrequires efficient processing of the sequential data and aims to provide\nrecommendations that maximize the long-term reward. To this end, we train a\nfarsighted recommender by using an offline RL algorithm with the policy network\nin our model architecture that has been initialized from a pre-trained\ntransformer model. The pre-trained model leverages the superb ability of the\ntransformer to process sequential information. Compared to prior works that\nrely on online interaction via simulation, we focus on implementing a fully\noffline RL framework that is able to converge in a fast and stable way. Through\nextensive experiments on public datasets, we show that our method is robust\nacross various recommendation regimes, including e-commerce and movie\nsuggestions. Compared to state-of-the-art supervised learning algorithms, our\nalgorithm yields recommendations of higher quality, demonstrating the clear\nadvantage of combining RL and transformers.",
        "translated": ""
    },
    {
        "title": "Measuring Americanization: A Global Quantitative Study of Interest in\n  American Topics on Wikipedia",
        "url": "http://arxiv.org/abs/2307.14401v1",
        "pub_date": "2023-07-26",
        "summary": "We conducted a global comparative analysis of the coverage of American topics\nin different language versions of Wikipedia, using over 90 million Wikidata\nitems and 40 million Wikipedia articles in 58 languages. Our study aimed to\ninvestigate whether Americanization is more or less dominant in different\nregions and cultures and to determine whether interest in American topics is\nuniversal.",
        "translated": ""
    },
    {
        "title": "Framework to Automatically Determine the Quality of Open Data Catalogs",
        "url": "http://arxiv.org/abs/2307.15464v1",
        "pub_date": "2023-07-28",
        "summary": "Data catalogs play a crucial role in modern data-driven organizations by\nfacilitating the discovery, understanding, and utilization of diverse data\nassets. However, ensuring their quality and reliability is complex, especially\nin open and large-scale data environments. This paper proposes a framework to\nautomatically determine the quality of open data catalogs, addressing the need\nfor efficient and reliable quality assessment mechanisms. Our framework can\nanalyze various core quality dimensions, such as accuracy, completeness,\nconsistency, scalability, and timeliness, offer several alternatives for the\nassessment of compatibility and similarity across such catalogs as well as the\nimplementation of a set of non-core quality dimensions such as provenance,\nreadability, and licensing. The goal is to empower data-driven organizations to\nmake informed decisions based on trustworthy and well-curated data assets. The\nsource code that illustrates our approach can be downloaded from\nhttps://www.github.com/jorge-martinez-gil/dataq/.",
        "translated": ""
    },
    {
        "title": "Toward Transparent Sequence Models with Model-Based Tree Markov Model",
        "url": "http://arxiv.org/abs/2307.15367v1",
        "pub_date": "2023-07-28",
        "summary": "In this study, we address the interpretability issue in complex, black-box\nMachine Learning models applied to sequence data. We introduce the Model-Based\ntree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model\naimed at detecting high mortality risk events and discovering hidden patterns\nassociated with the mortality risk in Intensive Care Units (ICU). This model\nleverages knowledge distilled from Deep Neural Networks (DNN) to enhance\npredictive performance while offering clear explanations. Our experimental\nresults indicate the improved performance of Model-Based trees (MOB trees) via\nemploying LSTM for learning sequential patterns, which are then transferred to\nMOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in\nthe MOB-HSMM enables uncovering potential and explainable sequences using\navailable information.",
        "translated": ""
    },
    {
        "title": "Staging E-Commerce Products for Online Advertising using Retrieval\n  Assisted Image Generation",
        "url": "http://arxiv.org/abs/2307.15326v1",
        "pub_date": "2023-07-28",
        "summary": "Online ads showing e-commerce products typically rely on the product images\nin a catalog sent to the advertising platform by an e-commerce platform. In the\nbroader ads industry such ads are called dynamic product ads (DPA). It is\ncommon for DPA catalogs to be in the scale of millions (corresponding to the\nscale of products which can be bought from the e-commerce platform). However,\nnot all product images in the catalog may be appealing when directly\nre-purposed as an ad image, and this may lead to lower click-through rates\n(CTRs). In particular, products just placed against a solid background may not\nbe as enticing and realistic as a product staged in a natural environment. To\naddress such shortcomings of DPA images at scale, we propose a generative\nadversarial network (GAN) based approach to generate staged backgrounds for\nun-staged product images. Generating the entire staged background is a\nchallenging task susceptible to hallucinations. To get around this, we\nintroduce a simpler approach called copy-paste staging using retrieval assisted\nGANs. In copy paste staging, we first retrieve (from the catalog) staged\nproducts similar to the un-staged input product, and then copy-paste the\nbackground of the retrieved product in the input image. A GAN based in-painting\nmodel is used to fill the holes left after this copy-paste operation. We show\nthe efficacy of our copy-paste staging method via offline metrics, and human\nevaluation. In addition, we show how our staging approach can enable animations\nof moving products leading to a video ad from a product image.",
        "translated": ""
    },
    {
        "title": "Reconciling the accuracy-diversity trade-off in recommendations",
        "url": "http://arxiv.org/abs/2307.15142v1",
        "pub_date": "2023-07-27",
        "summary": "In recommendation settings, there is an apparent trade-off between the goals\nof accuracy (to recommend items a user is most likely to want) and diversity\n(to recommend items representing a range of categories). As such, real-world\nrecommender systems often explicitly incorporate diversity separately from\naccuracy. This approach, however, leaves a basic question unanswered: Why is\nthere a trade-off in the first place?\n  We show how the trade-off can be explained via a user's consumption\nconstraints -- users typically only consume a few of the items they are\nrecommended. In a stylized model we introduce, objectives that account for this\nconstraint induce diverse recommendations, while objectives that do not account\nfor this constraint induce homogeneous recommendations. This suggests that\naccuracy and diversity appear misaligned because standard accuracy metrics do\nnot consider consumption constraints. Our model yields precise and\ninterpretable characterizations of diversity in different settings, giving\npractical insights into the design of diverse recommendations.",
        "translated": ""
    },
    {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative\n  Information-Seeking with Attribution",
        "url": "http://arxiv.org/abs/2307.16883v1",
        "pub_date": "2023-07-31",
        "summary": "The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.",
        "translated": ""
    },
    {
        "title": "Metric@CustomerN: Evaluating Metrics at a Customer Level in E-Commerce",
        "url": "http://arxiv.org/abs/2307.16832v1",
        "pub_date": "2023-07-31",
        "summary": "Accuracy measures such as Recall, Precision, and Hit Rate have been a\nstandard way of evaluating Recommendation Systems. The assumption is to use a\nfixed Top-N to represent them. We propose that median impressions viewed from\nhistorical sessions per diner be used as a personalized value for N. We present\npreliminary exploratory results and list future steps to improve upon and\nevaluate the efficacy of these personalized metrics.",
        "translated": ""
    },
    {
        "title": "Defense of Adversarial Ranking Attack in Text Retrieval: Benchmark and\n  Baseline via Detection",
        "url": "http://arxiv.org/abs/2307.16816v1",
        "pub_date": "2023-07-31",
        "summary": "Neural ranking models (NRMs) have undergone significant development and have\nbecome integral components of information retrieval (IR) systems.\nUnfortunately, recent research has unveiled the vulnerability of NRMs to\nadversarial document manipulations, potentially exploited by malicious search\nengine optimization practitioners. While progress in adversarial attack\nstrategies aids in identifying the potential weaknesses of NRMs before their\ndeployment, the defensive measures against such attacks, like the detection of\nadversarial documents, remain inadequately explored. To mitigate this gap, this\npaper establishes a benchmark dataset to facilitate the investigation of\nadversarial ranking defense and introduces two types of detection tasks for\nadversarial documents. A comprehensive investigation of the performance of\nseveral detection baselines is conducted, which involve examining the\nspamicity, perplexity, and linguistic acceptability, and utilizing supervised\nclassifiers. Experimental results demonstrate that a supervised classifier can\neffectively mitigate known attacks, but it performs poorly against unseen\nattacks. Furthermore, such classifier should avoid using query text to prevent\nlearning the classification on relevance, as it might lead to the inadvertent\ndiscarding of relevant documents.",
        "translated": ""
    },
    {
        "title": "Lexically-Accelerated Dense Retrieval",
        "url": "http://arxiv.org/abs/2307.16779v1",
        "pub_date": "2023-07-31",
        "summary": "Retrieval approaches that score documents based on learned dense vectors\n(i.e., dense retrieval) rather than lexical signals (i.e., conventional\nretrieval) are increasingly popular. Their ability to identify related\ndocuments that do not necessarily contain the same terms as those appearing in\nthe user's query (thereby improving recall) is one of their key advantages.\nHowever, to actually achieve these gains, dense retrieval approaches typically\nrequire an exhaustive search over the document collection, making them\nconsiderably more expensive at query-time than conventional lexical approaches.\nSeveral techniques aim to reduce this computational overhead by approximating\nthe results of a full dense retriever. Although these approaches reasonably\napproximate the top results, they suffer in terms of recall -- one of the key\nadvantages of dense retrieval. We introduce 'LADR' (Lexically-Accelerated Dense\nRetrieval), a simple-yet-effective approach that improves the efficiency of\nexisting dense retrieval models without compromising on retrieval\neffectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval\nexploration that uses a document proximity graph. We explore two variants of\nLADR: a proactive approach that expands the search space to the neighbors of\nall seed documents, and an adaptive approach that selectively searches the\ndocuments with the highest estimated relevance in an iterative fashion. Through\nextensive experiments across a variety of dense retrieval models, we find that\nLADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier\namong approximate k nearest neighbor techniques. Further, we find that when\ntuned to take around 8ms per query in retrieval latency on our hardware, LADR\nconsistently achieves both precision and recall that are on par with an\nexhaustive search on standard benchmarks.",
        "translated": ""
    },
    {
        "title": "AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of\n  Autism Spectrum Disorder",
        "url": "http://arxiv.org/abs/2307.16773v1",
        "pub_date": "2023-07-31",
        "summary": "To easily obtain the knowledge about autism spectrum disorder and help its\nearly screening and diagnosis, we create AsdKB, a Chinese knowledge base on\nautism spectrum disorder. The knowledge base is built on top of various\nsources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical\ndescriptions on mental and behavioural disorders, 2) the diagnostic knowledge\nfrom DSM-5 and different screening tools recommended by social organizations\nand medical institutes, and 3) the expert knowledge on professional physicians\nand hospitals from the Web. AsdKB contains both ontological and factual\nknowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The\npotential applications of AsdKB are question answering, auxiliary diagnosis,\nand expert recommendation, and we illustrate them with a prototype which can be\naccessed at http://asdkb.org.cn/.",
        "translated": ""
    },
    {
        "title": "NEON: Living Needs Prediction System in Meituan",
        "url": "http://arxiv.org/abs/2307.16644v1",
        "pub_date": "2023-07-31",
        "summary": "Living needs refer to the various needs in human's daily lives for survival\nand well-being, including food, housing, entertainment, etc. On life service\nplatforms that connect users to service providers, such as Meituan, the problem\nof living needs prediction is fundamental as it helps understand users and\nboost various downstream applications such as personalized recommendation.\nHowever, the problem has not been well explored and is faced with two critical\nchallenges. First, the needs are naturally connected to specific locations and\ntimes, suffering from complex impacts from the spatiotemporal context. Second,\nthere is a significant gap between users' actual living needs and their\nhistorical records on the platform. To address these two challenges, we design\na system of living NEeds predictiON named NEON, consisting of three phases:\nfeature mining, feature fusion, and multi-task prediction. In the feature\nmining phase, we carefully extract individual-level user features for\nspatiotemporal modeling, and aggregated-level behavioral features for enriching\ndata, which serve as the basis for addressing two challenges, respectively.\nFurther, in the feature fusion phase, we propose a neural network that\neffectively fuses two parts of features into the user representation. Moreover,\nwe design a multi-task prediction phase, where the auxiliary task of\nneeds-meeting way prediction can enhance the modeling of spatiotemporal\ncontext. Extensive offline evaluations verify that our NEON system can\neffectively predict users' living needs. Furthermore, we deploy NEON into\nMeituan's algorithm engine and evaluate how it enhances the three downstream\nprediction applications, via large-scale online A/B testing.",
        "translated": ""
    },
    {
        "title": "When Large Language Models Meet Personalization: Perspectives of\n  Challenges and Opportunities",
        "url": "http://arxiv.org/abs/2307.16376v1",
        "pub_date": "2023-07-31",
        "summary": "The advent of large language models marks a revolutionary breakthrough in\nartificial intelligence. With the unprecedented scale of training and model\nparameters, the capability of large language models has been dramatically\nimproved, leading to human-like performances in understanding, language\nsynthesizing, and common-sense reasoning, etc. Such a major leap-forward in\ngeneral AI capacity will change the pattern of how personalization is\nconducted. For one thing, it will reform the way of interaction between humans\nand personalization systems. Instead of being a passive medium of information\nfiltering, large language models present the foundation for active user\nengagement. On top of such a new foundation, user requests can be proactively\nexplored, and user's required information can be delivered in a natural and\nexplainable way. For another thing, it will also considerably expand the scope\nof personalization, making it grow from the sole function of collecting\npersonalized information to the compound function of providing personalized\nservices. By leveraging large language models as general-purpose interface, the\npersonalization systems may compile user requests into plans, calls the\nfunctions of external tools to execute the plans, and integrate the tools'\noutputs to complete the end-to-end personalization tasks. Today, large language\nmodels are still being developed, whereas the application in personalization is\nlargely unexplored. Therefore, we consider it to be the right time to review\nthe challenges in personalization and the opportunities to address them with\nLLMs. In particular, we dedicate this perspective paper to the discussion of\nthe following aspects: the development and challenges for the existing\npersonalization system, the newly emerged capabilities of large language\nmodels, and the potential ways of making use of large language models for\npersonalization.",
        "translated": ""
    },
    {
        "title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning",
        "url": "http://arxiv.org/abs/2307.16372v1",
        "pub_date": "2023-07-31",
        "summary": "Automatic music captioning, which generates natural language descriptions for\ngiven music tracks, holds significant potential for enhancing the understanding\nand organization of large volumes of musical data. Despite its importance,\nresearchers face challenges due to the costly and time-consuming collection\nprocess of existing music-language datasets, which are limited in size. To\naddress this data scarcity issue, we propose the use of large language models\n(LLMs) to artificially generate the description sentences from large-scale tag\ndatasets. This results in approximately 2.2M captions paired with 0.5M audio\nclips. We term it Large Language Model based Pseudo music caption dataset,\nshortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale\nmusic captioning dataset with various quantitative evaluation metrics used in\nthe field of natural language processing as well as human evaluation. In\naddition, we trained a transformer-based music captioning model with the\ndataset and evaluated it under zero-shot and transfer-learning settings. The\nresults demonstrate that our proposed approach outperforms the supervised\nbaseline model.",
        "translated": ""
    },
    {
        "title": "Workshop on Document Intelligence Understanding",
        "url": "http://arxiv.org/abs/2307.16369v1",
        "pub_date": "2023-07-31",
        "summary": "Document understanding and information extraction include different tasks to\nunderstand a document and extract valuable information automatically. Recently,\nthere has been a rising demand for developing document understanding among\ndifferent domains, including business, law, and medicine, to boost the\nefficiency of work that is associated with a large number of documents. This\nworkshop aims to bring together researchers and industry developers in the\nfield of document intelligence and understanding diverse document types to\nboost automatic document processing and understanding techniques. We also\nreleased a data challenge on the recently introduced document-level VQA\ndataset, PDFVQA. The PDFVQA challenge examines the structural and contextual\nunderstandings of proposed models on the natural full document level of\nmultiple consecutive document pages by including questions with a sequence of\nanswers extracted from multi-pages of the full document. This task helps to\nboost the document understanding step from the single-page level to the full\ndocument level understanding.",
        "translated": ""
    },
    {
        "title": "Time-Aware Item Weighting for the Next Basket Recommendations",
        "url": "http://arxiv.org/abs/2307.16297v1",
        "pub_date": "2023-07-30",
        "summary": "In this paper we study the next basket recommendation problem. Recent methods\nuse different approaches to achieve better performance. However, many of them\ndo not use information about the time of prediction and time intervals between\nbaskets. To fill this gap, we propose a novel method, Time-Aware Item-based\nWeighting (TAIW), which takes timestamps and intervals into account. We provide\nexperiments on three real-world datasets, and TAIW outperforms well-tuned\nstate-of-the-art baselines for next-basket recommendations. In addition, we\nshow the results of an ablation study and a case study of a few items.",
        "translated": ""
    },
    {
        "title": "TimePool: Visually Answer \"Which and When\" Questions On Univariate Time\n  Series",
        "url": "http://arxiv.org/abs/2308.00682v1",
        "pub_date": "2023-08-01",
        "summary": "When exploring time series datasets, analysts often pose \"which and when\"\nquestions. For example, with world life expectancy data over one hundred years,\nthey may inquire about the top 10 countries in life expectancy and the time\nperiod when they achieved this status, or which countries have had longer life\nexpectancy than Ireland and when. This paper proposes TimePool, a new\nvisualization prototype, to address this need for univariate time series\nanalysis. It allows users to construct interactive \"which and when\" queries and\nvisually explore the results for insights.",
        "translated": ""
    },
    {
        "title": "Explainable Graph Spectral Clustering of Text Documents",
        "url": "http://arxiv.org/abs/2308.00504v1",
        "pub_date": "2023-08-01",
        "summary": "Spectral clustering methods are known for their ability to represent clusters\nof diverse shapes, densities etc. However, results of such algorithms, when\napplied e.g. to text documents, are hard to explain to the user, especially due\nto embedding in the spectral space which has no obvious relation to document\ncontents. Therefore there is an urgent need to elaborate methods for explaining\nthe outcome of the clustering. This paper presents a contribution towards this\ngoal. We present a proposal of explanation of results of combinatorial\nLaplacian based graph spectral clustering. It is based on showing (approximate)\nequivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in\nthis paper) and term vector space embedding. Hence a bridge is constructed\nbetween the textual contents and the clustering results. We provide theoretical\nbackground for this approach. We performed experimental study showing that\n$K$-embedding approximates well Laplacian embedding under favourable block\nmatrix conditions and show that approximation is good enough under other\nconditions.",
        "translated": ""
    },
    {
        "title": "On the Effects of Regional Spelling Conventions in Retrieval Models",
        "url": "http://arxiv.org/abs/2308.00480v1",
        "pub_date": "2023-08-01",
        "summary": "One advantage of neural ranking models is that they are meant to generalise\nwell in situations of synonymity i.e. where two words have similar or identical\nmeanings. In this paper, we investigate and quantify how well various ranking\nmodels perform in a clear-cut case of synonymity: when words are simply\nexpressed in different surface forms due to regional differences in spelling\nconventions (e.g., color vs colour). We first explore the prevalence of\nAmerican and British English spelling conventions in datasets used for the\npre-training, training and evaluation of neural retrieval methods, and find\nthat American spelling conventions are far more prevalent. Despite these biases\nin the training data, we find that retrieval models often generalise well in\nthis case of synonymity. We explore the effect of document spelling\nnormalisation in retrieval and observe that all models are affected by\nnormalising the document's spelling. While they all experience a drop in\nperformance when normalised to a different spelling convention than that of the\nquery, we observe varied behaviour when the document is normalised to share the\nquery spelling convention: lexical models show improvements, dense retrievers\nremain unaffected, and re-rankers exhibit contradictory behaviour.",
        "translated": ""
    },
    {
        "title": "Generative Query Reformulation for Effective Adhoc Search",
        "url": "http://arxiv.org/abs/2308.00415v1",
        "pub_date": "2023-08-01",
        "summary": "Performing automatic reformulations of a user's query is a popular paradigm\nused in information retrieval (IR) for improving effectiveness -- as\nexemplified by the pseudo-relevance feedback approaches, which expand the query\nin order to alleviate the vocabulary mismatch problem. Recent advancements in\ngenerative language models have demonstrated their ability in generating\nresponses that are relevant to a given prompt. In light of this success, we\nseek to study the capacity of such models to perform query reformulation and\nhow they compare with long-standing query reformulation methods that use\npseudo-relevance feedback. In particular, we investigate two representative\nquery reformulation frameworks, GenQR and GenPRF. GenQR directly reformulates\nthe user's input query, while GenPRF provides additional context for the query\nby making use of pseudo-relevance feedback information. For each reformulation\nmethod, we leverage different techniques, including fine-tuning and direct\nprompting, to harness the knowledge of language models. The reformulated\nqueries produced by the generative models are demonstrated to markedly benefit\nthe effectiveness of a state-of-the-art retrieval pipeline on four TREC test\ncollections (varying from TREC 2004 Robust to the TREC 2019 Deep Learning).\nFurthermore, our results indicate that our studied generative models can\noutperform various statistical query expansion approaches while remaining\ncomparable to other existing complex neural query reformulation models, with\nthe added benefit of being simpler to implement.",
        "translated": ""
    },
    {
        "title": "Challenging the Myth of Graph Collaborative Filtering: a Reasoned and\n  Reproducibility-driven Analysis",
        "url": "http://arxiv.org/abs/2308.00404v1",
        "pub_date": "2023-08-01",
        "summary": "The success of graph neural network-based models (GNNs) has significantly\nadvanced recommender systems by effectively modeling users and items as a\nbipartite, undirected graph. However, many original graph-based works often\nadopt results from baseline papers without verifying their validity for the\nspecific configuration under analysis. Our work addresses this issue by\nfocusing on the replicability of results. We present a code that successfully\nreplicates results from six popular and recent graph recommendation models\n(NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark\ndatasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these\ngraph models with traditional collaborative filtering models that historically\nperformed well in offline evaluations. Furthermore, we extend our study to two\nnew datasets (Allrecipes and BookCrossing) that lack established setups in\nexisting literature. As the performance on these datasets differs from the\nprevious benchmarks, we analyze the impact of specific dataset characteristics\non recommendation accuracy. By investigating the information flow from users'\nneighborhoods, we aim to identify which models are influenced by intrinsic\nfeatures in the dataset structure. The code to reproduce our experiments is\navailable at: https://github.com/sisinflab/Graph-RSs-Reproducibility.",
        "translated": ""
    },
    {
        "title": "Masked and Swapped Sequence Modeling for Next Novel Basket\n  Recommendation in Grocery Shopping",
        "url": "http://arxiv.org/abs/2308.01308v1",
        "pub_date": "2023-08-02",
        "summary": "Next basket recommendation (NBR) is the task of predicting the next set of\nitems based on a sequence of already purchased baskets. It is a recommendation\ntask that has been widely studied, especially in the context of grocery\nshopping. In next basket recommendation (NBR), it is useful to distinguish\nbetween repeat items, i.e., items that a user has consumed before, and explore\nitems, i.e., items that a user has not consumed before. Most NBR work either\nignores this distinction or focuses on repeat items. We formulate the next\nnovel basket recommendation (NNBR) task, i.e., the task of recommending a\nbasket that only consists of novel items, which is valuable for both real-world\napplication and NBR evaluation. We evaluate how existing NBR methods perform on\nthe NNBR task and find that, so far, limited progress has been made w.r.t. the\nNNBR task. To address the NNBR task, we propose a simple bi-directional\ntransformer basket recommendation model (BTBR), which is focused on directly\nmodeling item-to-item correlations within and across baskets instead of\nlearning complex basket representations. To properly train BTBR, we propose and\ninvestigate several masking strategies and training objectives: (i) item-level\nrandom masking, (ii) item-level select masking, (iii) basket-level all masking,\n(iv) basket-level explore masking, and (v) joint masking. In addition, an\nitem-basket swapping strategy is proposed to enrich the item interactions\nwithin the same baskets. We conduct extensive experiments on three open\ndatasets with various characteristics. The results demonstrate the\neffectiveness of BTBR and our masking and swapping strategies for the NNBR\ntask. BTBR with a properly selected masking and swapping strategy can\nsubstantially improve NNBR performance.",
        "translated": ""
    },
    {
        "title": "A Survey on Popularity Bias in Recommender Systems",
        "url": "http://arxiv.org/abs/2308.01118v1",
        "pub_date": "2023-08-02",
        "summary": "Recommender systems help people find relevant content in a personalized way.\nOne main promise of such systems is that they are able to increase the\nvisibility of items in the long tail, i.e., the lesser-known items in a\ncatalogue. Existing research, however, suggests that in many situations today's\nrecommendation algorithms instead exhibit a popularity bias, meaning that they\noften focus on rather popular items in their recommendations. Such a bias may\nnot only lead to limited value of the recommendations for consumers and\nproviders in the short run, but it may also cause undesired reinforcement\neffects over time. In this paper, we discuss the potential reasons for\npopularity bias and we review existing approaches to detect, quantify and\nmitigate popularity bias in recommender systems. Our survey therefore includes\nboth an overview of the computational metrics used in the literature as well as\na review of the main technical approaches to reduce the bias. We furthermore\ncritically discuss today's literature, where we observe that the research is\nalmost entirely based on computational experiments and on certain assumptions\nregarding the practical effects of including long-tail items in the\nrecommendations.",
        "translated": ""
    },
    {
        "title": "Towards Better Query Classification with Multi-Expert Knowledge\n  Condensation in JD Ads Search",
        "url": "http://arxiv.org/abs/2308.01098v1",
        "pub_date": "2023-08-02",
        "summary": "Search query classification, as an effective way to understand user intents,\nis of great importance in real-world online ads systems. To ensure a lower\nlatency, a shallow model (e.g. FastText) is widely used for efficient online\ninference. However, the representation ability of the FastText model is\ninsufficient, resulting in poor classification performance, especially on some\nlow-frequency queries and tailed categories. Using a deeper and more complex\nmodel (e.g. BERT) is an effective solution, but it will cause a higher online\ninference latency and more expensive computing costs. Thus, how to juggle both\ninference efficiency and classification performance is obviously of great\npractical importance. To overcome this challenge, in this paper, we propose\nknowledge condensation (KC), a simple yet effective knowledge distillation\nframework to boost the classification performance of the online FastText model\nunder strict low latency constraints. Specifically, we propose to train an\noffline BERT model to retrieve more potentially relevant data. Benefiting from\nits powerful semantic representation, more relevant labels not exposed in the\nhistorical data will be added into the training set for better FastText model\ntraining. Moreover, a novel distribution-diverse multi-expert learning strategy\nis proposed to further improve the mining ability of relevant data. By training\nmultiple BERT models from different data distributions, it can respectively\nperform better at high, middle, and low-frequency search queries. The model\nensemble from multi-distribution makes its retrieval ability more powerful. We\nhave deployed two versions of this framework in JD search, and both offline\nexperiments and online A/B testing from multiple datasets have validated the\neffectiveness of the proposed approach.",
        "translated": ""
    },
    {
        "title": "Rethinking Similarity Search: Embracing Smarter Mechanisms over Smarter\n  Data",
        "url": "http://arxiv.org/abs/2308.00909v1",
        "pub_date": "2023-08-02",
        "summary": "In this vision paper, we propose a shift in perspective for improving the\neffectiveness of similarity search. Rather than focusing solely on enhancing\nthe data quality, particularly machine learning-generated embeddings, we\nadvocate for a more comprehensive approach that also enhances the underpinning\nsearch mechanisms. We highlight three novel avenues that call for a\nredefinition of the similarity search problem: exploiting implicit data\nstructures and distributions, engaging users in an iterative feedback loop, and\nmoving beyond a single query vector. These novel pathways have gained relevance\nin emerging applications such as large-scale language models, video clip\nretrieval, and data labeling. We discuss the corresponding research challenges\nposed by these new problem areas and share insights from our preliminary\ndiscoveries.",
        "translated": ""
    },
    {
        "title": "User-Controllable Recommendation via Counterfactual Retrospective and\n  Prospective Explanations",
        "url": "http://arxiv.org/abs/2308.00894v1",
        "pub_date": "2023-08-02",
        "summary": "Modern recommender systems utilize users' historical behaviors to generate\npersonalized recommendations. However, these systems often lack user\ncontrollability, leading to diminished user satisfaction and trust in the\nsystems. Acknowledging the recent advancements in explainable recommender\nsystems that enhance users' understanding of recommendation mechanisms, we\npropose leveraging these advancements to improve user controllability. In this\npaper, we present a user-controllable recommender system that seamlessly\nintegrates explainability and controllability within a unified framework. By\nproviding both retrospective and prospective explanations through\ncounterfactual reasoning, users can customize their control over the system by\ninteracting with these explanations.\n  Furthermore, we introduce and assess two attributes of controllability in\nrecommendation systems: the complexity of controllability and the accuracy of\ncontrollability. Experimental evaluations on MovieLens and Yelp datasets\nsubstantiate the effectiveness of our proposed framework. Additionally, our\nexperiments demonstrate that offering users control options can potentially\nenhance recommendation accuracy in the future. Source code and data are\navailable at \\url{https://github.com/chrisjtan/ucr}.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Contrastive BERT Fine-tuning for Fusion-based\n  Reviewed-Item Retrieval",
        "url": "http://arxiv.org/abs/2308.00762v1",
        "pub_date": "2023-08-01",
        "summary": "As natural language interfaces enable users to express increasingly complex\nnatural language queries, there is a parallel explosion of user review content\nthat can allow users to better find items such as restaurants, books, or movies\nthat match these expressive queries. While Neural Information Retrieval (IR)\nmethods have provided state-of-the-art results for matching queries to\ndocuments, they have not been extended to the task of Reviewed-Item Retrieval\n(RIR), where query-review scores must be aggregated (or fused) into item-level\nscores for ranking. In the absence of labeled RIR datasets, we extend Neural IR\nmethodology to RIR by leveraging self-supervised methods for contrastive\nlearning of BERT embeddings for both queries and reviews. Specifically,\ncontrastive learning requires a choice of positive and negative samples, where\nthe unique two-level structure of our item-review data combined with meta-data\naffords us a rich structure for the selection of these samples. For contrastive\nlearning in a Late Fusion scenario, we investigate the use of positive review\nsamples from the same item and/or with the same rating, selection of hard\npositive samples by choosing the least similar reviews from the same anchor\nitem, and selection of hard negative samples by choosing the most similar\nreviews from different items. We also explore anchor sub-sampling and\naugmenting with meta-data. For a more end-to-end Early Fusion approach, we\nintroduce contrastive item embedding learning to fuse reviews into single item\nembeddings. Experimental results show that Late Fusion contrastive learning for\nNeural RIR outperforms all other contrastive IR configurations, Neural IR, and\nsparse retrieval baselines, thus demonstrating the power of exploiting the\ntwo-level structure in Neural RIR approaches as well as the importance of\npreserving the nuance of individual review content via Late Fusion methods.",
        "translated": ""
    },
    {
        "title": "A Knowledge-Oriented Approach to Enhance Integration and Communicability\n  in the Polkadot Ecosystem",
        "url": "http://arxiv.org/abs/2308.00735v1",
        "pub_date": "2023-08-01",
        "summary": "The Polkadot ecosystem is a disruptive and highly complex multi-chain\narchitecture that poses challenges in terms of data analysis and\ncommunicability. Currently, there is a lack of standardized and holistic\napproaches to retrieve and analyze data across parachains and applications,\nmaking it difficult for general users and developers to access ecosystem data\nconsistently. This paper proposes a conceptual framework that includes a domain\nontology called POnto (a Polkadot Ontology) to address these challenges. POnto\nprovides a structured representation of the ecosystem's concepts and\nrelationships, enabling a formal understanding of the platform. The proposed\nknowledge-oriented approach enhances integration and communicability, enabling\na wider range of users to participate in the ecosystem and facilitating the\ndevelopment of AI-based applications. The paper presents a case study\nmethodology to validate the proposed framework, which includes expert feedback\nand insights from the Polkadot community. The POnto ontology and the roadmap\nfor a query engine based on a Controlled Natural Language using the ontology,\nprovide valuable contributions to the growth and adoption of the Polkadot\necosystem in heterogeneous socio-technical environments.",
        "translated": ""
    },
    {
        "title": "Adaptive Collaborative Filtering with Personalized Time Decay Functions\n  for Financial Product Recommendation",
        "url": "http://arxiv.org/abs/2308.01208v1",
        "pub_date": "2023-08-01",
        "summary": "Classical recommender systems often assume that historical data are\nstationary and fail to account for the dynamic nature of user preferences,\nlimiting their ability to provide reliable recommendations in time-sensitive\nsettings. This assumption is particularly problematic in finance, where\nfinancial products exhibit continuous changes in valuations, leading to\nfrequent shifts in client interests. These evolving interests, summarized in\nthe past client-product interactions, see their utility fade over time with a\ndegree that might differ from one client to another. To address this challenge,\nwe propose a time-dependent collaborative filtering algorithm that can\nadaptively discount distant client-product interactions using personalized\ndecay functions. Our approach is designed to handle the non-stationarity of\nfinancial data and produce reliable recommendations by modeling the dynamic\ncollaborative signals between clients and products. We evaluate our method\nusing a proprietary dataset from BNP Paribas and demonstrate significant\nimprovements over state-of-the-art benchmarks from relevant literature. Our\nfindings emphasize the importance of incorporating time explicitly in the model\nto enhance the accuracy of financial product recommendation.",
        "translated": ""
    },
    {
        "title": "MAP: A Model-agnostic Pretraining Framework for Click-through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2308.01737v1",
        "pub_date": "2023-08-03",
        "summary": "With the widespread application of personalized online services,\nclick-through rate (CTR) prediction has received more and more attention and\nresearch. The most prominent features of CTR prediction are its multi-field\ncategorical data format, and vast and daily-growing data volume. The large\ncapacity of neural models helps digest such massive amounts of data under the\nsupervised learning paradigm, yet they fail to utilize the substantial data to\nits full potential, since the 1-bit click signal is not sufficient to guide the\nmodel to learn capable representations of features and instances. The\nself-supervised learning paradigm provides a more promising pretrain-finetune\nsolution to better exploit the large amount of user click logs, and learn more\ngeneralized and effective representations. However, self-supervised learning\nfor CTR prediction is still an open question, since current works on this line\nare only preliminary and rudimentary. To this end, we propose a Model-agnostic\npretraining (MAP) framework that applies feature corruption and recovery on\nmulti-field categorical data, and more specifically, we derive two practical\nalgorithms: masked feature prediction (MFP) and replaced feature detection\n(RFD). MFP digs into feature interactions within each instance through masking\nand predicting a small portion of input features, and introduces noise\ncontrastive estimation (NCE) to handle large feature spaces. RFD further turns\nMFP into a binary classification mode through replacing and detecting changes\nin input features, making it even simpler and more effective for CTR\npretraining. Our extensive experiments on two real-world large-scale datasets\n(i.e., Avazu, Criteo) demonstrate the advantages of these two methods on\nseveral strong backbones (e.g., DCNv2, DeepFM), and achieve new\nstate-of-the-art performance in terms of both effectiveness and efficiency for\nCTR prediction.",
        "translated": ""
    },
    {
        "title": "Evaluating ChatGPT text-mining of clinical records for obesity\n  monitoring",
        "url": "http://arxiv.org/abs/2308.01666v1",
        "pub_date": "2023-08-03",
        "summary": "Background: Veterinary clinical narratives remain a largely untapped resource\nfor addressing complex diseases. Here we compare the ability of a large\nlanguage model (ChatGPT) and a previously developed regular expression (RegexT)\nto identify overweight body condition scores (BCS) in veterinary narratives.\nMethods: BCS values were extracted from 4,415 anonymised clinical narratives\nusing either RegexT or by appending the narrative to a prompt sent to ChatGPT\ncoercing the model to return the BCS information. Data were manually reviewed\nfor comparison. Results: The precision of RegexT was higher (100%, 95% CI\n94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall\nof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of\nRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is\nneeded to improve ChatGPT output. Conclusions: Large language models create\ndiverse opportunities and, whilst complex, present an intuitive interface to\ninformation but require careful implementation to avoid unpredictable errors.",
        "translated": ""
    },
    {
        "title": "Fast Slate Policy Optimization: Going Beyond Plackett-Luce",
        "url": "http://arxiv.org/abs/2308.01566v1",
        "pub_date": "2023-08-03",
        "summary": "An increasingly important building block of large scale machine learning\nsystems is based on returning slates; an ordered lists of items given a query.\nApplications of this technology include: search, information retrieval and\nrecommender systems. When the action space is large, decision systems are\nrestricted to a particular structure to complete online queries quickly. This\npaper addresses the optimization of these large scale decision systems given an\narbitrary reward function. We cast this learning problem in a policy\noptimization framework and propose a new class of policies, born from a novel\nrelaxation of decision functions. This results in a simple, yet efficient\nlearning algorithm that scales to massive action spaces. We compare our method\nto the commonly adopted Plackett-Luce policy class and demonstrate the\neffectiveness of our approach on problems with action space sizes in the order\nof millions.",
        "translated": ""
    },
    {
        "title": "Density Weighting for Multi-Interest Personalized Recommendation",
        "url": "http://arxiv.org/abs/2308.01563v1",
        "pub_date": "2023-08-03",
        "summary": "Using multiple user representations (MUR) to model user behavior instead of a\nsingle user representation (SUR) has been shown to improve personalization in\nrecommendation systems. However, the performance gains observed with MUR can be\nsensitive to the skewness in the item and/or user interest distribution. When\nthe data distribution is highly skewed, the gains observed by learning multiple\nrepresentations diminish since the model dominates on head items/interests,\nleading to poor performance on tail items. Robustness to data sparsity is\ntherefore essential for MUR-based approaches to achieve good performance for\nrecommendations. Yet, research in MUR and data imbalance have largely been done\nindependently. In this paper, we delve deeper into the shortcomings of MUR\ninferred from imbalanced data distributions. We make several contributions: (1)\nUsing synthetic datasets, we demonstrate the sensitivity of MUR with respect to\ndata imbalance, (2) To improve MUR for tail items, we propose an iterative\ndensity weighting scheme (IDW) with user tower calibration to mitigate the\neffect of training over long-tail distribution on personalization, and (3)\nThrough extensive experiments on three real-world benchmarks, we demonstrate\nIDW outperforms other alternatives that address data imbalance.",
        "translated": ""
    },
    {
        "title": "Adaptive Preferential Attached kNN Graph With Distribution-Awareness",
        "url": "http://arxiv.org/abs/2308.02442v1",
        "pub_date": "2023-08-04",
        "summary": "Graph-based kNN algorithms have garnered widespread popularity for machine\nlearning tasks, due to their simplicity and effectiveness. However, the\nconventional kNN graph's reliance on a fixed value of k can hinder its\nperformance, especially in scenarios involving complex data distributions.\nMoreover, like other classification models, the presence of ambiguous samples\nalong decision boundaries often presents a challenge, as they are more prone to\nincorrect classification. To address these issues, we propose the Preferential\nAttached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with\ndistribution-based graph construction. By incorporating distribution\ninformation, paNNG can significantly improve performance for ambiguous samples\nby \"pulling\" them towards their original classes and hence enable enhanced\noverall accuracy and generalization capability. Through rigorous evaluations on\ndiverse benchmark datasets, paNNG outperforms state-of-the-art algorithms,\nshowcasing its adaptability and efficacy across various real-world scenarios.",
        "translated": ""
    },
    {
        "title": "RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph\n  Classification",
        "url": "http://arxiv.org/abs/2308.02335v1",
        "pub_date": "2023-08-04",
        "summary": "Graph classification is a crucial task in many real-world multimedia\napplications, where graphs can represent various multimedia data types such as\nimages, videos, and social networks. Previous efforts have applied graph neural\nnetworks (GNNs) in balanced situations where the class distribution is\nbalanced. However, real-world data typically exhibit long-tailed class\ndistributions, resulting in a bias towards the head classes when using GNNs and\nlimited generalization ability over the tail classes. Recent approaches mainly\nfocus on re-balancing different classes during model training, which fails to\nexplicitly introduce new knowledge and sacrifices the performance of the head\nclasses. To address these drawbacks, we propose a novel framework called\nRetrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature\nextractor and an unbiased classifier in a decoupled manner. In the feature\nextractor training stage, we develop a graph retrieval module to search for\nrelevant graphs that directly enrich the intra-class diversity for the tail\nclasses. Moreover, we innovatively optimize a category-centered supervised\ncontrastive loss to obtain discriminative representations, which is more\nsuitable for long-tailed scenarios. In the classifier fine-tuning stage, we\nbalance the classifier weights with two weight regularization techniques, i.e.,\nMax-norm and weight decay. Experiments on various popular benchmarks verify the\nsuperiority of the proposed method against state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Learning to Select the Relevant History Turns in Conversational Question\n  Answering",
        "url": "http://arxiv.org/abs/2308.02294v1",
        "pub_date": "2023-08-04",
        "summary": "The increasing demand for the web-based digital assistants has given a rapid\nrise in the interest of the Information Retrieval (IR) community towards the\nfield of conversational question answering (ConvQA). However, one of the\ncritical aspects of ConvQA is the effective selection of conversational history\nturns to answer the question at hand. The dependency between relevant history\nselection and correct answer prediction is an intriguing but under-explored\narea. The selected relevant context can better guide the system so as to where\nexactly in the passage to look for an answer. Irrelevant context, on the other\nhand, brings noise to the system, thereby resulting in a decline in the model's\nperformance. In this paper, we propose a framework, DHS-ConvQA (Dynamic History\nSelection in Conversational Question Answering), that first generates the\ncontext and question entities for all the history turns, which are then pruned\non the basis of similarity they share in common with the question at hand. We\nalso propose an attention-based mechanism to re-rank the pruned terms based on\ntheir calculated weights of how useful they are in answering the question. In\nthe end, we further aid the model by highlighting the terms in the re-ranked\nconversational history using a binary classification task and keeping the\nuseful terms (predicted as 1) and ignoring the irrelevant terms (predicted as\n0). We demonstrate the efficacy of our proposed framework with extensive\nexperimental results on CANARD and QuAC -- the two popularly utilized datasets\nin ConvQA. We demonstrate that selecting relevant turns works better than\nrewriting the original question. We also investigate how adding the irrelevant\nhistory turns negatively impacts the model's performance and discuss the\nresearch challenges that demand more attention from the IR community.",
        "translated": ""
    },
    {
        "title": "Optimally Computing Compressed Indexing Arrays Based on the Compact\n  Directed Acyclic Word Graph",
        "url": "http://arxiv.org/abs/2308.02269v1",
        "pub_date": "2023-08-04",
        "summary": "In this paper, we present the first study of the computational complexity of\nconverting an automata-based text index structure, called the Compact Directed\nAcyclic Word Graph (CDAWG), of size $e$ for a text $T$ of length $n$ into other\ntext indexing structures for the same text, suitable for highly repetitive\ntexts: the run-length BWT of size $r$, the irreducible PLCP array of size $r$,\nand the quasi-irreducible LPF array of size $e$, as well as the lex-parse of\nsize $O(r)$ and the LZ77-parse of size $z$, where $r, z \\le e$. As main\nresults, we showed that the above structures can be optimally computed from\neither the CDAWG for $T$ stored in read-only memory or its self-index version\nof size $e$ without a text in $O(e)$ worst-case time and words of working\nspace. To obtain the above results, we devised techniques for enumerating a\nparticular subset of suffixes in the lexicographic and text orders using the\nforward and backward search on the CDAWG by extending the results by\nBelazzougui et al. in 2015.",
        "translated": ""
    },
    {
        "title": "Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song",
        "url": "http://arxiv.org/abs/2308.02249v1",
        "pub_date": "2023-08-04",
        "summary": "In this paper, we introduce a computational analysis of the field recording\ndataset of approximately 700 hours of Korean folk songs, which were recorded\naround 1980-90s. Because most of the songs were sung by non-expert musicians\nwithout accompaniment, the dataset provides several challenges. To address this\nchallenge, we utilized self-supervised learning with convolutional neural\nnetwork based on pitch contour, then analyzed how the musical concept of tori,\na classification system defined by a specific scale, ornamental notes, and an\nidiomatic melodic contour, is captured by the model. The experimental result\nshows that our approach can better capture the characteristics of tori compared\nto traditional pitch histograms. Using our approaches, we have examined how\nmusical discussions proposed in existing academia manifest in the actual field\nrecordings of Korean folk songs.",
        "translated": ""
    },
    {
        "title": "Towards Personalized Prompt-Model Retrieval for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.02205v1",
        "pub_date": "2023-08-04",
        "summary": "Recommender Systems are built to retrieve relevant items to satisfy users'\ninformation needs. The candidate corpus usually consists of a finite set of\nitems that are ready to be served, such as videos, products, or articles. With\nrecent advances in Generative AI such as GPT and Diffusion models, a new form\nof recommendation task is yet to be explored where items are to be created by\ngenerative models with personalized prompts. Taking image generation as an\nexample, with a single prompt from the user and access to a generative model,\nit is possible to generate hundreds of new images in a few minutes. How shall\nwe attain personalization in the presence of \"infinite\" items? In this\npreliminary study, we propose a two-stage framework, namely Prompt-Model\nRetrieval and Generated Item Ranking, to approach this new task formulation. We\nrelease GEMRec-18K, a prompt-model interaction dataset with 18K images\ngenerated by 200 publicly-available generative models paired with a diverse set\nof 90 textual prompts. Our findings demonstrate the promise of generative model\nrecommendation as a novel personalization problem and the limitations of\nexisting evaluation metrics. We highlight future directions for the RecSys\ncommunity to advance towards generative recommender systems. Our code and\ndataset are available at https://github.com/MAPS-research/GEMRec.",
        "translated": ""
    },
    {
        "title": "Incorporating Recklessness to Collaborative Filtering based Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2308.02058v1",
        "pub_date": "2023-08-03",
        "summary": "Recommender systems that include some reliability measure of their\npredictions tend to be more conservative in forecasting, due to their\nconstraint to preserve reliability. This leads to a significant drop in the\ncoverage and novelty that these systems can provide. In this paper, we propose\nthe inclusion of a new term in the learning process of matrix\nfactorization-based recommender systems, called recklessness, which enables the\ncontrol of the risk level desired when making decisions about the reliability\nof a prediction. Experimental results demonstrate that recklessness not only\nallows for risk regulation but also improves the quantity and quality of\npredictions provided by the recommender system.",
        "translated": ""
    },
    {
        "title": "Seasonality Based Reranking of E-commerce Autocomplete Using Natural\n  Language Queries",
        "url": "http://arxiv.org/abs/2308.02055v1",
        "pub_date": "2023-08-03",
        "summary": "Query autocomplete (QAC) also known as typeahead, suggests list of complete\nqueries as user types prefix in the search box. It is one of the key features\nof modern search engines specially in e-commerce. One of the goals of typeahead\nis to suggest relevant queries to users which are seasonally important. In this\npaper we propose a neural network based natural language processing (NLP)\nalgorithm to incorporate seasonality as a signal and present end to end\nevaluation of the QAC ranking model. Incorporating seasonality into\nautocomplete ranking model can improve autocomplete relevance and business\nmetric.",
        "translated": ""
    },
    {
        "title": "Domain specificity and data efficiency in typo tolerant spell checkers:\n  the case of search in online marketplaces",
        "url": "http://arxiv.org/abs/2308.01976v1",
        "pub_date": "2023-08-03",
        "summary": "Typographical errors are a major source of frustration for visitors of online\nmarketplaces. Because of the domain-specific nature of these marketplaces and\nthe very short queries users tend to search for, traditional spell cheking\nsolutions do not perform well in correcting typos. We present a data\naugmentation method to address the lack of annotated typo data and train a\nrecurrent neural network to learn context-limited domain-specific embeddings.\nThose embeddings are deployed in a real-time inferencing API for the Microsoft\nAppSource marketplace to find the closest match between a misspelled user query\nand the available product names. Our data efficient solution shows that\ncontrolled high quality synthetic data may be a powerful tool especially\nconsidering the current climate of large language models which rely on\nprohibitively huge and often uncontrolled datasets.",
        "translated": ""
    },
    {
        "title": "Randomized algorithms for precise measurement of differentially-private,\n  personalized recommendations",
        "url": "http://arxiv.org/abs/2308.03735v1",
        "pub_date": "2023-08-07",
        "summary": "Personalized recommendations form an important part of today's internet\necosystem, helping artists and creators to reach interested users, and helping\nusers to discover new and engaging content. However, many users today are\nskeptical of platforms that personalize recommendations, in part due to\nhistorically careless treatment of personal data and data privacy. Now,\nbusinesses that rely on personalized recommendations are entering a new\nparadigm, where many of their systems must be overhauled to be privacy-first.\nIn this article, we propose an algorithm for personalized recommendations that\nfacilitates both precise and differentially-private measurement. We consider\nadvertising as an example application, and conduct offline experiments to\nquantify how the proposed privacy-preserving algorithm affects key metrics\nrelated to user experience, advertiser value, and platform revenue compared to\nthe extremes of both (private) non-personalized and non-private, personalized\nimplementations.",
        "translated": ""
    },
    {
        "title": "Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity\n  Resolution",
        "url": "http://arxiv.org/abs/2308.03734v1",
        "pub_date": "2023-08-07",
        "summary": "The entity resolution problem requires finding pairs across datasets that\nbelong to different owners but refer to the same entity in the real world. To\ntrain and evaluate solutions (either rule-based or machine-learning-based) to\nthe entity resolution problem, generating a ground truth dataset with entity\npairs or clusters is needed. However, such a data annotation process involves\nhumans as domain oracles to review the plaintext data for all candidate record\npairs from different parties, which inevitably infringes the privacy of data\nowners, especially in privacy-sensitive cases like medical records. To the best\nof our knowledge, there is no prior work on privacy-preserving ground truth\ndataset generation, especially in the domain of entity resolution. We propose a\nnovel blind annotation protocol based on homomorphic encryption that allows\ndomain oracles to collaboratively label ground truths without sharing data in\nplaintext with other parties. In addition, we design a domain-specific\neasy-to-use language that hides the sophisticated underlying homomorphic\nencryption layer. Rigorous proof of the privacy guarantee is provided and our\nempirical experiments via an annotation simulator indicate the feasibility of\nour privacy-preserving protocol (f-measure on average achieves more than 90\\%\ncompared with the real ground truths).",
        "translated": ""
    },
    {
        "title": "Multi-View Graph Convolutional Network for Multimedia Recommendation",
        "url": "http://arxiv.org/abs/2308.03588v1",
        "pub_date": "2023-08-07",
        "summary": "Multimedia recommendation has received much attention in recent years. It\nmodels user preferences based on both behavior information and item multimodal\ninformation. Though current GCN-based methods achieve notable success, they\nsuffer from two limitations: (1) Modality noise contamination to the item\nrepresentations. Existing methods often mix modality features and behavior\nfeatures in a single view (e.g., user-item view) for propagation, the noise in\nthe modality features may be amplified and coupled with behavior features. In\nthe end, it leads to poor feature discriminability; (2) Incomplete user\npreference modeling caused by equal treatment of modality features. Users often\nexhibit distinct modality preferences when purchasing different items. Equally\nfusing each modality feature ignores the relative importance among different\nmodalities, leading to the suboptimal user preference modeling. To tackle the\nabove issues, we propose a novel Multi-View Graph Convolutional Network for the\nmultimedia recommendation. Specifically, to avoid modality noise contamination,\nthe modality features are first purified with the aid of item behavior\ninformation. Then, the purified modality features of items and behavior\nfeatures are enriched in separate views, including the user-item view and the\nitem-item view. In this way, the distinguishability of features is enhanced.\nMeanwhile, a behavior-aware fuser is designed to comprehensively model user\npreferences by adaptively learning the relative importance of different\nmodality features. Furthermore, we equip the fuser with a self-supervised\nauxiliary task. This task is expected to maximize the mutual information\nbetween the fused multimodal features and behavior features, so as to capture\ncomplementary and supplementary preference information simultaneously.\nExtensive experiments on three public datasets demonstrate the effectiveness of\nour methods.",
        "translated": ""
    },
    {
        "title": "TeraHAC: Hierarchical Agglomerative Clustering of Trillion-Edge Graphs",
        "url": "http://arxiv.org/abs/2308.03578v1",
        "pub_date": "2023-08-07",
        "summary": "We introduce TeraHAC, a $(1+\\epsilon)$-approximate hierarchical agglomerative\nclustering (HAC) algorithm which scales to trillion-edge graphs. Our algorithm\nis based on a new approach to computing $(1+\\epsilon)$-approximate HAC, which\nis a novel combination of the nearest-neighbor chain algorithm and the notion\nof $(1+\\epsilon)$-approximate HAC. Our approach allows us to partition the\ngraph among multiple machines and make significant progress in computing the\nclustering within each partition before any communication with other partitions\nis needed.\n  We evaluate TeraHAC on a number of real-world and synthetic graphs of up to 8\ntrillion edges. We show that TeraHAC requires over 100x fewer rounds compared\nto previously known approaches for computing HAC. It is up to 8.3x faster than\nSCC, the state-of-the-art distributed algorithm for hierarchical clustering,\nwhile achieving 1.16x higher quality. In fact, TeraHAC essentially retains the\nquality of the celebrated HAC algorithm while significantly improving the\nrunning time.",
        "translated": ""
    },
    {
        "title": "Global cognitive graph properties dynamics of hippocampal formation",
        "url": "http://arxiv.org/abs/2308.03563v1",
        "pub_date": "2023-08-07",
        "summary": "In the present study we have used a set of methods and metrics to build a\ngraph of relative neural connections in a hippocampus of a rodent. A set of\ngraphs was built on top of time-sequenced data and analyzed in terms of\ndynamics of a connection genesis. The analysis has shown that during the\nprocess of a rodent exploring a novel environment, the relations between\nneurons constantly change which indicates that globally memory is constantly\nupdated even for known areas of space. Even if some neurons gain cognitive\nspecialization, the global network though remains relatively stable.\nAdditionally we suggest a set of methods for building a graph of cognitive\nneural network.",
        "translated": ""
    },
    {
        "title": "Uncertainty-aware Consistency Learning for Cold-Start Item\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.03470v1",
        "pub_date": "2023-08-07",
        "summary": "Graph Neural Network (GNN)-based models have become the mainstream approach\nfor recommender systems. Despite the effectiveness, they are still suffering\nfrom the cold-start problem, i.e., recommend for few-interaction items.\nExisting GNN-based recommendation models to address the cold-start problem\nmainly focus on utilizing auxiliary features of users and items, leaving the\nuser-item interactions under-utilized. However, embeddings distributions of\ncold and warm items are still largely different, since cold items' embeddings\nare learned from lower-popularity interactions, while warm items' embeddings\nare from higher-popularity interactions. Thus, there is a seesaw phenomenon,\nwhere the recommendation performance for the cold and warm items cannot be\nimproved simultaneously. To this end, we proposed a Uncertainty-aware\nConsistency learning framework for Cold-start item recommendation (shorten as\nUCC) solely based on user-item interactions. Under this framework, we train the\nteacher model (generator) and student model (recommender) with consistency\nlearning, to ensure the cold items with additionally generated low-uncertainty\ninteractions can have similar distribution with the warm items. Therefore, the\nproposed framework improves the recommendation of cold and warm items at the\nsame time, without hurting any one of them. Extensive experiments on benchmark\ndatasets demonstrate that our proposed method significantly outperforms\nstate-of-the-art methods on both warm and cold items, with an average\nperformance improvement of 27.6%.",
        "translated": ""
    },
    {
        "title": "Doubly Robust Estimator for Off-Policy Evaluation with Large Action\n  Spaces",
        "url": "http://arxiv.org/abs/2308.03443v1",
        "pub_date": "2023-08-07",
        "summary": "We study Off-Policy Evaluation (OPE) in contextual bandit settings with large\naction spaces. The benchmark estimators suffer from severe bias and variance\ntradeoffs. Parametric approaches suffer from bias due to difficulty specifying\nthe correct model, whereas ones with importance weight suffer from variance. To\novercome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was\nproposed to mitigate the estimator's variance via embeddings of an action. To\nmake the estimator more accurate, we propose the doubly robust estimator of\nMIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical\nanalysis shows that the proposed estimator is unbiased under weaker assumptions\nthan MIPS while maintaining variance reduction against IPS, which was the main\nadvantage of MIPS. The empirical experiment verifies the supremacy of MDR\nagainst existing estimators.",
        "translated": ""
    },
    {
        "title": "Hierarchical Contrastive Learning with Multiple Augmentation for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.03400v1",
        "pub_date": "2023-08-07",
        "summary": "Sequential recommendation addresses the issue of preference drift by\npredicting the next item based on the user's previous behaviors. Recently, a\npromising approach using contrastive learning has emerged, demonstrating its\neffectiveness in recommending items under sparse user-item interactions.\nSignificantly, the effectiveness of combinations of various augmentation\nmethods has been demonstrated in different domains, particularly in computer\nvision. However, when it comes to augmentation within a contrastive learning\nframework in sequential recommendation, previous research has only focused on\nlimited conditions and simple structures. Thus, it is still possible to extend\nexisting approaches to boost the effects of augmentation methods by using\nprogressed structures with the combinations of multiple augmentation methods.\nIn this work, we propose a novel framework called Hierarchical Contrastive\nLearning with Multiple Augmentation for Sequential Recommendation(HCLRec) to\novercome the aforementioned limitation. Our framework leverages existing\naugmentation methods hierarchically to improve performance. By combining\naugmentation methods continuously, we generate low-level and high-level view\npairs. We employ a Transformers-based model to encode the input sequence\neffectively. Furthermore, we introduce additional blocks consisting of\nTransformers and position-wise feed-forward network(PFFN) layers to learn the\ninvariance of the original sequences from hierarchically augmented views. We\npass the input sequence to subsequent layers based on the number of increment\nlevels applied to the views to handle various augmentation levels. Within each\nlayer, we compute contrastive loss between pairs of views at the same level.\nExtensive experiments demonstrate that our proposed method outperforms\nstate-of-the-art approaches and that HCLRec is robust even when faced with the\nproblem of sparse interaction.",
        "translated": ""
    },
    {
        "title": "POSIT: Promotion of Semantic Item Tail via Adversarial Learning",
        "url": "http://arxiv.org/abs/2308.03366v1",
        "pub_date": "2023-08-07",
        "summary": "In many recommender problems, a handful of popular items (e.g. movies/TV\nshows, news etc.) can be dominant in recommendations for many users. However,\nwe know that in a large catalog of items, users are likely interested in more\nthan what is popular. The dominance of popular items may mean that users will\nnot see items they would likely enjoy. In this paper, we propose a technique to\novercome this problem using adversarial machine learning. We define a metric to\ntranslate user-level utility metric in terms of an advantage/disadvantage over\nitems. We subsequently use that metric in an adversarial learning framework to\nsystematically promote disadvantaged items. The resulting algorithm identifies\nsemantically meaningful items that get promoted in the learning algorithm. In\nthe empirical study, we evaluate the proposed technique on three publicly\navailable datasets and four competitive baselines. The result shows that our\nproposed method not only improves the coverage, but also, surprisingly,\nimproves the overall performance.",
        "translated": ""
    },
    {
        "title": "Heterogeneous Knowledge Fusion: A Novel Approach for Personalized\n  Recommendation via LLM",
        "url": "http://arxiv.org/abs/2308.03333v1",
        "pub_date": "2023-08-07",
        "summary": "The analysis and mining of user heterogeneous behavior are of paramount\nimportance in recommendation systems. However, the conventional approach of\nincorporating various types of heterogeneous behavior into recommendation\nmodels leads to feature sparsity and knowledge fragmentation issues. To address\nthis challenge, we propose a novel approach for personalized recommendation via\nLarge Language Model (LLM), by extracting and fusing heterogeneous knowledge\nfrom user heterogeneous behavior information. In addition, by combining\nheterogeneous knowledge and recommendation tasks, instruction tuning is\nperformed on LLM for personalized recommendations. The experimental results\ndemonstrate that our method can effectively integrate user heterogeneous\nbehavior and significantly improve recommendation performance.",
        "translated": ""
    },
    {
        "title": "Your Negative May not Be True Negative: Boosting Image-Text Matching\n  with False Negative Elimination",
        "url": "http://arxiv.org/abs/2308.04380v1",
        "pub_date": "2023-08-08",
        "summary": "Most existing image-text matching methods adopt triplet loss as the\noptimization objective, and choosing a proper negative sample for the triplet\nof &lt;anchor, positive, negative&gt; is important for effectively training the\nmodel, e.g., hard negatives make the model learn efficiently and effectively.\nHowever, we observe that existing methods mainly employ the most similar\nsamples as hard negatives, which may not be true negatives. In other words, the\nsamples with high similarity but not paired with the anchor may reserve\npositive semantic associations, and we call them false negatives. Repelling\nthese false negatives in triplet loss would mislead the semantic representation\nlearning and result in inferior retrieval performance. In this paper, we\npropose a novel False Negative Elimination (FNE) strategy to select negatives\nvia sampling, which could alleviate the problem introduced by false negatives.\nSpecifically, we first construct the distributions of positive and negative\nsamples separately via their similarities with the anchor, based on the\nfeatures extracted from image and text encoders. Then we calculate the false\nnegative probability of a given sample based on its similarity with the anchor\nand the above distributions via the Bayes' rule, which is employed as the\nsampling weight during negative sampling process. Since there may not exist any\nfalse negative in a small batch size, we design a memory module with momentum\nto retain a large negative buffer and implement our negative sampling strategy\nspanning over the buffer. In addition, to make the model focus on hard\nnegatives, we reassign the sampling weights for the simple negatives with a\ncut-down strategy. The extensive experiments are conducted on Flickr30K and\nMS-COCO, and the results demonstrate the superiority of our proposed false\nnegative elimination strategy. The code is available at\nhttps://github.com/LuminosityX/FNE.",
        "translated": ""
    },
    {
        "title": "Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval",
        "url": "http://arxiv.org/abs/2308.04343v1",
        "pub_date": "2023-08-08",
        "summary": "Most existing cross-modal retrieval methods employ two-stream encoders with\ndifferent architectures for images and texts, \\textit{e.g.}, CNN for images and\nRNN/Transformer for texts. Such discrepancy in architectures may induce\ndifferent semantic distribution spaces and limit the interactions between\nimages and texts, and further result in inferior alignment between images and\ntexts. To fill this research gap, inspired by recent advances of Transformers\nin vision tasks, we propose to unify the encoder architectures with\nTransformers for both modalities. Specifically, we design a cross-modal\nretrieval framework purely based on two-stream Transformers, dubbed\n\\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image\nTransformer, a text Transformer, and a hierarchical alignment module. With such\nidentical architectures, the encoders could produce representations with more\nsimilar characteristics for images and texts, and make the interactions and\nalignments between them much easier. Besides, to leverage the rich semantics,\nwe devise a hierarchical alignment scheme to explore multi-level\ncorrespondences of different layers between images and texts. To evaluate the\neffectiveness of the proposed HAT, we conduct extensive experiments on two\nbenchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that\nHAT outperforms SOTA baselines by a large margin. Specifically, on two key\ntasks, \\textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves\n7.6\\% and 16.7\\% relative score improvement of Recall@1 on MSCOCO, and 4.4\\%\nand 11.6\\% on Flickr30k respectively. The code is available at\n\\url{https://github.com/LuminosityX/HAT}.",
        "translated": ""
    },
    {
        "title": "Advancing Natural-Language Based Audio Retrieval with PaSST and Large\n  Audio-Caption Data Sets",
        "url": "http://arxiv.org/abs/2308.04258v1",
        "pub_date": "2023-08-08",
        "summary": "This work presents a text-to-audio-retrieval system based on pre-trained text\nand spectrogram transformers. Our method projects recordings and textual\ndescriptions into a shared audio-caption space in which related examples from\ndifferent modalities are close. Through a systematic analysis, we examine how\neach component of the system influences retrieval performance. As a result, we\nidentify two key components that play a crucial role in driving performance:\nthe self-attention-based audio encoder for audio embedding and the utilization\nof additional human-generated and synthetic data sets during pre-training. We\nfurther experimented with augmenting ClothoV2 captions with available keywords\nto increase their variety; however, this only led to marginal improvements. Our\nsystem ranked first in the 2023's DCASE Challenge, and it outperforms the\ncurrent state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.",
        "translated": ""
    },
    {
        "title": "UniRecSys: A Unified Framework for Personalized, Group, Package, and\n  Package-to-Group Recommendations",
        "url": "http://arxiv.org/abs/2308.04247v1",
        "pub_date": "2023-08-08",
        "summary": "Recommender systems aim to enhance the overall user experience by providing\ntailored recommendations for a variety of products and services. These systems\nhelp users make more informed decisions, leading to greater user satisfaction\nwith the platform. However, the implementation of these systems largely depends\non the context, which can vary from recommending an item or package to a user\nor a group. This requires careful exploration of several models during the\ndeployment, as there is no comprehensive and unified approach that deals with\nrecommendations at different levels. Furthermore, these individual models must\nbe closely attuned to their generated recommendations depending on the context\nto prevent significant variation in their generated recommendations. In this\npaper, we propose a novel unified recommendation framework that addresses all\nfour recommendation tasks, namely personalized, group, package, or\npackage-to-group recommendation, filling the gap in the current research\nlandscape. The proposed framework can be integrated with most of the\ntraditional matrix factorization-based collaborative filtering models. The idea\nis to enhance the formulation of the existing approaches by incorporating\ncomponents focusing on the exploitation of the group and package latent\nfactors. These components also help in exploiting a rich latent representation\nof the user/item by enforcing them to align closely with their corresponding\ngroup/package representation. We consider two prominent CF techniques,\nRegularized Matrix Factorization and Maximum Margin Matrix factorization, as\nthe baseline models and demonstrate their customization to various\nrecommendation tasks. Experiment results on two publicly available datasets are\nreported, comparing them to other baseline approaches that consider individual\nrating feedback for group or package recommendations.",
        "translated": ""
    },
    {
        "title": "OpinionConv: Conversational Product Search with Grounded Opinions",
        "url": "http://arxiv.org/abs/2308.04226v1",
        "pub_date": "2023-08-08",
        "summary": "When searching for products, the opinions of others play an important role in\nmaking informed decisions. Subjective experiences about a product can be a\nvaluable source of information. This is also true in sales conversations, where\na customer and a sales assistant exchange facts and opinions about products.\nHowever, training an AI for such conversations is complicated by the fact that\nlanguage models do not possess authentic opinions for their lack of real-world\nexperience. We address this problem by leveraging product reviews as a rich\nsource of product opinions to ground conversational AI in true subjective\nnarratives. With OpinionConv, we develop the first conversational AI for\nsimulating sales conversations. To validate the generated conversations, we\nconduct several user studies showing that the generated opinions are perceived\nas realistic. Our assessors also confirm the importance of opinions as an\ninformative basis for decision-making.",
        "translated": ""
    },
    {
        "title": "Understanding and Modeling Passive-Negative Feedback for Short-video\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.04086v1",
        "pub_date": "2023-08-08",
        "summary": "Sequential recommendation is one of the most important tasks in recommender\nsystems, which aims to recommend the next interacted item with historical\nbehaviors as input. Traditional sequential recommendation always mainly\nconsiders the collected positive feedback such as click, purchase, etc.\nHowever, in short-video platforms such as TikTok, video viewing behavior may\nnot always represent positive feedback. Specifically, the videos are played\nautomatically, and users passively receive the recommended videos. In this new\nscenario, users passively express negative feedback by skipping over videos\nthey do not like, which provides valuable information about their preferences.\nDifferent from the negative feedback studied in traditional recommender\nsystems, this passive-negative feedback can reflect users' interests and serve\nas an important supervision signal in extracting users' preferences. Therefore,\nit is essential to carefully design and utilize it in this novel recommendation\nscenario. In this work, we first conduct analyses based on a large-scale\nreal-world short-video behavior dataset and illustrate the significance of\nleveraging passive feedback. We then propose a novel method that deploys the\nsub-interest encoder, which incorporates positive feedback and passive-negative\nfeedback as supervision signals to learn the user's current active\nsub-interest. Moreover, we introduce an adaptive fusion layer to integrate\nvarious sub-interests effectively. To enhance the robustness of our model, we\nthen introduce a multi-task learning module to simultaneously optimize two\nkinds of feedback -- passive-negative feedback and traditional randomly-sampled\nnegative feedback. The experiments on two large-scale datasets verify that the\nproposed method can significantly outperform state-of-the-art approaches. The\ncode is released at https://github.com/tsinghua-fib-lab/RecSys2023-SINE.",
        "translated": ""
    },
    {
        "title": "Online Distillation-enhanced Multi-modal Transformer for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.04067v1",
        "pub_date": "2023-08-08",
        "summary": "Multi-modal recommendation systems, which integrate diverse types of\ninformation, have gained widespread attention in recent years. However,\ncompared to traditional collaborative filtering-based multi-modal\nrecommendation systems, research on multi-modal sequential recommendation is\nstill in its nascent stages. Unlike traditional sequential recommendation\nmodels that solely rely on item identifier (ID) information and focus on\nnetwork structure design, multi-modal recommendation models need to emphasize\nitem representation learning and the fusion of heterogeneous data sources. This\npaper investigates the impact of item representation learning on downstream\nrecommendation tasks and examines the disparities in information fusion at\ndifferent stages. Empirical experiments are conducted to demonstrate the need\nto design a framework suitable for collaborative learning and fusion of diverse\ninformation. Based on this, we propose a new model-agnostic framework for\nmulti-modal sequential recommendation tasks, called Online\nDistillation-enhanced Multi-modal Transformer (ODMT), to enhance feature\ninteraction and mutual learning among multi-source input (ID, text, and image),\nwhile avoiding conflicts among different features during training, thereby\nimproving recommendation accuracy. To be specific, we first introduce an\nID-aware Multi-modal Transformer module in the item representation learning\nstage to facilitate information interaction among different features. Secondly,\nwe employ an online distillation training strategy in the prediction\noptimization stage to make multi-source data learn from each other and improve\nprediction robustness. Experimental results on a video content recommendation\ndataset and three e-commerce recommendation datasets demonstrate the\neffectiveness of the proposed two modules, which is approximately 10%\nimprovement in performance compared to baseline models.",
        "translated": ""
    },
    {
        "title": "Adapting Foundation Models for Information Synthesis of Wireless\n  Communication Specifications",
        "url": "http://arxiv.org/abs/2308.04033v1",
        "pub_date": "2023-08-08",
        "summary": "Existing approaches to understanding, developing and researching modern\nwireless communication technologies involves time-intensive and arduous process\nof sifting through numerous webpages and technical specification documents,\ngathering the required information and synthesizing it. This paper presents\nNextGen Communications Copilot, a conversational artificial intelligence tool\nfor information synthesis of wireless communication specifications. The system\nbuilds on top of recent advancements in foundation models and consists of three\nkey additional components: a domain-specific database, a context extractor, and\na feedback mechanism. The system appends user queries with concise and\nquery-dependent contextual information extracted from a database of wireless\ntechnical specifications and incorporates tools for expert feedback and data\ncontributions. On evaluation using a benchmark dataset of queries and reference\nresponses created by subject matter experts, the system demonstrated more\nrelevant and accurate answers with an average BLEU score and BERTScore\nF1-measure of 0.37 and 0.79 respectively compared to the corresponding values\nof 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.",
        "translated": ""
    },
    {
        "title": "Top K Relevant Passage Retrieval for Biomedical Question Answering",
        "url": "http://arxiv.org/abs/2308.04028v1",
        "pub_date": "2023-08-08",
        "summary": "Question answering is a task that answers factoid questions using a large\ncollection of documents. It aims to provide precise answers in response to the\nuser's questions in natural language. Question answering relies on efficient\npassage retrieval to select candidate contexts, where traditional sparse vector\nspace models, such as TF-IDF or BM25, are the de facto method. On the web,\nthere is no single article that could provide all the possible answers\navailable on the internet to the question of the problem asked by the user. The\nexisting Dense Passage Retrieval model has been trained on Wikipedia dump from\nDec. 20, 2018, as the source documents for answering questions. Question\nanswering (QA) has made big strides with several open-domain and machine\ncomprehension systems built using large-scale annotated datasets. However, in\nthe clinical domain, this problem remains relatively unexplored. According to\nmultiple surveys, Biomedical Questions cannot be answered correctly from\nWikipedia Articles. In this work, we work on the existing DPR framework for the\nbiomedical domain and retrieve answers from the Pubmed articles which is a\nreliable source to answer medical questions. When evaluated on a BioASQ QA\ndataset, our fine-tuned dense retriever results in a 0.81 F1 score.",
        "translated": ""
    },
    {
        "title": "Exploring the Spatiotemporal Features of Online Food Recommendation\n  Service",
        "url": "http://arxiv.org/abs/2308.04019v1",
        "pub_date": "2023-08-08",
        "summary": "Online Food Recommendation Service (OFRS) has remarkable spatiotemporal\ncharacteristics and the advantage of being able to conveniently satisfy users'\nneeds in a timely manner. There have been a variety of studies that have begun\nto explore its spatiotemporal properties, but a comprehensive and in-depth\nanalysis of the OFRS spatiotemporal features is yet to be conducted. Therefore,\nthis paper studies the OFRS based on three questions: how spatiotemporal\nfeatures play a role; why self-attention cannot be used to model the\nspatiotemporal sequences of OFRS; and how to combine spatiotemporal features to\nimprove the efficiency of OFRS. Firstly, through experimental analysis, we\nsystemically extracted the spatiotemporal features of OFRS, identified the most\nvaluable features and designed an effective combination method. Secondly, we\nconducted a detailed analysis of the spatiotemporal sequences, which revealed\nthe shortcomings of self-attention in OFRS, and proposed a more optimized\nspatiotemporal sequence method for replacing self-attention. In addition, we\nalso designed a Dynamic Context Adaptation Model to further improve the\nefficiency and performance of OFRS. Through the offline experiments on two\nlarge datasets and online experiments for a week, the feasibility and\nsuperiority of our model were proven.",
        "translated": ""
    },
    {
        "title": "Dual Intents Graph Modeling for User-centric Group Discovery",
        "url": "http://arxiv.org/abs/2308.05013v1",
        "pub_date": "2023-08-09",
        "summary": "Online groups have become increasingly prevalent, providing users with space\nto share experiences and explore interests. Therefore, user-centric group\ndiscovery task, i.e., recommending groups to users can help both users' online\nexperiences and platforms' long-term developments. Existing recommender methods\ncan not deal with this task as modeling user-group participation into a\nbipartite graph overlooks their item-side interests. Although there exist a few\nworks attempting to address this task, they still fall short in fully\npreserving the social context and ensuring effective interest representation\nlearning.\n  In this paper, we focus on exploring the intents that motivate users to\nparticipate in groups, which can be categorized into different types, like the\nsocial-intent and the personal interest-intent. The former refers to users\njoining a group affected by their social links, while the latter relates to\nusers joining groups with like-minded people for self-enjoyment. To comprehend\ndifferent intents, we propose a novel model, DiRec, that first models each\nintent separately and then fuses them together for predictions. Specifically,\nfor social-intent, we introduce the hypergraph structure to model the\nrelationship between groups and members, leading to a richer understanding of\nthe social context. As for interest-intent, we employ novel structural\nrefinement on the interactive graph to uncover more intricate user behaviors\nand group interests, realizing better representation learning of interests.\nFurthermore, we also observe the intent overlapping in real-world scenarios and\ndevise a novel self-supervised learning loss that encourages such alignment for\nfinal recommendations. Extensive experiments on three public datasets show the\nsignificant improvement of DiRec over the state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction\n  Following",
        "url": "http://arxiv.org/abs/2308.04913v1",
        "pub_date": "2023-08-09",
        "summary": "E-commerce authoring involves creating attractive, abundant, and targeted\npromotional content to drive product sales. The emergence of large language\nmodels (LLMs) introduces an innovative paradigm, offering a unified solution to\naddress various authoring tasks within this scenario. However, mainstream LLMs\ntrained on general corpora with common sense knowledge reveal limitations in\nfitting complex and personalized features unique to e-commerce products and\ncustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,\nraising concerns about safeguarding voluminous customer privacy data during\ntransmission. This paper proposes the LLaMA-E, the unified and customized\ninstruction-following language models focusing on diverse e-commerce authoring\ntasks. Specifically, the domain experts create the seed instruction set from\nthe tasks of ads generation, query-enhanced product title rewriting, product\nclassification, purchase intent speculation, and general Q&amp;A. These tasks\nenable the models to comprehensively understand precise e-commerce authoring\nknowledge by interleaving features covering typical service aspects of\ncustomers, sellers, and platforms. The GPT-3.5 is introduced as a teacher\nmodel, which expands the seed instructions to form a training set for the\nLLaMA-E models with various scales. The experimental results show that the\nproposed LLaMA-E models achieve state-of-the-art results in quantitative and\nqualitative evaluations, also exhibiting the advantage in zero-shot scenes. To\nthe best of our knowledge, this study is the first to serve the LLMs to\nspecific e-commerce authoring scenarios.",
        "translated": ""
    },
    {
        "title": "Parallel Knowledge Enhancement based Framework for Multi-behavior\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.04807v1",
        "pub_date": "2023-08-09",
        "summary": "Multi-behavior recommendation algorithms aim to leverage the multiplex\ninteractions between users and items to learn users' latent preferences. Recent\nmulti-behavior recommendation frameworks contain two steps: fusion and\nprediction. In the fusion step, advanced neural networks are used to model the\nhierarchical correlations between user behaviors. In the prediction step,\nmultiple signals are utilized to jointly optimize the model with a multi-task\nlearning (MTL) paradigm. However, recent approaches have not addressed the\nissue caused by imbalanced data distribution in the fusion step, resulting in\nthe learned relationships being dominated by high-frequency behaviors. In the\nprediction step, the existing methods use a gate mechanism to directly\naggregate expert information generated by coupling input, leading to negative\ninformation transfer. To tackle these issues, we propose a Parallel Knowledge\nEnhancement Framework (PKEF) for multi-behavior recommendation. Specifically,\nwe enhance the hierarchical information propagation in the fusion step using\nparallel knowledge (PKF). Meanwhile, in the prediction step, we decouple the\nrepresentations to generate expert information and introduce a projection\nmechanism during aggregation to eliminate gradient conflicts and alleviate\nnegative transfer (PME). We conduct comprehensive experiments on three\nreal-world datasets to validate the effectiveness of our model. The results\nfurther demonstrate the rationality and effectiveness of the designed PKF and\nPME modules. The source code and datasets are available at\nhttps://github.com/MC-CV/PKEF.",
        "translated": ""
    },
    {
        "title": "DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels\n  from User Comments for Music",
        "url": "http://arxiv.org/abs/2308.04805v1",
        "pub_date": "2023-08-09",
        "summary": "Towards sufficient music searching, it is vital to form a complete set of\nlabels for each song. However, current solutions fail to resolve it as they\ncannot produce diverse enough mappings to make up for the information missed by\nthe gold labels. Based on the observation that such missing information may\nalready be presented in user comments, we propose to study the automated music\nlabeling in an essential but under-explored setting, where the model is\nrequired to harvest more diverse and valid labels from the users' comments\ngiven limited gold labels. To this end, we design an iterative framework (DiVa)\nto harvest more $\\underline{\\text{Di}}$verse and $\\underline{\\text{Va}}$lid\nlabels from user comments for music. The framework makes a classifier able to\nform complete sets of labels for songs via pseudo-labels inferred from\npre-trained classifiers and a novel joint score function. The experiment on a\ndensely annotated testing set reveals the superiority of the Diva over\nstate-of-the-art solutions in producing more diverse labels missed by the gold\nlabels. We hope our work can inspire future research on automated music\nlabeling.",
        "translated": ""
    },
    {
        "title": "Entire Space Cascade Delayed Feedback Modeling for Effective Conversion\n  Rate Prediction",
        "url": "http://arxiv.org/abs/2308.04768v1",
        "pub_date": "2023-08-09",
        "summary": "Conversion rate (CVR) prediction is an essential task for large-scale\ne-commerce platforms. However, refund behaviors frequently occur after\nconversion in online shopping systems, which drives us to pay attention to\neffective conversion for building healthier shopping services. This paper\ndefines the probability of item purchasing without any subsequent refund as an\neffective conversion rate (ECVR). A simple paradigm for ECVR prediction is to\ndecompose it into two sub-tasks: CVR prediction and post-conversion refund rate\n(RFR) prediction. However, RFR prediction suffers from data sparsity (DS) and\nsample selection bias (SSB) issues, as the refund behaviors are only available\nafter user purchase. Furthermore, there is delayed feedback in both conversion\nand refund events and they are sequentially dependent, named cascade delayed\nfeedback (CDF), which significantly harms data freshness for model training.\nPrevious studies mainly focus on tackling DS and SSB or delayed feedback for a\nsingle event. To jointly tackle these issues in ECVR prediction, we propose an\nEntire space CAscade Delayed feedback modeling (ECAD) method. Specifically,\nECAD deals with DS and SSB by constructing two tasks including CVR prediction\nand conversion \\&amp; refund rate (CVRFR) prediction using the entire space\nmodeling framework. In addition, it carefully schedules auxiliary tasks to\nleverage both conversion and refund time within data to alleviate CDF.\nExperimental results on the offline industrial dataset and online A/B testing\ndemonstrate the effectiveness of ECAD. In addition, ECAD has been deployed in\none of the recommender systems in Alibaba, contributing to a significant\nimprovement of ECVR.",
        "translated": ""
    },
    {
        "title": "Building Interpretable and Reliable Open Information Retriever for New\n  Domains Overnight",
        "url": "http://arxiv.org/abs/2308.04756v1",
        "pub_date": "2023-08-09",
        "summary": "Information retrieval (IR) or knowledge retrieval, is a critical component\nfor many down-stream tasks such as open-domain question answering (QA). It is\nalso very challenging, as it requires succinctness, completeness, and\ncorrectness. In recent works, dense retrieval models have achieved\nstate-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by\nrepresenting queries and knowledge passages with dense vectors and learning the\nlexical and semantic similarity. However, using single dense vectors and\nend-to-end supervision are not always optimal because queries may require\nattention to multiple aspects and event implicit knowledge. In this work, we\npropose an information retrieval pipeline that uses entity/event linking model\nand query decomposition model to focus more accurately on different information\nunits of the query. We show that, while being more interpretable and reliable,\nour proposed pipeline significantly improves passage coverages and denotation\naccuracies across five IR and QA benchmarks. It will be the go-to system to use\nfor applications that need to perform IR on a new domain without much dedicated\neffort, because of its superior interpretability and cross-domain performance.",
        "translated": ""
    },
    {
        "title": "Self-supervised Learning of Rotation-invariant 3D Point Set Features\n  using Transformer and its Self-distillation",
        "url": "http://arxiv.org/abs/2308.04725v1",
        "pub_date": "2023-08-09",
        "summary": "Invariance against rotations of 3D objects is an important property in\nanalyzing 3D point set data. Conventional 3D point set DNNs having rotation\ninvariance typically obtain accurate 3D shape features via supervised learning\nby using labeled 3D point sets as training samples. However, due to the rapid\nincrease in 3D point set data and the high cost of labeling, a framework to\nlearn rotation-invariant 3D shape features from numerous unlabeled 3D point\nsets is required. This paper proposes a novel self-supervised learning\nframework for acquiring accurate and rotation-invariant 3D point set features\nat object-level. Our proposed lightweight DNN architecture decomposes an input\n3D point set into multiple global-scale regions, called tokens, that preserve\nthe spatial layout of partial shapes composing the 3D object. We employ a\nself-attention mechanism to refine the tokens and aggregate them into an\nexpressive rotation-invariant feature per 3D point set. Our DNN is effectively\ntrained by using pseudo-labels generated by a self-distillation framework. To\nfacilitate the learning of accurate features, we propose to combine multi-crop\nand cut-mix data augmentation techniques to diversify 3D point sets for\ntraining. Through a comprehensive evaluation, we empirically demonstrate that,\n(1) existing rotation-invariant DNN architectures designed for supervised\nlearning do not necessarily learn accurate 3D shape features under a\nself-supervised learning scenario, and (2) our proposed algorithm learns\nrotation-invariant 3D point set features that are more accurate than those\nlearned by existing algorithms. Code will be available at\nhttps://github.com/takahikof/RIPT_SDMM",
        "translated": ""
    },
    {
        "title": "Pareto Invariant Representation Learning for Multimedia Recommendation",
        "url": "http://arxiv.org/abs/2308.04706v1",
        "pub_date": "2023-08-09",
        "summary": "Multimedia recommendation involves personalized ranking tasks, where\nmultimedia content is usually represented using a generic encoder. However,\nthese generic representations introduce spurious correlations that fail to\nreveal users' true preferences. Existing works attempt to alleviate this\nproblem by learning invariant representations, but overlook the balance between\nindependent and identically distributed (IID) and out-of-distribution (OOD)\ngeneralization. In this paper, we propose a framework called Pareto Invariant\nRepresentation Learning (PaInvRL) to mitigate the impact of spurious\ncorrelations from an IID-OOD multi-objective optimization perspective, by\nlearning invariant representations (intrinsic factors that attract user\nattention) and variant representations (other factors) simultaneously.\nSpecifically, PaInvRL includes three iteratively executed modules: (i)\nheterogeneous identification module, which identifies the heterogeneous\nenvironments to reflect distributional shifts for user-item interactions; (ii)\ninvariant mask generation module, which learns invariant masks based on the\nPareto-optimal solutions that minimize the adaptive weighted Invariant Risk\nMinimization (IRM) and Empirical Risk (ERM) losses; (iii) convert module, which\ngenerates both variant representations and item-invariant representations for\ntraining a multi-modal recommendation model that mitigates spurious\ncorrelations and balances the generalization performance within and cross the\nenvironmental distributions. We compare the proposed PaInvRL with\nstate-of-the-art recommendation models on three public multimedia\nrecommendation datasets (Movielens, Tiktok, and Kwai), and the experimental\nresults validate the effectiveness of PaInvRL for both within- and\ncross-environmental learning.",
        "translated": ""
    },
    {
        "title": "Evaluating and Optimizing the Effectiveness of Neural Machine\n  Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark",
        "url": "http://arxiv.org/abs/2308.04693v1",
        "pub_date": "2023-08-09",
        "summary": "Neural Machine Translation (NMT) is widely applied in software engineering\ntasks. The effectiveness of NMT for code retrieval relies on the ability to\nlearn from the sequence of tokens in the source language to the sequence of\ntokens in the target language. While NMT performs well in pseudocode-to-code\ntranslation, it might have challenges in learning to translate from natural\nlanguage query to source code in newly curated real-world code documentation/\nimplementation datasets. In this work, we analyze the performance of NMT in\nnatural language-to-code translation in the newly curated CAT benchmark that\nincludes the optimized versions of three Java datasets TLCodeSum,\nCodeSearchNet, Funcom, and a Python dataset PCSD. Our evaluation shows that NMT\nhas low accuracy, measured by CrystalBLEU and Meteor metrics in this task. To\nalleviate the duty of NMT in learning complex representation of source code, we\npropose ASTTrans Representation, a tailored representation of an Abstract\nSyntax Tree (AST) using a subset of non-terminal nodes. We show that the\nclassical approach NMT performs significantly better in learning ASTTrans\nRepresentation over code tokens with up to 36% improvement on Meteor score.\nMoreover, we leverage ASTTrans Representation to conduct combined code search\nprocesses from the state-of-the-art code search processes using GraphCodeBERT\nand UniXcoder. Our NMT models of learning ASTTrans Representation can boost the\nMean Reciprocal Rank of these state-of-the-art code search processes by up to\n3.08% and improve 23.08% of queries' results over the CAT benchmark.",
        "translated": ""
    },
    {
        "title": "web crawler strategies for web pages under robot.txt restriction",
        "url": "http://arxiv.org/abs/2308.04689v1",
        "pub_date": "2023-08-09",
        "summary": "In the present time, all know about World Wide Web and work over the Internet\ndaily. In this paper, we introduce the search engines working for keywords that\nare entered by users to find something. The search engine uses different search\nalgorithms for convenient results for providing to the net surfer. Net surfers\ngo with the top search results but how did the results of web pages get higher\nranks over search engines? how the search engine got that all the web pages in\nthe database? This paper gives the answers to all these kinds of basic\nquestions. Web crawlers working for search engines and robot exclusion protocol\nrules for web crawlers are also addressed in this research paper. Webmaster\nuses different restriction facts in robot.txt file to instruct web crawler,\nsome basic formats of robot.txt are also mentioned in this paper.",
        "translated": ""
    },
    {
        "title": "SSLRec: A Self-Supervised Learning Library for Recommendation",
        "url": "http://arxiv.org/abs/2308.05697v1",
        "pub_date": "2023-08-10",
        "summary": "Self-supervised learning (SSL) has gained significant interest in recent\nyears as a solution to address the challenges posed by sparse and noisy data in\nrecommender systems. Despite the growing number of SSL algorithms designed to\nprovide state-of-the-art performance in various recommendation scenarios (e.g.,\ngraph collaborative filtering, sequential recommendation, social\nrecommendation, KG-enhanced recommendation), there is still a lack of unified\nframeworks that integrate recommendation algorithms across different domains.\nSuch a framework could serve as the cornerstone for self-supervised\nrecommendation algorithms, unifying the validation of existing methods and\ndriving the design of new ones. To address this gap, we introduce SSLRec, a\nnovel benchmark platform that provides a standardized, flexible, and\ncomprehensive framework for evaluating various SSL-enhanced recommenders. The\nSSLRec library features a modular architecture that allows users to easily\nevaluate state-of-the-art models and a complete set of data augmentation and\nself-supervised toolkits to help create SSL recommendation models with specific\nneeds. Furthermore, SSLRec simplifies the process of training and evaluating\ndifferent recommendation models with consistent and fair settings. Our SSLRec\nplatform covers a comprehensive set of state-of-the-art SSL-enhanced\nrecommendation models across different scenarios, enabling researchers to\nevaluate these cutting-edge models and drive further innovation in the field.\nOur implemented SSLRec framework is available at the source code repository\nhttps://github.com/HKUDS/SSLRec.",
        "translated": ""
    },
    {
        "title": "Finding Already Debunked Narratives via Multistage Retrieval: Enabling\n  Cross-Lingual, Cross-Dataset and Zero-Shot Learning",
        "url": "http://arxiv.org/abs/2308.05680v1",
        "pub_date": "2023-08-10",
        "summary": "The task of retrieving already debunked narratives aims to detect stories\nthat have already been fact-checked. The successful detection of claims that\nhave already been debunked not only reduces the manual efforts of professional\nfact-checkers but can also contribute to slowing the spread of misinformation.\nMainly due to the lack of readily available data, this is an understudied\nproblem, particularly when considering the cross-lingual task, i.e. the\nretrieval of fact-checking articles in a language different from the language\nof the online post being checked. This paper fills this gap by (i) creating a\nnovel dataset to enable research on cross-lingual retrieval of already debunked\nnarratives, using tweets as queries to a database of fact-checking articles;\n(ii) presenting an extensive experiment to benchmark fine-tuned and\noff-the-shelf multilingual pre-trained Transformer models for this task; and\n(iii) proposing a novel multistage framework that divides this cross-lingual\ndebunk retrieval task into refinement and re-ranking stages. Results show that\nthe task of cross-lingual retrieval of already debunked narratives is\nchallenging and off-the-shelf Transformer models fail to outperform a strong\nlexical-based baseline (BM25). Nevertheless, our multistage retrieval framework\nis robust, outperforming BM25 in most scenarios and enabling cross-domain and\nzero-shot learning, without significantly harming the model's performance.",
        "translated": ""
    },
    {
        "title": "LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition",
        "url": "http://arxiv.org/abs/2308.05609v1",
        "pub_date": "2023-08-10",
        "summary": "Biomedical Natural Language Processing (NLP) tends to become cumbersome for\nmost researchers, frequently due to the amount and heterogeneity of text to be\nprocessed. To address this challenge, the industry is continuously developing\nhighly efficient tools and creating more flexible engineering solutions. This\nwork presents the integration between industry data engineering solutions for\nefficient data processing and academic systems developed for Named Entity\nRecognition (LasigeUnicage\\_NER) and Relation Extraction (BiOnt). Our design\nreflects an integration of those components with external knowledge in the form\nof additional training data from other datasets and biomedical ontologies. We\nused this pipeline in the 2022 LitCoin NLP Challenge, where our team\nLasigeUnicage was awarded the 7th Prize out of approximately 200 participating\nteams, reflecting a successful collaboration between the academia (LASIGE) and\nthe industry (Unicage). The software supporting this work is available at\n\\url{https://github.com/lasigeBioTM/Litcoin-Lasige_Unicage}.",
        "translated": ""
    },
    {
        "title": "Multi-domain Recommendation with Embedding Disentangling and Domain\n  Alignment",
        "url": "http://arxiv.org/abs/2308.05508v1",
        "pub_date": "2023-08-10",
        "summary": "Multi-domain recommendation (MDR) aims to provide recommendations for\ndifferent domains (e.g., types of products) with overlapping users/items and is\ncommon for platforms such as Amazon, Facebook, and LinkedIn that host multiple\nservices. Existing MDR models face two challenges: First, it is difficult to\ndisentangle knowledge that generalizes across domains (e.g., a user likes cheap\nitems) and knowledge specific to a single domain (e.g., a user likes blue\nclothing but not blue cars). Second, they have limited ability to transfer\nknowledge across domains with small overlaps. We propose a new MDR method named\nEDDA with two key components, i.e., embedding disentangling recommender and\ndomain alignment, to tackle the two challenges respectively. In particular, the\nembedding disentangling recommender separates both the model and embedding for\nthe inter-domain part and the intra-domain part, while most existing MDR\nmethods only focus on model-level disentangling. The domain alignment leverages\nrandom walks from graph processing to identify similar user/item pairs from\ndifferent domains and encourages similar user/item pairs to have similar\nembeddings, enhancing knowledge transfer. We compare EDDA with 12\nstate-of-the-art baselines on 3 real datasets. The results show that EDDA\nconsistently outperforms the baselines on all datasets and domains. All\ndatasets and codes are available at https://github.com/Stevenn9981/EDDA.",
        "translated": ""
    },
    {
        "title": "Bringing order into the realm of Transformer-based language models for\n  artificial intelligence and law",
        "url": "http://arxiv.org/abs/2308.05502v1",
        "pub_date": "2023-08-10",
        "summary": "Transformer-based language models (TLMs) have widely been recognized to be a\ncutting-edge technology for the successful development of deep-learning-based\nsolutions to problems and applications that require natural language processing\nand understanding. Like for other textual domains, TLMs have indeed pushed the\nstate-of-the-art of AI approaches for many tasks of interest in the legal\ndomain. Despite the first Transformer model being proposed about six years ago,\nthere has been a rapid progress of this technology at an unprecedented rate,\nwhereby BERT and related models represent a major reference, also in the legal\ndomain. This article provides the first systematic overview of TLM-based\nmethods for AI-driven problems and tasks in the legal sphere. A major goal is\nto highlight research advances in this field so as to understand, on the one\nhand, how the Transformers have contributed to the success of AI in supporting\nlegal processes, and on the other hand, what are the current limitations and\nopportunities for further research development.",
        "translated": ""
    },
    {
        "title": "Product Review Image Ranking for Fashion E-commerce",
        "url": "http://arxiv.org/abs/2308.05390v1",
        "pub_date": "2023-08-10",
        "summary": "In a fashion e-commerce platform where customers can't physically examine the\nproducts on their own, being able to see other customers' text and image\nreviews of the product is critical while making purchase decisions. Given the\nhigh reliance on these reviews, over the years we have observed customers\nproactively sharing their reviews. With an increase in the coverage of User\nGenerated Content (UGC), there has been a corresponding increase in the number\nof customer images. It is thus imperative to display the most relevant images\non top as it may influence users' online shopping choices and behavior. In this\npaper, we propose a simple yet effective training procedure for ranking\ncustomer images. We created a dataset consisting of Myntra (A Major Indian\nFashion e-commerce company) studio posts and highly engaged (upvotes/downvotes)\nUGC images as our starting point and used selected distortion techniques on the\nimages of the above dataset to bring their quality at par with those of bad UGC\nimages. We train our network to rank bad-quality images lower than high-quality\nones. Our proposed method outperforms the baseline models on two metrics,\nnamely correlation coefficient, and accuracy, by substantial margins.",
        "translated": ""
    },
    {
        "title": "Beyond Semantics: Learning a Behavior Augmented Relevance Model with\n  Self-supervised Learning",
        "url": "http://arxiv.org/abs/2308.05379v1",
        "pub_date": "2023-08-10",
        "summary": "Relevance modeling aims to locate desirable items for corresponding queries,\nwhich is crucial for search engines to ensure user experience. Although most\nconventional approaches address this problem by assessing the semantic\nsimilarity between the query and item, pure semantic matching is not\neverything. In reality, auxiliary query-item interactions extracted from user\nhistorical behavior data of the search log could provide hints to reveal users'\nsearch intents further. Drawing inspiration from this, we devise a novel\nBehavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that\nleverages neighbor queries of target item and neighbor items of target query to\ncomplement target query-item semantic matching. Specifically, our model builds\nmulti-level co-attention for distilling coarse-grained and fine-grained\nsemantic representations from both neighbor and target views. The model\nsubsequently employs neighbor-target self-supervised learning to improve the\naccuracy and robustness of BARL-ASe by strengthening representation and logit\nlearning. Furthermore, we discuss how to deal with the long-tail query-item\nmatching of the mini apps search scenario of Alipay practically. Experiments on\nreal-world industry data and online A/B testing demonstrate our proposal\nachieves promising performance with low latency.",
        "translated": ""
    },
    {
        "title": "Investigating disaster response through social media data and the\n  Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S.\n  wildfire season",
        "url": "http://arxiv.org/abs/2308.05281v1",
        "pub_date": "2023-08-10",
        "summary": "Effective disaster response is critical for affected communities. Responders\nand decision-makers would benefit from reliable, timely measures of the issues\nimpacting their communities during a disaster, and social media offers a\npotentially rich data source. Social media can reflect public concerns and\ndemands during a disaster, offering valuable insights for decision-makers to\nunderstand evolving situations and optimize resource allocation. We used\nBidirectional Encoder Representations from Transformers (BERT) topic modeling\nto cluster topics from Twitter data. Then, we conducted a temporal-spatial\nanalysis to examine the distribution of these topics across different regions\nduring the 2020 western U.S. wildfire season. Our results show that Twitter\nusers mainly focused on three topics:\"health impact,\" \"damage,\" and\n\"evacuation.\" We used the Susceptible-Infected-Recovered (SIR) theory to\nexplore the magnitude and velocity of topic diffusion on Twitter. The results\ndisplayed a clear relationship between topic trends and wildfire propagation\npatterns. The estimated parameters obtained from the SIR model in selected\ncities revealed that residents exhibited a high level of several concerns\nduring the wildfire. Our study details how the SIR model and topic modeling\nusing social media data can provide decision-makers with a quantitative\napproach to measure disaster response and support their decision-making\nprocesses.",
        "translated": ""
    },
    {
        "title": "A Large Language Model Enhanced Conversational Recommender System",
        "url": "http://arxiv.org/abs/2308.06212v1",
        "pub_date": "2023-08-11",
        "summary": "Conversational recommender systems (CRSs) aim to recommend high-quality items\nto users through a dialogue interface. It usually contains multiple sub-tasks,\nsuch as user preference elicitation, recommendation, explanation, and item\ninformation search. To develop effective CRSs, there are some challenges: 1)\nhow to properly manage sub-tasks; 2) how to effectively solve different\nsub-tasks; and 3) how to correctly generate responses that interact with users.\nRecently, Large Language Models (LLMs) have exhibited an unprecedented ability\nto reason and generate, presenting a new opportunity to develop more powerful\nCRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to\naddress the above challenges. For sub-task management, we leverage the\nreasoning ability of LLM to effectively manage sub-task. For sub-task solving,\nwe collaborate LLM with expert models of different sub-tasks to achieve the\nenhanced performance. For response generation, we utilize the generation\nability of LLM as a language interface to better interact with users.\nSpecifically, LLMCRS divides the workflow into four stages: sub-task detection,\nmodel matching, sub-task execution, and response generation. LLMCRS also\ndesigns schema-based instruction, demonstration-based instruction, dynamic\nsub-task and model matching, and summary-based generation to instruct LLM to\ngenerate desired results in the workflow. Finally, to adapt LLM to\nconversational recommendations, we also propose to fine-tune LLM with\nreinforcement learning from CRSs performance feedback, referred to as RLPF.\nExperimental results on benchmark datasets show that LLMCRS with RLPF\noutperforms the existing methods.",
        "translated": ""
    },
    {
        "title": "Identification of the Relevance of Comments in Codes Using Bag of Words\n  and Transformer Based Models",
        "url": "http://arxiv.org/abs/2308.06144v1",
        "pub_date": "2023-08-11",
        "summary": "The Forum for Information Retrieval (FIRE) started a shared task this year\nfor classification of comments of different code segments. This is binary text\nclassification task where the objective is to identify whether comments given\nfor certain code segments are relevant or not. The BioNLP-IISERB group at the\nIndian Institute of Science Education and Research Bhopal (IISERB) participated\nin this task and submitted five runs for five different models. The paper\npresents the overview of the models and other significant findings on the\ntraining corpus. The methods involve different feature engineering schemes and\ntext classification techniques. The performance of the classical bag of words\nmodel and transformer-based models were explored to identify significant\nfeatures from the given training corpus. We have explored different classifiers\nviz., random forest, support vector machine and logistic regression using the\nbag of words model. Furthermore, the pre-trained transformer based models like\nBERT, RoBERT and ALBERT were also used by fine-tuning them on the given\ntraining corpus. The performance of different such models over the training\ncorpus were reported and the best five models were implemented on the given\ntest corpus. The empirical results show that the bag of words model outperforms\nthe transformer based models, however, the performance of our runs are not\nreasonably well in both training and test corpus. This paper also addresses the\nlimitations of the models and scope for further improvement.",
        "translated": ""
    },
    {
        "title": "Toward a Better Understanding of Loss Functions for Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2308.06091v1",
        "pub_date": "2023-08-11",
        "summary": "Collaborative filtering (CF) is a pivotal technique in modern recommender\nsystems. The learning process of CF models typically consists of three\ncomponents: interaction encoder, loss function, and negative sampling. Although\nmany existing studies have proposed various CF models to design sophisticated\ninteraction encoders, recent work shows that simply reformulating the loss\nfunctions can achieve significant performance gains. This paper delves into\nanalyzing the relationship among existing loss functions. Our mathematical\nanalysis reveals that the previous loss functions can be interpreted as\nalignment and uniformity functions: (i) the alignment matches user and item\nrepresentations, and (ii) the uniformity disperses user and item distributions.\nInspired by this analysis, we propose a novel loss function that improves the\ndesign of alignment and uniformity considering the unique patterns of datasets\ncalled Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty\nof MAWU is two-fold: (i) margin-aware alignment (MA) mitigates\nuser/item-specific popularity biases, and (ii) weighted uniformity (WU) adjusts\nthe significance between user and item uniformities to reflect the inherent\ncharacteristics of datasets. Extensive experimental results show that MF and\nLightGCN equipped with MAWU are comparable or superior to state-of-the-art CF\nmodels with various loss functions on three public datasets.",
        "translated": ""
    },
    {
        "title": "Deep Context Interest Network for Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2308.06037v1",
        "pub_date": "2023-08-11",
        "summary": "Click-Through Rate (CTR) prediction, estimating the probability of a user\nclicking on an item, is essential in industrial applications, such as online\nadvertising. Many works focus on user behavior modeling to improve CTR\nprediction performance. However, most of those methods only model users'\npositive interests from users' click items while ignoring the context\ninformation, which is the display items around the clicks, resulting in\ninferior performance. In this paper, we highlight the importance of context\ninformation on user behavior modeling and propose a novel model named Deep\nContext Interest Network (DCIN), which integrally models the click and its\ndisplay context to learn users' context-aware interests. DCIN consists of three\nkey modules: 1) Position-aware Context Aggregation Module (PCAM), which\nperforms aggregation of display items with an attention mechanism; 2)\nFeedback-Context Fusion Module (FCFM), which fuses the representation of clicks\nand display contexts through non-linear feature interaction; 3) Interest\nMatching Module (IMM), which activates interests related with the target item.\nMoreover, we provide our hands-on solution to implement our DCIN model on\nlarge-scale industrial systems. The significant improvements in both offline\nand online evaluations demonstrate the superiority of our proposed DCIN method.\nNotably, DCIN has been deployed on our online advertising system serving the\nmain traffic, which brings 1.5% CTR and 1.5% RPM lift.",
        "translated": ""
    },
    {
        "title": "Designing a User Contextual Profile Ontology: A Focus on the Vehicle\n  Sales Domain",
        "url": "http://arxiv.org/abs/2308.06018v1",
        "pub_date": "2023-08-11",
        "summary": "In the digital age, it is crucial to understand and tailor experiences for\nusers interacting with systems and applications. This requires the creation of\nuser contextual profiles that combine user profiles with contextual\ninformation. However, there is a lack of research on the integration of\ncontextual information with different user profiles. This study aims to address\nthis gap by designing a user contextual profile ontology that considers both\nuser profiles and contextual information on each profile. Specifically, we\npresent a design and development of the user contextual profile ontology with a\nfocus on the vehicle sales domain. Our designed ontology serves as a structural\nfoundation for standardizing the representation of user profiles and contextual\ninformation, enhancing the system's ability to capture user preferences and\ncontextual information of the user accurately. Moreover, we illustrate a case\nstudy using the User Contextual Profile Ontology in generating personalized\nrecommendations for vehicle sales domain.",
        "translated": ""
    },
    {
        "title": "Augmented Negative Sampling for Collaborative Filtering",
        "url": "http://arxiv.org/abs/2308.05972v1",
        "pub_date": "2023-08-11",
        "summary": "Negative sampling is essential for implicit-feedback-based collaborative\nfiltering, which is used to constitute negative signals from massive unlabeled\ndata to guide supervised learning. The state-of-the-art idea is to utilize hard\nnegative samples that carry more useful information to form a better decision\nboundary. To balance efficiency and effectiveness, the vast majority of\nexisting methods follow the two-pass approach, in which the first pass samples\na fixed number of unobserved items by a simple static distribution and then the\nsecond pass selects the final negative items using a more sophisticated\nnegative sampling strategy. However, selecting negative samples from the\noriginal items is inherently restricted, and thus may not be able to contrast\npositive samples well. In this paper, we confirm this observation via\nexperiments and introduce two limitations of existing solutions: ambiguous trap\nand information discrimination. Our response to such limitations is to\nintroduce augmented negative samples. This direction renders a substantial\ntechnical challenge because constructing unconstrained negative samples may\nintroduce excessive noise that distorts the decision boundary. To this end, we\nintroduce a novel generic augmented negative sampling paradigm and provide a\nconcrete instantiation. First, we disentangle hard and easy factors of negative\nitems. Next, we generate new candidate negative samples by augmenting only the\neasy factors in a regulated manner: the direction and magnitude of the\naugmentation are carefully calibrated. Finally, we design an advanced negative\nsampling strategy to identify the final augmented negative samples, which\nconsiders not only the score function used in existing methods but also a new\nmetric called augmentation gain. Extensive experiments on real-world datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "LittleMu: Deploying an Online Virtual Teaching Assistant via\n  Heterogeneous Sources Integration and Chain of Teach Prompts",
        "url": "http://arxiv.org/abs/2308.05935v1",
        "pub_date": "2023-08-11",
        "summary": "Teaching assistants have played essential roles in the long history of\neducation. However, few MOOC platforms are providing human or virtual teaching\nassistants to support learning for massive online students due to the\ncomplexity of real-world online education scenarios and the lack of training\ndata. In this paper, we present a virtual MOOC teaching assistant, LittleMu\nwith minimum labeled training data, to provide question answering and chit-chat\nservices. Consisting of two interactive modules of heterogeneous retrieval and\nlanguage model prompting, LittleMu first integrates structural, semi- and\nunstructured knowledge sources to support accurate answers for a wide range of\nquestions. Then, we design delicate demonstrations named \"Chain of Teach\"\nprompts to exploit the large-scale pre-trained model to handle complex\nuncollected questions. Except for question answering, we develop other\neducational services such as knowledge-grounded chit-chat. We test the system's\nperformance via both offline evaluation and online deployment. Since May 2020,\nour LittleMu system has served over 80,000 users with over 300,000 queries from\nover 500 courses on XuetangX MOOC platform, which continuously contributes to a\nmore convenient and fair education. Our code, services, and dataset will be\navailable at https://github.com/THU-KEG/VTA.",
        "translated": ""
    },
    {
        "title": "LTP-MMF: Towards Long-term Provider Max-min Fairness Under\n  Recommendation Feedback Loops",
        "url": "http://arxiv.org/abs/2308.05902v1",
        "pub_date": "2023-08-11",
        "summary": "Multi-stakeholder recommender systems involve various roles, such as users,\nproviders. Previous work pointed out that max-min fairness (MMF) is a better\nmetric to support weak providers. However, when considering MMF, the features\nor parameters of these roles vary over time, how to ensure long-term provider\nMMF has become a significant challenge. We observed that recommendation\nfeedback loops (named RFL) will influence the provider MMF greatly in the long\nterm. RFL means that recommender system can only receive feedback on exposed\nitems from users and update recommender models incrementally based on this\nfeedback. When utilizing the feedback, the recommender model will regard\nunexposed item as negative. In this way, tail provider will not get the\nopportunity to be exposed, and its items will always be considered as negative\nsamples. Such phenomenons will become more and more serious in RFL. To\nalleviate the problem, this paper proposes an online ranking model named\nLong-Term Provider Max-min Fairness (named LTP-MMF). Theoretical analysis shows\nthat the long-term regret of LTP-MMF enjoys a sub-linear bound. Experimental\nresults on three public recommendation benchmarks demonstrated that LTP-MMF can\noutperform the baselines in the long term.",
        "translated": ""
    },
    {
        "title": "Cross-Attribute Matrix Factorization Model with Shared User Embedding",
        "url": "http://arxiv.org/abs/2308.07284v1",
        "pub_date": "2023-08-14",
        "summary": "Over the past few years, deep learning has firmly established its prowess\nacross various domains, including computer vision, speech recognition, and\nnatural language processing. Motivated by its outstanding success, researchers\nhave been directing their efforts towards applying deep learning techniques to\nrecommender systems. Neural collaborative filtering (NCF) and Neural Matrix\nFactorization (NeuMF) refreshes the traditional inner product in matrix\nfactorization with a neural architecture capable of learning complex and\ndata-driven functions. While these models effectively capture user-item\ninteractions, they overlook the specific attributes of both users and items.\nThis can lead to robustness issues, especially for items and users that belong\nto the \"long tail\". Such challenges are commonly recognized in recommender\nsystems as a part of the cold-start problem. A direct and intuitive approach to\naddress this issue is by leveraging the features and attributes of the items\nand users themselves. In this paper, we introduce a refined NeuMF model that\nconsiders not only the interaction between users and items, but also acrossing\nassociated attributes. Moreover, our proposed architecture features a shared\nuser embedding, seamlessly integrating with user embeddings to imporve the\nrobustness and effectively address the cold-start problem. Rigorous experiments\non both the Movielens and Pinterest datasets demonstrate the superiority of our\nCross-Attribute Matrix Factorization model, particularly in scenarios\ncharacterized by higher dataset sparsity.",
        "translated": ""
    },
    {
        "title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models",
        "url": "http://arxiv.org/abs/2308.07269v1",
        "pub_date": "2023-08-14",
        "summary": "Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to the outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners to apply knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub\nat https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and\ncomprehensive documentation for beginners to get started. Besides, we present\nan online system for real-time knowledge editing, and a demo video at\nhttp://knowlm.zjukg.cn/easyedit.mp4.",
        "translated": ""
    },
    {
        "title": "MM-GEF: Multi-modal representation meet collaborative filtering",
        "url": "http://arxiv.org/abs/2308.07222v1",
        "pub_date": "2023-08-14",
        "summary": "In modern e-commerce, item content features in various modalities offer\naccurate yet comprehensive information to recommender systems. The majority of\nprevious work either focuses on learning effective item representation during\nmodelling user-item interactions, or exploring item-item relationships by\nanalysing multi-modal features. Those methods, however, fail to incorporate the\ncollaborative item-user-item relationships into the multi-modal feature-based\nitem structure. In this work, we propose a graph-based item structure\nenhancement method MM-GEF: Multi-Modal recommendation with Graph Early-Fusion,\nwhich effectively combines the latent item structure underlying multi-modal\ncontents with the collaborative signals. Instead of processing the content\nfeature in different modalities separately, we show that the early-fusion of\nmulti-modal features provides significant improvement. MM-GEF learns refined\nitem representations by injecting structural information obtained from both\nmulti-modal and collaborative signals. Through extensive experiments on four\npublicly available datasets, we demonstrate systematical improvements of our\nmethod over state-of-the-art multi-modal recommendation methods.",
        "translated": ""
    },
    {
        "title": "gSASRec: Reducing Overconfidence in Sequential Recommendation Trained\n  with Negative Sampling",
        "url": "http://arxiv.org/abs/2308.07192v1",
        "pub_date": "2023-08-14",
        "summary": "A large catalogue size is one of the central challenges in training\nrecommendation models: a large number of items makes them memory and\ncomputationally inefficient to compute scores for all items during training,\nforcing these models to deploy negative sampling. However, negative sampling\nincreases the proportion of positive interactions in the training data, and\ntherefore models trained with negative sampling tend to overestimate the\nprobabilities of positive interactions a phenomenon we call overconfidence.\nWhile the absolute values of the predicted scores or probabilities are not\nimportant for the ranking of retrieved recommendations, overconfident models\nmay fail to estimate nuanced differences in the top-ranked items, resulting in\ndegraded performance. In this paper, we show that overconfidence explains why\nthe popular SASRec model underperforms when compared to BERT4Rec. This is\ncontrary to the BERT4Rec authors explanation that the difference in performance\nis due to the bi-directional attention mechanism. To mitigate overconfidence,\nwe propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) and\ntheoretically prove that it can mitigate overconfidence. We further propose the\ngSASRec model, an improvement over SASRec that deploys an increased number of\nnegatives and the gBCE loss. We show through detailed experiments on three\ndatasets that gSASRec does not exhibit the overconfidence problem. As a result,\ngSASRec can outperform BERT4Rec (e.g. +9.47% NDCG on the MovieLens-1M dataset),\nwhile requiring less training time (e.g. -73% training time on MovieLens-1M).\nMoreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets that\ncontain more than 1 million items.",
        "translated": ""
    },
    {
        "title": "Natural Language is All a Graph Needs",
        "url": "http://arxiv.org/abs/2308.07134v1",
        "pub_date": "2023-08-14",
        "summary": "The emergence of large-scale pre-trained language models, such as ChatGPT,\nhas revolutionized various research fields in artificial intelligence.\nTransformers-based large language models (LLMs) have gradually replaced CNNs\nand RNNs to unify fields of computer vision and natural language processing.\nCompared with the data that exists relatively independently such as images,\nvideos or texts, graph is a type of data that contains rich structural and\nrelational information. Meanwhile, natural language, as one of the most\nexpressive mediums, excels in describing complex structures. However, existing\nwork on incorporating graph learning problems into the generative language\nmodeling framework remains very limited. As the importance of language models\ncontinues to grow, it becomes essential to explore whether LLMs can also\nreplace GNNs as the foundational model for graphs. In this paper, we propose\nInstructGLM (Instruction-finetuned Graph Language Model), systematically design\nhighly scalable prompts based on natural language instructions, and use natural\nlanguage to describe the geometric structure and node features of the graph for\ninstruction tuning an LLMs to perform learning and inference on graphs in a\ngenerative manner. Our method exceeds all competitive GNN baselines on\nogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of\nour method and sheds light on generative language models replacing GNNs as the\nfoundation model for graph machine learning.",
        "translated": ""
    },
    {
        "title": "Large Language Models for Information Retrieval: A Survey",
        "url": "http://arxiv.org/abs/2308.07107v1",
        "pub_date": "2023-08-14",
        "summary": "As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions within\nthis expanding field.",
        "translated": ""
    },
    {
        "title": "UIPC-MF: User-Item Prototype Connection Matrix Factorization for\n  Explainable Collaborative Filtering",
        "url": "http://arxiv.org/abs/2308.07048v1",
        "pub_date": "2023-08-14",
        "summary": "Recommending items to potentially interested users has been an important\ncommercial task that faces two main challenges: accuracy and explainability.\nWhile most collaborative filtering models rely on statistical computations on a\nlarge scale of interaction data between users and items and can achieve high\nperformance, they often lack clear explanatory power. We propose UIPC-MF, a\nprototype-based matrix factorization method for explainable collaborative\nfiltering recommendations. In UIPC-MF, both users and items are associated with\nsets of prototypes, capturing general collaborative attributes. To enhance\nexplainability, UIPC-MF learns connection weights that reflect the associative\nrelations between user and item prototypes for recommendations. UIPC-MF\noutperforms other prototype-based baseline methods in terms of Hit Ratio and\nNormalized Discounted Cumulative Gain on three datasets, while also providing\nbetter transparency.",
        "translated": ""
    },
    {
        "title": "The Scientometrics and Reciprocality Underlying Co-Authorship Panels in\n  Google Scholar Profiles",
        "url": "http://arxiv.org/abs/2308.07001v1",
        "pub_date": "2023-08-14",
        "summary": "Online academic profiles are used by scholars to reflect a desired image to\ntheir online audience. In Google Scholar, scholars can select a subset of\nco-authors for presentation in a central location on their profile using a\nsocial feature called the Co-authroship panel. In this work, we examine whether\nscientometrics and reciprocality can explain the observed selections. To this\nend, we scrape and thoroughly analyze a novel set of 120,000 Google Scholar\nprofiles, ranging across four disciplines and various academic institutions.\nOur results suggest that scholars tend to favor co-authors with higher\nscientometrics over others for inclusion in their co-authorship panels.\nInterestingly, as one's own scientometrics are higher, the tendency to include\nco-authors with high scientometrics is diminishing. Furthermore, we find that\nreciprocality is central to explaining scholars' selections.",
        "translated": ""
    },
    {
        "title": "Discrete Conditional Diffusion for Reranking in Recommendation",
        "url": "http://arxiv.org/abs/2308.06982v1",
        "pub_date": "2023-08-14",
        "summary": "Reranking plays a crucial role in modern multi-stage recommender systems by\nrearranging the initial ranking list to model interplay between items.\nConsidering the inherent challenges of reranking such as combinatorial\nsearching space, some previous studies have adopted the evaluator-generator\nparadigm, with a generator producing feasible sequences and a evaluator\nselecting the best one based on estimated listwise utility. Inspired by the\nremarkable success of diffusion generative models, this paper explores the\npotential of diffusion models for generating high-quality sequences in\nreranking. However, we argue that it is nontrivial to take diffusion models as\nthe generator in the context of recommendation. Firstly, diffusion models\nprimarily operate in continuous data space, differing from the discrete data\nspace of item permutations. Secondly, the recommendation task is different from\nconventional generation tasks as the purpose of recommender systems is to\nfulfill user interests. Lastly, real-life recommender systems require\nefficiency, posing challenges for the inference of diffusion models. To\novercome these challenges, we propose a novel Discrete Conditional Diffusion\nReranking (DCDR) framework for recommendation. DCDR extends traditional\ndiffusion models by introducing a discrete forward process with tractable\nposteriors, which adds noise to item sequences through step-wise discrete\noperations (e.g., swapping). Additionally, DCDR incorporates a conditional\nreverse process that generates item sequences conditioned on expected user\nresponses. Extensive offline experiments conducted on public datasets\ndemonstrate that DCDR outperforms state-of-the-art reranking methods.\nFurthermore, DCDR has been deployed in a real-world video app with over 300\nmillion daily active users, significantly enhancing online recommendation\nquality.",
        "translated": ""
    },
    {
        "title": "AutoAssign+: Automatic Shared Embedding Assignment in Streaming\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.06965v1",
        "pub_date": "2023-08-14",
        "summary": "In the domain of streaming recommender systems, conventional methods for\naddressing new user IDs or item IDs typically involve assigning initial ID\nembeddings randomly. However, this practice results in two practical\nchallenges: (i) Items or users with limited interactive data may yield\nsuboptimal prediction performance. (ii) Embedding new IDs or low-frequency IDs\nnecessitates consistently expanding the embedding table, leading to unnecessary\nmemory consumption. In light of these concerns, we introduce a reinforcement\nlearning-driven framework, namely AutoAssign+, that facilitates Automatic\nShared Embedding Assignment Plus. To be specific, AutoAssign+ utilizes an\nIdentity Agent as an actor network, which plays a dual role: (i) Representing\nlow-frequency IDs field-wise with a small set of shared embeddings to enhance\nthe embedding initialization, and (ii) Dynamically determining which ID\nfeatures should be retained or eliminated in the embedding table. The policy of\nthe agent is optimized with the guidance of a critic network. To evaluate the\neffectiveness of our approach, we perform extensive experiments on three\ncommonly used benchmark datasets. Our experiment results demonstrate that\nAutoAssign+ is capable of significantly enhancing recommendation performance by\nmitigating the cold-start problem. Furthermore, our framework yields a\nreduction in memory usage of approximately 20-30%, verifying its practical\neffectiveness and efficiency for streaming recommender systems.",
        "translated": ""
    },
    {
        "title": "Investigation Toward The Economic Feasibility of Personalized Medicine\n  For Healthcare Service Providers: The Case of Bladder Cancer",
        "url": "http://arxiv.org/abs/2308.07924v1",
        "pub_date": "2023-08-15",
        "summary": "In today's complex healthcare landscape, the pursuit of delivering optimal\npatient care while navigating intricate economic dynamics poses a significant\nchallenge for healthcare service providers (HSPs). In this already complex\ndynamics, the emergence of clinically promising personalized medicine based\ntreatment aims to revolutionize medicine. While personalized medicine holds\ntremendous potential for enhancing therapeutic outcomes, its integration within\nresource-constrained HSPs presents formidable challenges. In this study, we\ninvestigate the economic feasibility of implementing personalized medicine. The\ncentral objective is to strike a balance between catering to individual patient\nneeds and making economically viable decisions. Unlike conventional binary\napproaches to personalized treatment, we propose a more nuanced perspective by\ntreating personalization as a spectrum. This approach allows for greater\nflexibility in decision-making and resource allocation. To this end, we propose\na mathematical framework to investigate our proposal, focusing on Bladder\nCancer (BC) as a case study. Our results show that while it is feasible to\nintroduce personalized medicine, a highly efficient but highly expensive one\nwould be short-lived relative to its less effective but cheaper alternative as\nthe latter can be provided to a larger cohort of patients, optimizing the HSP's\nobjective better.",
        "translated": ""
    },
    {
        "title": "Synthesizing Political Zero-Shot Relation Classification via Codebook\n  Knowledge, NLI, and ChatGPT",
        "url": "http://arxiv.org/abs/2308.07876v1",
        "pub_date": "2023-08-15",
        "summary": "Recent supervised models for event coding vastly outperform pattern-matching\nmethods. However, their reliance solely on new annotations disregards the vast\nknowledge within expert databases, hindering their applicability to\nfine-grained classification. To address these limitations, we explore zero-shot\napproaches for political event ontology relation classification, by leveraging\nknowledge from established annotation codebooks. Our study encompasses both\nChatGPT and a novel natural language inference (NLI) based approach named ZSP.\nZSP adopts a tree-query framework that deconstructs the task into context,\nmodality, and class disambiguation levels. This framework improves\ninterpretability, efficiency, and adaptability to schema changes. By conducting\nextensive experiments on our newly curated datasets, we pinpoint the\ninstability issues within ChatGPT and highlight the superior performance of\nZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained\nRootcode classification. ZSP demonstrates competitive performance compared to\nsupervised BERT models, positioning it as a valuable tool for event record\nvalidation and ontology development. Our work underscores the potential of\nleveraging transfer learning and existing expertise to enhance the efficiency\nand scalability of research in the field.",
        "translated": ""
    },
    {
        "title": "Impression-Aware Recommender Systems",
        "url": "http://arxiv.org/abs/2308.07857v1",
        "pub_date": "2023-08-15",
        "summary": "Novel data sources bring new opportunities to improve the quality of\nrecommender systems. Impressions are a novel data source containing past\nrecommendations (shown items) and traditional interactions. Researchers may use\nimpressions to refine user preferences and overcome the current limitations in\nrecommender systems research. The relevance and interest of impressions have\nincreased over the years; hence, the need for a review of relevant work on this\ntype of recommenders. We present a systematic literature review on recommender\nsystems using impressions, focusing on three fundamental angles in research:\nrecommenders, datasets, and evaluation methodologies. We provide three\ncategorizations of papers describing recommenders using impressions, present\neach reviewed paper in detail, describe datasets with impressions, and analyze\nthe existing evaluation methodologies. Lastly, we present open questions and\nfuture directions of interest, highlighting aspects missing in the literature\nthat can be addressed in future works.",
        "translated": ""
    },
    {
        "title": "Dynamic Embedding Size Search with Minimum Regret for Streaming\n  Recommender System",
        "url": "http://arxiv.org/abs/2308.07760v1",
        "pub_date": "2023-08-15",
        "summary": "With the continuous increase of users and items, conventional recommender\nsystems trained on static datasets can hardly adapt to changing environments.\nThe high-throughput data requires the model to be updated in a timely manner\nfor capturing the user interest dynamics, which leads to the emergence of\nstreaming recommender systems. Due to the prevalence of deep learning-based\nrecommender systems, the embedding layer is widely adopted to represent the\ncharacteristics of users, items, and other features in low-dimensional vectors.\nHowever, it has been proved that setting an identical and static embedding size\nis sub-optimal in terms of recommendation performance and memory cost,\nespecially for streaming recommendations. To tackle this problem, we first\nrethink the streaming model update process and model the dynamic embedding size\nsearch as a bandit problem. Then, we analyze and quantify the factors that\ninfluence the optimal embedding sizes from the statistics perspective. Based on\nthis, we propose the \\textbf{D}ynamic \\textbf{E}mbedding \\textbf{S}ize\n\\textbf{S}earch (\\textbf{DESS}) method to minimize the embedding size selection\nregret on both user and item sides in a non-stationary manner. Theoretically,\nwe obtain a sublinear regret upper bound superior to previous methods.\nEmpirical results across two recommendation tasks on four public datasets also\ndemonstrate that our approach can achieve better streaming recommendation\nperformance with lower memory cost and higher time efficiency.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Dynamic Hypergraph Recommendation based on\n  Hyper-Relational Knowledge Graph",
        "url": "http://arxiv.org/abs/2308.07752v1",
        "pub_date": "2023-08-15",
        "summary": "Knowledge graphs (KGs) are commonly used as side information to enhance\ncollaborative signals and improve recommendation quality. In the context of\nknowledge-aware recommendation (KGR), graph neural networks (GNNs) have emerged\nas promising solutions for modeling factual and semantic information in KGs.\nHowever, the long-tail distribution of entities leads to sparsity in\nsupervision signals, which weakens the quality of item representation when\nutilizing KG enhancement. Additionally, the binary relation representation of\nKGs simplifies hyper-relational facts, making it challenging to model complex\nreal-world information. Furthermore, the over-smoothing phenomenon results in\nindistinguishable representations and information loss. To address these\nchallenges, we propose the SDK (Self-Supervised Dynamic Hypergraph\nRecommendation based on Hyper-Relational Knowledge Graph) framework. This\nframework establishes a cross-view hypergraph self-supervised learning\nmechanism for KG enhancement. Specifically, we model hyper-relational facts in\nKGs to capture interdependencies between entities under complete semantic\nconditions. With the refined representation, a hypergraph is dynamically\nconstructed to preserve features in the deep vector space, thereby alleviating\nthe over-smoothing problem. Furthermore, we mine external supervision signals\nfrom both the global perspective of the hypergraph and the local perspective of\ncollaborative filtering (CF) to guide the model prediction process. Extensive\nexperiments conducted on different datasets demonstrate the superiority of the\nSDK framework over state-of-the-art models. The results showcase its ability to\nalleviate the effects of over-smoothing and supervision signal sparsity.",
        "translated": ""
    },
    {
        "title": "SPM: Structured Pretraining and Matching Architectures for Relevance\n  Modeling in Meituan Search",
        "url": "http://arxiv.org/abs/2308.07711v1",
        "pub_date": "2023-08-15",
        "summary": "In e-commerce search, relevance between query and documents is an essential\nrequirement for satisfying user experience. Different from traditional\ne-commerce platforms that offer products, users search on life service\nplatforms such as Meituan mainly for product providers, which usually have\nabundant structured information, e.g. name, address, category, thousands of\nproducts. Modeling search relevance with these rich structured contents is\nchallenging due to the following issues: (1) there is language distribution\ndiscrepancy among different fields of structured document, making it difficult\nto directly adopt off-the-shelf pretrained language model based methods like\nBERT. (2) different fields usually have different importance and their length\nvary greatly, making it difficult to extract document information helpful for\nrelevance matching.\n  To tackle these issues, in this paper we propose a novel two-stage\npretraining and matching architecture for relevance matching with rich\nstructured documents. At pretraining stage, we propose an effective pretraining\nmethod that employs both query and multiple fields of document as inputs,\nincluding an effective information compression method for lengthy fields. At\nrelevance matching stage, a novel matching method is proposed by leveraging\ndomain knowledge in search query to generate more effective document\nrepresentations for relevance scoring. Extensive offline experiments and online\nA/B tests on millions of users verify that the proposed architectures\neffectively improve the performance of relevance modeling. The model has\nalready been deployed online, serving the search traffic of Meituan for over a\nyear.",
        "translated": ""
    },
    {
        "title": "Learning from All Sides: Diversified Positive Augmentation via\n  Self-distillation in Recommendation",
        "url": "http://arxiv.org/abs/2308.07629v1",
        "pub_date": "2023-08-15",
        "summary": "Personalized recommendation relies on user historical behaviors to provide\nuser-interested items, and thus seriously struggles with the data sparsity\nissue. A powerful positive item augmentation is beneficial to address the\nsparsity issue, while few works could jointly consider both the accuracy and\ndiversity of these augmented training labels. In this work, we propose a novel\nmodel-agnostic Diversified self-distillation guided positive augmentation\n(DivSPA) for accurate and diverse positive item augmentations. Specifically,\nDivSPA first conducts three types of retrieval strategies to collect\nhigh-quality and diverse positive item candidates according to users' overall\ninterests, short-term intentions, and similar users. Next, a self-distillation\nmodule is conducted to double-check and rerank these candidates as the final\npositive augmentations. Extensive offline and online evaluations verify the\neffectiveness of our proposed DivSPA on both accuracy and diversity. DivSPA is\nsimple and effective, which could be conveniently adapted to other base models\nand systems. Currently, DivSPA has been deployed on multiple widely-used\nreal-world recommender systems.",
        "translated": ""
    },
    {
        "title": "Delphic Costs and Benefits in Web Search: A utilitarian and historical\n  analysis",
        "url": "http://arxiv.org/abs/2308.07525v1",
        "pub_date": "2023-08-15",
        "summary": "We present a new framework to conceptualize and operationalize the total user\nexperience of search, by studying the entirety of a search journey from an\nutilitarian point of view.\n  Web search engines are widely perceived as \"free\". But search requires time\nand effort: in reality there are many intermingled non-monetary costs (e.g.\ntime costs, cognitive costs, interactivity costs) and the benefits may be\nmarred by various impairments, such as misunderstanding and misinformation.\nThis characterization of costs and benefits appears to be inherent to the human\nsearch for information within the pursuit of some larger task: most of the\ncosts and impairments can be identified in interactions with any web search\nengine, interactions with public libraries, and even in interactions with\nancient oracles. To emphasize this innate connection, we call these costs and\nbenefits Delphic, in contrast to explicitly financial costs and benefits.\n  Our main thesis is that the users' satisfaction with a search engine mostly\ndepends on their experience of Delphic cost and benefits, in other words on\ntheir utility. The consumer utility is correlated with classic measures of\nsearch engine quality, such as ranking, precision, recall, etc., but is not\ncompletely determined by them. To argue our thesis, we catalog the Delphic\ncosts and benefits and show how the development of search engines over the last\nquarter century, from classic Information Retrieval roots to the integration of\nLarge Language Models, was driven to a great extent by the quest of decreasing\nDelphic costs and increasing Delphic benefits.\n  We hope that the Delphic costs framework will engender new ideas and new\nresearch for evaluating and improving the web experience for everyone.",
        "translated": ""
    },
    {
        "title": "A Survey on Point-of-Interest Recommendations Leveraging Heterogeneous\n  Data",
        "url": "http://arxiv.org/abs/2308.07426v1",
        "pub_date": "2023-08-14",
        "summary": "Tourism is an important application domain for recommender systems. In this\ndomain, recommender systems are for example tasked with providing personalized\nrecommendations for transportation, accommodation, points-of-interest (POIs),\nor tourism services. Among these tasks, in particular the problem of\nrecommending POIs that are of likely interest to individual tourists has gained\ngrowing attention in recent years. Providing POI recommendations to tourists\n\\emph{during their trip} can however be especially challenging due to the\nvariability of the users' context. With the rapid development of the Web and\ntoday's multitude of online services, vast amounts of data from various sources\nhave become available, and these heterogeneous data sources represent a huge\npotential to better address the challenges of in-trip POI recommendation\nproblems. In this work, we provide a comprehensive survey of published research\non POI recommendation between 2017 and 2022 from the perspective of\nheterogeneous data sources. Specifically, we investigate which types of data\nare used in the literature and which technical approaches and evaluation\nmethods are predominant. Among other aspects, we find that today's research\nworks often focus on a narrow range of data sources, leaving great potential\nfor future works that better utilize heterogeneous data sources and diverse\ndata types for improved in-trip recommendations.",
        "translated": ""
    },
    {
        "title": "A Bi-Step Grounding Paradigm for Large Language Models in Recommendation\n  Systems",
        "url": "http://arxiv.org/abs/2308.08434v1",
        "pub_date": "2023-08-16",
        "summary": "As the focus on Large Language Models (LLMs) in the field of recommendation\nintensifies, the optimization of LLMs for recommendation purposes (referred to\nas LLM4Rec) assumes a crucial role in augmenting their effectiveness in\nproviding recommendations. However, existing approaches for LLM4Rec often\nassess performance using restricted sets of candidates, which may not\naccurately reflect the models' overall ranking capabilities. In this paper, our\nobjective is to investigate the comprehensive ranking capacity of LLMs and\npropose a two-step grounding framework known as BIGRec (Bi-step Grounding\nParadigm for Recommendation). It initially grounds LLMs to the recommendation\nspace by fine-tuning them to generate meaningful tokens for items and\nsubsequently identifies appropriate actual items that correspond to the\ngenerated tokens. By conducting extensive experiments on two datasets, we\nsubstantiate the superior performance, capacity for handling few-shot\nscenarios, and versatility across multiple domains exhibited by BIGRec.\nFurthermore, we observe that the marginal benefits derived from increasing the\nquantity of training samples are modest for BIGRec, implying that LLMs possess\nthe limited capability to assimilate statistical information, such as\npopularity and collaborative filtering, due to their robust semantic priors.\nThese findings also underline the efficacy of integrating diverse statistical\ninformation into the LLM4Rec framework, thereby pointing towards a potential\navenue for future research. Our code and data are available at\nhttps://github.com/SAI990323/Grounding4Rec.",
        "translated": ""
    },
    {
        "title": "Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value\n  Extraction",
        "url": "http://arxiv.org/abs/2308.08413v1",
        "pub_date": "2023-08-16",
        "summary": "Existing attribute-value extraction (AVE) models require large quantities of\nlabeled data for training. However, new products with new attribute-value pairs\nenter the market every day in real-world e-Commerce. Thus, we formulate AVE in\nmulti-label few-shot learning (FSL), aiming to extract unseen attribute value\npairs based on a small number of training examples. We propose a\nKnowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks,\nleveraging the generated label description and category information to learn\nmore discriminative prototypes. Besides, KEAF integrates with hybrid attention\nto reduce noise and capture more informative semantics for each class by\ncalculating the label-relevant and query-related weights. To achieve\nmulti-label inference, KEAF further learns a dynamic threshold by integrating\nthe semantic information from both the support set and the query set. Extensive\nexperiments with ablation studies conducted on two datasets demonstrate that\nKEAF outperforms other SOTA models for information extraction in FSL. The code\ncan be found at: https://github.com/gjiaying/KEAF",
        "translated": ""
    },
    {
        "title": "Content-based Recommendation Engine for Video Streaming Platform",
        "url": "http://arxiv.org/abs/2308.08406v1",
        "pub_date": "2023-08-16",
        "summary": "Recommendation engine suggest content, product or services to the user by\nusing machine learning algorithm. This paper proposed a content-based\nrecommendation engine for providing video suggestion to the user based on their\nprevious interests and choices. We will use TF-IDF text vectorization method to\ndetermine the relevance of words in a document. Then we will find out the\nsimilarity between each content by calculating cosine similarity between them.\nFinally, engine will recommend videos to the users based on the obtained\nsimilarity score value. In addition, we will measure the engine's performance\nby computing precision, recall, and F1 core of the proposed system.",
        "translated": ""
    },
    {
        "title": "Advancing continual lifelong learning in neural information retrieval:\n  definition, dataset, framework, and empirical evaluation",
        "url": "http://arxiv.org/abs/2308.08378v1",
        "pub_date": "2023-08-16",
        "summary": "Continual learning refers to the capability of a machine learning model to\nlearn and adapt to new information, without compromising its performance on\npreviously learned tasks. Although several studies have investigated continual\nlearning methods for information retrieval tasks, a well-defined task\nformulation is still lacking, and it is unclear how typical learning strategies\nperform in this context. To address this challenge, a systematic task\nformulation of continual neural information retrieval is presented, along with\na multiple-topic dataset that simulates continuous information retrieval. A\ncomprehensive continual neural information retrieval framework consisting of\ntypical retrieval models and continual learning strategies is then proposed.\nEmpirical evaluations illustrate that the proposed framework can successfully\nprevent catastrophic forgetting in neural information retrieval and enhance\nperformance on previously learned tasks. The results indicate that\nembedding-based retrieval models experience a decline in their continual\nlearning performance as the topic shift distance and dataset volume of new\ntasks increase. In contrast, pretraining-based models do not show any such\ncorrelation. Adopting suitable learning strategies can mitigate the effects of\ntopic shift and data augmentation.",
        "translated": ""
    },
    {
        "title": "Is Meta-Learning the Right Approach for the Cold-Start Problem in\n  Recommender Systems?",
        "url": "http://arxiv.org/abs/2308.08354v1",
        "pub_date": "2023-08-16",
        "summary": "Recommender systems have become fundamental building blocks of modern online\nproducts and services, and have a substantial impact on user experience. In the\npast few years, deep learning methods have attracted a lot of research, and are\nnow heavily used in modern real-world recommender systems. Nevertheless,\ndealing with recommendations in the cold-start setting, e.g., when a user has\ndone limited interactions in the system, is a problem that remains far from\nsolved. Meta-learning techniques, and in particular optimization-based\nmeta-learning, have recently become the most popular approaches in the academic\nresearch literature for tackling the cold-start problem in deep learning models\nfor recommender systems. However, current meta-learning approaches are not\npractical for real-world recommender systems, which have billions of users and\nitems, and strict latency requirements. In this paper we show that it is\npossible to obtaining similar, or higher, performance on commonly used\nbenchmarks for the cold-start problem without using meta-learning techniques.\nIn more detail, we show that, when tuned correctly, standard and widely adopted\ndeep learning models perform just as well as newer meta-learning models. We\nfurther show that an extremely simple modular approach using common\nrepresentation learning techniques, can perform comparably to meta-learning\ntechniques specifically designed for the cold-start setting while being much\nmore easily deployable in real-world applications.",
        "translated": ""
    },
    {
        "title": "Phase Retrieval with Background Information: Decreased References and\n  Efficient Methods",
        "url": "http://arxiv.org/abs/2308.08328v1",
        "pub_date": "2023-08-16",
        "summary": "Fourier phase retrieval(PR) is a severely ill-posed inverse problem that\narises in various applications. To guarantee a unique solution and relieve the\ndependence on the initialization, background information can be exploited as a\nstructural priors. However, the requirement for the background information may\nbe challenging when moving to the high-resolution imaging. At the same time,\nthe previously proposed projected gradient descent(PGD) method also demands\nmuch background information.\n  In this paper, we present an improved theoretical result about the demand for\nthe background information, along with two Douglas Rachford(DR) based methods.\nAnalytically, we demonstrate that the background required to ensure a unique\nsolution can be decreased by nearly $1/2$ for the 2-D signals compared to the\n1-D signals. By generalizing the results into $d$-dimension, we show that the\nlength of the background information more than $(2^{\\frac{d+1}{d}}-1)$ folds of\nthe signal is sufficient to ensure the uniqueness. At the same time, we also\nanalyze the stability and robustness of the model when measurements and\nbackground information are corrupted by the noise. Furthermore, two methods\ncalled Background Douglas-Rachford (BDR) and Convex Background Douglas-Rachford\n(CBDR) are proposed. BDR which is a kind of non-convex method is proven to have\nthe local R-linear convergence rate under mild assumptions. Instead, CBDR\nmethod uses the techniques of convexification and can be proven to own a global\nconvergence guarantee as long as the background information is sufficient. To\nsupport this, a new property called F-RIP is established. We test the\nperformance of the proposed methods through simulations as well as real\nexperimental measurements, and demonstrate that they achieve a higher recovery\nrate with less background information compared to the PGD method.",
        "translated": ""
    },
    {
        "title": "Pre-training with Large Language Model-based Document Expansion for\n  Dense Passage Retrieval",
        "url": "http://arxiv.org/abs/2308.08285v1",
        "pub_date": "2023-08-16",
        "summary": "In this paper, we systematically study the potential of pre-training with\nLarge Language Model(LLM)-based document expansion for dense passage retrieval.\nConcretely, we leverage the capabilities of LLMs for document expansion, i.e.\nquery generation, and effectively transfer expanded knowledge to retrievers\nusing pre-training strategies tailored for passage retrieval. These strategies\ninclude contrastive learning and bottlenecked query generation. Furthermore, we\nincorporate a curriculum learning strategy to reduce the reliance on LLM\ninferences. Experimental results demonstrate that pre-training with LLM-based\ndocument expansion significantly boosts the retrieval performance on\nlarge-scale web-search tasks. Our work shows strong zero-shot and out-of-domain\nretrieval abilities, making it more widely applicable for retrieval when\ninitializing with no human-labeled data.",
        "translated": ""
    },
    {
        "title": "Uncovering User Interest from Biased and Noised Watch Time in Video\n  Recommendation",
        "url": "http://arxiv.org/abs/2308.08120v1",
        "pub_date": "2023-08-16",
        "summary": "In the video recommendation, watch time is commonly adopted as an indicator\nof user interest. However, watch time is not only influenced by the matching of\nusers' interests but also by other factors, such as duration bias and noisy\nwatching. Duration bias refers to the tendency for users to spend more time on\nvideos with longer durations, regardless of their actual interest level. Noisy\nwatching, on the other hand, describes users taking time to determine whether\nthey like a video or not, which can result in users spending time watching\nvideos they do not like. Consequently, the existence of duration bias and noisy\nwatching make watch time an inadequate label for indicating user interest.\nFurthermore, current methods primarily address duration bias and ignore the\nimpact of noisy watching, which may limit their effectiveness in uncovering\nuser interest from watch time. In this study, we first analyze the generation\nmechanism of users' watch time from a unified causal viewpoint. Specifically,\nwe considered the watch time as a mixture of the user's actual interest level,\nthe duration-biased watch time, and the noisy watch time. To mitigate both the\nduration bias and noisy watching, we propose Debiased and Denoised watch time\nCorrection (D$^2$Co), which can be divided into two steps: First, we employ a\nduration-wise Gaussian Mixture Model plus frequency-weighted moving average for\nestimating the bias and noise terms; then we utilize a sensitivity-controlled\ncorrection function to separate the user interest from the watch time, which is\nrobust to the estimation error of bias and noise terms. The experiments on two\npublic video recommendation datasets and online A/B testing indicate the\neffectiveness of the proposed method.",
        "translated": ""
    },
    {
        "title": "Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme\n  Detection",
        "url": "http://arxiv.org/abs/2308.08088v1",
        "pub_date": "2023-08-16",
        "summary": "Hateful meme detection is a challenging multimodal task that requires\ncomprehension of both vision and language, as well as cross-modal interactions.\nRecent studies have tried to fine-tune pre-trained vision-language models\n(PVLMs) for this task. However, with increasing model sizes, it becomes\nimportant to leverage powerful PVLMs more efficiently, rather than simply\nfine-tuning them. Recently, researchers have attempted to convert meme images\ninto textual captions and prompt language models for predictions. This approach\nhas shown good performance but suffers from non-informative image captions.\nConsidering the two factors mentioned above, we propose a probing-based\ncaptioning approach to leverage PVLMs in a zero-shot visual question answering\n(VQA) manner. Specifically, we prompt a frozen PVLM by asking hateful\ncontent-related questions and use the answers as image captions (which we call\nPro-Cap), so that the captions contain information critical for hateful content\ndetection. The good performance of models with Pro-Cap on three benchmarks\nvalidates the effectiveness and generalization of the proposed method.",
        "translated": ""
    },
    {
        "title": "Decentralized Graph Neural Network for Privacy-Preserving Recommendation",
        "url": "http://arxiv.org/abs/2308.08072v1",
        "pub_date": "2023-08-15",
        "summary": "Building a graph neural network (GNN)-based recommender system without\nviolating user privacy proves challenging. Existing methods can be divided into\nfederated GNNs and decentralized GNNs. But both methods have undesirable\neffects, i.e., low communication efficiency and privacy leakage. This paper\nproposes DGREC, a novel decentralized GNN for privacy-preserving\nrecommendations, where users can choose to publicize their interactions. It\nincludes three stages, i.e., graph construction, local gradient calculation,\nand global gradient passing. The first stage builds a local inner-item\nhypergraph for each user and a global inter-user graph. The second stage models\nuser preference and calculates gradients on each local device. The third stage\ndesigns a local differential privacy mechanism named secure gradient-sharing,\nwhich proves strong privacy-preserving of users' private data. We conduct\nextensive experiments on three public datasets to validate the consistent\nsuperiority of our framework.",
        "translated": ""
    },
    {
        "title": "MUSE: Music Recommender System with Shuffle Play Recommendation\n  Enhancement",
        "url": "http://arxiv.org/abs/2308.09649v1",
        "pub_date": "2023-08-18",
        "summary": "Recommender systems have become indispensable in music streaming services,\nenhancing user experiences by personalizing playlists and facilitating the\nserendipitous discovery of new music. However, the existing recommender systems\noverlook the unique challenges inherent in the music domain, specifically\nshuffle play, which provides subsequent tracks in a random sequence. Based on\nour observation that the shuffle play sessions hinder the overall training\nprocess of music recommender systems mainly due to the high unique transition\nrates of shuffle play sessions, we propose a Music Recommender System with\nShuffle Play Recommendation Enhancement (MUSE). MUSE employs the\nself-supervised learning framework that maximizes the agreement between the\noriginal session and the augmented session, which is augmented by our novel\nsession augmentation method, called transition-based augmentation. To further\nfacilitate the alignment of the representations between the two views, we\ndevise two fine-grained matching strategies, i.e., item- and similarity-based\nmatching strategies. Through rigorous experiments conducted across diverse\nenvironments, we demonstrate MUSE's efficacy over 12 baseline models on a\nlarge-scale Music Streaming Sessions Dataset (MSSD) from Spotify. The source\ncode of MUSE is available at \\url{https://github.com/yunhak0/MUSE}.",
        "translated": ""
    },
    {
        "title": "ReCon: Reducing Congestion in Job Recommendation using Optimal Transport",
        "url": "http://arxiv.org/abs/2308.09516v1",
        "pub_date": "2023-08-18",
        "summary": "Recommender systems may suffer from congestion, meaning that there is an\nunequal distribution of the items in how often they are recommended. Some items\nmay be recommended much more than others. Recommenders are increasingly used in\ndomains where items have limited availability, such as the job market, where\ncongestion is especially problematic: Recommending a vacancy -- for which\ntypically only one person will be hired -- to a large number of job seekers may\nlead to frustration for job seekers, as they may be applying for jobs where\nthey are not hired. This may also leave vacancies unfilled and result in job\nmarket inefficiency.\n  We propose a novel approach to job recommendation called ReCon, accounting\nfor the congestion problem. Our approach is to use an optimal transport\ncomponent to ensure a more equal spread of vacancies over job seekers, combined\nwith a job recommendation model in a multi-objective optimization problem. We\nevaluated our approach on two real-world job market datasets. The evaluation\nresults show that ReCon has good performance on both congestion-related (e.g.,\nCongestion) and desirability (e.g., NDCG) measures.",
        "translated": ""
    },
    {
        "title": "Attention Calibration for Transformer-based Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.09419v1",
        "pub_date": "2023-08-18",
        "summary": "Transformer-based sequential recommendation (SR) has been booming in recent\nyears, with the self-attention mechanism as its key component. Self-attention\nhas been widely believed to be able to effectively select those informative and\nrelevant items from a sequence of interacted items for next-item prediction via\nlearning larger attention weights for these items. However, this may not always\nbe true in reality. Our empirical analysis of some representative\nTransformer-based SR models reveals that it is not uncommon for large attention\nweights to be assigned to less relevant items, which can result in inaccurate\nrecommendations. Through further in-depth analysis, we find two factors that\nmay contribute to such inaccurate assignment of attention weights: sub-optimal\nposition encoding and noisy input. To this end, in this paper, we aim to\naddress this significant yet challenging gap in existing works. To be specific,\nwe propose a simple yet effective framework called Attention Calibration for\nTransformer-based Sequential Recommendation (AC-TSR). In AC-TSR, a novel\nspatial calibrator and adversarial calibrator are designed respectively to\ndirectly calibrates those incorrectly assigned attention weights. The former is\ndevised to explicitly capture the spatial relationships (i.e., order and\ndistance) among items for more precise calculation of attention weights. The\nlatter aims to redistribute the attention weights based on each item's\ncontribution to the next-item prediction. AC-TSR is readily adaptable and can\nbe seamlessly integrated into various existing transformer-based SR models.\nExtensive experimental results on four benchmark real-world datasets\ndemonstrate the superiority of our proposed ACTSR via significant\nrecommendation performance enhancements. The source code is available at\nhttps://github.com/AIM-SE/AC-TSR.",
        "translated": ""
    },
    {
        "title": "SHARK: A Lightweight Model Compression Approach for Large-scale\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2308.09395v1",
        "pub_date": "2023-08-18",
        "summary": "Increasing the size of embedding layers has shown to be effective in\nimproving the performance of recommendation models, yet gradually causing their\nsizes to exceed terabytes in industrial recommender systems, and hence the\nincrease of computing and storage costs. To save resources while maintaining\nmodel performances, we propose SHARK, the model compression practice we have\nsummarized in the recommender system of industrial scenarios. SHARK consists of\ntwo main components. First, we use the novel first-order component of Taylor\nexpansion as importance scores to prune the number of embedding tables (feature\nfields). Second, we introduce a new row-wise quantization method to apply\ndifferent quantization strategies to each embedding. We conduct extensive\nexperiments on both public and industrial datasets, demonstrating that each\ncomponent of our proposed SHARK framework outperforms previous approaches. We\nconduct A/B tests in multiple models on Kuaishou, such as short video,\ne-commerce, and advertising recommendation models. The results of the online\nA/B test showed SHARK can effectively reduce the memory footprint of the\nembedded layer. For the short-video scenarios, the compressed model without any\nperformance drop significantly saves 70% storage and thousands of machines,\nimproves 30\\% queries per second (QPS), and has been deployed to serve hundreds\nof millions of users and process tens of billions of requests every day.",
        "translated": ""
    },
    {
        "title": "How Discriminative Are Your Qrels? How To Study the Statistical\n  Significance of Document Adjudication Methods",
        "url": "http://arxiv.org/abs/2308.09340v1",
        "pub_date": "2023-08-18",
        "summary": "Creating test collections for offline retrieval evaluation requires human\neffort to judge documents' relevance. This expensive activity motivated much\nwork in developing methods for constructing benchmarks with fewer assessment\ncosts. In this respect, adjudication methods actively decide both which\ndocuments and the order in which experts review them, in order to better\nexploit the assessment budget or to lower it. Researchers evaluate the quality\nof those methods by measuring the correlation between the known gold ranking of\nsystems under the full collection and the observed ranking of systems under the\nlower-cost one. This traditional analysis ignores whether and how the low-cost\njudgements impact on the statistically significant differences among systems\nwith respect to the full collection. We fill this void by proposing a novel\nmethodology to evaluate how the low-cost adjudication methods preserve the\npairwise significant differences between systems as the full collection. In\nother terms, while traditional approaches look for stability in answering the\nquestion \"is system A better than system B?\", our proposed approach looks for\nstability in answering the question \"is system A significantly better than\nsystem B?\", which is the ultimate questions researchers need to answer to\nguarantee the generalisability of their results. Among other results, we found\nthat the best methods in terms of ranking of systems correlation do not always\nmatch those preserving statistical significance.",
        "translated": ""
    },
    {
        "title": "Meta-learning enhanced next POI recommendation by leveraging check-ins\n  from auxiliary cities",
        "url": "http://arxiv.org/abs/2308.09309v1",
        "pub_date": "2023-08-18",
        "summary": "Most existing point-of-interest (POI) recommenders aim to capture user\npreference by employing city-level user historical check-ins, thus facilitating\nusers' exploration of the city. However, the scarcity of city-level user\ncheck-ins brings a significant challenge to user preference learning. Although\nprior studies attempt to mitigate this challenge by exploiting various context\ninformation, e.g., spatio-temporal information, they ignore to transfer the\nknowledge (i.e., common behavioral pattern) from other relevant cities (i.e.,\nauxiliary cities). In this paper, we investigate the effect of knowledge\ndistilled from auxiliary cities and thus propose a novel Meta-learning Enhanced\nnext POI Recommendation framework (MERec). The MERec leverages the correlation\nof check-in behaviors among various cities into the meta-learning paradigm to\nhelp infer user preference in the target city, by holding the principle of\n\"paying more attention to more correlated knowledge\". Particularly, a\ncity-level correlation strategy is devised to attentively capture common\npatterns among cities, so as to transfer more relevant knowledge from more\ncorrelated cities. Extensive experiments verify the superiority of the proposed\nMERec against state-of-the-art algorithms.",
        "translated": ""
    },
    {
        "title": "Differentiable Retrieval Augmentation via Generative Language Modeling\n  for E-commerce Query Intent Classification",
        "url": "http://arxiv.org/abs/2308.09308v1",
        "pub_date": "2023-08-18",
        "summary": "Retrieval augmentation, which enhances downstream models by a knowledge\nretriever and an external corpus instead of by merely increasing the number of\nmodel parameters, has been successfully applied to many natural language\nprocessing (NLP) tasks such as text classification, question answering and so\non. However, existing methods that separately or asynchronously train the\nretriever and downstream model mainly due to the non-differentiability between\nthe two parts, usually lead to degraded performance compared to end-to-end\njoint training.",
        "translated": ""
    },
    {
        "title": "Graph-based Alignment and Uniformity for Recommendation",
        "url": "http://arxiv.org/abs/2308.09292v1",
        "pub_date": "2023-08-18",
        "summary": "Collaborative filtering-based recommender systems (RecSys) rely on learning\nrepresentations for users and items to predict preferences accurately.\nRepresentation learning on the hypersphere is a promising approach due to its\ndesirable properties, such as alignment and uniformity. However, the sparsity\nissue arises when it encounters RecSys. To address this issue, we propose a\nnovel approach, graph-based alignment and uniformity (GraphAU), that explicitly\nconsiders high-order connectivities in the user-item bipartite graph. GraphAU\naligns the user/item embedding to the dense vector representations of\nhigh-order neighbors using a neighborhood aggregator, eliminating the need to\ncompute the burdensome alignment to high-order neighborhoods individually. To\naddress the discrepancy in alignment losses, GraphAU includes a layer-wise\nalignment pooling module to integrate alignment losses layer-wise. Experiments\non four datasets show that GraphAU significantly alleviates the sparsity issue\nand achieves state-of-the-art performance. We open-source GraphAU at\nhttps://github.com/YangLiangwei/GraphAU.",
        "translated": ""
    },
    {
        "title": "A Model-Agnostic Framework for Recommendation via Interest-aware Item\n  Embeddings",
        "url": "http://arxiv.org/abs/2308.09202v1",
        "pub_date": "2023-08-17",
        "summary": "Item representation holds significant importance in recommendation systems,\nwhich encompasses domains such as news, retail, and videos. Retrieval and\nranking models utilise item representation to capture the user-item\nrelationship based on user behaviours. While existing representation learning\nmethods primarily focus on optimising item-based mechanisms, such as attention\nand sequential modelling. However, these methods lack a modelling mechanism to\ndirectly reflect user interests within the learned item representations.\nConsequently, these methods may be less effective in capturing user interests\nindirectly. To address this challenge, we propose a novel Interest-aware\nCapsule network (IaCN) recommendation model, a model-agnostic framework that\ndirectly learns interest-oriented item representations. IaCN serves as an\nauxiliary task, enabling the joint learning of both item-based and\ninterest-based representations. This framework adopts existing recommendation\nmodels without requiring substantial redesign. We evaluate the proposed\napproach on benchmark datasets, exploring various scenarios involving different\ndeep neural networks, behaviour sequence lengths, and joint learning ratios of\ninterest-oriented item representations. Experimental results demonstrate\nsignificant performance enhancements across diverse recommendation models,\nvalidating the effectiveness of our approach.",
        "translated": ""
    },
    {
        "title": "Identity-Aware Semi-Supervised Learning for Comic Character\n  Re-Identification",
        "url": "http://arxiv.org/abs/2308.09096v1",
        "pub_date": "2023-08-17",
        "summary": "Character re-identification, recognizing characters consistently across\ndifferent panels in comics, presents significant challenges due to limited\nannotated data and complex variations in character appearances. To tackle this\nissue, we introduce a robust semi-supervised framework that combines metric\nlearning with a novel 'Identity-Aware' self-supervision method by contrastive\nlearning of face and body pairs of characters. Our approach involves processing\nboth facial and bodily features within a unified network architecture,\nfacilitating the extraction of identity-aligned character embeddings that\ncapture individual identities while preserving the effectiveness of face and\nbody features. This integrated character representation enhances feature\nextraction and improves character re-identification compared to\nre-identification by face or body independently, offering a parameter-efficient\nsolution. By extensively validating our method using in-series and inter-series\nevaluation metrics, we demonstrate its effectiveness in consistently\nre-identifying comic characters. Compared to existing methods, our approach not\nonly addresses the challenge of character re-identification but also serves as\na foundation for downstream tasks since it can produce character embeddings\nwithout restrictions of face and body availability, enriching the comprehension\nof comic books. In our experiments, we leverage two newly curated datasets: the\n'Comic Character Instances Dataset', comprising over a million character\ninstances and the 'Comic Sequence Identity Dataset', containing annotations of\nidentities within more than 3000 sets of four consecutive comic panels that we\ncollected.",
        "translated": ""
    },
    {
        "title": "Leveraging Large Language Models for Pre-trained Recommender Systems",
        "url": "http://arxiv.org/abs/2308.10837v1",
        "pub_date": "2023-08-21",
        "summary": "Recent advancements in recommendation systems have shifted towards more\ncomprehensive and personalized recommendations by utilizing large language\nmodels (LLM). However, effectively integrating LLM's commonsense knowledge and\nreasoning abilities into recommendation systems remains a challenging problem.\nIn this paper, we propose RecSysLLM, a novel pre-trained recommendation model\nbased on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating\nrecommendation domain knowledge through unique designs of data, training, and\ninference. This allows RecSysLLM to leverage LLMs' capabilities for\nrecommendation tasks in an efficient, unified framework. We demonstrate the\neffectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM\nprovides a promising approach to developing unified recommendation systems by\nfully exploiting the power of pre-trained language models.",
        "translated": ""
    },
    {
        "title": "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
        "url": "http://arxiv.org/abs/2308.10835v1",
        "pub_date": "2023-08-21",
        "summary": "Recommendation systems aim to provide users with relevant suggestions, but\noften lack interpretability and fail to capture higher-level semantic\nrelationships between user behaviors and profiles. In this paper, we propose a\nnovel approach that leverages large language models (LLMs) to construct\npersonalized reasoning graphs. These graphs link a user's profile and\nbehavioral sequences through causal and logical inferences, representing the\nuser's interests in an interpretable way. Our approach, LLM reasoning graphs\n(LLMRG), has four components: chained graph reasoning, divergent extension,\nself-verification and scoring, and knowledge base self-improvement. The\nresulting reasoning graph is encoded using graph neural networks, which serves\nas additional input to improve conventional recommender systems, without\nrequiring extra user or item information. Our approach demonstrates how LLMs\ncan enable more logical and interpretable recommender systems through\npersonalized reasoning graphs. LLMRG allows recommendations to benefit from\nboth engineered recommendation systems and LLM-derived reasoning graphs. We\ndemonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios\nin enhancing base recommendation models.",
        "translated": ""
    },
    {
        "title": "DynED: Dynamic Ensemble Diversification in Data Stream Classification",
        "url": "http://arxiv.org/abs/2308.10807v1",
        "pub_date": "2023-08-21",
        "summary": "Ensemble methods are commonly used in classification due to their remarkable\nperformance. Achieving high accuracy in a data stream environment is a\nchallenging task considering disruptive changes in the data distribution, also\nknown as concept drift. A greater diversity of ensemble components is known to\nenhance prediction accuracy in such settings. Despite the diversity of\ncomponents within an ensemble, not all contribute as expected to its overall\nperformance. This necessitates a method for selecting components that exhibit\nhigh performance and diversity. We present a novel ensemble construction and\nmaintenance approach based on MMR (Maximal Marginal Relevance) that dynamically\ncombines the diversity and prediction accuracy of components during the process\nof structuring an ensemble. The experimental results on both four real and 11\nsynthetic datasets demonstrate that the proposed approach (DynED) provides a\nhigher average mean accuracy compared to the five state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "LSCPM: communities in massive real-world Link Streams by Clique\n  Percolation Method",
        "url": "http://arxiv.org/abs/2308.10801v1",
        "pub_date": "2023-08-21",
        "summary": "Community detection is a popular approach to understand the organization of\ninteractions in static networks. For that purpose, the Clique Percolation\nMethod (CPM), which involves the percolation of k-cliques, is a well-studied\ntechnique that offers several advantages. Besides, studying interactions that\noccur over time is useful in various contexts, which can be modeled by the link\nstream formalism. The Dynamic Clique Percolation Method (DCPM) has been\nproposed for extending CPM to temporal networks.\n  However, existing implementations are unable to handle massive datasets. We\npresent a novel algorithm that adapts CPM to link streams, which has the\nadvantage that it allows us to speed up the computation time with respect to\nthe existing DCPM method. We evaluate it experimentally on real datasets and\nshow that it scales to massive link streams. For example, it allows to obtain a\ncomplete set of communities in under twenty-five minutes for a dataset with\nthirty million links, what the state of the art fails to achieve even after a\nweek of computation. We further show that our method provides communities\nsimilar to DCPM, but slightly more aggregated. We exhibit the relevance of the\nobtained communities in real world cases, and show that they provide\ninformation on the importance of vertices in the link streams.",
        "translated": ""
    },
    {
        "title": "A Topology-aware Analysis of Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2308.10778v1",
        "pub_date": "2023-08-21",
        "summary": "The successful integration of graph neural networks into recommender systems\n(RSs) has led to a novel paradigm in collaborative filtering (CF), graph\ncollaborative filtering (graph CF). By representing user-item data as an\nundirected, bipartite graph, graph CF utilizes short- and long-range\nconnections to extract collaborative signals that yield more accurate user\npreferences than traditional CF methods. Although the recent literature\nhighlights the efficacy of various algorithmic strategies in graph CF, the\nimpact of datasets and their topological features on recommendation performance\nis yet to be studied. To fill this gap, we propose a topology-aware analysis of\ngraph CF. In this study, we (i) take some widely-adopted recommendation\ndatasets and use them to generate a large set of synthetic sub-datasets through\ntwo state-of-the-art graph sampling methods, (ii) measure eleven of their\nclassical and topological characteristics, and (iii) estimate the accuracy\ncalculated on the generated sub-datasets considering four popular and recent\ngraph-based RSs (i.e., LightGCN, DGCF, UltraGCN, and SVD-GCN). Finally, the\ninvestigation presents an explanatory framework that reveals the linear\nrelationships between characteristics and accuracy measures. The results,\nstatistically validated under different graph sampling settings, confirm the\nexistence of solid dependencies between topological characteristics and\naccuracy in the graph-based recommendation, offering a new perspective on how\nto interpret graph CF.",
        "translated": ""
    },
    {
        "title": "DepreSym: A Depression Symptom Annotated Corpus and the Role of LLMs as\n  Assessors of Psychological Markers",
        "url": "http://arxiv.org/abs/2308.10758v1",
        "pub_date": "2023-08-21",
        "summary": "Computational methods for depression detection aim to mine traces of\ndepression from online publications posted by Internet users. However,\nsolutions trained on existing collections exhibit limited generalisation and\ninterpretability. To tackle these issues, recent studies have shown that\nidentifying depressive symptoms can lead to more robust models. The eRisk\ninitiative fosters research on this area and has recently proposed a new\nranking task focused on developing search methods to find sentences related to\ndepressive symptoms. This search challenge relies on the symptoms specified by\nthe Beck Depression Inventory-II (BDI-II), a questionnaire widely used in\nclinical practice. Based on the participant systems' results, we present the\nDepreSym dataset, consisting of 21580 sentences annotated according to their\nrelevance to the 21 BDI-II symptoms. The labelled sentences come from a pool of\ndiverse ranking methods, and the final dataset serves as a valuable resource\nfor advancing the development of models that incorporate depressive markers\nsuch as clinical symptoms. Due to the complex nature of this relevance\nannotation, we designed a robust assessment methodology carried out by three\nexpert assessors (including an expert psychologist). Additionally, we explore\nhere the feasibility of employing recent Large Language Models (ChatGPT and\nGPT4) as potential assessors in this complex task. We undertake a comprehensive\nexamination of their performance, determine their main limitations and analyze\ntheir role as a complement or replacement for human annotators.",
        "translated": ""
    },
    {
        "title": "Contrastive Graph Prompt-tuning for Cross-domain Recommendation",
        "url": "http://arxiv.org/abs/2308.10685v1",
        "pub_date": "2023-08-21",
        "summary": "Recommender systems are frequently challenged by the data sparsity problem.\nOne approach to mitigate this issue is through cross-domain recommendation\ntechniques. In a cross-domain context, sharing knowledge between domains can\nenhance the effectiveness in the target domain. Recent cross-domain methods\nhave employed a pre-training approach, but we argue that these methods often\nresult in suboptimal fine-tuning, especially with large neural models. Modern\nlanguage models utilize prompts for efficient model tuning. Such prompts act as\na tunable latent vector, allowing for the freezing of the main model\nparameters. In our research, we introduce the Personalised Graph Prompt-based\nRecommendation (PGPRec) framework. This leverages the advantages of\nprompt-tuning. Within this framework, we formulate personalized graph prompts\nitem-wise, rooted in items that a user has previously engaged with.\nSpecifically, we employ Contrastive Learning (CL) to produce pre-trained\nembeddings that offer greater generalizability in the pre-training phase,\nensuring robust training during the tuning phase. Our evaluation of PGPRec in\ncross-domain scenarios involves comprehensive testing with the top-k\nrecommendation tasks and a cold-start analysis. Our empirical findings, based\non four Amazon Review datasets, reveal that the PGPRec framework can decrease\nthe tuned parameters by as much as 74%, maintaining competitive performance.\nRemarkably, there's an 11.41% enhancement in performance against the leading\nbaseline in cold-start situations.",
        "translated": ""
    },
    {
        "title": "Evaluating Temporal Persistence Using Replicability Measures",
        "url": "http://arxiv.org/abs/2308.10549v1",
        "pub_date": "2023-08-21",
        "summary": "In real-world Information Retrieval (IR) experiments, the Evaluation\nEnvironment (EE) is exposed to constant change. Documents are added, removed,\nor updated, and the information need and the search behavior of users is\nevolving. Simultaneously, IR systems are expected to retain a consistent\nquality. The LongEval Lab seeks to investigate the longitudinal persistence of\nIR systems, and in this work, we describe our participation. We submitted runs\nof five advanced retrieval systems, namely a Reciprocal Rank Fusion (RRF)\napproach, ColBERT, monoT5, Doc2Query, and E5, to both sub-tasks. Further, we\ncast the longitudinal evaluation as a replicability study to better understand\nthe temporal change observed. As a result, we quantify the persistence of the\nsubmitted runs and see great potential in this evaluation method.",
        "translated": ""
    },
    {
        "title": "DPAN: Dynamic Preference-based and Attribute-aware Network for Relevant\n  Recommendations",
        "url": "http://arxiv.org/abs/2308.10527v1",
        "pub_date": "2023-08-21",
        "summary": "In e-commerce platforms, the relevant recommendation is a unique scenario\nproviding related items for a trigger item that users are interested in.\nHowever, users' preferences for the similarity and diversity of recommendation\nresults are dynamic and vary under different conditions. Moreover, individual\nitem-level diversity is too coarse-grained since all recommended items are\nrelated to the trigger item. Thus, the two main challenges are to learn\nfine-grained representations of similarity and diversity and capture users'\ndynamic preferences for them under different conditions. To address these\nchallenges, we propose a novel method called the Dynamic Preference-based and\nAttribute-aware Network (DPAN) for predicting Click-Through Rate (CTR) in\nrelevant recommendations. Specifically, based on Attribute-aware Activation\nValues Generation (AAVG), Bi-dimensional Compression-based Re-expression (BCR)\nis designed to obtain similarity and diversity representations of user\ninterests and item information. Then Shallow and Deep Union-based Fusion (SDUF)\nis proposed to capture users' dynamic preferences for the diverse degree of\nrecommendation results according to various conditions. DPAN has demonstrated\nits effectiveness through extensive offline experiments and online A/B testing,\nresulting in a significant 7.62% improvement in CTR. Currently, DPAN has been\nsuccessfully deployed on our e-commerce platform serving the primary traffic\nfor relevant recommendations. The code of DPAN has been made publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Single-User Injection for Invisible Shilling Attack against Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2308.10467v1",
        "pub_date": "2023-08-21",
        "summary": "Recommendation systems (RS) are crucial for alleviating the information\noverload problem. Due to its pivotal role in guiding users to make decisions,\nunscrupulous parties are lured to launch attacks against RS to affect the\ndecisions of normal users and gain illegal profits. Among various types of\nattacks, shilling attack is one of the most subsistent and profitable attacks.\nIn shilling attack, an adversarial party injects a number of well-designed fake\nuser profiles into the system to mislead RS so that the attack goal can be\nachieved. Although existing shilling attack methods have achieved promising\nresults, they all adopt the attack paradigm of multi-user injection, where some\nfake user profiles are required. This paper provides the first study of\nshilling attack in an extremely limited scenario: only one fake user profile is\ninjected into the victim RS to launch shilling attacks (i.e., single-user\ninjection). We propose a novel single-user injection method SUI-Attack for\ninvisible shilling attack. SUI-Attack is a graph based attack method that\nmodels shilling attack as a node generation task over the user-item bipartite\ngraph of the victim RS, and it constructs the fake user profile by generating\nuser features and edges that link the fake user to items. Extensive experiments\ndemonstrate that SUI-Attack can achieve promising attack results in single-user\ninjection. In addition to its attack power, SUI-Attack increases the\nstealthiness of shilling attack and reduces the risk of being detected. We\nprovide our implementation at: https://github.com/KDEGroup/SUI-Attack.",
        "translated": ""
    },
    {
        "title": "Multi-event Video-Text Retrieval",
        "url": "http://arxiv.org/abs/2308.11551v1",
        "pub_date": "2023-08-22",
        "summary": "Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive\nvideo-text data on the Internet. A plethora of work characterized by using a\ntwo-stream Vision-Language model architecture that learns a joint\nrepresentation of video-text pairs has become a prominent approach for the VTR\ntask. However, these models operate under the assumption of bijective\nvideo-text correspondences and neglect a more practical scenario where video\ncontent usually encompasses multiple events, while texts like user queries or\nwebpage metadata tend to be specific and correspond to single events. This\nestablishes a gap between the previous training objective and real-world\napplications, leading to the potential performance degradation of earlier\nmodels during inference. In this study, we introduce the Multi-event Video-Text\nRetrieval (MeVTR) task, addressing scenarios in which each video contains\nmultiple different events, as a niche scenario of the conventional Video-Text\nRetrieval Task. We present a simple model, Me-Retriever, which incorporates key\nevent video representation and a new MeVTR loss for the MeVTR task.\nComprehensive experiments show that this straightforward framework outperforms\nother models in the Video-to-Text and Text-to-Video tasks, effectively\nestablishing a robust baseline for the MeVTR task. We believe this work serves\nas a strong foundation for future studies. Code is available at\nhttps://github.com/gengyuanmax/MeVTR.",
        "translated": ""
    },
    {
        "title": "L^2R: Lifelong Learning for First-stage Retrieval with\n  Backward-Compatible Representations",
        "url": "http://arxiv.org/abs/2308.11512v1",
        "pub_date": "2023-08-22",
        "summary": "First-stage retrieval is a critical task that aims to retrieve relevant\ndocument candidates from a large-scale collection. While existing retrieval\nmodels have achieved impressive performance, they are mostly studied on static\ndata sets, ignoring that in the real-world, the data on the Web is continuously\ngrowing with potential distribution drift. Consequently, retrievers trained on\nstatic old data may not suit new-coming data well and inevitably produce\nsub-optimal results. In this work, we study lifelong learning for first-stage\nretrieval, especially focusing on the setting where the emerging documents are\nunlabeled since relevance annotation is expensive and may not keep up with data\nemergence. Under this setting, we aim to develop model updating with two goals:\n(1) to effectively adapt to the evolving distribution with the unlabeled\nnew-coming data, and (2) to avoid re-inferring all embeddings of old documents\nto efficiently update the index each time the model is updated.\n  We first formalize the task and then propose a novel Lifelong Learning method\nfor the first-stage Retrieval, namely L^2R. L^2R adopts the typical memory\nmechanism for lifelong learning, and incorporates two crucial components: (1)\nselecting diverse support negatives for model training and memory updating for\neffective model adaptation, and (2) a ranking alignment objective to ensure the\nbackward-compatibility of representations to save the cost of index rebuilding\nwithout hurting the model performance. For evaluation, we construct two new\nbenchmarks from LoTTE and Multi-CPR datasets to simulate the document\ndistribution drift in realistic retrieval scenarios. Extensive experiments show\nthat L^2R significantly outperforms competitive lifelong learning baselines.",
        "translated": ""
    },
    {
        "title": "Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect\n  Dense Retrieval",
        "url": "http://arxiv.org/abs/2308.11474v1",
        "pub_date": "2023-08-22",
        "summary": "Grounded on pre-trained language models (PLMs), dense retrieval has been\nstudied extensively on plain text. In contrast, there has been little research\non retrieving data with multiple aspects using dense models. In the scenarios\nsuch as product search, the aspect information plays an essential role in\nrelevance matching, e.g., category: Electronics, Computers, and Pet Supplies. A\ncommon way of leveraging aspect information for multi-aspect retrieval is to\nintroduce an auxiliary classification objective, i.e., using item contents to\npredict the annotated value IDs of item aspects. However, by learning the value\nembeddings from scratch, this approach may not capture the various semantic\nsimilarities between the values sufficiently. To address this limitation, we\nleverage the aspect information as text strings rather than class IDs during\npre-training so that their semantic similarities can be naturally captured in\nthe PLMs. To facilitate effective retrieval with the aspect strings, we propose\nmutual prediction objectives between the text of the item aspect and content.\nIn this way, our model makes more sufficient use of aspect information than\nconducting undifferentiated masked language modeling (MLM) on the concatenated\ntext of aspects and content. Extensive experiments on two real-world datasets\n(product and mini-program search) show that our approach can outperform\ncompetitive baselines both treating aspect values as classes and conducting the\nsame MLM for aspect and content strings. Code and related dataset will be\navailable at the URL \\footnote{https://github.com/sunxiaojie99/ATTEMPT}.",
        "translated": ""
    },
    {
        "title": "On the Opportunities and Challenges of Offline Reinforcement Learning\n  for Recommender Systems",
        "url": "http://arxiv.org/abs/2308.11336v1",
        "pub_date": "2023-08-22",
        "summary": "Reinforcement learning serves as a potent tool for modeling dynamic user\ninterests within recommender systems, garnering increasing research attention\nof late. However, a significant drawback persists: its poor data efficiency,\nstemming from its interactive nature. The training of reinforcement\nlearning-based recommender systems demands expensive online interactions to\namass adequate trajectories, essential for agents to learn user preferences.\nThis inefficiency renders reinforcement learning-based recommender systems a\nformidable undertaking, necessitating the exploration of potential solutions.\nRecent strides in offline reinforcement learning present a new perspective.\nOffline reinforcement learning empowers agents to glean insights from offline\ndatasets and deploy learned policies in online settings. Given that recommender\nsystems possess extensive offline datasets, the framework of offline\nreinforcement learning aligns seamlessly. Despite being a burgeoning field,\nworks centered on recommender systems utilizing offline reinforcement learning\nremain limited. This survey aims to introduce and delve into offline\nreinforcement learning within recommender systems, offering an inclusive review\nof existing literature in this domain. Furthermore, we strive to underscore\nprevalent challenges, opportunities, and future pathways, poised to propel\nresearch in this evolving field.",
        "translated": ""
    },
    {
        "title": "Test Time Embedding Normalization for Popularity Bias Mitigation",
        "url": "http://arxiv.org/abs/2308.11288v1",
        "pub_date": "2023-08-22",
        "summary": "Popularity bias is a widespread problem in the field of recommender systems,\nwhere popular items tend to dominate recommendation results. In this work, we\npropose 'Test Time Embedding Normalization' as a simple yet effective strategy\nfor mitigating popularity bias, which surpasses the performance of the previous\nmitigation approaches by a significant margin. Our approach utilizes the\nnormalized item embedding during the inference stage to control the influence\nof embedding magnitude, which is highly correlated with item popularity.\nThrough extensive experiments, we show that our method combined with the\nsampled softmax loss effectively reduces popularity bias compare to previous\napproaches for bias mitigation. We further investigate the relationship between\nuser and item embeddings and find that the angular similarity between\nembeddings distinguishes preferable and non-preferable items regardless of\ntheir popularity. The analysis explains the mechanism behind the success of our\napproach in eliminating the impact of popularity bias. Our code is available at\nhttps://github.com/ml-postech/TTEN.",
        "translated": ""
    },
    {
        "title": "MISSRec: Pre-training and Transferring Multi-modal Interest-aware\n  Sequence Representation for Recommendation",
        "url": "http://arxiv.org/abs/2308.11175v1",
        "pub_date": "2023-08-22",
        "summary": "The goal of sequential recommendation (SR) is to predict a user's potential\ninterested items based on her/his historical interaction sequences. Most\nexisting sequential recommenders are developed based on ID features, which,\ndespite their widespread use, often underperform with sparse IDs and struggle\nwith the cold-start problem. Besides, inconsistent ID mappings hinder the\nmodel's transferability, isolating similar recommendation domains that could\nhave been co-optimized. This paper aims to address these issues by exploring\nthe potential of multi-modal information in learning robust and generalizable\nsequence representations. We propose MISSRec, a multi-modal pre-training and\ntransfer learning framework for SR. On the user side, we design a\nTransformer-based encoder-decoder model, where the contextual encoder learns to\ncapture the sequence-level multi-modal synergy while a novel interest-aware\ndecoder is developed to grasp item-modality-interest relations for better\nsequence representation. On the candidate item side, we adopt a dynamic fusion\nmodule to produce user-adaptive item representation, providing more precise\nmatching between users and items. We pre-train the model with contrastive\nlearning objectives and fine-tune it in an efficient manner. Extensive\nexperiments demonstrate the effectiveness and flexibility of MISSRec, promising\nan practical solution for real-world recommendation scenarios.",
        "translated": ""
    },
    {
        "title": "Towards Validating Long-Term User Feedbacks in Interactive\n  Recommendation Systems",
        "url": "http://arxiv.org/abs/2308.11137v1",
        "pub_date": "2023-08-22",
        "summary": "Interactive Recommender Systems (IRSs) have attracted a lot of attention, due\nto their ability to model interactive processes between users and recommender\nsystems. Numerous approaches have adopted Reinforcement Learning (RL)\nalgorithms, as these can directly maximize users' cumulative rewards. In IRS,\nresearchers commonly utilize publicly available review datasets to compare and\nevaluate algorithms. However, user feedback provided in public datasets merely\nincludes instant responses (e.g., a rating), with no inclusion of delayed\nresponses (e.g., the dwell time and the lifetime value). Thus, the question\nremains whether these review datasets are an appropriate choice to evaluate the\nlong-term effects of the IRS. In this work, we revisited experiments on IRS\nwith review datasets and compared RL-based models with a simple reward model\nthat greedily recommends the item with the highest one-step reward. Following\nextensive analysis, we can reveal three main findings: First, a simple greedy\nreward model consistently outperforms RL-based models in maximizing cumulative\nrewards. Second, applying higher weighting to long-term rewards leads to a\ndegradation of recommendation performance. Third, user feedbacks have mere\nlong-term effects on the benchmark datasets. Based on our findings, we conclude\nthat a dataset has to be carefully verified and that a simple greedy baseline\nshould be included for a proper evaluation of RL-based IRS approaches.",
        "translated": ""
    },
    {
        "title": "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential\n  Behavior Comprehension in Recommendation",
        "url": "http://arxiv.org/abs/2308.11131v1",
        "pub_date": "2023-08-22",
        "summary": "With large language models (LLMs) achieving remarkable breakthroughs in\nnatural language processing (NLP) domains, LLM-enhanced recommender systems\nhave received much attention and have been actively explored currently. In this\npaper, we focus on adapting and empowering a pure large language model for\nzero-shot and few-shot recommendation tasks. First and foremost, we identify\nand formulate the lifelong sequential behavior incomprehension problem for LLMs\nin recommendation domains, i.e., LLMs fail to extract useful information from a\ntextual context of long user behavior sequence, even if the length of context\nis far from reaching the context limitation of LLMs. To address such an issue\nand improve the recommendation performance of LLMs, we propose a novel\nframework, namely Retrieval-enhanced Large Language models (ReLLa) for\nrecommendation tasks in both zero-shot and few-shot settings. For zero-shot\nrecommendation, we perform semantic user behavior retrieval (SUBR) to improve\nthe data quality of testing samples, which greatly reduces the difficulty for\nLLMs to extract the essential knowledge from user behavior sequences. As for\nfew-shot recommendation, we further design retrieval-enhanced instruction\ntuning (ReiT) by adopting SUBR as a data augmentation technique for training\nsamples. Specifically, we develop a mixed training dataset consisting of both\nthe original data samples and their retrieval-enhanced counterparts. We conduct\nextensive experiments on a real-world public dataset (i.e., MovieLens-1M) to\ndemonstrate the superiority of ReLLa compared with existing baseline models, as\nwell as its capability for lifelong sequential behavior comprehension.",
        "translated": ""
    },
    {
        "title": "How Expressive are Graph Neural Networks in Recommendation?",
        "url": "http://arxiv.org/abs/2308.11127v1",
        "pub_date": "2023-08-22",
        "summary": "Graph Neural Networks (GNNs) have demonstrated superior performance on\nvarious graph learning tasks, including recommendation, where they leverage\nuser-item collaborative filtering signals in graphs. However, theoretical\nformulations of their capability are scarce, despite their empirical\neffectiveness in state-of-the-art recommender models. Recently, research has\nexplored the expressiveness of GNNs in general, demonstrating that message\npassing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that\nGNNs combined with random node initialization are universal. Nevertheless, the\nconcept of \"expressiveness\" for GNNs remains vaguely defined. Most existing\nworks adopt the graph isomorphism test as the metric of expressiveness, but\nthis graph-level task may not effectively assess a model's ability in\nrecommendation, where the objective is to distinguish nodes of different\ncloseness. In this paper, we provide a comprehensive theoretical analysis of\nthe expressiveness of GNNs in recommendation, considering three levels of\nexpressiveness metrics: graph isomorphism (graph-level), node automorphism\n(node-level), and topological closeness (link-level). We propose the\ntopological closeness metric to evaluate GNNs' ability to capture the\nstructural distance between nodes, which aligns closely with the objective of\nrecommendation. To validate the effectiveness of this new metric in evaluating\nrecommendation performance, we introduce a learning-less GNN algorithm that is\noptimal on the new metric and can be optimal on the node-level metric with\nsuitable modification. We conduct extensive experiments comparing the proposed\nalgorithm against various types of state-of-the-art GNN models to explore the\nexplainability of the new metric in the recommendation task. For\nreproducibility, implementation codes are available at\nhttps://github.com/HKUDS/GTE.",
        "translated": ""
    },
    {
        "title": "Anonymity at Risk? Assessing Re-Identification Capabilities of Large\n  Language Models",
        "url": "http://arxiv.org/abs/2308.11103v1",
        "pub_date": "2023-08-22",
        "summary": "Anonymity of both natural and legal persons in court rulings is a critical\naspect of privacy protection in the European Union and Switzerland. With the\nadvent of LLMs, concerns about large-scale re-identification of anonymized\npersons are growing. In accordance with the Federal Supreme Court of\nSwitzerland, we explore the potential of LLMs to re-identify individuals in\ncourt rulings by constructing a proof-of-concept using actual legal data from\nthe Swiss federal supreme court. Following the initial experiment, we\nconstructed an anonymized Wikipedia dataset as a more rigorous testing ground\nto further investigate the findings. With the introduction and application of\nthe new task of re-identifying people in texts, we also introduce new metrics\nto measure performance. We systematically analyze the factors that influence\nsuccessful re-identifications, identifying model size, input length, and\ninstruction tuning among the most critical determinants. Despite high\nre-identification rates on Wikipedia, even the best LLMs struggled with court\ndecisions. The complexity is attributed to the lack of test datasets, the\nnecessity for substantial training resources, and data sparsity in the\ninformation used for re-identification. In conclusion, this study demonstrates\nthat re-identification using LLMs may not be feasible for now, but as the\nproof-of-concept on Wikipedia showed, it might become possible in the future.\nWe hope that our system can help enhance the confidence in the security of\nanonymized decisions, thus leading to the courts being more confident to\npublish decisions.",
        "translated": ""
    },
    {
        "title": "Learning from Negative User Feedback and Measuring Responsiveness for\n  Sequential Recommenders",
        "url": "http://arxiv.org/abs/2308.12256v1",
        "pub_date": "2023-08-23",
        "summary": "Sequential recommenders have been widely used in industry due to their\nstrength in modeling user preferences. While these models excel at learning a\nuser's positive interests, less attention has been paid to learning from\nnegative user feedback. Negative user feedback is an important lever of user\ncontrol, and comes with an expectation that recommenders should respond quickly\nand reduce similar recommendations to the user. However, negative feedback\nsignals are often ignored in the training objective of sequential retrieval\nmodels, which primarily aim at predicting positive user interactions. In this\nwork, we incorporate explicit and implicit negative user feedback into the\ntraining objective of sequential recommenders in the retrieval stage using a\n\"not-to-recommend\" loss function that optimizes for the log-likelihood of not\nrecommending items with negative feedback. We demonstrate the effectiveness of\nthis approach using live experiments on a large-scale industrial recommender\nsystem. Furthermore, we address a challenge in measuring recommender\nresponsiveness to negative feedback by developing a counterfactual simulation\nframework to compare recommender responses between different user actions,\nshowing improved responsiveness from the modeling change.",
        "translated": ""
    },
    {
        "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task",
        "url": "http://arxiv.org/abs/2308.12241v1",
        "pub_date": "2023-08-23",
        "summary": "Recently, the fast development of Large Language Models (LLMs) such as\nChatGPT has significantly advanced NLP tasks by enhancing the capabilities of\nconversational models. However, the application of LLMs in the recommendation\ndomain has not been thoroughly investigated. To bridge this gap, we propose\nLLMRec, a LLM-based recommender system designed for benchmarking LLMs on\nvarious recommendation tasks. Specifically, we benchmark several popular\noff-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation\ntasks, including rating prediction, sequential recommendation, direct\nrecommendation, explanation generation, and review summarization. Furthermore,\nwe investigate the effectiveness of supervised finetuning to improve LLMs'\ninstruction compliance ability. The benchmark results indicate that LLMs\ndisplayed only moderate proficiency in accuracy-based tasks such as sequential\nand direct recommendation. However, they demonstrated comparable performance to\nstate-of-the-art methods in explainability-based tasks. We also conduct\nqualitative evaluations to further evaluate the quality of contents generated\nby different models, and the results show that LLMs can truly understand the\nprovided information and generate clearer and more reasonable results. We\naspire that this benchmark will serve as an inspiration for researchers to\ndelve deeper into the potential of LLMs in enhancing recommendation\nperformance. Our codes, processed data and benchmark results are available at\nhttps://github.com/williamliujl/LLMRec.",
        "translated": ""
    },
    {
        "title": "Counterfactual Graph Augmentation for Consumer Unfairness Mitigation in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2308.12083v1",
        "pub_date": "2023-08-23",
        "summary": "In recommendation literature, explainability and fairness are becoming two\nprominent perspectives to consider. However, prior works have mostly addressed\nthem separately, for instance by explaining to consumers why a certain item was\nrecommended or mitigating disparate impacts in recommendation utility. None of\nthem has leveraged explainability techniques to inform unfairness mitigation.\nIn this paper, we propose an approach that relies on counterfactual\nexplanations to augment the set of user-item interactions, such that using them\nwhile inferring recommendations leads to fairer outcomes. Modeling user-item\ninteractions as a bipartite graph, our approach augments the latter by\nidentifying new user-item edges that not only can explain the original\nunfairness by design, but can also mitigate it. Experiments on two public data\nsets show that our approach effectively leads to a better trade-off between\nfairness and recommendation utility compared with state-of-the-art mitigation\nprocedures. We further analyze the characteristics of added edges to highlight\nkey unfairness patterns. Source code available at\nhttps://github.com/jackmedda/RS-BGExplainer/tree/cikm2023.",
        "translated": ""
    },
    {
        "title": "Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep\n  Learning Track",
        "url": "http://arxiv.org/abs/2308.12039v1",
        "pub_date": "2023-08-23",
        "summary": "Large-scale text retrieval technology has been widely used in various\npractical business scenarios. This paper presents our systems for the TREC 2022\nDeep Learning Track. We explain the hybrid text retrieval and multi-stage text\nranking method adopted in our solution. The retrieval stage combined the two\nstructures of traditional sparse retrieval and neural dense retrieval. In the\nranking stage, in addition to the full interaction-based ranking model built on\nlarge pre-trained language model, we also proposes a lightweight sub-ranking\nmodule to further enhance the final text ranking performance. Evaluation\nresults demonstrate the effectiveness of our proposed approach. Our models\nachieve the 1st and 4th rank on the test set of passage ranking and document\nranking respectively.",
        "translated": ""
    },
    {
        "title": "LKPNR: LLM and KG for Personalized News Recommendation Framework",
        "url": "http://arxiv.org/abs/2308.12028v1",
        "pub_date": "2023-08-23",
        "summary": "Accurately recommending candidate news articles to users is a basic challenge\nfaced by personalized news recommendation systems. Traditional methods are\nusually difficult to grasp the complex semantic information in news texts,\nresulting in unsatisfactory recommendation results. Besides, these traditional\nmethods are more friendly to active users with rich historical behaviors.\nHowever, they can not effectively solve the \"long tail problem\" of inactive\nusers. To address these issues, this research presents a novel general\nframework that combines Large Language Models (LLM) and Knowledge Graphs (KG)\ninto semantic representations of traditional methods. In order to improve\nsemantic understanding in complex news texts, we use LLMs' powerful text\nunderstanding ability to generate news representations containing rich semantic\ninformation. In addition, our method combines the information about news\nentities and mines high-order structural information through multiple hops in\nKG, thus alleviating the challenge of long tail distribution. Experimental\nresults demonstrate that compared with various traditional models, the\nframework significantly improves the recommendation effect. The successful\nintegration of LLM and KG in our framework has established a feasible path for\nachieving more accurate personalized recommendations in the news field. Our\ncode is available at https://github.com/Xuan-ZW/LKPNR.",
        "translated": ""
    },
    {
        "title": "Economic Recommender Systems -- A Systematic Review",
        "url": "http://arxiv.org/abs/2308.11998v1",
        "pub_date": "2023-08-23",
        "summary": "Many of today's online services provide personalized recommendations to their\nusers. Such recommendations are typically designed to serve certain user needs,\ne.g., to quickly find relevant content in situations of information overload.\nCorrespondingly, the academic literature in the field largely focuses on the\nvalue of recommender systems for the end user. In this context, one underlying\nassumption is that the improved service that is achieved through the\nrecommendations will in turn positively impact the organization's goals, e.g.,\nin the form of higher customer retention or loyalty. However, in reality,\nrecommender systems can be used to target organizational economic goals more\ndirectly by incorporating monetary considerations such as price awareness and\nprofitability aspects into the underlying recommendation models. In this work,\nwe survey the existing literature on what we call Economic Recommender Systems\nbased on a systematic review approach that helped us identify 133 relevant\npapers. We first categorize existing works along different dimensions and then\nreview the most important technical approaches from the literature.\nFurthermore, we discuss common methodologies to evaluate such systems and\nfinally outline the limitations of today's research and future directions.",
        "translated": ""
    },
    {
        "title": "Integrating the Wikidata Taxonomy into YAGO",
        "url": "http://arxiv.org/abs/2308.11884v1",
        "pub_date": "2023-08-23",
        "summary": "Wikidata is one of the largest public general-purpose Knowledge Bases (KBs).\nYet, due to its collaborative nature, its schema and taxonomy have become\nconvoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from\nSchema.org, which reduced and cleaned up the taxonomy and constraints and made\nit possible to run automated reasoners on the data. However, it also cut away\nlarge parts of the Wikidata taxonomy. In this paper, we present our effort to\nmerge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay\nparticular attention to logical constraints and a careful distinction of\nclasses and instances. Our work creates YAGO 4.5, which adds a rich layer of\ninformative classes to YAGO, while at the same time keeping the KB logically\nconsistent.",
        "translated": ""
    },
    {
        "title": "CLIP Multi-modal Hashing: A new baseline CLIPMH",
        "url": "http://arxiv.org/abs/2308.11797v1",
        "pub_date": "2023-08-22",
        "summary": "The multi-modal hashing method is widely used in multimedia retrieval. It can\nfuse multi-source data to generate binary hash code. However, the current\nmulti-modal methods have the problem of low retrieval accuracy. The reason is\nthat the individual backbone networks have limited feature expression\ncapabilities and are not jointly pre-trained on large-scale unsupervised\nmulti-modal data. To solve this problem, we propose a new baseline CLIP\nMulti-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and\nimage features, and then fuse to generate hash code. CLIP improves the\nexpressiveness of each modal feature. In this way, it can greatly improve the\nretrieval performance of multi-modal hashing methods. In comparison to\nstate-of-the-art unsupervised and supervised multi-modal hashing methods,\nexperiments reveal that the proposed CLIPMH can significantly enhance\nperformance (Maximum increase of 8.38%). CLIP also has great advantages over\nthe text and visual backbone networks commonly used before.",
        "translated": ""
    },
    {
        "title": "Knowledge Graph Prompting for Multi-Document Question Answering",
        "url": "http://arxiv.org/abs/2308.11730v1",
        "pub_date": "2023-08-22",
        "summary": "The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has\nachieved remarkable success in open-domain question answering (OD-QA). However,\nfew works explore this paradigm in the scenario of multi-document question\nanswering (MD-QA), a task demanding a thorough understanding of the logical\nassociations among the contents and structures of different documents. To fill\nthis crucial gap, we propose a Knowledge Graph Prompting (KGP) method to\nformulate the right context in prompting LLMs for MD-QA, which consists of a\ngraph construction module and a graph traversal module. For graph construction,\nwe create a knowledge graph (KG) over multiple documents with nodes symbolizing\npassages or document structures (e.g., pages/tables), and edges denoting the\nsemantic/lexical similarity between passages or intra-document structural\nrelations. For graph traversal, we design an LM-guided graph traverser that\nnavigates across nodes and gathers supporting passages assisting LLMs in MD-QA.\nThe constructed graph serves as the global ruler that regulates the\ntransitional space among passages and reduces retrieval latency. Concurrently,\nthe LM-guided traverser acts as a local navigator that gathers pertinent\ncontext to progressively approach the question and guarantee retrieval quality.\nExtensive experiments underscore the efficacy of KGP for MD-QA, signifying the\npotential of leveraging graphs in enhancing the prompt design for LLMs. Our\ncode is at https://github.com/YuWVandy/KG-LLM-MDQA.",
        "translated": ""
    },
    {
        "title": "Invariant representation learning for sequential recommendation",
        "url": "http://arxiv.org/abs/2308.11728v1",
        "pub_date": "2023-08-22",
        "summary": "Sequential recommendation involves automatically recommending the next item\nto users based on their historical item sequence. While most prior research\nemploys RNN or transformer methods to glean information from the item\nsequence-generating probabilities for each user-item pair and recommending the\ntop items, these approaches often overlook the challenge posed by spurious\nrelationships. This paper specifically addresses these spurious relations. We\nintroduce a novel sequential recommendation framework named Irl4Rec. This\nframework harnesses invariant learning and employs a new objective that factors\nin the relationship between spurious variables and adjustment variables during\nmodel training. This approach aids in identifying spurious relations.\nComparative analyses reveal that our framework outperforms three typical\nmethods, underscoring the effectiveness of our model. Moreover, an ablation\nstudy further demonstrates the critical role our model plays in detecting\nspurious relations.",
        "translated": ""
    },
    {
        "title": "On Popularity Bias of Multimodal-aware Recommender Systems: a\n  Modalities-driven Analysis",
        "url": "http://arxiv.org/abs/2308.12911v1",
        "pub_date": "2023-08-24",
        "summary": "Multimodal-aware recommender systems (MRSs) exploit multimodal content (e.g.,\nproduct images or descriptions) as items' side information to improve\nrecommendation accuracy. While most of such methods rely on factorization\nmodels (e.g., MFBPR) as base architecture, it has been shown that MFBPR may be\naffected by popularity bias, meaning that it inherently tends to boost the\nrecommendation of popular (i.e., short-head) items at the detriment of niche\n(i.e., long-tail) items from the catalog. Motivated by this assumption, in this\nwork, we provide one of the first analyses on how multimodality in\nrecommendation could further amplify popularity bias. Concretely, we evaluate\nthe performance of four state-of-the-art MRSs algorithms (i.e., VBPR, MMGCN,\nGRCN, LATTICE) on three datasets from Amazon by assessing, along with\nrecommendation accuracy metrics, performance measures accounting for the\ndiversity of recommended items and the portion of retrieved niche items. To\nbetter investigate this aspect, we decide to study the separate influence of\neach modality (i.e., visual and textual) on popularity bias in different\nevaluation dimensions. Results, which demonstrate how the single modality may\naugment the negative effect of popularity bias, shed light on the importance to\nprovide a more rigorous analysis of the performance of such models.",
        "translated": ""
    },
    {
        "title": "Towards Communication-Efficient Model Updating for On-Device\n  Session-Based Recommendation",
        "url": "http://arxiv.org/abs/2308.12777v1",
        "pub_date": "2023-08-24",
        "summary": "On-device recommender systems recently have garnered increasing attention due\nto their advantages of providing prompt response and securing privacy. To stay\ncurrent with evolving user interests, cloud-based recommender systems are\nperiodically updated with new interaction data. However, on-device models\nstruggle to retrain themselves because of limited onboard computing resources.\nAs a solution, we consider the scenario where the model retraining occurs on\nthe server side and then the updated parameters are transferred to edge devices\nvia network communication. While this eliminates the need for local retraining,\nit incurs a regular transfer of parameters that significantly taxes network\nbandwidth. To mitigate this issue, we develop an efficient approach based on\ncompositional codes to compress the model update. This approach ensures the\non-device model is updated flexibly with minimal additional parameters whilst\nutilizing previous knowledge. The extensive experiments conducted on multiple\nsession-based recommendation models with distinctive architectures demonstrate\nthat the on-device model can achieve comparable accuracy to the retrained\nserver-side counterpart through transferring an update 60x smaller in size. The\ncodes are available at \\url{https://github.com/xiaxin1998/ODUpdate}.",
        "translated": ""
    },
    {
        "title": "On the Consistency of Average Embeddings for Item Recommendation",
        "url": "http://arxiv.org/abs/2308.12767v1",
        "pub_date": "2023-08-24",
        "summary": "A prevalent practice in recommender systems consists of averaging item\nembeddings to represent users or higher-level concepts in the same embedding\nspace. This paper investigates the relevance of such a practice. For this\npurpose, we propose an expected precision score, designed to measure the\nconsistency of an average embedding relative to the items used for its\nconstruction. We subsequently analyze the mathematical expression of this score\nin a theoretical setting with specific assumptions, as well as its empirical\nbehavior on real-world data from music streaming services. Our results\nemphasize that real-world averages are less consistent for recommendation,\nwhich paves the way for future research to better align real-world embeddings\nwith assumptions from our theoretical setting.",
        "translated": ""
    },
    {
        "title": "Video Recommendation Using Social Network Analysis and User Viewing\n  Patterns",
        "url": "http://arxiv.org/abs/2308.12743v1",
        "pub_date": "2023-08-24",
        "summary": "With the meteoric rise of video-on-demand (VOD) platforms, users face the\nchallenge of sifting through an expansive sea of content to uncover shows that\nclosely match their preferences. To address this information overload dilemma,\nVOD services have increasingly incorporated recommender systems powered by\nalgorithms that analyze user behavior and suggest personalized content.\nHowever, a majority of existing recommender systems depend on explicit user\nfeedback in the form of ratings and reviews, which can be difficult and\ntime-consuming to collect at scale. This presents a key research gap, as\nleveraging users' implicit feedback patterns could provide an alternative\navenue for building effective video recommendation models, circumventing the\nneed for explicit ratings. However, prior literature lacks sufficient\nexploration into implicit feedback-based recommender systems, especially in the\ncontext of modeling video viewing behavior. Therefore, this paper aims to\nbridge this research gap by proposing a novel video recommendation technique\nthat relies solely on users' implicit feedback in the form of their content\nviewing percentages.",
        "translated": ""
    },
    {
        "title": "Out of the Box Thinking: Improving Customer Lifetime Value Modelling via\n  Expert Routing and Game Whale Detection",
        "url": "http://arxiv.org/abs/2308.12729v1",
        "pub_date": "2023-08-24",
        "summary": "Customer lifetime value (LTV) prediction is essential for mobile game\npublishers trying to optimize the advertising investment for each user\nacquisition based on the estimated worth. In mobile games, deploying\nmicrotransactions is a simple yet effective monetization strategy, which\nattracts a tiny group of game whales who splurge on in-game purchases. The\npresence of such game whales may impede the practicality of existing LTV\nprediction models, since game whales' purchase behaviours always exhibit varied\ndistribution from general users. Consequently, identifying game whales can open\nup new opportunities to improve the accuracy of LTV prediction models. However,\nlittle attention has been paid to applying game whale detection in LTV\nprediction, and existing works are mainly specialized for the long-term LTV\nprediction with the assumption that the high-quality user features are\navailable, which is not applicable in the UA stage. In this paper, we propose\nExpLTV, a novel multi-task framework to perform LTV prediction and game whale\ndetection in a unified way. In ExpLTV, we first innovatively design a deep\nneural network-based game whale detector that can not only infer the intrinsic\norder in accordance with monetary value, but also precisely identify high\nspenders (i.e., game whales) and low spenders. Then, by treating the game whale\ndetector as a gating network to decide the different mixture patterns of LTV\nexperts assembling, we can thoroughly leverage the shared information and\nscenario-specific information (i.e., game whales modelling and low spenders\nmodelling). Finally, instead of separately designing a purchase rate estimator\nfor two tasks, we design a shared estimator that can preserve the inner task\nrelationships. The superiority of ExpLTV is further validated via extensive\nexperiments on three industrial datasets.",
        "translated": ""
    },
    {
        "title": "Laying foundations to quantify the \"Effort of Reproducibility\"",
        "url": "http://arxiv.org/abs/2308.12580v1",
        "pub_date": "2023-08-24",
        "summary": "Why are some research studies easy to reproduce while others are difficult?\nCasting doubt on the accuracy of scientific work is not fruitful, especially\nwhen an individual researcher cannot reproduce the claims made in the paper.\nThere could be many subjective reasons behind the inability to reproduce a\nscientific paper. The field of Machine Learning (ML) faces a reproducibility\ncrisis, and surveying a portion of published articles has resulted in a group\nrealization that although sharing code repositories would be appreciable, code\nbases are not the end all be all for determining the reproducibility of an\narticle. Various parties involved in the publication process have come forward\nto address the reproducibility crisis and solutions such as badging articles as\nreproducible, reproducibility checklists at conferences (\\textit{NeurIPS, ICML,\nICLR, etc.}), and sharing artifacts on \\textit{OpenReview} come across as\npromising solutions to the core problem. The breadth of literature on\nreproducibility focuses on measures required to avoid ir-reproducibility, and\nthere is not much research into the effort behind reproducing these articles.\nIn this paper, we investigate the factors that contribute to the easiness and\ndifficulty of reproducing previously published studies and report on the\nfoundational framework to quantify effort of reproducibility.",
        "translated": ""
    },
    {
        "title": "Exploring the Integration Strategies of Retriever and Large Language\n  Models",
        "url": "http://arxiv.org/abs/2308.12574v1",
        "pub_date": "2023-08-24",
        "summary": "The integration of retrieved passages and large language models (LLMs), such\nas ChatGPTs, has significantly contributed to improving open-domain question\nanswering. However, there is still a lack of exploration regarding the optimal\napproach for incorporating retrieved passages into the answer generation\nprocess. This paper aims to fill this gap by investigating different methods of\ncombining retrieved passages with LLMs to enhance answer generation. We begin\nby examining the limitations of a commonly-used concatenation approach.\nSurprisingly, this approach often results in generating \"unknown\" outputs, even\nwhen the correct document is among the top-k retrieved passages. To address\nthis issue, we explore four alternative strategies for integrating the\nretrieved passages with the LLMs. These strategies include two single-round\nmethods that utilize chain-of-thought reasoning and two multi-round strategies\nthat incorporate feedback loops. Through comprehensive analyses and\nexperiments, we provide insightful observations on how to effectively leverage\nretrieved passages to enhance the answer generation capability of LLMs.",
        "translated": ""
    },
    {
        "title": "Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature",
        "url": "http://arxiv.org/abs/2308.12420v1",
        "pub_date": "2023-08-23",
        "summary": "Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating\ncomprehensive insights into their diverse components. However, a systematic\nliterature review that emphasizes the Environmental, Sustainability, and\nGovernance (ESG) components of DLT remains lacking. To bridge this gap, we\nselected 107 seed papers to build a citation network of 63,083 references and\nrefined it to a corpus of 24,539 publications for analysis. Then, we labeled\nthe named entities in 46 papers according to twelve top-level categories\nderived from an established technology taxonomy and enhanced the taxonomy by\npinpointing DLT's ESG elements. Leveraging transformer-based language models,\nwe fine-tuned a pre-trained language model for a Named Entity Recognition (NER)\ntask using our labeled dataset. We used our fine-tuned language model to\ndistill the corpus to 505 key papers, facilitating a literature review via\nnamed entities and temporal graph analysis on DLT evolution in the context of\nESG. Our contributions are a methodology to conduct a machine learning-driven\nsystematic literature review in the DLT field, placing a special emphasis on\nESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed\nof 54,808 named entities, designed for DLT and ESG-related explorations.",
        "translated": ""
    },
    {
        "title": "On the Practicality of Dynamic Updates in Fast Searchable Encryption",
        "url": "http://arxiv.org/abs/2308.13486v1",
        "pub_date": "2023-08-25",
        "summary": "Searchable encrypted (SE) indexing systems are a useful tool for utilizing\ncloud services to store and manage sensitive information. However, much of the\nwork on SE systems to date has remained theoretical. In order to make them of\npractical use, more work is needed to develop optimal protocols and working\nmodels for them. This includes, in particular, the creation of a working update\nmodel in order to maintain an encrypted index of a dynamic document set such as\nan email inbox. I have created a working, real-world end-to-end SE\nimplementation that satisfies these needs, including the first empirical\nperformance evaluation of the dynamic SE update operation. In doing so, I show\na viable path to move from the theoretical concepts described by previous\nresearchers to a future production-worthy implementation and identify issues\nfor follow-on investigation.",
        "translated": ""
    },
    {
        "title": "Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability\n  of Language Models",
        "url": "http://arxiv.org/abs/2308.13467v1",
        "pub_date": "2023-08-25",
        "summary": "The Natural Language Processing(NLP) community has been using crowd sourcing\ntechniques to create benchmark datasets such as General Language Understanding\nand Evaluation(GLUE) for training modern Language Models such as BERT. GLUE\ntasks measure the reliability scores using inter annotator metrics i.e. Cohens\nKappa. However, the reliability aspect of LMs has often been overlooked. To\ncounter this problem, we explore a knowledge-guided LM ensembling approach that\nleverages reinforcement learning to integrate knowledge from ConceptNet and\nWikipedia as knowledge graph embeddings. This approach mimics human annotators\nresorting to external knowledge to compensate for information deficits in the\ndatasets. Across nine GLUE datasets, our research shows that ensembling\nstrengthens reliability and accuracy scores, outperforming state of the art.",
        "translated": ""
    },
    {
        "title": "A Bayesian Active Learning Approach to Comparative Judgement",
        "url": "http://arxiv.org/abs/2308.13292v1",
        "pub_date": "2023-08-25",
        "summary": "Assessment is a crucial part of education. Traditional marking is a source of\ninconsistencies and unconscious bias, placing a high cognitive load on the\nassessors. An approach to address these issues is comparative judgement (CJ).\nIn CJ, the assessor is presented with a pair of items and is asked to select\nthe better one. Following a series of comparisons, a rank is derived using a\nranking model, for example, the BTM, based on the results. While CJ is\nconsidered a reliable method for marking, there are concerns around\ntransparency, and the ideal number of pairwise comparisons to generate a\nreliable estimation of the rank order is not known. Additionally, there have\nbeen attempts to generate a method of selecting pairs that should be compared\nnext in an informative manner, but some existing methods are known to have\ncreated their own bias within results inflating the reliability metric used. As\na result, a random selection approach is usually deployed.\n  We propose a novel Bayesian approach to CJ (BCJ) for determining the ranks of\ncompared items alongside a new way to select the pairs to present to the\nmarker(s) using active learning (AL), addressing the key shortcomings of\ntraditional CJ. Furthermore, we demonstrate how the entire approach may provide\ntransparency by providing the user insights into how it is making its decisions\nand, at the same time, being more efficient. Results from our experiments\nconfirm that the proposed BCJ combined with entropy-driven AL pair-selection\nmethod is superior to other alternatives. We also find that the more\ncomparisons done, the more accurate BCJ becomes, which solves the issue the\ncurrent method has of the model deteriorating if too many comparisons are\nperformed. As our approach can generate the complete predicted rank\ndistribution for an item, we also show how this can be utilised in devising a\npredicted grade, guided by the assessor.",
        "translated": ""
    },
    {
        "title": "Learning and Optimization of Implicit Negative Feedback for Industrial\n  Short-video Recommender System",
        "url": "http://arxiv.org/abs/2308.13249v1",
        "pub_date": "2023-08-25",
        "summary": "Short-video recommendation is one of the most important recommendation\napplications in today's industrial information systems. Compared with other\nrecommendation tasks, the enormous amount of feedback is the most typical\ncharacteristic. Specifically, in short-video recommendation, the\neasiest-to-collect user feedback is from the skipping behaviors, which leads to\ntwo critical challenges for the recommendation model. First, the skipping\nbehavior reflects implicit user preferences, and thus it is challenging for\ninterest extraction. Second, the kind of special feedback involves multiple\nobjectives, such as total watching time, which is also very challenging. In\nthis paper, we present our industrial solution in Kuaishou, which serves\nbillion-level users every day. Specifically, we deploy a feedback-aware\nencoding module which well extracts user preference taking the impact of\ncontext into consideration. We further design a multi-objective prediction\nmodule which well distinguishes the relation and differences among different\nmodel objectives in the short-video recommendation. We conduct extensive online\nA/B testing, along with detailed and careful analysis, which verifies the\neffectiveness of our solution.",
        "translated": ""
    },
    {
        "title": "Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and\n  Ex-Post Fairness",
        "url": "http://arxiv.org/abs/2308.13242v1",
        "pub_date": "2023-08-25",
        "summary": "In learning-to-rank (LTR), optimizing only the relevance (or the expected\nranking utility) can cause representational harm to certain categories of\nitems. Moreover, if there is implicit bias in the relevance scores, LTR models\nmay fail to optimize for true relevance. Previous works have proposed efficient\nalgorithms to train stochastic ranking models that achieve fairness of exposure\nto the groups ex-ante (or, in expectation), which may not guarantee\nrepresentation fairness to the groups ex-post, that is, after realizing a\nranking from the stochastic ranking model. Typically, ex-post fairness is\nachieved by post-processing, but previous work does not train stochastic\nranking models that are aware of this post-processing.\n  In this paper, we propose a novel objective that maximizes expected relevance\nonly over those rankings that satisfy given representation constraints to\nensure ex-post fairness. Building upon recent work on an efficient sampler for\nex-post group-fair rankings, we propose a group-fair Plackett-Luce model and\nshow that it can be efficiently optimized for our objective in the LTR\nframework.\n  Experiments on three real-world datasets show that our group-fair algorithm\nguarantees fairness alongside usually having better relevance compared to the\nLTR baselines. In addition, our algorithm also achieves better relevance than\npost-processing baselines, which also ensures ex-post fairness. Further, when\nimplicit bias is injected into the training data, our algorithm typically\noutperforms existing LTR baselines in relevance.",
        "translated": ""
    },
    {
        "title": "MMBAttn: Max-Mean and Bit-wise Attention for CTR Prediction",
        "url": "http://arxiv.org/abs/2308.13187v1",
        "pub_date": "2023-08-25",
        "summary": "With the increasing complexity and scale of click-through rate (CTR)\nprediction tasks in online advertising and recommendation systems, accurately\nestimating the importance of features has become a critical aspect of\ndeveloping effective models. In this paper, we propose an attention-based\napproach that leverages max and mean pooling operations, along with a bit-wise\nattention mechanism, to enhance feature importance estimation in CTR\nprediction. Traditionally, pooling operations such as max and mean pooling have\nbeen widely used to extract relevant information from features. However, these\noperations can lead to information loss and hinder the accurate determination\nof feature importance. To address this challenge, we propose a novel attention\narchitecture that utilizes a bit-based attention structure that emphasizes the\nrelationships between all bits in features, together with maximum and mean\npooling. By considering the fine-grained interactions at the bit level, our\nmethod aims to capture intricate patterns and dependencies that might be\noverlooked by traditional pooling operations. To examine the effectiveness of\nthe proposed method, experiments have been conducted on three public datasets.\nThe experiments demonstrated that the proposed method significantly improves\nthe performance of the base models to achieve state-of-the-art results.",
        "translated": ""
    },
    {
        "title": "Multi-BERT for Embeddings for Recommendation System",
        "url": "http://arxiv.org/abs/2308.13050v1",
        "pub_date": "2023-08-24",
        "summary": "In this paper, we propose a novel approach for generating document embeddings\nusing a combination of Sentence-BERT (SBERT) and RoBERTa, two state-of-the-art\nnatural language processing models. Our approach treats sentences as tokens and\ngenerates embeddings for them, allowing the model to capture both\nintra-sentence and inter-sentence relations within a document. We evaluate our\nmodel on a book recommendation task and demonstrate its effectiveness in\ngenerating more semantically rich and accurate document embeddings. To assess\nthe performance of our approach, we conducted experiments on a book\nrecommendation task using the Goodreads dataset. We compared the document\nembeddings generated using our MULTI-BERT model to those generated using SBERT\nalone. We used precision as our evaluation metric to compare the quality of the\ngenerated embeddings. Our results showed that our model consistently\noutperformed SBERT in terms of the quality of the generated embeddings.\nFurthermore, we found that our model was able to capture more nuanced semantic\nrelations within documents, leading to more accurate recommendations. Overall,\nour results demonstrate the effectiveness of our approach and suggest that it\nis a promising direction for improving the performance of recommendation\nsystems",
        "translated": ""
    },
    {
        "title": "Financial News Analytics Using Fine-Tuned Llama 2 GPT Model",
        "url": "http://arxiv.org/abs/2308.13032v1",
        "pub_date": "2023-08-24",
        "summary": "The paper considers the possibility to fine-tune Llama 2 Large Language Model\n(LLM) for the multitask analysis of financial news. For fine-tuning, the\nPEFT/LoRA based approach was used. In the study, the model was fine-tuned for\nthe following tasks: analysing a text from financial market perspectives,\nhighlighting main points of a text, summarizing a text and extracting named\nentities with appropriate sentiments. The obtained results show that the\nfine-tuned Llama 2 model can perform a multitask financial news analysis with a\nspecified structure of response, part of response can be a structured text and\nanother part of data can have JSON format for further processing. Extracted\nsentiments for named entities can be considered as predictive features in\nsupervised machine learning models with quantitative target variables.",
        "translated": ""
    },
    {
        "title": "TRIVEA: Transparent Ranking Interpretation using Visual Explanation of\n  Black-Box Algorithmic Rankers",
        "url": "http://arxiv.org/abs/2308.14622v1",
        "pub_date": "2023-08-28",
        "summary": "Ranking schemes drive many real-world decisions, like, where to study, whom\nto hire, what to buy, etc. Many of these decisions often come with high\nconsequences. For example, a university can be deemed less prestigious if not\nfeatured in a top-k list, and consumers might not even explore products that do\nnot get recommended to buyers. At the heart of most of these decisions are\nopaque ranking schemes, which dictate the ordering of data entities, but their\ninternal logic is inaccessible or proprietary. Drawing inferences about the\nranking differences is like a guessing game to the stakeholders, like, the\nrankees (i.e., the entities who are ranked, like product companies) and the\ndecision-makers (i.e., who use the rankings, like buyers). In this paper, we\naim to enable transparency in ranking interpretation by using algorithmic\nrankers that learn from available data and by enabling human reasoning about\nthe learned ranking differences using explainable AI (XAI) methods. To realize\nthis aim, we leverage the exploration-explanation paradigm of human-data\ninteraction to let human stakeholders explore subsets and groupings of complex\nmulti-attribute ranking data using visual explanations of model fit and\nattribute influence on rankings. We realize this explanation paradigm for\ntransparent ranking interpretation in TRIVEA, a visual analytic system that is\nfueled by: i) visualizations of model fit derived from algorithmic rankers that\nlearn the associations between attributes and rankings from available data and\nii) visual explanations derived from XAI methods that help abstract important\npatterns, like, the relative influence of attributes in different ranking\nranges. Using TRIVEA, end users not trained in data science have the agency to\ntransparently reason about the global and local behavior of the rankings\nwithout the need to open black-box ranking models and develop confidence in the\nresulting attribute-based inferences. We demonstrate the efficacy of TRIVEA\nusing multiple usage scenarios and subjective feedback from researchers with\ndiverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank,\nExplainable ML, Ranking",
        "translated": ""
    },
    {
        "title": "Fairness Through Domain Awareness: Mitigating Popularity Bias For Music\n  Discovery",
        "url": "http://arxiv.org/abs/2308.14601v1",
        "pub_date": "2023-08-28",
        "summary": "As online music platforms grow, music recommender systems play a vital role\nin helping users navigate and discover content within their vast musical\ndatabases. At odds with this larger goal, is the presence of popularity bias,\nwhich causes algorithmic systems to favor mainstream content over, potentially\nmore relevant, but niche items. In this work we explore the intrinsic\nrelationship between music discovery and popularity bias. To mitigate this\nissue we propose a domain-aware, individual fairness-based approach which\naddresses popularity bias in graph neural network (GNNs) based recommender\nsystems. Our approach uses individual fairness to reflect a ground truth\nlistening experience, i.e., if two songs sound similar, this similarity should\nbe reflected in their representations. In doing so, we facilitate meaningful\nmusic discovery that is robust to popularity bias and grounded in the music\ndomain. We apply our BOOST methodology to two discovery based tasks, performing\nrecommendations at both the playlist level and user level. Then, we ground our\nevaluation in the cold start setting, showing that our approach outperforms\nexisting fairness benchmarks in both performance and recommendation of\nlesser-known content. Finally, our analysis explains why our proposed\nmethodology is a novel and promising approach to mitigating popularity bias and\nimproving the discovery of new and niche content in music recommender systems.",
        "translated": ""
    },
    {
        "title": "Efficient and Accurate Tree Detection from 3D Point Clouds through Paid\n  Crowdsourcing",
        "url": "http://arxiv.org/abs/2308.14499v1",
        "pub_date": "2023-08-28",
        "summary": "Accurate tree detection is of growing importance in applications such as\nurban planning, forest inventory, and environmental monitoring. In this\narticle, we present an approach to creating tree maps by annotating them in 3D\npoint clouds. Point cloud representations allow the precise identification of\ntree positions, particularly stem locations, and their heights. Our method\nleverages human computational power through paid crowdsourcing, employing a web\ntool designed to enable even non-experts to effectively tackle the task. The\nprimary focus of this paper is to discuss the web tool's development and\nstrategies to ensure high-quality tree annotations despite encountering noise\nin the crowdsourced data. Following our methodology, we achieve quality\nmeasures surpassing 90% for various challenging test sets of diverse\ncomplexities. We emphasize that our tree map creation process, including\ninitial point cloud collection, can be completed within 1-2 days.",
        "translated": ""
    },
    {
        "title": "Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware\n  Pre-training for KBQA",
        "url": "http://arxiv.org/abs/2308.14436v1",
        "pub_date": "2023-08-28",
        "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with factual information such as entities and relations in KBs.\nHowever, traditional Pre-trained Language Models (PLMs) are directly\npre-trained on large-scale natural language corpus, which poses challenges for\nthem in understanding and representing complex subgraphs in structured KBs. To\nbridge the gap between texts and structured KBs, we propose a Structured\nKnowledge-aware Pre-training method (SKP). In the pre-training stage, we\nintroduce two novel structured knowledge-aware tasks, guiding the model to\neffectively learn the implicit relationship and better representations of\ncomplex subgraphs. In downstream KBQA task, we further design an efficient\nlinearization strategy and an interval attention mechanism, which assist the\nmodel to better encode complex subgraphs and shield the interference of\nirrelevant subgraphs during reasoning respectively. Detailed experiments and\nanalyses on WebQSP verify the effectiveness of SKP, especially the significant\nimprovement in subgraph retrieval (+4.08% H@10).",
        "translated": ""
    },
    {
        "title": "Can Transformer and GNN Help Each Other?",
        "url": "http://arxiv.org/abs/2308.14355v1",
        "pub_date": "2023-08-28",
        "summary": "Although Transformer has achieved great success in natural language process\nand computer vision, it has difficulty generalizing to medium and large-scale\ngraph data for two important reasons: (i) High complexity. (ii) Failing to\ncapture the complex and entangled structure information. In graph\nrepresentation learning, Graph Neural Networks(GNNs) can fuse the graph\nstructure and node attributes but have limited receptive fields. Therefore, we\nquestion whether can we combine Transformers and GNNs to help each other. In\nthis paper, we propose a new model named TransGNN where the Transformer layer\nand GNN layer are used alternately to improve each other. Specifically, to\nexpand the receptive field and disentangle the information aggregation from\nedges, we propose using Transformer to aggregate more relevant nodes'\ninformation to improve the message passing of GNNs. Besides, to capture the\ngraph structure information, we utilize positional encoding and make use of the\nGNN layer to fuse the structure into node attributes, which improves the\nTransformer in graph data. We also propose to sample the most relevant nodes\nfor Transformer and two efficient samples update strategies to lower the\ncomplexity. At last, we theoretically prove that TransGNN is more expressive\nthan GNNs only with extra linear complexity. The experiments on eight datasets\ncorroborate the effectiveness of TransGNN on node and graph classification\ntasks.",
        "translated": ""
    },
    {
        "title": "RecMind: Large Language Model Powered Agent For Recommendation",
        "url": "http://arxiv.org/abs/2308.14296v1",
        "pub_date": "2023-08-28",
        "summary": "Recent advancements in instructing Large Language Models (LLMs) to utilize\nexternal tools and execute multi-step plans have significantly enhanced their\nability to solve intricate tasks, ranging from mathematical problems to\ncreative writing. Yet, there remains a notable gap in studying the capacity of\nLLMs in responding to personalized queries such as a recommendation request. To\nbridge this gap, we have designed an LLM-powered autonomous recommender agent,\nRecMind, which is capable of providing precise personalized recommendations\nthrough careful planning, utilizing tools for obtaining external knowledge, and\nleveraging individual data. We propose a novel algorithm, Self-Inspiring, to\nimprove the planning ability of the LLM agent. At each intermediate planning\nstep, the LLM 'self-inspires' to consider all previously explored states to\nplan for next step. This mechanism greatly improves the model's ability to\ncomprehend and utilize historical planning information for recommendation. We\nevaluate RecMind's performance in various recommendation scenarios, including\nrating prediction, sequential recommendation, direct recommendation,\nexplanation generation, and review summarization. Our experiment shows that\nRecMind outperforms existing zero/few-shot LLM-based recommendation methods in\ndifferent recommendation tasks and achieves competitive performance to a recent\nmodel P5, which requires fully pre-train for the recommendation tasks.",
        "translated": ""
    },
    {
        "title": "Alleviating Video-Length Effect for Micro-video Recommendation",
        "url": "http://arxiv.org/abs/2308.14276v1",
        "pub_date": "2023-08-28",
        "summary": "Micro-videos platforms such as TikTok are extremely popular nowadays. One\nimportant feature is that users no longer select interested videos from a set,\ninstead they either watch the recommended video or skip to the next one. As a\nresult, the time length of users' watching behavior becomes the most important\nsignal for identifying preferences. However, our empirical data analysis has\nshown a video-length effect that long videos are easier to receive a higher\nvalue of average view time, thus adopting such view-time labels for measuring\nuser preferences can easily induce a biased model that favors the longer\nvideos. In this paper, we propose a Video Length Debiasing Recommendation\n(VLDRec) method to alleviate such an effect for micro-video recommendation.\nVLDRec designs the data labeling approach and the sample generation module that\nbetter capture user preferences in a view-time oriented manner. It further\nleverages the multi-task learning technique to jointly optimize the above\nsamples with original biased ones. Extensive experiments show that VLDRec can\nimprove the users' view time by 1.81% and 11.32% on two real-world datasets,\ngiven a recommendation list of a fixed overall video length, compared with the\nbest baseline method. Moreover, VLDRec is also more effective in matching\nusers' interests in terms of the video content.",
        "translated": ""
    },
    {
        "title": "Cross-Modal Retrieval: A Systematic Review of Methods and Future\n  Directions",
        "url": "http://arxiv.org/abs/2308.14263v1",
        "pub_date": "2023-08-28",
        "summary": "With the exponential surge in diverse multi-modal data, traditional uni-modal\nretrieval methods struggle to meet the needs of users demanding access to data\nfrom various modalities. To address this, cross-modal retrieval has emerged,\nenabling interaction across modalities, facilitating semantic matching, and\nleveraging complementarity and consistency between different modal data.\nAlthough prior literature undertook a review of the cross-modal retrieval\nfield, it exhibits numerous deficiencies pertaining to timeliness, taxonomy,\nand comprehensiveness. This paper conducts a comprehensive review of\ncross-modal retrieval's evolution, spanning from shallow statistical analysis\ntechniques to vision-language pre-training models. Commencing with a\ncomprehensive taxonomy grounded in machine learning paradigms, mechanisms, and\nmodels, the paper then delves deeply into the principles and architectures\nunderpinning existing cross-modal retrieval methods. Furthermore, it offers an\noverview of widely used benchmarks, metrics, and performances. Lastly, the\npaper probes the prospects and challenges that confront contemporary\ncross-modal retrieval, while engaging in a discourse on potential directions\nfor further progress in the field. To facilitate the research on cross-modal\nretrieval, we develop an open-source code repository at\nhttps://github.com/BMC-SDNU/Cross-Modal-Retrieval.",
        "translated": ""
    },
    {
        "title": "Distributional Off-Policy Evaluation for Slate Recommendations",
        "url": "http://arxiv.org/abs/2308.14165v1",
        "pub_date": "2023-08-27",
        "summary": "Recommendation strategies are typically evaluated by using previously logged\ndata, employing off-policy evaluation methods to estimate their expected\nperformance. However, for strategies that present users with slates of multiple\nitems, the resulting combinatorial action space renders many of these methods\nimpractical. Prior work has developed estimators that leverage the structure in\nslates to estimate the expected off-policy performance, but the estimation of\nthe entire performance distribution remains elusive. Estimating the complete\ndistribution allows for a more comprehensive evaluation of recommendation\nstrategies, particularly along the axes of risk and fairness that employ\nmetrics computable from the distribution. In this paper, we propose an\nestimator for the complete off-policy performance distribution for slates and\nestablish conditions under which the estimator is unbiased and consistent. This\nbuilds upon prior work on off-policy evaluation for slates and off-policy\ndistribution estimation in reinforcement learning. We validate the efficacy of\nour method empirically on synthetic data as well as on a slate recommendation\nsimulator constructed from real-world data (MovieLens-20M). Our results show a\nsignificant reduction in estimation variance and improved sample efficiency\nover prior work across a range of slate structures.",
        "translated": ""
    },
    {
        "title": "Only Encode Once: Making Content-based News Recommender Greener",
        "url": "http://arxiv.org/abs/2308.14155v1",
        "pub_date": "2023-08-27",
        "summary": "Large pretrained language models (PLM) have become de facto news encoders in\nmodern news recommender systems, due to their strong ability in comprehending\ntextual content. These huge Transformer-based architectures, when finetuned on\nrecommendation tasks, can greatly improve news recommendation performance.\nHowever, the PLM-based pretrain-finetune framework incurs high computational\ncost and energy consumption, primarily due to the extensive redundant\nprocessing of news encoding during each training epoch. In this paper, we\npropose the ``Only Encode Once'' framework for news recommendation (OLEO), by\ndecoupling news representation learning from downstream recommendation task\nlearning. The decoupled design makes content-based news recommender as green\nand efficient as id-based ones, leading to great reduction in computational\ncost and training resources. Extensive experiments show that our OLEO framework\ncan reduce carbon emissions by up to 13 times compared with the\nstate-of-the-art pretrain-finetune framework and maintain a competitive or even\nsuperior performance level. The source code is released for reproducibility.",
        "translated": ""
    },
    {
        "title": "Robust Long-Tailed Learning via Label-Aware Bounded CVaR",
        "url": "http://arxiv.org/abs/2308.15405v1",
        "pub_date": "2023-08-29",
        "summary": "Data in the real-world classification problems are always imbalanced or\nlong-tailed, wherein the majority classes have the most of the samples that\ndominate the model training. In such setting, the naive model tends to have\npoor performance on the minority classes. Previously, a variety of loss\nmodifications have been proposed to address the long-tailed leaning problem,\nwhile these methods either treat the samples in the same class\nindiscriminatingly or lack a theoretical guarantee. In this paper, we propose\ntwo novel approaches based on CVaR (Conditional Value at Risk) to improve the\nperformance of long-tailed learning with a solid theoretical ground.\nSpecifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss\nto overcome the pessimistic result of the original CVaR, and further design the\noptimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we\nadditionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to\nstabilize the optimization process, where we also offer the theoretical\nsupport. Extensive experiments on real-world datasets with long-tailed label\ndistributions verify the superiority of our proposed methods.",
        "translated": ""
    },
    {
        "title": "A Multi-Perspective Learning to Rank Approach to Support Children's\n  Information Seeking in the Classroom",
        "url": "http://arxiv.org/abs/2308.15265v1",
        "pub_date": "2023-08-29",
        "summary": "We introduce a novel re-ranking model that aims to augment the functionality\nof standard search engines to support classroom search activities for children\n(ages 6 to 11). This model extends the known listwise learning-to-rank\nframework by balancing risk and reward. Doing so enables the model to\nprioritize Web resources of high educational alignment, appropriateness, and\nadequate readability by analyzing the URLs, snippets, and page titles of Web\nresources retrieved by a given mainstream search engine. Experimental results,\nincluding an ablation study and comparisons with existing baselines, showcase\nthe correctness of the proposed model. The outcomes of this work demonstrate\nthe value of considering multiple perspectives inherent to the classroom\nsetting, e.g., educational alignment, readability, and objectionability, when\napplied to the design of algorithms that can better support children's\ninformation discovery.",
        "translated": ""
    },
    {
        "title": "Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation",
        "url": "http://arxiv.org/abs/2308.15244v1",
        "pub_date": "2023-08-29",
        "summary": "Since Knowledge Graphs (KGs) contain rich semantic information, recently\nthere has been an influx of KG-enhanced recommendation methods. Most of\nexisting methods are entirely designed based on euclidean space without\nconsidering curvature. However, recent studies have revealed that a tremendous\ngraph-structured data exhibits highly non-euclidean properties. Motivated by\nthese observations, in this work, we propose a knowledge-based multiple\nadaptive spaces fusion method for recommendation, namely MCKG. Unlike existing\nmethods that solely adopt a specific manifold, we introduce the unified space\nthat is compatible with hyperbolic, euclidean and spherical spaces.\nFurthermore, we fuse the multiple unified spaces in an attention manner to\nobtain the high-quality embeddings for better knowledge propagation. In\naddition, we propose a geometry-aware optimization strategy which enables the\npull and push processes benefited from both hyperbolic and spherical spaces.\nSpecifically, in hyperbolic space, we set smaller margins in the area near to\nthe origin, which is conducive to distinguishing between highly similar\npositive items and negative ones. At the same time, we set larger margins in\nthe area far from the origin to ensure the model has sufficient error\ntolerance. The similar manner also applies to spherical spaces. Extensive\nexperiments on three real-world datasets demonstrate that the MCKG has a\nsignificant improvement over state-of-the-art recommendation methods. Further\nablation experiments verify the importance of multi-space fusion and\ngeometry-aware optimization strategy, justifying the rationality and\neffectiveness of MCKG.",
        "translated": ""
    },
    {
        "title": "Classification-Aware Neural Topic Model Combined With Interpretable\n  Analysis -- For Conflict Classification",
        "url": "http://arxiv.org/abs/2308.15232v1",
        "pub_date": "2023-08-29",
        "summary": "A large number of conflict events are affecting the world all the time. In\norder to analyse such conflict events effectively, this paper presents a\nClassification-Aware Neural Topic Model (CANTM-IA) for Conflict Information\nClassification and Topic Discovery. The model provides a reliable\ninterpretation of classification results and discovered topics by introducing\ninterpretability analysis. At the same time, interpretation is introduced into\nthe model architecture to improve the classification performance of the model\nand to allow interpretation to focus further on the details of the data.\nFinally, the model architecture is optimised to reduce the complexity of the\nmodel.",
        "translated": ""
    },
    {
        "title": "Providing Previously Unseen Users Fair Recommendations Using Variational\n  Autoencoders",
        "url": "http://arxiv.org/abs/2308.15230v1",
        "pub_date": "2023-08-29",
        "summary": "An emerging definition of fairness in machine learning requires that models\nare oblivious to demographic user information, e.g., a user's gender or age\nshould not influence the model. Personalized recommender systems are\nparticularly prone to violating this definition through their explicit user\nfocus and user modelling. Explicit user modelling is also an aspect that makes\nmany recommender systems incapable of providing hitherto unseen users with\nrecommendations. We propose novel approaches for mitigating discrimination in\nVariational Autoencoder-based recommender systems by limiting the encoding of\ndemographic information. The approaches are capable of, and evaluated on,\nproviding users that are not represented in the training data with fair\nrecommendations.",
        "translated": ""
    },
    {
        "title": "CAGRA: Highly Parallel Graph Construction and Approximate Nearest\n  Neighbor Search for GPUs",
        "url": "http://arxiv.org/abs/2308.15136v1",
        "pub_date": "2023-08-29",
        "summary": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in various\ndisciplines spanning data mining and artificial intelligence, from information\nretrieval and computer vision to natural language processing and recommender\nsystems. Data volumes have soared in recent years and the computational cost of\nan exhaustive exact nearest neighbor search is often prohibitive, necessitating\nthe adoption of approximate techniques. The balanced performance and recall of\ngraph-based approaches have more recently garnered significant attention in\nANNS algorithms, however, only a few studies have explored harnessing the power\nof GPUs and multi-core processors despite the widespread use of massively\nparallel and general-purpose computing. To bridge this gap, we introduce a\nnovel parallel computing hardware-based proximity graph and search algorithm.\nBy leveraging the high-performance capabilities of modern hardware, our\napproach achieves remarkable efficiency gains. In particular, our method\nsurpasses existing CPU and GPU-based methods in constructing the proximity\ngraph, demonstrating higher throughput in both large- and small-batch searches\nwhile maintaining compatible accuracy. In graph construction time, our method,\nCAGRA, is 2.2~27x faster than HNSW, which is one of the CPU SOTA\nimplementations. In large-batch query throughput in the 90% to 95% recall\nrange, our method is 33~77x faster than HNSW, and is 3.8~8.8x faster than the\nSOTA implementations for GPU. For a single query, our method is 3.4~53x faster\nthan HNSW at 95% recall.",
        "translated": ""
    },
    {
        "title": "Killing two birds with one stone: Can an audio captioning system also be\n  used for audio-text retrieval?",
        "url": "http://arxiv.org/abs/2308.15090v1",
        "pub_date": "2023-08-29",
        "summary": "Automated Audio Captioning (AAC) aims to develop systems capable of\ndescribing an audio recording using a textual sentence. In contrast, Audio-Text\nRetrieval (ATR) systems seek to find the best matching audio recording(s) for a\ngiven textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks\nrequire different types of systems: AAC employs a sequence-to-sequence model,\nwhile ATR utilizes a ranking model that compares audio and text representations\nwithin a shared projection subspace. However, this work investigates the\nrelationship between AAC and ATR by exploring the ATR capabilities of an\nunmodified AAC system, without fine-tuning for the new task. Our AAC system\nconsists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio\ntagging, and a transformer decoder responsible for generating sentences. For\nAAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on\nAudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss\nvalues obtained for any audio/caption pair. Experimental results on the Clotho\nand AudioCaps datasets demonstrate decent recall values using this simple\napproach. For instance, we obtained a Text-to-Audio R@1 value of 0.382 for\nAu-dioCaps, which is above the current state-of-the-art method without external\ndata. Interestingly, we observe that normalizing the loss values was necessary\nfor Audio-to-Text retrieval.",
        "translated": ""
    },
    {
        "title": "STEC: See-Through Transformer-based Encoder for CTR Prediction",
        "url": "http://arxiv.org/abs/2308.15033v1",
        "pub_date": "2023-08-29",
        "summary": "Click-Through Rate (CTR) prediction holds a pivotal place in online\nadvertising and recommender systems since CTR prediction performance directly\ninfluences the overall satisfaction of the users and the revenue generated by\ncompanies. Even so, CTR prediction is still an active area of research since it\ninvolves accurately modelling the preferences of users based on sparse and\nhigh-dimensional features where the higher-order interactions of multiple\nfeatures can lead to different outcomes. Most CTR prediction models have relied\non a single fusion and interaction learning strategy. The few CTR prediction\nmodels that have utilized multiple interaction modelling strategies have\ntreated each interaction to be self-contained. In this paper, we propose a\nnovel model named STEC that reaps the benefits of multiple interaction learning\napproaches in a single unified architecture. Additionally, our model introduces\nresidual connections from different orders of interactions which boosts the\nperformance by allowing lower level interactions to directly affect the\npredictions. Through extensive experiments on four real-world datasets, we\ndemonstrate that STEC outperforms existing state-of-the-art approaches for CTR\nprediction thanks to its greater expressive capabilities.",
        "translated": ""
    },
    {
        "title": "Improving Neural Ranking Models with Traditional IR Methods",
        "url": "http://arxiv.org/abs/2308.15027v1",
        "pub_date": "2023-08-29",
        "summary": "Neural ranking methods based on large transformer models have recently gained\nsignificant attention in the information retrieval community, and have been\nadopted by major commercial solutions. Nevertheless, they are computationally\nexpensive to create, and require a great deal of labeled data for specialized\ncorpora. In this paper, we explore a low resource alternative which is a\nbag-of-embedding model for document retrieval and find that it is competitive\nwith large transformer models fine tuned on information retrieval tasks. Our\nresults show that a simple combination of TF-IDF, a traditional keyword\nmatching method, with a shallow embedding model provides a low cost path to\ncompete well with the performance of complex neural ranking models on 3\ndatasets. Furthermore, adding TF-IDF measures improves the performance of\nlarge-scale fine tuned models on these tasks.",
        "translated": ""
    },
    {
        "title": "CAPS: A Practical Partition Index for Filtered Similarity Search",
        "url": "http://arxiv.org/abs/2308.15014v1",
        "pub_date": "2023-08-29",
        "summary": "With the surging popularity of approximate near-neighbor search (ANNS),\ndriven by advances in neural representation learning, the ability to serve\nqueries accompanied by a set of constraints has become an area of intense\ninterest. While the community has recently proposed several algorithms for\nconstrained ANNS, almost all of these methods focus on integration with\ngraph-based indexes, the predominant class of algorithms achieving\nstate-of-the-art performance in latency-recall tradeoffs. In this work, we take\na different approach and focus on developing a constrained ANNS algorithm via\nspace partitioning as opposed to graphs. To that end, we introduce Constrained\nApproximate Partitioned Search (CAPS), an index for ANNS with filters via space\npartitions that not only retains the benefits of a partition-based algorithm\nbut also outperforms state-of-the-art graph-based constrained search techniques\nin recall-latency tradeoffs, with only 10% of the index size.",
        "translated": ""
    },
    {
        "title": "Adaptive Multi-Modalities Fusion in Sequential Recommendation Systems",
        "url": "http://arxiv.org/abs/2308.15980v1",
        "pub_date": "2023-08-30",
        "summary": "In sequential recommendation, multi-modal information (e.g., text or image)\ncan provide a more comprehensive view of an item's profile. The optimal stage\n(early or late) to fuse modality features into item representations is still\ndebated. We propose a graph-based approach (named MMSR) to fuse modality\nfeatures in an adaptive order, enabling each modality to prioritize either its\ninherent sequential nature or its interplay with other modalities. MMSR\nrepresents each user's history as a graph, where the modality features of each\nitem in a user's history sequence are denoted by cross-linked nodes. The edges\nbetween homogeneous nodes represent intra-modality sequential relationships,\nand the ones between heterogeneous nodes represent inter-modality\ninterdependence relationships. During graph propagation, MMSR incorporates dual\nattention, differentiating homogeneous and heterogeneous neighbors. To\nadaptively assign nodes with distinct fusion orders, MMSR allows each node's\nrepresentation to be asynchronously updated through an update gate. In\nscenarios where modalities exhibit stronger sequential relationships, the\nupdate gate prioritizes updates among homogeneous nodes. Conversely, when the\ninterdependent relationships between modalities are more pronounced, the update\ngate prioritizes updates among heterogeneous nodes. Consequently, MMSR\nestablishes a fusion order that spans a spectrum from early to late modality\nfusion. In experiments across six datasets, MMSR consistently outperforms\nstate-of-the-art models, and our graph propagation methods surpass other graph\nneural networks. Additionally, MMSR naturally manages missing modalities.",
        "translated": ""
    },
    {
        "title": "Denoising Attention for Query-aware User Modeling in Personalized Search",
        "url": "http://arxiv.org/abs/2308.15968v1",
        "pub_date": "2023-08-30",
        "summary": "The personalization of search results has gained increasing attention in the\npast few years, thanks to the development of Neural Networks-based approaches\nfor Information Retrieval and the importance of personalization in many search\nscenarios. Recent works have proposed to build user models at query time by\nleveraging the Attention mechanism, which allows weighing the contribution of\nthe user-related information w.r.t. the current query. This approach allows\ntaking into account the diversity of the user's interests by giving more\nimportance to those related to the current search performed by the user.\n  In this paper, we first discuss some shortcomings of the standard Attention\nformulation when employed for personalization. In particular, we focus on\nissues related to its normalization mechanism and its inability to entirely\nfilter out noisy user-related information. Then, we introduce the Denoising\nAttention mechanism: an Attention variant that directly tackles the above\nshortcomings by adopting a robust normalization scheme and introducing a\nfiltering mechanism. The reported experimental evaluation shows the benefits of\nthe proposed approach over other Attention-based variants.",
        "translated": ""
    },
    {
        "title": "DRGame: Diversified Recommendation for Multi-category Video Games with\n  Balanced Implicit Preferences",
        "url": "http://arxiv.org/abs/2308.15823v1",
        "pub_date": "2023-08-30",
        "summary": "The growing popularity of subscription services in video game consumption has\nemphasized the importance of offering diversified recommendations. Providing\nusers with a diverse range of games is essential for ensuring continued\nengagement and fostering long-term subscriptions. However, existing\nrecommendation models face challenges in effectively handling highly imbalanced\nimplicit feedback in gaming interactions. Additionally, they struggle to take\ninto account the distinctive characteristics of multiple categories and the\nlatent user interests associated with these categories. In response to these\nchallenges, we propose a novel framework, named DRGame, to obtain diversified\nrecommendation. It is centered on multi-category video games, consisting of two\n{components}: Balance-driven Implicit Preferences Learning for data\npre-processing and Clustering-based Diversified Recommendation {Module} for\nfinal prediction. The first module aims to achieve a balanced representation of\nimplicit feedback in game time, thereby discovering a comprehensive view of\nplayer interests across different categories. The second module adopts\ncategory-aware representation learning to cluster and select players and games\nbased on balanced implicit preferences, and then employs asymmetric neighbor\naggregation to achieve diversified recommendations. Experimental results on a\nreal-world dataset demonstrate the superiority of our proposed method over\nexisting approaches in terms of game diversity recommendations.",
        "translated": ""
    },
    {
        "title": "Knowledge-grounded Natural Language Recommendation Explanation",
        "url": "http://arxiv.org/abs/2308.15813v1",
        "pub_date": "2023-08-30",
        "summary": "Explanations accompanied by a recommendation can assist users in\nunderstanding the decision made by recommendation systems, which in turn\nincreases a user's confidence and trust in the system. Recently, research has\nfocused on generating natural language explanations in a human-readable format.\nThus far, the proposed approaches leverage item reviews written by users, which\nare often subjective, sparse in language, and unable to account for new items\nthat have not been purchased or reviewed before. Instead, we aim to generate\nfact-grounded recommendation explanations that are objectively described with\nitem features while implicitly considering a user's preferences, based on the\nuser's purchase history. To achieve this, we propose a knowledge graph (KG)\napproach to natural language explainable recommendation. Our approach draws on\nuser-item features through a novel collaborative filtering-based KG\nrepresentation to produce fact-grounded, personalized explanations, while\njointly learning user-item representations for recommendation scoring.\nExperimental results show that our approach consistently outperforms previous\nstate-of-the-art models on natural language explainable recommendation.",
        "translated": ""
    },
    {
        "title": "Fragment and Integrate Network (FIN): A Novel Spatial-Temporal Modeling\n  Based on Long Sequential Behavior for Online Food Ordering Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2308.15703v1",
        "pub_date": "2023-08-30",
        "summary": "Spatial-temporal information has been proven to be of great significance for\nclick-through rate prediction tasks in online Location-Based Services (LBS),\nespecially in mainstream food ordering platforms such as DoorDash, Uber Eats,\nMeituan, and Ele.me. Modeling user spatial-temporal preferences with sequential\nbehavior data has become a hot topic in recommendation systems and online\nadvertising. However, most of existing methods either lack the representation\nof rich spatial-temporal information or only handle user behaviors with limited\nlength, e.g. 100. In this paper, we tackle these problems by designing a new\nspatial-temporal modeling paradigm named Fragment and Integrate Network (FIN).\nFIN consists of two networks: (i) Fragment Network (FN) extracts Multiple\nSub-Sequences (MSS) from lifelong sequential behavior data, and captures the\nspecific spatial-temporal representation by modeling each MSS respectively.\nHere both a simplified attention and a complicated attention are adopted to\nbalance the performance gain and resource consumption. (ii) Integrate Network\n(IN) builds a new integrated sequence by utilizing spatial-temporal interaction\non MSS and captures the comprehensive spatial-temporal representation by\nmodeling the integrated sequence with a complicated attention. Both public\ndatasets and production datasets have demonstrated the accuracy and scalability\nof FIN. Since 2022, FIN has been fully deployed in the recommendation\nadvertising system of Ele.me, one of the most popular online food ordering\nplatforms in China, obtaining 5.7% improvement on Click-Through Rate (CTR) and\n7.3% increase on Revenue Per Mille (RPM).",
        "translated": ""
    },
    {
        "title": "A Survey on Multi-Behavior Sequential Recommendation",
        "url": "http://arxiv.org/abs/2308.15701v1",
        "pub_date": "2023-08-30",
        "summary": "Recommender systems is set up to address the issue of information overload in\ntraditional information retrieval systems, which is focused on recommending\ninformation that is of most interest to users from massive information.\nGenerally, there is a sequential nature and heterogeneity to the behavior of a\nperson interacting with a system, leading to the proposal of multi-behavior\nsequential recommendation (MBSR). MBSR is a relatively new and worthy direction\nfor in-depth research, which can achieve state-of-the-art recommendation\nthrough suitable modeling, and some related works have been proposed. This\nsurvey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in\ndetail, including its problem definition, application scenarios and challenges\nfaced. Secondly, we detail the classification of MBSR, including\nneighborhood-based methods, matrix factorization-based methods and deep\nlearning-based methods, where we further classify the deep learning-based\nmethods into different learning architectures based on RNN, GNN, Transformer,\nand generic architectures as well as architectures that integrate hybrid\ntechniques. In each method, we present related works based on the data\nperspective and the modeling perspective, as well as analyze the strengths,\nweaknesses and features of these works. Finally, we discuss some promising\nfuture research directions to address the challenges and improve the current\nstatus of MBSR.",
        "translated": ""
    },
    {
        "title": "Ensuring User-side Fairness in Dynamic Recommender Systems",
        "url": "http://arxiv.org/abs/2308.15651v1",
        "pub_date": "2023-08-29",
        "summary": "User-side group fairness is crucial for modern recommender systems, as it\naims to alleviate performance disparity between groups of users defined by\nsensitive attributes such as gender, race, or age. We find that the disparity\ntends to persist or even increase over time. This calls for effective ways to\naddress user-side fairness in a dynamic environment, which has been\ninfrequently explored in the literature. However, fairness-constrained\nre-ranking, a typical method to ensure user-side fairness (i.e., reducing\nperformance disparity), faces two fundamental challenges in the dynamic\nsetting: (1) non-differentiability of the ranking-based fairness constraint,\nwhich hinders the end-to-end training paradigm, and (2) time-inefficiency,\nwhich impedes quick adaptation to changes in user preferences. In this paper,\nwe propose FAir Dynamic rEcommender (FADE), an end-to-end framework with\nfine-tuning strategy to dynamically alleviate performance disparity. To tackle\nthe above challenges, FADE uses a novel fairness loss designed to be\ndifferentiable and lightweight to fine-tune model parameters to ensure both\nuser-side fairness and high-quality recommendations. Via extensive experiments\non the real-world dataset, we empirically demonstrate that FADE effectively and\nefficiently reduces performance disparity, and furthermore, FADE improves\noverall recommendation quality over time compared to not using any new data.",
        "translated": ""
    },
    {
        "title": "Dimensionality Reduction Using pseudo-Boolean polynomials For Cluster\n  Analysis",
        "url": "http://arxiv.org/abs/2308.15553v1",
        "pub_date": "2023-08-29",
        "summary": "We introduce usage of a reduction property of penalty-based formulation of\npseudo-Boolean polynomials as a mechanism for invariant dimensionality\nreduction in cluster analysis processes. In our experiments, we show that\nmultidimensional data, like 4-dimensional Iris Flower dataset can be reduced to\n2-dimensional space while the 30-dimensional Wisconsin Diagnostic Breast Cancer\n(WDBC) dataset can be reduced to 3-dimensional space, and by searching lines or\nplanes that lie between reduced samples we can extract clusters in a linear and\nunbiased manner with competitive accuracies, reproducibility and clear\ninterpretation.",
        "translated": ""
    },
    {
        "title": "Co-evolving Vector Quantization for ID-based Recommendation",
        "url": "http://arxiv.org/abs/2308.16761v1",
        "pub_date": "2023-08-31",
        "summary": "Category information plays a crucial role in enhancing the quality and\npersonalization of recommendations. Nevertheless, the availability of item\ncategory information is not consistently present, particularly in the context\nof ID-based recommendations. In this work, we propose an alternative approach\nto automatically learn and generate entity (i.e., user and item) categorical\ninformation at different levels of granularity, specifically for ID-based\nrecommendation. Specifically, we devise a co-evolving vector quantization\nframework, namely COVE, which enables the simultaneous learning and refinement\nof code representation and entity embedding in an end-to-end manner, starting\nfrom the randomly initialized states. With its high adaptability, COVE can be\neasily integrated into existing recommendation models. We validate the\neffectiveness of COVE on various recommendation tasks including list\ncompletion, collaborative filtering, and click-through rate prediction, across\ndifferent recommendation models. We will publish the code and data for other\nresearchers to reproduce our work.",
        "translated": ""
    },
    {
        "title": "Context Aware Query Rewriting for Text Rankers using LLM",
        "url": "http://arxiv.org/abs/2308.16753v1",
        "pub_date": "2023-08-31",
        "summary": "Query rewriting refers to an established family of approaches that are\napplied to underspecified and ambiguous queries to overcome the vocabulary\nmismatch problem in document ranking. Queries are typically rewritten during\nquery processing time for better query modelling for the downstream ranker.\nWith the advent of large-language models (LLMs), there have been initial\ninvestigations into using generative approaches to generate pseudo documents to\ntackle this inherent vocabulary gap. In this work, we analyze the utility of\nLLMs for improved query rewriting for text ranking tasks. We find that there\nare two inherent limitations of using LLMs as query re-writers -- concept drift\nwhen using only queries as prompts and large inference costs during query\nprocessing. We adopt a simple, yet surprisingly effective, approach called\ncontext aware query rewriting (CAR) to leverage the benefits of LLMs for query\nunderstanding. Firstly, we rewrite ambiguous training queries by context-aware\nprompting of LLMs, where we use only relevant documents as context.Unlike\nexisting approaches, we use LLM-based query rewriting only during the training\nphase. Eventually, a ranker is fine-tuned on the rewritten queries instead of\nthe original queries during training. In our extensive experiments, we find\nthat fine-tuning a ranker using re-written queries offers a significant\nimprovement of up to 33% on the passage ranking task and up to 28% on the\ndocument ranking task when compared to the baseline performance of using\noriginal queries.",
        "translated": ""
    },
    {
        "title": "Concentrating on the Impact: Consequence-based Explanations in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2308.16708v1",
        "pub_date": "2023-08-31",
        "summary": "Recommender systems assist users in decision-making, where the presentation\nof recommended items and their explanations are critical factors for enhancing\nthe overall user experience. Although various methods for generating\nexplanations have been proposed, there is still room for improvement,\nparticularly for users who lack expertise in a specific item domain. In this\nstudy, we introduce the novel concept of \\textit{consequence-based\nexplanations}, a type of explanation that emphasizes the individual impact of\nconsuming a recommended item on the user, which makes the effect of following\nrecommendations clearer. We conducted an online user study to examine our\nassumption about the appreciation of consequence-based explanations and their\nimpacts on different explanation aims in recommender systems. Our findings\nhighlight the importance of consequence-based explanations, which were\nwell-received by users and effectively improved user satisfaction in\nrecommender systems. These results provide valuable insights for designing\nengaging explanations that can enhance the overall user experience in\ndecision-making.",
        "translated": ""
    },
    {
        "title": "Towards Long-Tailed Recognition for Graph Classification via\n  Collaborative Experts",
        "url": "http://arxiv.org/abs/2308.16609v1",
        "pub_date": "2023-08-31",
        "summary": "Graph classification, aiming at learning the graph-level representations for\neffective class assignments, has received outstanding achievements, which\nheavily relies on high-quality datasets that have balanced class distribution.\nIn fact, most real-world graph data naturally presents a long-tailed form,\nwhere the head classes occupy much more samples than the tail classes, it thus\nis essential to study the graph-level classification over long-tailed data\nwhile still remaining largely unexplored. However, most existing long-tailed\nlearning methods in visions fail to jointly optimize the representation\nlearning and classifier training, as well as neglect the mining of the\nhard-to-classify classes. Directly applying existing methods to graphs may lead\nto sub-optimal performance, since the model trained on graphs would be more\nsensitive to the long-tailed distribution due to the complex topological\ncharacteristics. Hence, in this paper, we propose a novel long-tailed\ngraph-level classification framework via Collaborative Multi-expert Learning\n(CoMe) to tackle the problem. To equilibrate the contributions of head and tail\nclasses, we first develop balanced contrastive learning from the view of\nrepresentation learning, and then design an individual-expert classifier\ntraining based on hard class mining. In addition, we execute gated fusion and\ndisentangled knowledge distillation among the multiple experts to promote the\ncollaboration in a multi-expert framework. Comprehensive experiments are\nperformed on seven widely-used benchmark datasets to demonstrate the\nsuperiority of our method CoMe over state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Recommender AI Agent: Integrating Large Language Models for Interactive\n  Recommendations",
        "url": "http://arxiv.org/abs/2308.16505v1",
        "pub_date": "2023-08-31",
        "summary": "Recommender models excel at providing domain-specific item recommendations by\nleveraging extensive user behavior data. Despite their ability to act as\nlightweight domain experts, they struggle to perform versatile tasks such as\nproviding explanations and engaging in conversations. On the other hand, large\nlanguage models (LLMs) represent a significant step towards artificial general\nintelligence, showcasing remarkable capabilities in instruction comprehension,\ncommonsense reasoning, and human interaction. However, LLMs lack the knowledge\nof domain-specific item catalogs and behavioral patterns, particularly in areas\nthat diverge from general world knowledge, such as online e-commerce.\nFinetuning LLMs for each domain is neither economic nor efficient.\n  In this paper, we bridge the gap between recommender models and LLMs,\ncombining their respective strengths to create a versatile and interactive\nrecommender system. We introduce an efficient framework called RecAgent, which\nemploys LLMs as the brain and recommender models as tools. We first outline a\nminimal set of essential tools required to transform LLMs into RecAgent. We\nthen propose an efficient workflow within RecAgent for task execution,\nincorporating key components such as a memory bus, dynamic\ndemonstration-augmented task planning, and reflection. RecAgent enables\ntraditional recommender systems, such as those ID-based matrix factorization\nmodels, to become interactive systems with a natural language interface through\nthe integration of LLMs. Experimental results on several public datasets show\nthat RecAgent achieves satisfying performance as a conversational recommender\nsystem, outperforming general-purpose LLMs.",
        "translated": ""
    },
    {
        "title": "AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR\n  Prediction",
        "url": "http://arxiv.org/abs/2308.16437v1",
        "pub_date": "2023-08-31",
        "summary": "Click-through rate (CTR) prediction is a crucial issue in recommendation\nsystems. There has been an emergence of various public CTR datasets. However,\nexisting datasets primarily suffer from the following limitations. Firstly,\nusers generally click different types of items from multiple scenarios, and\nmodeling from multiple scenarios can provide a more comprehensive understanding\nof users. Existing datasets only include data for the same type of items from a\nsingle scenario. Secondly, multi-modal features are essential in multi-scenario\nprediction as they address the issue of inconsistent ID encoding between\ndifferent scenarios. The existing datasets are based on ID features and lack\nmulti-modal features. Third, a large-scale dataset can provide a more reliable\nevaluation of models, fully reflecting the performance differences between\nmodels. The scale of existing datasets is around 100 million, which is\nrelatively small compared to the real-world CTR prediction. To address these\nlimitations, we propose AntM$^{2}$C, a Multi-Scenario Multi-Modal CTR dataset\nbased on industrial data from Alipay. Specifically, AntM$^{2}$C provides the\nfollowing advantages: 1) It covers CTR data of 5 different types of items,\nproviding insights into the preferences of users for different items, including\nadvertisements, vouchers, mini-programs, contents, and videos. 2) Apart from\nID-based features, AntM$^{2}$C also provides 2 multi-modal features, raw text\nand image features, which can effectively establish connections between items\nwith different IDs. 3) AntM$^{2}$C provides 1 billion CTR data with 200\nfeatures, including 200 million users and 6 million items. It is currently the\nlargest-scale CTR dataset available. Based on AntM$^{2}$C, we construct several\ntypical CTR tasks and provide comparisons with baseline methods. The dataset\nhomepage is available at https://www.atecup.cn/home.",
        "translated": ""
    },
    {
        "title": "NeMig -- A Bilingual News Collection and Knowledge Graph about Migration",
        "url": "http://arxiv.org/abs/2309.00550v1",
        "pub_date": "2023-09-01",
        "summary": "News recommendation plays a critical role in shaping the public's worldviews\nthrough the way in which it filters and disseminates information about\ndifferent topics. Given the crucial impact that media plays in opinion\nformation, especially for sensitive topics, understanding the effects of\npersonalized recommendation beyond accuracy has become essential in today's\ndigital society. In this work, we present NeMig, a bilingual news collection on\nthe topic of migration, and corresponding rich user data. In comparison to\nexisting news recommendation datasets, which comprise a large variety of\nmonolingual news, NeMig covers articles on a single controversial topic,\npublished in both Germany and the US. We annotate the sentiment polarization of\nthe articles and the political leanings of the media outlets, in addition to\nextracting subtopics and named entities disambiguated through Wikidata. These\nfeatures can be used to analyze the effects of algorithmic news curation beyond\naccuracy-based performance, such as recommender biases and the creation of\nfilter bubbles. We construct domain-specific knowledge graphs from the news\ntext and metadata, thus encoding knowledge-level connections between articles.\nImportantly, while existing datasets include only click behavior, we collect\nuser socio-demographic and political information in addition to explicit click\nfeedback. We demonstrate the utility of NeMig through experiments on the tasks\nof news recommenders benchmarking, analysis of biases in recommenders, and news\ntrends analysis. NeMig aims to provide a useful resource for the news\nrecommendation community and to foster interdisciplinary research into the\nmultidimensional effects of algorithmic news curation.",
        "translated": ""
    },
    {
        "title": "General and Practical Tuning Method for Off-the-Shelf Graph-Based Index:\n  SISAP Indexing Challenge Report by Team UTokyo",
        "url": "http://arxiv.org/abs/2309.00472v1",
        "pub_date": "2023-09-01",
        "summary": "Despite the efficacy of graph-based algorithms for Approximate Nearest\nNeighbor (ANN) searches, the optimal tuning of such systems remains unclear.\nThis study introduces a method to tune the performance of off-the-shelf\ngraph-based indexes, focusing on the dimension of vectors, database size, and\nentry points of graph traversal. We utilize a black-box optimization algorithm\nto perform integrated tuning to meet the required levels of recall and Queries\nPer Second (QPS). We applied our approach to Task A of the SISAP 2023 Indexing\nChallenge and got second place in the 10M and 30M tracks. It improves\nperformance substantially compared to brute force methods. This research offers\na universally applicable tuning method for graph-based indexes, extending\nbeyond the specific conditions of the competition to broader uses.",
        "translated": ""
    },
    {
        "title": "Explainable Active Learning for Preference Elicitation",
        "url": "http://arxiv.org/abs/2309.00356v1",
        "pub_date": "2023-09-01",
        "summary": "Gaining insights into the preferences of new users and subsequently\npersonalizing recommendations necessitate managing user interactions\nintelligently, namely, posing pertinent questions to elicit valuable\ninformation effectively. In this study, our focus is on a specific scenario of\nthe cold-start problem, where the recommendation system lacks adequate user\npresence or access to other users' data is restricted, obstructing employing\nuser profiling methods utilizing existing data in the system. We employ Active\nLearning (AL) to solve the addressed problem with the objective of maximizing\ninformation acquisition with minimal user effort. AL operates for selecting\ninformative data from a large unlabeled set to inquire an oracle to label them\nand eventually updating a machine learning (ML) model. We operate AL in an\nintegrated process of unsupervised, semi-supervised, and supervised ML within\nan explanatory preference elicitation process. It harvests user feedback (given\nfor the system's explanations on the presented items) over informative samples\nto update an underlying ML model estimating user preferences. The designed user\ninteraction facilitates personalizing the system by incorporating user feedback\ninto the ML model and also enhances user trust by refining the system's\nexplanations on recommendations. We implement the proposed preference\nelicitation methodology for food recommendation. We conducted human experiments\nto assess its efficacy in the short term and also experimented with several AL\nstrategies over synthetic user profiles that we created for two food datasets,\naiming for long-term performance analysis. The experimental results demonstrate\nthe efficiency of the proposed preference elicitation with limited user-labeled\ndata while also enhancing user trust through accurate explanations.",
        "translated": ""
    },
    {
        "title": "Towards Contrastive Learning in Music Video Domain",
        "url": "http://arxiv.org/abs/2309.00347v1",
        "pub_date": "2023-09-01",
        "summary": "Contrastive learning is a powerful way of learning multimodal representations\nacross various domains such as image-caption retrieval and audio-visual\nrepresentation learning. In this work, we investigate if these findings\ngeneralize to the domain of music videos. Specifically, we create a dual\nen-coder for the audio and video modalities and train it using a bidirectional\ncontrastive loss. For the experiments, we use an industry dataset containing\n550 000 music videos as well as the public Million Song Dataset, and evaluate\nthe quality of learned representations on the downstream tasks of music tagging\nand genre classification. Our results indicate that pre-trained networks\nwithout contrastive fine-tuning outperform our contrastive learning approach\nwhen evaluated on both tasks. To gain a better understanding of the reasons\ncontrastive learning was not successful for music videos, we perform a\nqualitative analysis of the learned representations, revealing why contrastive\nlearning might have difficulties uniting embeddings from two modalities. Based\non these findings, we outline possible directions for future work. To\nfacilitate the reproducibility of our results, we share our code and the\npre-trained model.",
        "translated": ""
    },
    {
        "title": "Fairness of Exposure in Dynamic Recommendation",
        "url": "http://arxiv.org/abs/2309.02322v1",
        "pub_date": "2023-09-05",
        "summary": "Exposure bias is a well-known issue in recommender systems where the exposure\nis not fairly distributed among items in the recommendation results. This is\nespecially problematic when bias is amplified over time as a few items (e.g.,\npopular ones) are repeatedly over-represented in recommendation lists and\nusers' interactions with those items will amplify bias towards those items over\ntime resulting in a feedback loop. This issue has been extensively studied in\nthe literature in static recommendation environment where a single round of\nrecommendation result is processed to improve the exposure fairness. However,\nless work has been done on addressing exposure bias in a dynamic recommendation\nsetting where the system is operating over time, the recommendation model and\nthe input data are dynamically updated with ongoing user feedback on\nrecommended items at each round. In this paper, we study exposure bias in a\ndynamic recommendation setting. Our goal is to show that existing bias\nmitigation methods that are designed to operate in a static recommendation\nsetting are unable to satisfy fairness of exposure for items in long run. In\nparticular, we empirically study one of these methods and show that repeatedly\napplying this method fails to fairly distribute exposure among items in long\nrun. To address this limitation, we show how this method can be adapted to\neffectively operate in a dynamic recommendation setting and achieve exposure\nfairness for items in long run. Experiments on a real-world dataset confirm\nthat our solution is superior in achieving long-term exposure fairness for the\nitems while maintaining the recommendation accuracy.",
        "translated": ""
    },
    {
        "title": "STGIN: Spatial-Temporal Graph Interaction Network for Large-scale POI\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.02251v1",
        "pub_date": "2023-09-05",
        "summary": "In Location-Based Services, Point-Of-Interest(POI) recommendation plays a\ncrucial role in both user experience and business opportunities. Graph neural\nnetworks have been proven effective in providing personalized POI\nrecommendation services. However, there are still two critical challenges.\nFirst, existing graph models attempt to capture users' diversified interests\nthrough a unified graph, which limits their ability to express interests in\nvarious spatial-temporal contexts. Second, the efficiency limitations of graph\nconstruction and graph sampling in large-scale systems make it difficult to\nadapt quickly to new real-time interests. To tackle the above challenges, we\npropose a novel Spatial-Temporal Graph Interaction Network. Specifically, we\nconstruct subgraphs of spatial, temporal, spatial-temporal, and global views\nrespectively to precisely characterize the user's interests in various\ncontexts. In addition, we design an industry-friendly framework to track the\nuser's latest interests. Extensive experiments on the real-world dataset show\nthat our method outperforms state-of-the-art models. This work has been\nsuccessfully deployed in a large e-commerce platform, delivering a 1.1% CTR and\n6.3% RPM improvement.",
        "translated": ""
    },
    {
        "title": "TensorBank:Tensor Lakehouse for Foundation Model Training",
        "url": "http://arxiv.org/abs/2309.02094v1",
        "pub_date": "2023-09-05",
        "summary": "Storing and streaming high dimensional data for foundation model training\nbecame a critical requirement with the rise of foundation models beyond natural\nlanguage. In this paper we introduce TensorBank, a petabyte scale tensor\nlakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU\nmemory at wire speed based on complex relational queries. We use Hierarchical\nStatistical Indices (HSI) for query acceleration. Our architecture allows to\ndirectly address tensors on block level using HTTP range reads. Once in GPU\nmemory, data can be transformed using PyTorch transforms. We provide a generic\nPyTorch dataset type with a corresponding dataset factory translating\nrelational queries and requested transformations as an instance. By making use\nof the HSI, irrelevant blocks can be skipped without reading them as those\nindices contain statistics on their content at different hierarchical\nresolution levels. This is an opinionated architecture powered by open\nstandards and making heavy use of open-source technology. Although, hardened\nfor production use using geospatial-temporal data, this architecture\ngeneralizes to other use case like computer vision, computational neuroscience,\nbiological sequence analysis and more.",
        "translated": ""
    },
    {
        "title": "MvFS: Multi-view Feature Selection for Recommender System",
        "url": "http://arxiv.org/abs/2309.02064v1",
        "pub_date": "2023-09-05",
        "summary": "Feature selection, which is a technique to select key features in recommender\nsystems, has received increasing research attention. Recently, Adaptive Feature\nSelection (AdaFS) has shown remarkable performance by adaptively selecting\nfeatures for each data instance, considering that the importance of a given\nfeature field can vary significantly across data. However, this method still\nhas limitations in that its selection process could be easily biased to major\nfeatures that frequently occur. To address these problems, we propose\nMulti-view Feature Selection (MvFS), which selects informative features for\neach instance more effectively. Most importantly, MvFS employs a multi-view\nnetwork consisting of multiple sub-networks, each of which learns to measure\nthe feature importance of a part of data with different feature patterns. By\ndoing so, MvFS promotes a more balanced feature selection process mitigating\nthe bias problem towards dominant patterns. Moreover, MvFS adopts an effective\nimportance score modeling strategy which is applied independently to each field\nwithout incurring dependency among features. Experimental results on real-world\ndatasets demonstrate the effectiveness of MvFS compared to state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "Scenario-Aware Hierarchical Dynamic Network for Multi-Scenario\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.02061v1",
        "pub_date": "2023-09-05",
        "summary": "Click-Through Rate (CTR) prediction is a fundamental technique in\nrecommendation and advertising systems. Recent studies have shown that\nimplementing multi-scenario recommendations contributes to strengthening\ninformation sharing and improving overall performance. However, existing\nmulti-scenario models only consider coarse-grained explicit scenario modeling\nthat depends on pre-defined scenario identification from manual prior rules,\nwhich is biased and sub-optimal. To address these limitations, we propose a\nScenario-Aware Hierarchical Dynamic Network for Multi-Scenario Recommendations\n(HierRec), which perceives implicit patterns adaptively and conducts explicit\nand implicit scenario modeling jointly. In particular, HierRec designs a basic\nscenario-oriented module based on the dynamic weight to capture\nscenario-specific information. Then the hierarchical explicit and implicit\nscenario-aware modules are proposed to model hybrid-grained scenario\ninformation. The multi-head implicit modeling design contributes to perceiving\ndistinctive patterns from different perspectives. Our experiments on two public\ndatasets and real-world industrial applications on a mainstream online\nadvertising platform demonstrate that our HierRec outperforms existing models\nsignificantly.",
        "translated": ""
    },
    {
        "title": "Robust Recommender System: A Survey and Future Directions",
        "url": "http://arxiv.org/abs/2309.02057v1",
        "pub_date": "2023-09-05",
        "summary": "With the rapid growth of information, recommender systems have become\nintegral for providing personalized suggestions and overcoming information\noverload. However, their practical deployment often encounters \"dirty\" data,\nwhere noise or malicious information can lead to abnormal recommendations.\nResearch on improving recommender systems' robustness against such dirty data\nhas thus gained significant attention. This survey provides a comprehensive\nreview of recent work on recommender systems' robustness. We first present a\ntaxonomy to organize current techniques for withstanding malicious attacks and\nnatural noise. We then explore state-of-the-art methods in each category,\nincluding fraudster detection, adversarial training, certifiable robust\ntraining against malicious attacks, and regularization, purification,\nself-supervised learning against natural noise. Additionally, we summarize\nevaluation metrics and common datasets used to assess robustness. We discuss\nrobustness across varying recommendation scenarios and its interplay with other\nproperties like accuracy, interpretability, privacy, and fairness. Finally, we\ndelve into open issues and future research directions in this emerging field.\nOur goal is to equip readers with a holistic understanding of robust\nrecommender systems and spotlight pathways for future research and development.",
        "translated": ""
    },
    {
        "title": "Towards Individual and Multistakeholder Fairness in Tourism Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2309.02052v1",
        "pub_date": "2023-09-05",
        "summary": "This position paper summarizes our published review on individual and\nmultistakeholder fairness in Tourism Recommender Systems (TRS). Recently, there\nhas been growing attention to fairness considerations in recommender systems\n(RS). It has been acknowledged in research that fairness in RS is often closely\ntied to the presence of multiple stakeholders, such as end users, item\nproviders, and platforms, as it raises concerns for the fair treatment of all\nparties involved. Hence, fairness in RS is a multi-faceted concept that\nrequires consideration of the perspectives and needs of the different\nstakeholders to ensure fair outcomes for them. However, there may often be\ninstances where achieving the goals of one stakeholder could conflict with\nthose of another, resulting in trade-offs.\n  In this paper, we emphasized addressing the unique challenges of ensuring\nfairness in RS within the tourism domain. We aimed to discuss potential\nstrategies for mitigating the aforementioned challenges and examine the\napplicability of solutions from other domains to tackle fairness issues in\ntourism. By exploring cross-domain approaches and strategies for incorporating\nS-Fairness, we can uncover valuable insights and determine how these solutions\ncan be adapted and implemented effectively in the context of tourism to enhance\nfairness in RS.",
        "translated": ""
    },
    {
        "title": "DiscoverPath: A Knowledge Refinement and Retrieval System for\n  Interdisciplinarity on Biomedical Research",
        "url": "http://arxiv.org/abs/2309.01808v1",
        "pub_date": "2023-09-04",
        "summary": "The exponential growth in scholarly publications necessitates advanced tools\nfor efficient article retrieval, especially in interdisciplinary fields where\ndiverse terminologies are used to describe similar research. Traditional\nkeyword-based search engines often fall short in assisting users who may not be\nfamiliar with specific terminologies. To address this, we present a knowledge\ngraph-based paper search engine for biomedical research to enhance the user\nexperience in discovering relevant queries and articles. The system, dubbed\nDiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS)\ntagging to extract terminologies and relationships from article abstracts to\ncreate a KG. To reduce information overload, DiscoverPath presents users with a\nfocused subgraph containing the queried entity and its neighboring nodes and\nincorporates a query recommendation system, enabling users to iteratively\nrefine their queries. The system is equipped with an accessible Graphical User\nInterface that provides an intuitive visualization of the KG, query\nrecommendations, and detailed article information, enabling efficient article\nretrieval, thus fostering interdisciplinary knowledge exploration. DiscoverPath\nis open-sourced at https://github.com/ynchuang/DiscoverPath.",
        "translated": ""
    },
    {
        "title": "CRUISE-Screening: Living Literature Reviews Toolbox",
        "url": "http://arxiv.org/abs/2309.01684v1",
        "pub_date": "2023-09-04",
        "summary": "Keeping up with research and finding related work is still a time-consuming\ntask for academics. Researchers sift through thousands of studies to identify a\nfew relevant ones. Automation techniques can help by increasing the efficiency\nand effectiveness of this task. To this end, we developed CRUISE-Screening, a\nweb-based application for conducting living literature reviews - a type of\nliterature review that is continuously updated to reflect the latest research\nin a particular field. CRUISE-Screening is connected to several search engines\nvia an API, which allows for updating the search results periodically.\nMoreover, it can facilitate the process of screening for relevant publications\nby using text classification and question answering models. CRUISE-Screening\ncan be used both by researchers conducting literature reviews and by those\nworking on automating the citation screening process to validate their\nalgorithms. The application is open-source:\nhttps://github.com/ProjectDoSSIER/cruise-screening, and a demo is available\nunder this URL: https://citation-screening.ec.tuwien.ac.at. We discuss the\nlimitations of our tool in Appendix A.",
        "translated": ""
    },
    {
        "title": "Fair Ranking under Disparate Uncertainty",
        "url": "http://arxiv.org/abs/2309.01610v1",
        "pub_date": "2023-09-04",
        "summary": "Ranking is a ubiquitous method for focusing the attention of human evaluators\non a manageable subset of options. Its use ranges from surfacing potentially\nrelevant products on an e-commerce site to prioritizing college applications\nfor human review. While ranking can make human evaluation far more effective by\nfocusing attention on the most promising options, we argue that it can\nintroduce unfairness if the uncertainty of the underlying relevance model\ndiffers between groups of options. Unfortunately, such disparity in uncertainty\nappears widespread, since the relevance estimates for minority groups tend to\nhave higher uncertainty due to a lack of data or appropriate features. To\novercome this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a\nnew fairness criterion for ranking that provably corrects for the disparity in\nuncertainty between groups. Furthermore, we present a practical algorithm for\ncomputing EOR rankings in time $O(n \\log(n))$ and prove its close approximation\nguarantee to the globally optimal solution. In a comprehensive empirical\nevaluation on synthetic data, a US Census dataset, and a real-world case study\nof Amazon search queries, we find that the algorithm reliably guarantees EOR\nfairness while providing effective rankings.",
        "translated": ""
    },
    {
        "title": "Impression-Informed Multi-Behavior Recommender System: A Hierarchical\n  Graph Attention Approach",
        "url": "http://arxiv.org/abs/2309.03169v1",
        "pub_date": "2023-09-06",
        "summary": "While recommender systems have significantly benefited from implicit\nfeedback, they have often missed the nuances of multi-behavior interactions\nbetween users and items. Historically, these systems either amalgamated all\nbehaviors, such as \\textit{impression} (formerly \\textit{view}),\n\\textit{add-to-cart}, and \\textit{buy}, under a singular 'interaction' label,\nor prioritized only the target behavior, often the \\textit{buy} action,\ndiscarding valuable auxiliary signals. Although recent advancements tried\naddressing this simplification, they primarily gravitated towards optimizing\nthe target behavior alone, battling with data scarcity. Additionally, they\ntended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these\ngaps, we introduce the \\textbf{H}ierarchical \\textbf{M}ulti-behavior\n\\textbf{G}raph Attention \\textbf{N}etwork (HMGN). This pioneering framework\nleverages attention mechanisms to discern information from both inter and\nintra-behaviors while employing a multi-task Hierarchical Bayesian Personalized\nRanking (HBPR) for optimization. Recognizing the need for scalability, our\napproach integrates a specialized multi-behavior sub-graph sampling technique.\nMoreover, the adaptability of HMGN allows for the seamless inclusion of\nknowledge metadata and time-series data. Empirical results attest to our\nmodel's prowess, registering a notable performance boost of up to 64\\% in\nNDCG@100 metrics over conventional graph neural network methods.",
        "translated": ""
    },
    {
        "title": "Helper Recommendation with seniority control in Online Health Community",
        "url": "http://arxiv.org/abs/2309.02978v1",
        "pub_date": "2023-09-06",
        "summary": "Online health communities (OHCs) are forums where patients with similar\nconditions communicate their experiences and provide moral support. Social\nsupport in OHCs plays a crucial role in easing and rehabilitating patients.\nHowever, many time-sensitive questions from patients often remain unanswered\ndue to the multitude of threads and the random nature of patient visits in\nOHCs. To address this issue, it is imperative to propose a recommender system\nthat assists solution seekers in finding appropriate problem helpers.\nNevertheless, developing a recommendation algorithm to enhance social support\nin OHCs remains an under-explored area. Traditional recommender systems cannot\nbe directly adapted due to the following obstacles. First, unlike user-item\nlinks in traditional recommender systems, it is hard to model the social\nsupport behind helper-seeker links in OHCs since they are formed based on\nvarious heterogeneous reasons. Second, it is difficult to distinguish the\nimpact of historical activities in characterizing patients. Third, it is\nsignificantly challenging to ensure that the recommended helpers possess\nsufficient expertise to assist the seekers. To tackle the aforementioned\nchallenges, we develop a Monotonically regularIzed diseNTangled Variational\nAutoencoders (MINT) model to strengthen social support in OHCs.",
        "translated": ""
    },
    {
        "title": "Prompt-based Effective Input Reformulation for Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2309.02962v1",
        "pub_date": "2023-09-06",
        "summary": "Legal case retrieval plays an important role for legal practitioners to\neffectively retrieve relevant cases given a query case. Most existing neural\nlegal case retrieval models directly encode the whole legal text of a case to\ngenerate a case representation, which is then utilised to conduct a nearest\nneighbour search for retrieval. Although these straightforward methods have\nachieved improvement over conventional statistical methods in retrieval\naccuracy, two significant challenges are identified in this paper: (1) Legal\nfeature alignment: the usage of the whole case text as the input will generally\nincorporate redundant and noisy information because, from the legal\nperspective, the determining factor of relevant cases is the alignment of key\nlegal features instead of whole text matching; (2) Legal context preservation:\nfurthermore, since the existing text encoding models usually have an input\nlength limit shorter than the case, the whole case text needs to be truncated\nor divided into paragraphs, which leads to the loss of the global context of\nlegal information. In this paper, a novel legal case retrieval framework,\nPromptCase, is proposed to tackle these challenges. Firstly, legal facts and\nlegal issues are identified and formally defined as the key features\nfacilitating legal case retrieval based on a thorough study of the definition\nof relevant cases from a legal perspective. Secondly, with the determining\nlegal features, a prompt-based encoding scheme is designed to conduct an\neffective encoding with language models. Extensive zero-shot experiments have\nbeen conducted on two benchmark datasets in legal case retrieval, which\ndemonstrate the superior retrieval effectiveness of the proposed PromptCase.\nThe code has been released on https://github.com/yanran-tang/PromptCase.",
        "translated": ""
    },
    {
        "title": "Tidying Up the Conversational Recommender Systems' Biases",
        "url": "http://arxiv.org/abs/2309.02550v1",
        "pub_date": "2023-09-05",
        "summary": "The growing popularity of language models has sparked interest in\nconversational recommender systems (CRS) within both industry and research\ncircles. However, concerns regarding biases in these systems have emerged.\nWhile individual components of CRS have been subject to bias studies, a\nliterature gap remains in understanding specific biases unique to CRS and how\nthese biases may be amplified or reduced when integrated into complex CRS\nmodels. In this paper, we provide a concise review of biases in CRS by\nsurveying recent literature. We examine the presence of biases throughout the\nsystem's pipeline and consider the challenges that arise from combining\nmultiple models. Our study investigates biases in classic recommender systems\nand their relevance to CRS. Moreover, we address specific biases in CRS,\nconsidering variations with and without natural language understanding\ncapabilities, along with biases related to dialogue systems and language\nmodels. Through our findings, we highlight the necessity of adopting a holistic\nperspective when dealing with biases in complex CRS models.",
        "translated": ""
    },
    {
        "title": "Extending Transductive Knowledge Graph Embedding Models for Inductive\n  Logical Relational Inference",
        "url": "http://arxiv.org/abs/2309.03773v1",
        "pub_date": "2023-09-07",
        "summary": "Many downstream inference tasks for knowledge graphs, such as relation\nprediction, have been handled successfully by knowledge graph embedding\ntechniques in the transductive setting. To address the inductive setting\nwherein new entities are introduced into the knowledge graph at inference time,\nmore recent work opts for models which learn implicit representations of the\nknowledge graph through a complex function of a network's subgraph structure,\noften parametrized by graph neural network architectures. These come at the\ncost of increased parametrization, reduced interpretability and limited\ngeneralization to other downstream inference tasks. In this work, we bridge the\ngap between traditional transductive knowledge graph embedding approaches and\nmore recent inductive relation prediction models by introducing a generalized\nform of harmonic extension which leverages representations learned through\ntransductive embedding methods to infer representations of new entities\nintroduced at inference time as in the inductive setting. This harmonic\nextension technique provides the best such approximation, can be implemented\nvia an efficient iterative scheme, and can be employed to answer a family of\nconjunctive logical queries over the knowledge graph, further expanding the\ncapabilities of transductive embedding methods. In experiments on a number of\nlarge-scale knowledge graph embedding benchmarks, we find that this approach\nfor extending the functionality of transductive knowledge graph embedding\nmodels to perform knowledge graph completion and answer logical queries in the\ninductive setting is competitive with--and in some scenarios\noutperforms--several state-of-the-art models derived explicitly for such\ninductive tasks.",
        "translated": ""
    },
    {
        "title": "VideolandGPT: A User Study on a Conversational Recommender System",
        "url": "http://arxiv.org/abs/2309.03645v1",
        "pub_date": "2023-09-07",
        "summary": "This paper investigates how large language models (LLMs) can enhance\nrecommender systems, with a specific focus on Conversational Recommender\nSystems that leverage user preferences and personalised candidate selections\nfrom existing ranking models. We introduce VideolandGPT, a recommender system\nfor a Video-on-Demand (VOD) platform, Videoland, which uses ChatGPT to select\nfrom a predetermined set of contents, considering the additional context\nindicated by users' interactions with a chat interface. We evaluate ranking\nmetrics, user experience, and fairness of recommendations, comparing a\npersonalised and a non-personalised version of the system, in a between-subject\nuser study. Our results indicate that the personalised version outperforms the\nnon-personalised in terms of accuracy and general user satisfaction, while both\nversions increase the visibility of items which are not in the top of the\nrecommendation lists. However, both versions present inconsistent behavior in\nterms of fairness, as the system may generate recommendations which are not\navailable on Videoland.",
        "translated": ""
    },
    {
        "title": "Evaluating ChatGPT as a Recommender System: A Rigorous Approach",
        "url": "http://arxiv.org/abs/2309.03613v1",
        "pub_date": "2023-09-07",
        "summary": "Recent popularity surrounds large AI language models due to their impressive\nnatural language capabilities. They contribute significantly to\nlanguage-related tasks, including prompt-based learning, making them valuable\nfor various specific tasks. This approach unlocks their full potential,\nenhancing precision and generalization. Research communities are actively\nexploring their applications, with ChatGPT receiving recognition. Despite\nextensive research on large language models, their potential in recommendation\nscenarios still needs to be explored. This study aims to fill this gap by\ninvestigating ChatGPT's capabilities as a zero-shot recommender system. Our\ngoals include evaluating its ability to use user preferences for\nrecommendations, reordering existing recommendation lists, leveraging\ninformation from similar users, and handling cold-start situations. We assess\nChatGPT's performance through comprehensive experiments using three datasets\n(MovieLens Small, Last.FM, and Facebook Book). We compare ChatGPT's performance\nagainst standard recommendation algorithms and other large language models,\nsuch as GPT-3.5 and PaLM-2. To measure recommendation effectiveness, we employ\nwidely-used evaluation metrics like Mean Average Precision (MAP), Recall,\nPrecision, F1, normalized Discounted Cumulative Gain (nDCG), Item Coverage,\nExpected Popularity Complement (EPC), Average Coverage of Long Tail (ACLT),\nAverage Recommendation Popularity (ARP), and Popularity-based Ranking-based\nEqual Opportunity (PopREO). Through thoroughly exploring ChatGPT's abilities in\nrecommender systems, our study aims to contribute to the growing body of\nresearch on the versatility and potential applications of large language\nmodels. Our experiment code is available on the GitHub repository:\nhttps://github.com/sisinflab/Recommender-ChatGPT",
        "translated": ""
    },
    {
        "title": "Learning Compact Compositional Embeddings via Regularized Pruning for\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.03518v1",
        "pub_date": "2023-09-07",
        "summary": "Latent factor models are the dominant backbones of contemporary recommender\nsystems (RSs) given their performance advantages, where a unique vector\nembedding with a fixed dimensionality (e.g., 128) is required to represent each\nentity (commonly a user/item). Due to the large number of users and items on\ne-commerce sites, the embedding table is arguably the least memory-efficient\ncomponent of RSs. For any lightweight recommender that aims to efficiently\nscale with the growing size of users/items or to remain applicable in\nresource-constrained settings, existing solutions either reduce the number of\nembeddings needed via hashing, or sparsify the full embedding table to switch\noff selected embedding dimensions. However, as hash collision arises or\nembeddings become overly sparse, especially when adapting to a tighter memory\nbudget, those lightweight recommenders inevitably have to compromise their\naccuracy. To this end, we propose a novel compact embedding framework for RSs,\nnamely Compositional Embedding with Regularized Pruning (CERP). Specifically,\nCERP represents each entity by combining a pair of embeddings from two\nindependent, substantially smaller meta-embedding tables, which are then\njointly pruned via a learnable element-wise threshold. In addition, we\ninnovatively design a regularized pruning mechanism in CERP, such that the two\nsparsified meta-embedding tables are encouraged to encode information that is\nmutually complementary. Given the compatibility with agnostic latent factor\nmodels, we pair CERP with two popular recommendation models for extensive\nexperiments, where results on two real-world datasets under different memory\nbudgets demonstrate its superiority against state-of-the-art baselines. The\ncodebase of CERP is available in https://github.com/xurong-liang/CERP.",
        "translated": ""
    },
    {
        "title": "Behind Recommender Systems: the Geography of the ACM RecSys Community",
        "url": "http://arxiv.org/abs/2309.03512v1",
        "pub_date": "2023-09-07",
        "summary": "The amount and dissemination rate of media content accessible online is\nnowadays overwhelming. Recommender Systems filter this information into\nmanageable streams or feeds, adapted to our personal needs or preferences. It\nis of utter importance that algorithms employed to filter information do not\ndistort or cut out important elements from our perspectives of the world. Under\nthis principle, it is essential to involve diverse views and teams from the\nearliest stages of their design and development. This has been highlighted, for\ninstance, in recent European Union regulations such as the Digital Services\nAct, via the requirement of risk monitoring, including the risk of\ndiscrimination, and the AI Act, through the requirement to involve people with\ndiverse backgrounds in the development of AI systems. We look into the\ngeographic diversity of the recommender systems research community,\nspecifically by analyzing the affiliation countries of the authors who\ncontributed to the ACM Conference on Recommender Systems (RecSys) during the\nlast 15 years. This study has been carried out in the framework of the\nDiversity in AI - DivinAI project, whose main objective is the long-term\nmonitoring of diversity in AI forums through a set of indexes.",
        "translated": ""
    },
    {
        "title": "Provider Fairness and Beyond-Accuracy Trade-offs in Recommender Systems",
        "url": "http://arxiv.org/abs/2309.04250v1",
        "pub_date": "2023-09-08",
        "summary": "Recommender systems, while transformative in online user experiences, have\nraised concerns over potential provider-side fairness issues. These systems may\ninadvertently favor popular items, thereby marginalizing less popular ones and\ncompromising provider fairness. While previous research has recognized\nprovider-side fairness issues, the investigation into how these biases affect\nbeyond-accuracy aspects of recommendation systems - such as diversity, novelty,\ncoverage, and serendipity - has been less emphasized. In this paper, we address\nthis gap by introducing a simple yet effective post-processing re-ranking model\nthat prioritizes provider fairness, while simultaneously maintaining user\nrelevance and recommendation quality. We then conduct an in-depth evaluation of\nthe model's impact on various aspects of recommendation quality across multiple\ndatasets. Specifically, we apply the post-processing algorithm to four distinct\nrecommendation models across four varied domain datasets, assessing the\nimprovement in each metric, encompassing both accuracy and beyond-accuracy\naspects. This comprehensive analysis allows us to gauge the effectiveness of\nour approach in mitigating provider biases. Our findings underscore the\neffectiveness of the adopted method in improving provider fairness and\nrecommendation quality. They also provide valuable insights into the trade-offs\ninvolved in achieving fairness in recommender systems, contributing to a more\nnuanced understanding of this complex issue.",
        "translated": ""
    },
    {
        "title": "Offline Recommender System Evaluation under Unobserved Confounding",
        "url": "http://arxiv.org/abs/2309.04222v1",
        "pub_date": "2023-09-08",
        "summary": "Off-Policy Estimation (OPE) methods allow us to learn and evaluate\ndecision-making policies from logged data. This makes them an attractive choice\nfor the offline evaluation of recommender systems, and several recent works\nhave reported successful adoption of OPE methods to this end. An important\nassumption that makes this work is the absence of unobserved confounders:\nrandom variables that influence both actions and rewards at data collection\ntime. Because the data collection policy is typically under the practitioner's\ncontrol, the unconfoundedness assumption is often left implicit, and its\nviolations are rarely dealt with in the existing literature.\n  This work aims to highlight the problems that arise when performing\noff-policy estimation in the presence of unobserved confounders, specifically\nfocusing on a recommendation use-case. We focus on policy-based estimators,\nwhere the logging propensities are learned from logged data. We characterise\nthe statistical bias that arises due to confounding, and show how existing\ndiagnostics are unable to uncover such cases. Because the bias depends directly\non the true and unobserved logging propensities, it is non-identifiable. As the\nunconfoundedness assumption is famously untestable, this becomes especially\nproblematic. This paper emphasises this common, yet often overlooked issue.\nThrough synthetic data, we empirically show how na\\\"ive propensity estimation\nunder confounding can lead to severely biased metric estimates that are allowed\nto fly under the radar. We aim to cultivate an awareness among researchers and\npractitioners of this important problem, and touch upon potential research\ndirections towards mitigating its effects.",
        "translated": ""
    },
    {
        "title": "Receiving an algorithmic recommendation based on documentary filmmaking\n  techniques",
        "url": "http://arxiv.org/abs/2309.04184v1",
        "pub_date": "2023-09-08",
        "summary": "This article analyzes the reception of a novel algorithmic recommendation of\ndocumentary films by a panel of moviegoers of the T{\\\"e}nk platform. In order\nto propose an alternative to recommendations based on a thematic\nclassification, the director or the production period, a set of metadata has\nbeen elaborated within the framework of this experimentation in order to\ncharacterize the great variety of ``documentary filmmaking dispositifs'' . The\ngoal is to investigate the different ways in which the platform's film lovers\nappropriate a personalized recommendation of 4 documentaries with similar or\nsimilar filmmaking dispositifs. To conclude, the contributions and limits of\nthis proof of concept are discussed in order to sketch out avenues of\nreflection for improving the instrumented mediation of documentary films.",
        "translated": ""
    },
    {
        "title": "A Long-Tail Friendly Representation Framework for Artist and Music\n  Similarity",
        "url": "http://arxiv.org/abs/2309.04182v1",
        "pub_date": "2023-09-08",
        "summary": "The investigation of the similarity between artists and music is crucial in\nmusic retrieval and recommendation, and addressing the challenge of the\nlong-tail phenomenon is increasingly important. This paper proposes a Long-Tail\nFriendly Representation Framework (LTFRF) that utilizes neural networks to\nmodel the similarity relationship. Our approach integrates music, user,\nmetadata, and relationship data into a unified metric learning framework, and\nemploys a meta-consistency relationship as a regular term to introduce the\nMulti-Relationship Loss. Compared to the Graph Neural Network (GNN), our\nproposed framework improves the representation performance in long-tail\nscenarios, which are characterized by sparse relationships between artists and\nmusic. We conduct experiments and analysis on the AllMusic dataset, and the\nresults demonstrate that our framework provides a favorable generalization of\nartist and music representation. Specifically, on similar artist/music\nrecommendation tasks, the LTFRF outperforms the baseline by 9.69%/19.42% in Hit\nRatio@10, and in long-tail cases, the framework achieves 11.05%/14.14% higher\nthan the baseline in Consistent@10.",
        "translated": ""
    },
    {
        "title": "PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded\n  Diffraction Patterns Phase Retrieval",
        "url": "http://arxiv.org/abs/2309.04171v1",
        "pub_date": "2023-09-08",
        "summary": "The problem of phase retrieval (PR) involves recovering an unknown image from\nlimited amplitude measurement data and is a challenge nonlinear inverse problem\nin computational imaging and image processing. However, many of the PR methods\nare based on black-box network models that lack interpretability and\nplug-and-play (PnP) frameworks that are computationally complex and require\ncareful parameter tuning. To address this, we have developed PRISTA-Net, a deep\nunfolding network (DUN) based on the first-order iterative shrinkage\nthresholding algorithm (ISTA). This network utilizes a learnable nonlinear\ntransformation to address the proximal-point mapping sub-problem associated\nwith the sparse priors, and an attention mechanism to focus on phase\ninformation containing image edges, textures, and structures. Additionally, the\nfast Fourier transform (FFT) is used to learn global features to enhance local\ninformation, and the designed logarithmic-based loss function leads to\nsignificant improvements when the noise level is low. All parameters in the\nproposed PRISTA-Net framework, including the nonlinear transformation,\nthreshold parameters, and step size, are learned end-to-end instead of being\nmanually set. This method combines the interpretability of traditional methods\nwith the fast inference ability of deep learning and is able to handle noise at\neach iteration during the unfolding stage, thus improving recovery quality.\nExperiments on Coded Diffraction Patterns (CDPs) measurements demonstrate that\nour approach outperforms the existing state-of-the-art methods in terms of\nqualitative and quantitative evaluations. Our source codes are available at\n\\emph{https://github.com/liuaxou/PRISTA-Net}.",
        "translated": ""
    },
    {
        "title": "D2WFP: A Novel Protocol for Forensically Identifying, Extracting, and\n  Analysing Deep and Dark Web Browsing Activities",
        "url": "http://arxiv.org/abs/2309.05537v1",
        "pub_date": "2023-09-11",
        "summary": "The use of the un-indexed web, commonly known as the deep web and dark web,\nto commit or facilitate criminal activity has drastically increased over the\npast decade. The dark web is an in-famously dangerous place where all kinds of\ncriminal activities take place [1-2], despite advances in web forensics\ntechniques, tools, and methodologies, few studies have formally tackled the\ndark and deep web forensics and the technical differences in terms of\ninvestigative techniques and artefacts identification and extraction. This\nresearch proposes a novel and comprehensive protocol to guide and assist\ndigital forensics professionals in investigating crimes committed on or via the\ndeep and dark web, The protocol named D2WFP establishes a new sequential\napproach for performing investigative activities by observing the order of\nvolatility and implementing a systemic approach covering all browsing related\nhives and artefacts which ultimately resulted into improv-ing the accuracy and\neffectiveness. Rigorous quantitative and qualitative research has been\nconducted by assessing D2WFP following a scientifically-sound and comprehensive\nprocess in different scenarios and the obtained results show an apparent\nincrease in the number of artefacts re-covered when adopting D2WFP which\noutperform any current industry or opensource browsing forensics tools. The\nsecond contribution of D2WFP is the robust formulation of artefact correlation\nand cross-validation within D2WFP which enables digital forensics professionals\nto better document and structure their analysis of host-based deep and dark web\nbrowsing artefacts.",
        "translated": ""
    },
    {
        "title": "Re-formalization of Individual Fairness",
        "url": "http://arxiv.org/abs/2309.05521v1",
        "pub_date": "2023-09-11",
        "summary": "The notion of individual fairness is a formalization of an ethical principle,\n\"Treating like cases alike,\" which has been argued such as by Aristotle. In a\nfairness-aware machine learning context, Dwork et al. firstly formalized the\nnotion. In their formalization, a similar pair of data in an unfair space\nshould be mapped to similar positions in a fair space. We propose to\nre-formalize individual fairness by the statistical independence conditioned by\nindividuals. This re-formalization has the following merits. First, our\nformalization is compatible with that of Dwork et al. Second, our formalization\nenables to combine individual fairness with the fairness notion, equalized odds\nor sufficiency, as well as statistical parity. Third, though their\nformalization implicitly assumes a pre-process approach for making fair\nprediction, our formalization is applicable to an in-process or post-process\napproach.",
        "translated": ""
    },
    {
        "title": "Towards Content-based Pixel Retrieval in Revisited Oxford and Paris",
        "url": "http://arxiv.org/abs/2309.05438v1",
        "pub_date": "2023-09-11",
        "summary": "This paper introduces the first two pixel retrieval benchmarks. Pixel\nretrieval is segmented instance retrieval. Like semantic segmentation extends\nclassification to the pixel level, pixel retrieval is an extension of image\nretrieval and offers information about which pixels are related to the query\nobject. In addition to retrieving images for the given query, it helps users\nquickly identify the query object in true positive images and exclude false\npositive images by denoting the correlated pixels. Our user study results show\npixel-level annotation can significantly improve the user experience.\n  Compared with semantic and instance segmentation, pixel retrieval requires a\nfine-grained recognition capability for variable-granularity targets. To this\nend, we propose pixel retrieval benchmarks named PROxford and PRParis, which\nare based on the widely used image retrieval datasets, ROxford and RParis.\nThree professional annotators label 5,942 images with two rounds of\ndouble-checking and refinement. Furthermore, we conduct extensive experiments\nand analysis on the SOTA methods in image search, image matching, detection,\nsegmentation, and dense matching using our pixel retrieval benchmarks. Results\nshow that the pixel retrieval task is challenging to these approaches and\ndistinctive from existing problems, suggesting that further research can\nadvance the content-based pixel-retrieval and thus user search experience. The\ndatasets can be downloaded from\n\\href{https://github.com/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval}{this\nlink}.",
        "translated": ""
    },
    {
        "title": "Formalizing Multimedia Recommendation through Multimodal Deep Learning",
        "url": "http://arxiv.org/abs/2309.05273v1",
        "pub_date": "2023-09-11",
        "summary": "Recommender systems (RSs) offer personalized navigation experiences on online\nplatforms, but recommendation remains a challenging task, particularly in\nspecific scenarios and domains. Multimodality can help tap into richer\ninformation sources and construct more refined user/item profiles for\nrecommendations. However, existing literature lacks a shared and universal\nschema for modeling and solving the recommendation problem through the lens of\nmultimodality. This work aims to formalize a general multimodal schema for\nmultimedia recommendation. It provides a comprehensive literature review of\nmultimodal approaches for multimedia recommendation from the last eight years,\noutlines the theoretical foundations of a multimodal pipeline, and demonstrates\nits rationale by applying it to selected state-of-the-art approaches. The work\nalso conducts a benchmarking analysis of recent algorithms for multimedia\nrecommendation within Elliot, a rigorous framework for evaluating recommender\nsystems. The main aim is to provide guidelines for designing and implementing\nthe next generation of multimodal approaches in multimedia recommendation.",
        "translated": ""
    },
    {
        "title": "Generating Natural Language Queries for More Effective Systematic Review\n  Screening Prioritisation",
        "url": "http://arxiv.org/abs/2309.05238v1",
        "pub_date": "2023-09-11",
        "summary": "Screening prioritisation in medical systematic reviews aims to rank the set\nof documents retrieved by complex Boolean queries. The goal is to prioritise\nthe most important documents so that subsequent review steps can be carried out\nmore efficiently and effectively. The current state of the art uses the final\ntitle of the review to rank documents using BERT-based neural neural rankers.\nHowever, the final title is only formulated at the end of the review process,\nwhich makes this approach impractical as it relies on ex post facto\ninformation. At the time of screening, only a rough working title is available,\nwith which the BERT-based ranker achieves is significantly worse than the final\ntitle. In this paper, we explore alternative sources of queries for screening\nprioritisation, such as the Boolean query used to retrieve the set of documents\nto be screened, and queries generated by instruction-based generative large\nlanguage models such as ChatGPT and Alpaca. Our best approach is not only\npractical based on the information available at screening time, but is similar\nin effectiveness with the final title.",
        "translated": ""
    },
    {
        "title": "Learning Personalized User Preference from Cold Start in Multi-turn\n  Conversations",
        "url": "http://arxiv.org/abs/2309.05127v1",
        "pub_date": "2023-09-10",
        "summary": "This paper presents a novel teachable conversation interaction system that is\ncapable of learning users preferences from cold start by gradually adapting to\npersonal preferences. In particular, the TAI system is able to automatically\nidentify and label user preference in live interactions, manage dialogue flows\nfor interactive teaching sessions, and reuse learned preference for preference\nelicitation. We develop the TAI system by leveraging BERT encoder models to\nencode both dialogue and relevant context information, and build action\nprediction (AP), argument filling (AF) and named entity recognition (NER)\nmodels to understand the teaching session. We adopt a seeker-provider\ninteraction loop mechanism to generate diverse dialogues from cold-start. TAI\nis capable of learning user preference, which achieves 0.9122 turn level\naccuracy on out-of-sample dataset, and has been successfully adopted in\nproduction.",
        "translated": ""
    },
    {
        "title": "Personalized Search Via Neural Contextual Semantic Relevance Ranking",
        "url": "http://arxiv.org/abs/2309.05113v1",
        "pub_date": "2023-09-10",
        "summary": "Existing neural relevance models do not give enough consideration for query\nand item context information which diversifies the search results to adapt for\npersonal preference. To bridge this gap, this paper presents a neural learning\nframework to personalize document ranking results by leveraging the signals to\ncapture how the document fits into users' context. In particular, it models the\nrelationships between document content and user query context using both\nlexical representations and semantic embeddings such that the user's intent can\nbe better understood by data enrichment of personalized query context\ninformation. Extensive experiments performed on the search dataset, demonstrate\nthe effectiveness of the proposed method.",
        "translated": ""
    },
    {
        "title": "Duplicate Question Retrieval and Confirmation Time Prediction in\n  Software Communities",
        "url": "http://arxiv.org/abs/2309.05035v1",
        "pub_date": "2023-09-10",
        "summary": "Community Question Answering (CQA) in different domains is growing at a large\nscale because of the availability of several platforms and huge shareable\ninformation among users. With the rapid growth of such online platforms, a\nmassive amount of archived data makes it difficult for moderators to retrieve\npossible duplicates for a new question and identify and confirm existing\nquestion pairs as duplicates at the right time. This problem is even more\ncritical in CQAs corresponding to large software systems like askubuntu where\nmoderators need to be experts to comprehend something as a duplicate. Note that\nthe prime challenge in such CQA platforms is that the moderators are themselves\nexperts and are therefore usually extremely busy with their time being\nextraordinarily expensive. To facilitate the task of the moderators, in this\nwork, we have tackled two significant issues for the askubuntu CQA platform:\n(1) retrieval of duplicate questions given a new question and (2) duplicate\nquestion confirmation time prediction. In the first task, we focus on\nretrieving duplicate questions from a question pool for a particular newly\nposted question. In the second task, we solve a regression problem to rank a\npair of questions that could potentially take a long time to get confirmed as\nduplicates. For duplicate question retrieval, we propose a Siamese neural\nnetwork based approach by exploiting both text and network-based features,\nwhich outperforms several state-of-the-art baseline techniques. Our method\noutperforms DupPredictor and DUPE by 5% and 7% respectively. For duplicate\nconfirmation time prediction, we have used both the standard machine learning\nmodels and neural network along with the text and graph-based features. We\nobtain Spearman's rank correlation of 0.20 and 0.213 (statistically\nsignificant) for text and graph based features respectively.",
        "translated": ""
    },
    {
        "title": "Streamlined Data Fusion: Unleashing the Power of Linear Combination with\n  Minimal Relevance Judgments",
        "url": "http://arxiv.org/abs/2309.04981v1",
        "pub_date": "2023-09-10",
        "summary": "Linear combination is a potent data fusion method in information retrieval\ntasks, thanks to its ability to adjust weights for diverse scenarios. However,\nachieving optimal weight training has traditionally required manual relevance\njudgments on a large percentage of documents, a labor-intensive and expensive\nprocess. In this study, we investigate the feasibility of obtaining\nnear-optimal weights using a mere 20\\%-50\\% of relevant documents. Through\nexperiments on four TREC datasets, we find that weights trained with multiple\nlinear regression using this reduced set closely rival those obtained with\nTREC's official \"qrels.\" Our findings unlock the potential for more efficient\nand affordable data fusion, empowering researchers and practitioners to reap\nits full benefits with significantly less effort.",
        "translated": ""
    },
    {
        "title": "Multi-modal Extreme Classification",
        "url": "http://arxiv.org/abs/2309.04961v1",
        "pub_date": "2023-09-10",
        "summary": "This paper develops the MUFIN technique for extreme classification (XC) tasks\nwith millions of labels where datapoints and labels are endowed with visual and\ntextual descriptors. Applications of MUFIN to product-to-product recommendation\nand bid query prediction over several millions of products are presented.\nContemporary multi-modal methods frequently rely on purely embedding-based\nmethods. On the other hand, XC methods utilize classifier architectures to\noffer superior accuracies than embedding only methods but mostly focus on\ntext-based categorization tasks. MUFIN bridges this gap by reformulating\nmulti-modal categorization as an XC problem with several millions of labels.\nThis presents the twin challenges of developing multi-modal architectures that\ncan offer embeddings sufficiently expressive to allow accurate categorization\nover millions of labels; and training and inference routines that scale\nlogarithmically in the number of labels. MUFIN develops an architecture based\non cross-modal attention and trains it in a modular fashion using pre-training\nand positive and negative mining. A novel product-to-product recommendation\ndataset MM-AmazonTitles-300K containing over 300K products was curated from\npublicly available amazon.com listings with each product endowed with a title\nand multiple images. On the all datasets MUFIN offered at least 3% higher\naccuracy than leading text-based, image-based and multi-modal techniques. Code\nfor MUFIN is available at https://github.com/Extreme-classification/MUFIN",
        "translated": ""
    },
    {
        "title": "Human Action Co-occurrence in Lifestyle Vlogs using Graph Link\n  Prediction",
        "url": "http://arxiv.org/abs/2309.06219v1",
        "pub_date": "2023-09-12",
        "summary": "We introduce the task of automatic human action co-occurrence identification,\ni.e., determine whether two human actions can co-occur in the same interval of\ntime. We create and make publicly available the ACE (Action Co-occurrencE)\ndataset, consisting of a large graph of ~12k co-occurring pairs of visual\nactions and their corresponding video clips. We describe graph link prediction\nmodels that leverage visual and textual information to automatically infer if\ntwo actions are co-occurring. We show that graphs are particularly well suited\nto capture relations between human actions, and the learned graph\nrepresentations are effective for our task and capture novel and relevant\ninformation across different data domains. The ACE dataset and the code\nintroduced in this paper are publicly available at\nhttps://github.com/MichiganNLP/vlog_action_co-occurrence.",
        "translated": ""
    },
    {
        "title": "HAMUR: Hyper Adapter for Multi-Domain Recommendation",
        "url": "http://arxiv.org/abs/2309.06217v1",
        "pub_date": "2023-09-12",
        "summary": "Multi-Domain Recommendation (MDR) has gained significant attention in recent\nyears, which leverages data from multiple domains to enhance their performance\nconcurrently.However, current MDR models are confronted with two limitations.\nFirstly, the majority of these models adopt an approach that explicitly shares\nparameters between domains, leading to mutual interference among them.\nSecondly, due to the distribution differences among domains, the utilization of\nstatic parameters in existing methods limits their flexibility to adapt to\ndiverse domains. To address these challenges, we propose a novel model Hyper\nAdapter for Multi-Domain Recommendation (HAMUR). Specifically, HAMUR consists\nof two components: (1). Domain-specific adapter, designed as a pluggable module\nthat can be seamlessly integrated into various existing multi-domain backbone\nmodels, and (2). Domain-shared hyper-network, which implicitly captures shared\ninformation among domains and dynamically generates the parameters for the\nadapter. We conduct extensive experiments on two public datasets using various\nbackbone networks. The experimental results validate the effectiveness and\nscalability of the proposed model.",
        "translated": ""
    },
    {
        "title": "Improving and Evaluating the Detection of Fragmentation in News\n  Recommendations with the Clustering of News Story Chains",
        "url": "http://arxiv.org/abs/2309.06192v1",
        "pub_date": "2023-09-12",
        "summary": "News recommender systems play an increasingly influential role in shaping\ninformation access within democratic societies. However, tailoring\nrecommendations to users' specific interests can result in the divergence of\ninformation streams. Fragmented access to information poses challenges to the\nintegrity of the public sphere, thereby influencing democracy and public\ndiscourse. The Fragmentation metric quantifies the degree of fragmentation of\ninformation streams in news recommendations. Accurate measurement of this\nmetric requires the application of Natural Language Processing (NLP) to\nidentify distinct news events, stories, or timelines. This paper presents an\nextensive investigation of various approaches for quantifying Fragmentation in\nnews recommendations. These approaches are evaluated both intrinsically, by\nmeasuring performance on news story clustering, and extrinsically, by assessing\nthe Fragmentation scores of different simulated news recommender scenarios. Our\nfindings demonstrate that agglomerative hierarchical clustering coupled with\nSentenceBERT text representation is substantially better at detecting\nFragmentation than earlier implementations. Additionally, the analysis of\nsimulated scenarios yields valuable insights and recommendations for\nstakeholders concerning the measurement and interpretation of Fragmentation.",
        "translated": ""
    },
    {
        "title": "AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity\n  Recognition and Linking",
        "url": "http://arxiv.org/abs/2309.06175v1",
        "pub_date": "2023-09-12",
        "summary": "This paper presents a novel approach to address the Entity Recognition and\nLinking Challenge at NLPCC 2015. The task involves extracting named entity\nmentions from short search queries and linking them to entities within a\nreference Chinese knowledge base. To tackle this problem, we first expand the\nexisting knowledge base and utilize external knowledge to identify candidate\nentities, thereby improving the recall rate. Next, we extract features from the\ncandidate entities and utilize Support Vector Regression and Multiple Additive\nRegression Tree as scoring functions to filter the results. Additionally, we\napply rules to further refine the results and enhance precision. Our method is\ncomputationally efficient and achieves an F1 score of 0.535.",
        "translated": ""
    },
    {
        "title": "Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning\n  Strategies are not Better than Random Selection",
        "url": "http://arxiv.org/abs/2309.06131v1",
        "pub_date": "2023-09-12",
        "summary": "Search methods based on Pretrained Language Models (PLM) have demonstrated\ngreat effectiveness gains compared to statistical and early neural ranking\nmodels. However, fine-tuning PLM-based rankers requires a great amount of\nannotated training data. Annotating data involves a large manual effort and\nthus is expensive, especially in domain specific tasks. In this paper we\ninvestigate fine-tuning PLM-based rankers under limited training data and\nbudget. We investigate two scenarios: fine-tuning a ranker from scratch, and\ndomain adaptation starting with a ranker already fine-tuned on general data,\nand continuing fine-tuning on a target dataset. We observe a great variability\nin effectiveness when fine-tuning on different randomly selected subsets of\ntraining data. This suggests that it is possible to achieve effectiveness gains\nby actively selecting a subset of the training data that has the most positive\neffect on the rankers. This way, it would be possible to fine-tune effective\nPLM rankers at a reduced annotation budget. To investigate this, we adapt\nexisting Active Learning (AL) strategies to the task of fine-tuning PLM rankers\nand investigate their effectiveness, also considering annotation and\ncomputational costs. Our extensive analysis shows that AL strategies do not\nsignificantly outperform random selection of training subsets in terms of\neffectiveness. We further find that gains provided by AL strategies come at the\nexpense of more assessments (thus higher annotation costs) and AL strategies\nunderperform random selection when comparing effectiveness given a fixed\nannotation cost. Our results highlight that ``optimal'' subsets of training\ndata that provide high effectiveness at low annotation cost do exist, but\ncurrent mainstream AL strategies applied to PLM rankers are not capable of\nidentifying them.",
        "translated": ""
    },
    {
        "title": "Characterizing Latent Perspectives of Media Houses Towards Public\n  Figures",
        "url": "http://arxiv.org/abs/2309.06112v1",
        "pub_date": "2023-09-12",
        "summary": "Media houses reporting on public figures, often come with their own biases\nstemming from their respective worldviews. A characterization of these\nunderlying patterns helps us in better understanding and interpreting news\nstories. For this, we need diverse or subjective summarizations, which may not\nbe amenable for classifying into predefined class labels. This work proposes a\nzero-shot approach for non-extractive or generative characterizations of person\nentities from a corpus using GPT-2. We use well-articulated articles from\nseveral well-known news media houses as a corpus to build a sound argument for\nthis approach. First, we fine-tune a GPT-2 pre-trained language model with a\ncorpus where specific person entities are characterized. Second, we further\nfine-tune this with demonstrations of person entity characterizations, created\nfrom a corpus of programmatically constructed characterizations. This twice\nfine-tuned model is primed with manual prompts consisting of entity names that\nwere not previously encountered in the second fine-tuning, to generate a simple\nsentence about the entity. The results were encouraging, when compared against\nactual characterizations from the corpus.",
        "translated": ""
    },
    {
        "title": "Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering\n  Trends across Diverse Platforms",
        "url": "http://arxiv.org/abs/2309.05961v1",
        "pub_date": "2023-09-12",
        "summary": "Community Question Answering (CQA) platforms steadily gain popularity as they\nprovide users with fast responses to their queries. The swiftness of these\nresponses is contingent on a mixture of query-specific and user-related\nelements. This paper scrutinizes these contributing factors within the context\nof six highly popular CQA platforms, identified through their standout\nanswering speed. Our investigation reveals a correlation between the time taken\nto yield the first response to a question and several variables: the metadata,\nthe formulation of the questions, and the level of interaction among users.\nAdditionally, by employing conventional machine learning models to analyze\nthese metadata and patterns of user interaction, we endeavor to predict which\nqueries will receive their initial responses promptly.",
        "translated": ""
    },
    {
        "title": "GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection",
        "url": "http://arxiv.org/abs/2309.05953v1",
        "pub_date": "2023-09-12",
        "summary": "Logs play a crucial role in system monitoring and debugging by recording\nvaluable system information, including events and states. Although various\nmethods have been proposed to detect anomalies in log sequences, they often\noverlook the significance of considering relations among system components,\nsuch as services and users, which can be identified from log contents.\nUnderstanding these relations is vital for detecting anomalies and their\nunderlying causes. To address this issue, we introduce GLAD, a Graph-based Log\nAnomaly Detection framework designed to detect relational anomalies in system\nlogs. GLAD incorporates log semantics, relational patterns, and sequential\npatterns into a unified framework for anomaly detection. Specifically, GLAD\nfirst introduces a field extraction module that utilizes prompt-based few-shot\nlearning to identify essential fields from log contents. Then GLAD constructs\ndynamic log graphs for sliding windows by interconnecting extracted fields and\nlog events parsed from the log parser. These graphs represent events and fields\nas nodes and their relations as edges. Subsequently, GLAD utilizes a\ntemporal-attentive graph edge anomaly detection model for identifying anomalous\nrelations in these dynamic log graphs. This model employs a Graph Neural\nNetwork (GNN)-based encoder enhanced with transformers to capture content,\nstructural and temporal features. We evaluate our proposed method on three\ndatasets, and the results demonstrate the effectiveness of GLAD in detecting\nanomalies indicated by varying relational patterns.",
        "translated": ""
    },
    {
        "title": "A Survey of Hallucination in Large Foundation Models",
        "url": "http://arxiv.org/abs/2309.05922v1",
        "pub_date": "2023-09-12",
        "summary": "Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large'' Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.",
        "translated": ""
    },
    {
        "title": "SAGE: Structured Attribute Value Generation for Billion-Scale Product\n  Catalogs",
        "url": "http://arxiv.org/abs/2309.05920v1",
        "pub_date": "2023-09-12",
        "summary": "We introduce SAGE; a Generative LLM for inferring attribute values for\nproducts across world-wide e-Commerce catalogs. We introduce a novel\nformulation of the attribute-value prediction problem as a Seq2Seq\nsummarization task, across languages, product types and target attributes. Our\nnovel modeling approach lifts the restriction of predicting attribute values\nwithin a pre-specified set of choices, as well as, the requirement that the\nsought attribute values need to be explicitly mentioned in the text. SAGE can\ninfer attribute values even when such values are mentioned implicitly using\nperiphrastic language, or not-at-all-as is the case for common-sense defaults.\nAdditionally, SAGE is capable of predicting whether an attribute is\ninapplicable for the product at hand, or non-obtainable from the available\ninformation. SAGE is the first method able to tackle all aspects of the\nattribute-value-prediction task as they arise in practical settings in\ne-Commerce catalogs. A comprehensive set of experiments demonstrates the\neffectiveness of the proposed approach, as well as, its superiority against\nstate-of-the-art competing alternatives. Moreover, our experiments highlight\nSAGE's ability to tackle the task of predicting attribute values in zero-shot\nsetting; thereby, opening up opportunities for significantly reducing the\noverall number of labeled examples required for training.",
        "translated": ""
    },
    {
        "title": "Résumé Parsing as Hierarchical Sequence Labeling: An Empirical Study",
        "url": "http://arxiv.org/abs/2309.07015v1",
        "pub_date": "2023-09-13",
        "summary": "Extracting information from r\\'esum\\'es is typically formulated as a\ntwo-stage problem, where the document is first segmented into sections and then\neach section is processed individually to extract the target entities. Instead,\nwe cast the whole problem as sequence labeling in two levels -- lines and\ntokens -- and study model architectures for solving both tasks simultaneously.\nWe build high-quality r\\'esum\\'e parsing corpora in English, French, Chinese,\nSpanish, German, Portuguese, and Swedish. Based on these corpora, we present\nexperimental results that demonstrate the effectiveness of the proposed models\nfor the information extraction task, outperforming approaches introduced in\nprevious work. We conduct an ablation study of the proposed architectures. We\nalso analyze both model performance and resource efficiency, and describe the\ntrade-offs for model deployment in the context of a production environment.",
        "translated": ""
    },
    {
        "title": "Modeling Dislocation Dynamics Data Using Semantic Web Technologies",
        "url": "http://arxiv.org/abs/2309.06930v1",
        "pub_date": "2023-09-13",
        "summary": "Research in the field of Materials Science and Engineering focuses on the\ndesign, synthesis, properties, and performance of materials. An important class\nof materials that is widely investigated are crystalline materials, including\nmetals and semiconductors. Crystalline material typically contains a distinct\ntype of defect called \"dislocation\". This defect significantly affects various\nmaterial properties, including strength, fracture toughness, and ductility.\nResearchers have devoted a significant effort in recent years to understanding\ndislocation behavior through experimental characterization techniques and\nsimulations, e.g., dislocation dynamics simulations. This paper presents how\ndata from dislocation dynamics simulations can be modeled using semantic web\ntechnologies through annotating data with ontologies. We extend the already\nexisting Dislocation Ontology by adding missing concepts and aligning it with\ntwo other domain-related ontologies (i.e., the Elementary Multi-perspective\nMaterial Ontology and the Materials Design Ontology) allowing for representing\nthe dislocation simulation data efficiently. Moreover, we show a real-world use\ncase by representing the discrete dislocation dynamics data as a knowledge\ngraph (DisLocKG) that illustrates the relationship between them. We also\ndeveloped a SPARQL endpoint that brings extensive flexibility to query\nDisLocKG.",
        "translated": ""
    },
    {
        "title": "Multi-behavior Recommendation with SVD Graph Neural Networks",
        "url": "http://arxiv.org/abs/2309.06912v1",
        "pub_date": "2023-09-13",
        "summary": "Graph Neural Networks (GNNs) has been extensively employed in the field of\nrecommender systems, offering users personalized recommendations and yielding\nremarkable outcomes. Recently, GNNs incorporating contrastive learning have\ndemonstrated promising performance in handling sparse data problem of\nrecommendation system. However, existing contrastive learning methods still\nhave limitations in addressing the cold-start problem and resisting noise\ninterference especially for multi-behavior recommendation. To mitigate the\naforementioned issues, the present research posits a GNNs based multi-behavior\nrecommendation model MB-SVD that utilizes Singular Value Decomposition (SVD)\ngraphs to enhance model performance. In particular, MB-SVD considers user\npreferences under different behaviors, improving recommendation effectiveness\nwhile better addressing the cold-start problem. Our model introduces an\ninnovative methodology, which subsume multi-behavior contrastive learning\nparadigm to proficiently discern the intricate interconnections among\nheterogeneous manifestations of user behavior and generates SVD graphs to\nautomate the distillation of crucial multi-behavior self-supervised information\nfor robust graph augmentation. Furthermore, the SVD based framework reduces the\nembedding dimensions and computational load. Thorough experimentation showcases\nthe remarkable performance of our proposed MB-SVD approach in multi-behavior\nrecommendation endeavors across diverse real-world datasets.",
        "translated": ""
    },
    {
        "title": "Towards the TopMost: A Topic Modeling System Toolkit",
        "url": "http://arxiv.org/abs/2309.06908v1",
        "pub_date": "2023-09-13",
        "summary": "Topic models have been proposed for decades with various applications and\nrecently refreshed by the neural variational inference. However, these topic\nmodels adopt totally distinct dataset, implementation, and evaluation settings,\nwhich hinders their quick utilization and fair comparisons. This greatly\nhinders the research progress of topic models. To address these issues, in this\npaper we propose a Topic Modeling System Toolkit (TopMost). Compared to\nexisting toolkits, TopMost stands out by covering a wider range of topic\nmodeling scenarios including complete lifecycles with dataset pre-processing,\nmodel training, testing, and evaluations. The highly cohesive and decoupled\nmodular design of TopMost enables quick utilization, fair comparisons, and\nflexible extensions of different topic models. This can facilitate the research\nand applications of topic models. Our code, tutorials, and documentation are\navailable at https://github.com/bobxwu/topmost.",
        "translated": ""
    },
    {
        "title": "ProMap: Datasets for Product Mapping in E-commerce",
        "url": "http://arxiv.org/abs/2309.06882v1",
        "pub_date": "2023-09-13",
        "summary": "The goal of product mapping is to decide, whether two listings from two\ndifferent e-shops describe the same products. Existing datasets of matching and\nnon-matching pairs of products, however, often suffer from incomplete product\ninformation or contain only very distant non-matching products. Therefore,\nwhile predictive models trained on these datasets achieve good results on them,\nin practice, they are unusable as they cannot distinguish very similar but\nnon-matching pairs of products. This paper introduces two new datasets for\nproduct mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn\nconsisting of 1,555 English product pairs of matching and non-matching products\nmanually scraped from two pairs of e-shops. The datasets contain both images\nand textual descriptions of the products, including their specifications,\nmaking them one of the most complete datasets for product mapping.\nAdditionally, the non-matching products were selected in two phases, creating\ntwo types of non-matches -- close non-matches and medium non-matches. Even the\nmedium non-matches are pairs of products that are much more similar than\nnon-matches in other datasets -- for example, they still need to have the same\nbrand and similar name and price. After simple data preprocessing, several\nmachine learning algorithms were trained on these and two the other datasets to\ndemonstrate the complexity and completeness of ProMap datasets. ProMap datasets\nare presented as a golden standard for further research of product mapping\nfilling the gaps in existing ones.",
        "translated": ""
    },
    {
        "title": "An Image Dataset for Benchmarking Recommender Systems with Raw Pixels",
        "url": "http://arxiv.org/abs/2309.06789v1",
        "pub_date": "2023-09-13",
        "summary": "Recommender systems (RS) have achieved significant success by leveraging\nexplicit identification (ID) features. However, the full potential of content\nfeatures, especially the pure image pixel features, remains relatively\nunexplored. The limited availability of large, diverse, and content-driven\nimage recommendation datasets has hindered the use of raw images as item\nrepresentations. In this regard, we present PixelRec, a massive image-centric\nrecommendation dataset that includes approximately 200 million user-image\ninteractions, 30 million users, and 400,000 high-quality cover images. By\nproviding direct access to raw image pixels, PixelRec enables recommendation\nmodels to learn item representation directly from them. To demonstrate its\nutility, we begin by presenting the results of several classical pure ID-based\nbaseline models, termed IDNet, trained on PixelRec. Then, to show the\neffectiveness of the dataset's image features, we substitute the itemID\nembeddings (from IDNet) with a powerful vision encoder that represents items\nusing their raw image pixels. This new model is dubbed PixelNet.Our findings\nindicate that even in standard, non-cold start recommendation settings where\nIDNet is recognized as highly effective, PixelNet can already perform equally\nwell or even better than IDNet. Moreover, PixelNet has several other notable\nadvantages over IDNet, such as being more effective in cold-start and\ncross-domain recommendation scenarios. These results underscore the importance\nof visual features in PixelRec. We believe that PixelRec can serve as a\ncritical resource and testing ground for research on recommendation models that\nemphasize image pixel content. The dataset, code, and leaderboard will be\navailable at https://github.com/website-pixelrec/PixelRec.",
        "translated": ""
    },
    {
        "title": "CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data\n  Generation",
        "url": "http://arxiv.org/abs/2309.06748v1",
        "pub_date": "2023-09-13",
        "summary": "Conversational search provides a natural interface for information retrieval\n(IR). Recent approaches have demonstrated promising results in applying dense\nretrieval to conversational IR. However, training dense retrievers requires\nlarge amounts of in-domain paired data. This hinders the development of\nconversational dense retrievers, as abundant in-domain conversations are\nexpensive to collect. In this paper, we propose CONVERSER, a framework for\ntraining conversational dense retrievers with at most 6 examples of in-domain\ndialogues. Specifically, we utilize the in-context learning capability of large\nlanguage models to generate conversational queries given a passage in the\nretrieval corpus. Experimental results on conversational retrieval benchmarks\nOR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable\nperformance to fully-supervised models, demonstrating the effectiveness of our\nproposed framework in few-shot conversational dense retrieval. All source code\nand generated datasets are available at https://github.com/MiuLab/CONVERSER",
        "translated": ""
    },
    {
        "title": "Hierarchical Multi-Task Learning Framework for Session-based\n  Recommendations",
        "url": "http://arxiv.org/abs/2309.06533v1",
        "pub_date": "2023-09-12",
        "summary": "While session-based recommender systems (SBRSs) have shown superior\nrecommendation performance, multi-task learning (MTL) has been adopted by SBRSs\nto enhance their prediction accuracy and generalizability further. Hierarchical\nMTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds\noutputs from auxiliary tasks to main tasks. This hierarchy leads to richer\ninput features for main tasks and higher interpretability of predictions,\ncompared to existing MTL frameworks. However, the H-MTL framework has not been\ninvestigated in SBRSs yet. In this paper, we propose HierSRec which\nincorporates the H-MTL architecture into SBRSs. HierSRec encodes a given\nsession with a metadata-aware Transformer and performs next-category prediction\n(i.e., auxiliary task) with the session encoding. Next, HierSRec conducts\nnext-item prediction (i.e., main task) with the category prediction result and\nsession encoding. For scalable inference, HierSRec creates a compact set of\ncandidate items (e.g., 4% of total items) per test example using the category\nprediction. Experiments show that HierSRec outperforms existing SBRSs as per\nnext-item prediction accuracy on two session-based recommendation datasets. The\naccuracy of HierSRec measured with the carefully-curated candidate items aligns\nwith the accuracy of HierSRec calculated with all items, which validates the\nusefulness of our candidate generation scheme via H-MTL.",
        "translated": ""
    },
    {
        "title": "Ambiguity-Aware In-Context Learning with Large Language Models",
        "url": "http://arxiv.org/abs/2309.07900v1",
        "pub_date": "2023-09-14",
        "summary": "In-context learning (ICL) i.e. showing LLMs only a few task-specific\ndemonstrations has led to downstream gains with no task-specific fine-tuning\nrequired. However, LLMs are sensitive to the choice of prompts, and therefore a\ncrucial research question is how to select good demonstrations for ICL. One\neffective strategy is leveraging semantic similarity between the ICL\ndemonstrations and test inputs by using a text retriever, which however is\nsub-optimal as that does not consider the LLM's existing knowledge about that\ntask. From prior work (Min et al., 2022), we already know that labels paired\nwith the demonstrations bias the model predictions. This leads us to our\nhypothesis whether considering LLM's existing knowledge about the task,\nespecially with respect to the output label space can help in a better\ndemonstration selection strategy. Through extensive experimentation on three\ntext classification tasks, we find that it is beneficial to not only choose\nsemantically similar ICL demonstrations but also to choose those demonstrations\nthat help resolve the inherent label ambiguity surrounding the test example.\nInterestingly, we find that including demonstrations that the LLM previously\nmis-classified and also fall on the test example's decision boundary, brings\nthe most performance gain.",
        "translated": ""
    },
    {
        "title": "NineRec: A Benchmark Dataset Suite for Evaluating Transferable\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.07705v1",
        "pub_date": "2023-09-14",
        "summary": "Learning a recommender system model from an item's raw modality features\n(such as image, text, audio, etc.), called MoRec, has attracted growing\ninterest recently. One key advantage of MoRec is that it can easily benefit\nfrom advances in other fields, such as natural language processing (NLP) and\ncomputer vision (CV). Moreover, it naturally supports transfer learning across\ndifferent systems through modality features, known as transferable recommender\nsystems, or TransRec.\n  However, so far, TransRec has made little progress, compared to\ngroundbreaking foundation models in the fields of NLP and CV. The lack of\nlarge-scale, high-quality recommendation datasets poses a major obstacle. To\nthis end, we introduce NineRec, a TransRec dataset suite that includes a\nlarge-scale source domain recommendation dataset and nine diverse target domain\nrecommendation datasets. Each item in NineRec is represented by a text\ndescription and a high-resolution cover image. With NineRec, we can implement\nTransRec models in an end-to-end training manner instead of using pre-extracted\ninvariant features. We conduct a benchmark study and empirical analysis of\nTransRec using NineRec, and our findings provide several valuable insights. To\nsupport further research, we make our code, datasets, benchmarks, and\nleaderboards publicly available at\nhttps://github.com/anonymous?ninerec/NineRec.",
        "translated": ""
    },
    {
        "title": "A Conversation is Worth A Thousand Recommendations: A Survey of Holistic\n  Conversational Recommender Systems",
        "url": "http://arxiv.org/abs/2309.07682v1",
        "pub_date": "2023-09-14",
        "summary": "Conversational recommender systems (CRS) generate recommendations through an\ninteractive process. However, not all CRS approaches use human conversations as\ntheir source of interaction data; the majority of prior CRS work simulates\ninteractions by exchanging entity-level information. As a result, claims of\nprior CRS work do not generalise to real-world settings where conversations\ntake unexpected turns, or where conversational and intent understanding is not\nperfect. To tackle this challenge, the research community has started to\nexamine holistic CRS, which are trained using conversational data collected\nfrom real-world scenarios. Despite their emergence, such holistic approaches\nare under-explored.\n  We present a comprehensive survey of holistic CRS methods by summarizing the\nliterature in a structured manner. Our survey recognises holistic CRS\napproaches as having three components: 1) a backbone language model, the\noptional use of 2) external knowledge, and/or 3) external guidance. We also\ngive a detailed analysis of CRS datasets and evaluation methods in real\napplication scenarios. We offer our insight as to the current challenges of\nholistic CRS and possible future trends.",
        "translated": ""
    },
    {
        "title": "Feature Engineering in Learning-to-Rank for Community Question Answering\n  Task",
        "url": "http://arxiv.org/abs/2309.07610v1",
        "pub_date": "2023-09-14",
        "summary": "Community question answering (CQA) forums are Internet-based platforms where\nusers ask questions about a topic and other expert users try to provide\nsolutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer,\nStackExchange exist with a lot of user-generated data. These data are leveraged\nin automated CQA ranking systems where similar questions (and answers) are\npresented in response to the query of the user. In this work, we empirically\ninvestigate a few aspects of this domain. Firstly, in addition to traditional\nfeatures like TF-IDF, BM25 etc., we introduce a BERT-based feature that\ncaptures the semantic similarity between the question and answer. Secondly,\nmost of the existing research works have focused on features extracted only\nfrom the question part; features extracted from answers have not been explored\nextensively. We combine both types of features in a linear fashion. Thirdly,\nusing our proposed concepts, we conduct an empirical investigation with\ndifferent rank-learning algorithms, some of which have not been used so far in\nCQA domain. On three standard CQA datasets, our proposed framework achieves\nstate-of-the-art performance. We also analyze importance of the features we use\nin our investigation. This work is expected to guide the practitioners to\nselect a better set of features for the CQA retrieval task.",
        "translated": ""
    },
    {
        "title": "Zero-shot Audio Topic Reranking using Large Language Models",
        "url": "http://arxiv.org/abs/2309.07606v1",
        "pub_date": "2023-09-14",
        "summary": "The Multimodal Video Search by Examples (MVSE) project investigates using\nvideo clips as the query term for information retrieval, rather than the more\ntraditional text query. This enables far richer search modalities such as\nimages, speaker, content, topic, and emotion. A key element for this process is\nhighly rapid, flexible, search to support large archives, which in MVSE is\nfacilitated by representing video attributes by embeddings. This work aims to\nmitigate any performance loss from this rapid archive search by examining\nreranking approaches. In particular, zero-shot reranking methods using large\nlanguage models are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking can achieve improved retrieval ranking without the need for any\ntask-specific training data.",
        "translated": ""
    },
    {
        "title": "Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?",
        "url": "http://arxiv.org/abs/2309.07602v1",
        "pub_date": "2023-09-14",
        "summary": "Recently sequential recommendations and next-item prediction task has become\nincreasingly popular in the field of recommender systems. Currently, two\nstate-of-the-art baselines are Transformer-based models SASRec and BERT4Rec.\nOver the past few years, there have been quite a few publications comparing\nthese two algorithms and proposing new state-of-the-art models. In most of the\npublications, BERT4Rec achieves better performance than SASRec. But BERT4Rec\nuses cross-entropy over softmax for all items, while SASRec uses negative\nsampling and calculates binary cross-entropy loss for one positive and one\nnegative item. In our work, we show that if both models are trained with the\nsame loss, which is used by BERT4Rec, then SASRec will significantly outperform\nBERT4Rec both in terms of quality and training speed. In addition, we show that\nSASRec could be effectively trained with negative sampling and still outperform\nBERT4Rec, but the number of negative examples should be much larger than one.",
        "translated": ""
    },
    {
        "title": "C-Pack: Packaged Resources To Advance General Chinese Embedding",
        "url": "http://arxiv.org/abs/2309.07597v1",
        "pub_date": "2023-09-14",
        "summary": "We introduce C-Pack, a package of resources that significantly advance the\nfield of general Chinese embeddings. C-Pack includes three critical resources.\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\nC-TEM is a family of embedding models covering multiple sizes. Our models\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\ntime of the release. We also integrate and optimize the entire suite of\ntraining methods for C-TEM. Along with our resources on general Chinese\nembedding, we release our data and models for English text embeddings. The\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\nmeanwhile, our released English data is 2 times larger than the Chinese data.\nAll these resources are made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.",
        "translated": ""
    },
    {
        "title": "Neuro-Symbolic Recommendation Model based on Logic Query",
        "url": "http://arxiv.org/abs/2309.07594v1",
        "pub_date": "2023-09-14",
        "summary": "A recommendation system assists users in finding items that are relevant to\nthem. Existing recommendation models are primarily based on predicting\nrelationships between users and items and use complex matching models or\nincorporate extensive external information to capture association patterns in\ndata. However, recommendation is not only a problem of inductive statistics\nusing data; it is also a cognitive task of reasoning decisions based on\nknowledge extracted from information. Hence, a logic system could naturally be\nincorporated for the reasoning in a recommendation task. However, although\nhard-rule approaches based on logic systems can provide powerful reasoning\nability, they struggle to cope with inconsistent and incomplete knowledge in\nreal-world tasks, especially for complex tasks such as recommendation.\nTherefore, in this paper, we propose a neuro-symbolic recommendation model,\nwhich transforms the user history interactions into a logic expression and then\ntransforms the recommendation prediction into a query task based on this logic\nexpression. The logic expressions are then computed based on the modular logic\noperations of the neural network. We also construct an implicit logic encoder\nto reasonably reduce the complexity of the logic computation. Finally, a user's\ninterest items can be queried in the vector space based on the computation\nresults. Experiments on three well-known datasets verified that our method\nperforms better compared to state of the art shallow, deep, session, and\nreasoning models.",
        "translated": ""
    },
    {
        "title": "MMEAD: MS MARCO Entity Annotations and Disambiguations",
        "url": "http://arxiv.org/abs/2309.07574v1",
        "pub_date": "2023-09-14",
        "summary": "MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for\nentity links for the MS MARCO datasets. We specify a format to store and share\nlinks for both document and passage collections of MS MARCO. Following this\nspecification, we release entity links to Wikipedia for documents and passages\nin both MS MARCO collections (v1 and v2). Entity links have been produced by\nthe REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing\nusers to load the link data and entity embeddings effortlessly. Using MMEAD\ntakes only a few lines of code. Finally, we show how MMEAD can be used for IR\nresearch that uses entity information. We show how to improve recall@1000 and\nMRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this\nresource. We also demonstrate how entity expansions can be used for interactive\nsearch applications.",
        "translated": ""
    },
    {
        "title": "When do Generative Query and Document Expansions Fail? A Comprehensive\n  Study Across Methods, Retrievers, and Datasets",
        "url": "http://arxiv.org/abs/2309.08541v1",
        "pub_date": "2023-09-15",
        "summary": "Using large language models (LMs) for query or document expansion can improve\ngeneralization in information retrieval. However, it is unknown whether these\ntechniques are universally beneficial or only effective in specific settings,\nsuch as for particular retrieval models, dataset domains, or query types. To\nanswer this, we conduct the first comprehensive analysis of LM-based expansion.\nWe find that there exists a strong negative correlation between retriever\nperformance and gains from expansion: expansion improves scores for weaker\nmodels, but generally harms stronger models. We show this trend holds across a\nset of eleven expansion techniques, twelve datasets with diverse distribution\nshifts, and twenty-four retrieval models. Through qualitative error analysis,\nwe hypothesize that although expansions provide extra information (potentially\nimproving recall), they add additional noise that makes it difficult to discern\nbetween the top relevant documents (thus introducing false positives). Our\nresults suggest the following recipe: use expansions for weaker models or when\nthe target dataset significantly differs from training corpus in format;\notherwise, avoid expansions to keep the relevance signal clear.",
        "translated": ""
    },
    {
        "title": "SilverRetriever: Advancing Neural Passage Retrieval for Polish Question\n  Answering",
        "url": "http://arxiv.org/abs/2309.08469v1",
        "pub_date": "2023-09-15",
        "summary": "Modern open-domain question answering systems often rely on accurate and\nefficient retrieval components to find passages containing the facts necessary\nto answer the question. Recently, neural retrievers have gained popularity over\nlexical alternatives due to their superior performance. However, most of the\nwork concerns popular languages such as English or Chinese. For others, such as\nPolish, few models are available. In this work, we present SilverRetriever, a\nneural retriever for Polish trained on a diverse collection of manually or\nweakly labeled datasets. SilverRetriever achieves much better results than\nother Polish models and is competitive with larger multilingual models.\nTogether with the model, we open-source five new passage retrieval datasets.",
        "translated": ""
    },
    {
        "title": "Explaining Search Result Stances to Opinionated People",
        "url": "http://arxiv.org/abs/2309.08460v1",
        "pub_date": "2023-09-15",
        "summary": "People use web search engines to find information before forming opinions,\nwhich can lead to practical decisions with different levels of impact. The\ncognitive effort of search can leave opinionated users vulnerable to cognitive\nbiases, e.g., the confirmation bias. In this paper, we investigate whether\nstance labels and their explanations can help users consume more diverse search\nresults. We automatically classify and label search results on three topics\n(i.e., intellectual property rights, school uniforms, and atheism) as against,\nneutral, and in favor, and generate explanations for these labels. In a user\nstudy (N =203), we then investigate whether search result stance bias (balanced\nvs biased) and the level of explanation (plain text, label only, label and\nexplanation) influence the diversity of search results clicked. We find that\nstance labels and explanations lead to a more diverse search result\nconsumption. However, we do not find evidence for systematic opinion change\namong users in this context. We believe these results can help designers of\nsearch engines to make more informed design decisions.",
        "translated": ""
    },
    {
        "title": "FedDCSR: Federated Cross-domain Sequential Recommendation via\n  Disentangled Representation Learning",
        "url": "http://arxiv.org/abs/2309.08420v1",
        "pub_date": "2023-09-15",
        "summary": "Cross-domain Sequential Recommendation (CSR) which leverages user sequence\ndata from multiple domains has received extensive attention in recent years.\nHowever, the existing CSR methods require sharing origin user data across\ndomains, which violates the General Data Protection Regulation (GDPR). Thus, it\nis necessary to combine federated learning (FL) and CSR to fully utilize\nknowledge from different domains while preserving data privacy. Nonetheless,\nthe sequence feature heterogeneity across different domains significantly\nimpacts the overall performance of FL. In this paper, we propose FedDCSR, a\nnovel federated cross-domain sequential recommendation framework via\ndisentangled representation learning. Specifically, to address the sequence\nfeature heterogeneity across domains, we introduce an approach called\ninter-intra domain sequence representation disentanglement (SRD) to disentangle\nthe user sequence features into domain-shared and domain-exclusive features. In\naddition, we design an intra domain contrastive infomax (CIM) strategy to learn\nricher domain-exclusive features of users by performing data augmentation on\nuser sequences. Extensive experiments on three real-world scenarios demonstrate\nthat FedDCSR achieves significant improvements over existing baselines.",
        "translated": ""
    },
    {
        "title": "Structural Self-Supervised Objectives for Transformers",
        "url": "http://arxiv.org/abs/2309.08272v1",
        "pub_date": "2023-09-15",
        "summary": "This thesis focuses on improving the pre-training of natural language models\nusing unsupervised raw data to make them more efficient and aligned with\ndownstream applications.\n  In the first part, we introduce three alternative pre-training objectives to\nBERT's Masked Language Modeling (MLM), namely Random Token Substitution (RTS),\nCluster-based Random Token Substitution (C-RTS), and Swapped Language Modeling\n(SLM). These objectives involve token swapping instead of masking, with RTS and\nC-RTS aiming to predict token originality and SLM predicting the original token\nvalues. Results show that RTS and C-RTS require less pre-training time while\nmaintaining performance comparable to MLM. Surprisingly, SLM outperforms MLM on\ncertain tasks despite using the same computational budget.\n  In the second part, we proposes self-supervised pre-training tasks that align\nstructurally with downstream applications, reducing the need for labeled data.\nWe use large corpora like Wikipedia and CC-News to train models to recognize if\ntext spans originate from the same paragraph or document in several ways. By\ndoing continuous pre-training, starting from existing models like RoBERTa,\nELECTRA, DeBERTa, BART, and T5, we demonstrate significant performance\nimprovements in tasks like Fact Verification, Answer Sentence Selection, and\nSummarization. These improvements are especially pronounced when limited\nannotation data is available. The proposed objectives also achieve\nstate-of-the-art results on various benchmark datasets, including FEVER (dev\nset), ASNQ, WikiQA, and TREC-QA, as well as enhancing the quality of summaries.\nImportantly, these techniques can be easily integrated with other methods\nwithout altering the internal structure of Transformer models, making them\nversatile for various NLP applications.",
        "translated": ""
    },
    {
        "title": "AdSEE: Investigating the Impact of Image Style Editing on Advertisement\n  Attractiveness",
        "url": "http://arxiv.org/abs/2309.08159v1",
        "pub_date": "2023-09-15",
        "summary": "Online advertisements are important elements in e-commerce sites, social\nmedia platforms, and search engines. With the increasing popularity of mobile\nbrowsing, many online ads are displayed with visual information in the form of\na cover image in addition to text descriptions to grab the attention of users.\nVarious recent studies have focused on predicting the click rates of online\nadvertisements aware of visual features or composing optimal advertisement\nelements to enhance visibility. In this paper, we propose Advertisement Style\nEditing and Attractiveness Enhancement (AdSEE), which explores whether semantic\nediting to ads images can affect or alter the popularity of online\nadvertisements. We introduce StyleGAN-based facial semantic editing and\ninversion to ads images and train a click rate predictor attributing GAN-based\nface latent representations in addition to traditional visual and textual\nfeatures to click rates. Through a large collected dataset named QQ-AD,\ncontaining 20,527 online ads, we perform extensive offline tests to study how\ndifferent semantic directions and their edit coefficients may impact click\nrates. We further design a Genetic Advertisement Editor to efficiently search\nfor the optimal edit directions and intensity given an input ad cover image to\nenhance its projected click rates. Online A/B tests performed over a period of\n5 days have verified the increased click-through rates of AdSEE-edited samples\nas compared to a control group of original ads, verifying the relation between\nimage styles and ad popularity. We open source the code for AdSEE research at\nhttps://github.com/LiyaoJiang1998/adsee.",
        "translated": ""
    },
    {
        "title": "Uncertainty-Aware Multi-View Visual Semantic Embedding",
        "url": "http://arxiv.org/abs/2309.08154v1",
        "pub_date": "2023-09-15",
        "summary": "The key challenge in image-text retrieval is effectively leveraging semantic\ninformation to measure the similarity between vision and language data.\nHowever, using instance-level binary labels, where each image is paired with a\nsingle text, fails to capture multiple correspondences between different\nsemantic units, leading to uncertainty in multi-modal semantic understanding.\nAlthough recent research has captured fine-grained information through more\ncomplex model structures or pre-training techniques, few studies have directly\nmodeled uncertainty of correspondence to fully exploit binary labels. To\naddress this issue, we propose an Uncertainty-Aware Multi-View Visual Semantic\nEmbedding (UAMVSE)} framework that decomposes the overall image-text matching\ninto multiple view-text matchings. Our framework introduce an uncertainty-aware\nloss function (UALoss) to compute the weighting of each view-text loss by\nadaptively modeling the uncertainty in each view-text correspondence. Different\nweightings guide the model to focus on different semantic information,\nenhancing the model's ability to comprehend the correspondence of images and\ntexts. We also design an optimized image-text matching strategy by normalizing\nthe similarity matrix to improve model performance. Experimental results on the\nFlicker30k and MS-COCO datasets demonstrate that UAMVSE outperforms\nstate-of-the-art models.",
        "translated": ""
    },
    {
        "title": "iHAS: Instance-wise Hierarchical Architecture Search for Deep Learning\n  Recommendation Models",
        "url": "http://arxiv.org/abs/2309.07967v1",
        "pub_date": "2023-09-14",
        "summary": "Current recommender systems employ large-sized embedding tables with uniform\ndimensions for all features, leading to overfitting, high computational cost,\nand suboptimal generalizing performance. Many techniques aim to solve this\nissue by feature selection or embedding dimension search. However, these\ntechniques typically select a fixed subset of features or embedding dimensions\nfor all instances and feed all instances into one recommender model without\nconsidering heterogeneity between items or users. This paper proposes a novel\ninstance-wise Hierarchical Architecture Search framework, iHAS, which automates\nneural architecture search at the instance level. Specifically, iHAS\nincorporates three stages: searching, clustering, and retraining. The searching\nstage identifies optimal instance-wise embedding dimensions across different\nfield features via carefully designed Bernoulli gates with stochastic selection\nand regularizers. After obtaining these dimensions, the clustering stage\ndivides samples into distinct groups via a deterministic selection approach of\nBernoulli gates. The retraining stage then constructs different recommender\nmodels, each one designed with optimal dimensions for the corresponding group.\nWe conduct extensive experiments to evaluate the proposed iHAS on two public\nbenchmark datasets from a real-world recommender system. The experimental\nresults demonstrate the effectiveness of iHAS and its outstanding\ntransferability to widely-used deep recommendation models.",
        "translated": ""
    },
    {
        "title": "Predictive Uncertainty-based Bias Mitigation in Ranking",
        "url": "http://arxiv.org/abs/2309.09833v1",
        "pub_date": "2023-09-18",
        "summary": "Societal biases that are contained in retrieved documents have received\nincreased interest. Such biases, which are often prevalent in the training data\nand learned by the model, can cause societal harms, by misrepresenting certain\ngroups, and by enforcing stereotypes. Mitigating such biases demands algorithms\nthat balance the trade-off between maximized utility for the user with fairness\nobjectives, which incentivize unbiased rankings. Prior work on bias mitigation\noften assumes that ranking scores, which correspond to the utility that a\ndocument holds for a user, can be accurately determined. In reality, there is\nalways a degree of uncertainty in the estimate of expected document utility.\nThis uncertainty can be approximated by viewing ranking models through a\nBayesian perspective, where the standard deterministic score becomes a\ndistribution.\n  In this work, we investigate whether uncertainty estimates can be used to\ndecrease the amount of bias in the ranked results, while minimizing loss in\nmeasured utility. We introduce a simple method that uses the uncertainty of the\nranking scores for an uncertainty-aware, post hoc approach to bias mitigation.\nWe compare our proposed method with existing baselines for bias mitigation with\nrespect to the utility-fairness trade-off, the controllability of methods, and\ncomputational costs. We show that an uncertainty-based approach can provide an\nintuitive and flexible trade-off that outperforms all baselines without\nadditional training requirements, allowing for the post hoc use of this\napproach on top of arbitrary retrieval models.",
        "translated": ""
    },
    {
        "title": "How Much Freedom Does An Effectiveness Metric Really Have?",
        "url": "http://arxiv.org/abs/2309.09477v1",
        "pub_date": "2023-09-18",
        "summary": "It is tempting to assume that because effectiveness metrics have free choice\nto assign scores to search engine result pages (SERPs) there must thus be a\nsimilar degree of freedom as to the relative order that SERP pairs can be put\ninto. In fact that second freedom is, to a considerable degree, illusory.\nThat's because if one SERP in a pair has been given a certain score by a\nmetric, fundamental ordering constraints in many cases then dictate that the\nscore for the second SERP must be either not less than, or not greater than,\nthe score assigned to the first SERP. We refer to these fixed relationships as\ninnate pairwise SERP orderings. Our first goal in this work is to describe and\ndefend those pairwise SERP relationship constraints, and tabulate their\nrelative occurrence via both exhaustive and empirical experimentation.\n  We then consider how to employ such innate pairwise relationships in IR\nexperiments, leading to a proposal for a new measurement paradigm.\nSpecifically, we argue that tables of results in which many different metrics\nare listed for champion versus challenger system comparisons should be avoided;\nand that instead a single metric be argued for in principled terms, with any\nrelationships identified by that metric then reinforced via an assessment of\nthe innate relationship as to whether other metrics - indeed, all other metrics\n- are likely to yield the same system-vs-system outcome.",
        "translated": ""
    },
    {
        "title": "Selecting which Dense Retriever to use for Zero-Shot Search",
        "url": "http://arxiv.org/abs/2309.09403v1",
        "pub_date": "2023-09-18",
        "summary": "We propose the new problem of choosing which dense retrieval model to use\nwhen searching on a new collection for which no labels are available, i.e. in a\nzero-shot setting. Many dense retrieval models are readily available. Each\nmodel however is characterized by very differing search effectiveness -- not\njust on the test portion of the datasets in which the dense representations\nhave been learned but, importantly, also across different datasets for which\ndata was not used to learn the dense representations. This is because dense\nretrievers typically require training on a large amount of labeled data to\nachieve satisfactory search effectiveness in a specific dataset or domain.\nMoreover, effectiveness gains obtained by dense retrievers on datasets for\nwhich they are able to observe labels during training, do not necessarily\ngeneralise to datasets that have not been observed during training. This is\nhowever a hard problem: through empirical experimentation we show that methods\ninspired by recent work in unsupervised performance evaluation with the\npresence of domain shift in the area of computer vision and machine learning\nare not effective for choosing highly performing dense retrievers in our setup.\nThe availability of reliable methods for the selection of dense retrieval\nmodels in zero-shot settings that do not require the collection of labels for\nevaluation would allow to streamline the widespread adoption of dense\nretrieval. This is therefore an important new problem we believe the\ninformation retrieval community should consider. Implementation of methods,\nalong with raw result files and analysis scripts are made publicly available at\nhttps://www.github.com/anonymized.",
        "translated": ""
    },
    {
        "title": "ChatGPT Hallucinates when Attributing Answers",
        "url": "http://arxiv.org/abs/2309.09401v1",
        "pub_date": "2023-09-17",
        "summary": "Can ChatGPT provide evidence to support its answers? Does the evidence it\nsuggests actually exist and does it really support its answer? We investigate\nthese questions using a collection of domain-specific knowledge-based\nquestions, specifically prompting ChatGPT to provide both an answer and\nsupporting evidence in the form of references to external sources. We also\ninvestigate how different prompts impact answers and evidence. We find that\nChatGPT provides correct or partially correct answers in about half of the\ncases (50.6% of the times), but its suggested references only exist 14% of the\ntimes. We further provide insights on the generated references that reveal\ncommon traits among the references that ChatGPT generates, and show how even if\na reference provided by the model does exist, this reference often does not\nsupport the claims ChatGPT attributes to it. Our findings are important because\n(1) they are the first systematic analysis of the references created by ChatGPT\nin its answers; (2) they suggest that the model may leverage good quality\ninformation in producing correct answers, but is unable to attribute real\nevidence to support its answers. Prompts, raw result files and manual analysis\nare made publicly available.",
        "translated": ""
    },
    {
        "title": "Towards Debiasing Frame Length Bias in Text-Video Retrieval via Causal\n  Intervention",
        "url": "http://arxiv.org/abs/2309.09311v1",
        "pub_date": "2023-09-17",
        "summary": "Many studies focus on improving pretraining or developing new backbones in\ntext-video retrieval. However, existing methods may suffer from the learning\nand inference bias issue, as recent research suggests in other\ntext-video-related tasks. For instance, spatial appearance features on action\nrecognition or temporal object co-occurrences on video scene graph generation\ncould induce spurious correlations. In this work, we present a unique and\nsystematic study of a temporal bias due to frame length discrepancy between\ntraining and test sets of trimmed video clips, which is the first such attempt\nfor a text-video retrieval task, to the best of our knowledge. We first\nhypothesise and verify the bias on how it would affect the model illustrated\nwith a baseline study. Then, we propose a causal debiasing approach and perform\nextensive experiments and ablation studies on the Epic-Kitchens-100, YouCook2,\nand MSR-VTT datasets. Our model overpasses the baseline and SOTA on nDCG, a\nsemantic-relevancy-focused evaluation metric which proves the bias is\nmitigated, as well as on the other conventional metrics.",
        "translated": ""
    },
    {
        "title": "Fairness for All: Investigating Harms to Within-Group Individuals in\n  Producer Fairness Re-ranking Optimization -- A Reproducibility Study",
        "url": "http://arxiv.org/abs/2309.09277v1",
        "pub_date": "2023-09-17",
        "summary": "Recommender systems are widely used to provide personalized recommendations\nto users. Recent research has shown that recommender systems may be subject to\ndifferent types of biases, such as popularity bias, leading to an uneven\ndistribution of recommendation exposure among producer groups. To mitigate\nthis, producer-centered fairness re-ranking (PFR) approaches have been proposed\nto ensure equitable recommendation utility across groups. However, these\napproaches overlook the harm they may cause to within-group individuals\nassociated with colder items, which are items with few or no interactions.\n  This study reproduces previous PFR approaches and shows that they\nsignificantly harm colder items, leading to a fairness gap for these items in\nboth advantaged and disadvantaged groups. Surprisingly, the unfair base\nrecommendation models were providing greater exposure opportunities to these\nindividual cold items, even though at the group level, they appeared to be\nunfair. To address this issue, the study proposes an amendment to the PFR\napproach that regulates the number of colder items recommended by the system.\nThis modification achieves a balance between accuracy and producer fairness\nwhile optimizing the selection of colder items within each group, thereby\npreventing or reducing harm to within-group individuals and augmenting the\nnovelty of all recommended items. The proposed method is able to register an\nincrease in sub-group fairness (SGF) from 0.3104 to 0.3782, 0.6156, and 0.9442\nwhile also improving group-level fairness (GF) (112% and 37% with respect to\nbase models and traditional PFR). Moreover, the proposed method achieves these\nimprovements with minimal or no reduction in accuracy (or even an increase\nsometimes). We evaluate the proposed method on various recommendation datasets\nand demonstrate promising results independent of the underlying model or\ndatasets.",
        "translated": ""
    },
    {
        "title": "Leveraging Large Language Models for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.09261v1",
        "pub_date": "2023-09-17",
        "summary": "Sequential recommendation problems have received increasing attention in\nresearch during the past few years, leading to the inception of a large variety\nof algorithmic approaches. In this work, we explore how large language models\n(LLMs), which are nowadays introducing disruptive effects in many AI-based\napplications, can be used to build or improve sequential recommendation\napproaches. Specifically, we devise and evaluate three approaches to leverage\nthe power of LLMs in different ways. Our results from experiments on two\ndatasets show that initializing the state-of-the-art sequential recommendation\nmodel BERT4Rec with embeddings obtained from an LLM improves NDCG by 15-20%\ncompared to the vanilla BERT4Rec model. Furthermore, we find that a simple\napproach that leverages LLM embeddings for producing recommendations, can\nprovide competitive performance by highlighting semantically related items. We\npublicly share the code and data of our experiments to ensure reproducibility.",
        "translated": ""
    },
    {
        "title": "SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription",
        "url": "http://arxiv.org/abs/2309.09085v1",
        "pub_date": "2023-09-16",
        "summary": "Guitar tablature is a form of music notation widely used among guitarists. It\ncaptures not only the musical content of a piece, but also its implementation\nand ornamentation on the instrument. Guitar Tablature Transcription (GTT) is an\nimportant task with broad applications in music education and entertainment.\nExisting datasets are limited in size and scope, causing state-of-the-art GTT\nmodels trained on such datasets to suffer from overfitting and to fail in\ngeneralization across datasets. To address this issue, we developed a\nmethodology for synthesizing SynthTab, a large-scale guitar tablature\ntranscription dataset using multiple commercial acoustic and electric guitar\nplugins. This dataset is built on tablatures from DadaGP, which offers a vast\ncollection and the degree of specificity we wish to transcribe. The proposed\nsynthesis pipeline produces audio which faithfully adheres to the original\nfingerings, styles, and techniques specified in the tablature with diverse\ntimbre. Experiments show that pre-training state-of-the-art GTT model on\nSynthTab improves transcription accuracy in same-dataset tests. More\nimportantly, it significantly mitigates overfitting problems of GTT models in\ncross-dataset evaluation.",
        "translated": ""
    },
    {
        "title": "Bridging Dense and Sparse Maximum Inner Product Search",
        "url": "http://arxiv.org/abs/2309.09013v1",
        "pub_date": "2023-09-16",
        "summary": "Maximum inner product search (MIPS) over dense and sparse vectors have\nprogressed independently in a bifurcated literature for decades; the latter is\nbetter known as top-$k$ retrieval in Information Retrieval. This duality exists\nbecause sparse and dense vectors serve different end goals. That is despite the\nfact that they are manifestations of the same mathematical problem. In this\nwork, we ask if algorithms for dense vectors could be applied effectively to\nsparse vectors, particularly those that violate the assumptions underlying\ntop-$k$ retrieval methods. We study IVF-based retrieval where vectors are\npartitioned into clusters and only a fraction of clusters are searched during\nretrieval. We conduct a comprehensive analysis of dimensionality reduction for\nsparse vectors, and examine standard and spherical KMeans for partitioning. Our\nexperiments demonstrate that IVF serves as an efficient solution for sparse\nMIPS. As byproducts, we identify two research opportunities and demonstrate\ntheir potential. First, we cast the IVF paradigm as a dynamic pruning technique\nand turn that insight into a novel organization of the inverted index for\napproximate MIPS for general sparse vectors. Second, we offer a unified regime\nfor MIPS over vectors that have dense and sparse subspaces, and show its\nrobustness to query distributions.",
        "translated": ""
    },
    {
        "title": "An Unified Search and Recommendation Foundation Model for Cold-Start\n  Scenario",
        "url": "http://arxiv.org/abs/2309.08939v1",
        "pub_date": "2023-09-16",
        "summary": "In modern commercial search engines and recommendation systems, data from\nmultiple domains is available to jointly train the multi-domain model.\nTraditional methods train multi-domain models in the multi-task setting, with\nshared parameters to learn the similarity of multiple tasks, and task-specific\nparameters to learn the divergence of features, labels, and sample\ndistributions of individual tasks. With the development of large language\nmodels, LLM can extract global domain-invariant text features that serve both\nsearch and recommendation tasks. We propose a novel framework called S\\&amp;R\nMulti-Domain Foundation, which uses LLM to extract domain invariant features,\nand Aspect Gating Fusion to merge the ID feature, domain invariant text\nfeatures and task-specific heterogeneous sparse features to obtain the\nrepresentations of query and item. Additionally, samples from multiple search\nand recommendation scenarios are trained jointly with Domain Adaptive\nMulti-Task module to obtain the multi-domain foundation model. We apply the\nS\\&amp;R Multi-Domain foundation model to cold start scenarios in the\npretrain-finetune manner, which achieves better performance than other SOTA\ntransfer learning methods. The S\\&amp;R Multi-Domain Foundation model has been\nsuccessfully deployed in Alipay Mobile Application's online services, such as\ncontent query recommendation and service card recommendation, etc.",
        "translated": ""
    },
    {
        "title": "Interactive Distillation of Large Single-Topic Corpora of Scientific\n  Papers",
        "url": "http://arxiv.org/abs/2309.10772v1",
        "pub_date": "2023-09-19",
        "summary": "Highly specific datasets of scientific literature are important for both\nresearch and education. However, it is difficult to build such datasets at\nscale. A common approach is to build these datasets reductively by applying\ntopic modeling on an established corpus and selecting specific topics. A more\nrobust but time-consuming approach is to build the dataset constructively in\nwhich a subject matter expert (SME) handpicks documents. This method does not\nscale and is prone to error as the dataset grows. Here we showcase a new tool,\nbased on machine learning, for constructively generating targeted datasets of\nscientific literature. Given a small initial \"core\" corpus of papers, we build\na citation network of documents. At each step of the citation network, we\ngenerate text embeddings and visualize the embeddings through dimensionality\nreduction. Papers are kept in the dataset if they are \"similar\" to the core or\nare otherwise pruned through human-in-the-loop selection. Additional insight\ninto the papers is gained through sub-topic modeling using SeNMFk. We\ndemonstrate our new tool for literature review by applying it to two different\nfields in machine learning.",
        "translated": ""
    },
    {
        "title": "MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation",
        "url": "http://arxiv.org/abs/2309.10738v1",
        "pub_date": "2023-09-19",
        "summary": "Pre-trained language models have achieved impressive results in various music\nunderstanding and generation tasks. However, existing pre-training methods for\nsymbolic melody generation struggle to capture multi-scale, multi-dimensional\nstructural information in note sequences, due to the domain knowledge\ndiscrepancy between text and music. Moreover, the lack of available large-scale\nsymbolic melody datasets limits the pre-training improvement. In this paper, we\npropose MelodyGLM, a multi-task pre-training framework for generating melodies\nwith long-term structure. We design the melodic n-gram and long span sampling\nstrategies to create local and global blank infilling tasks for modeling the\nlocal and global structures in melodies. Specifically, we incorporate pitch\nn-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram\nblank infilling tasks for modeling the multi-dimensional structures in\nmelodies. To this end, we have constructed a large-scale symbolic melody\ndataset, MelodyNet, containing more than 0.4 million melody pieces. MelodyNet\nis utilized for large-scale pre-training and domain-specific n-gram lexicon\nconstruction. Both subjective and objective evaluations demonstrate that\nMelodyGLM surpasses the standard and previous pre-training methods. In\nparticular, subjective evaluations show that, on the melody continuation task,\nMelodyGLM achieves average improvements of 0.82, 0.87, 0.78, and 0.94 in\nconsistency, rhythmicity, structure, and overall quality, respectively.\nNotably, MelodyGLM nearly matches the quality of human-composed melodies on the\nmelody inpainting task.",
        "translated": ""
    },
    {
        "title": "Large language models can accurately predict searcher preferences",
        "url": "http://arxiv.org/abs/2309.10621v1",
        "pub_date": "2023-09-19",
        "summary": "Relevance labels, which indicate whether a search result is valuable to a\nsearcher, are key to evaluating and optimising search systems. The best way to\ncapture the true preferences of users is to ask them for their careful feedback\non which results would be useful, but this approach does not scale to produce a\nlarge number of labels. Getting relevance labels at scale is usually done with\nthird-party labellers, who judge on behalf of the user, but there is a risk of\nlow-quality data if the labeller doesn't understand user needs. To improve\nquality, one standard approach is to study real users through interviews, user\nstudies and direct feedback, find areas where labels are systematically\ndisagreeing with users, then educate labellers about user needs through judging\nguidelines, training and monitoring. This paper introduces an alternate\napproach for improving label quality. It takes careful feedback from real\nusers, which by definition is the highest-quality first-party gold data that\ncan be derived, and develops an large language model prompt that agrees with\nthat data.\n  We present ideas and observations from deploying language models for\nlarge-scale relevance labelling at Bing, and illustrate with data from TREC. We\nhave found large language models can be effective, with accuracy as good as\nhuman labellers and similar capability to pick the hardest queries, best runs,\nand best groups. Systematic changes to the prompts make a difference in\naccuracy, but so too do simple paraphrases. To measure agreement with real\nsearchers needs high-quality ``gold'' labels, but with these we find that\nmodels produce better labels than third-party workers, for a fraction of the\ncost, and these labels let us train notably better rankers.",
        "translated": ""
    },
    {
        "title": "A Hierarchical Neural Framework for Classification and its Explanation\n  in Large Unstructured Legal Documents",
        "url": "http://arxiv.org/abs/2309.10563v1",
        "pub_date": "2023-09-19",
        "summary": "Automatic legal judgment prediction and its explanation suffer from the\nproblem of long case documents exceeding tens of thousands of words, in\ngeneral, and having a non-uniform structure. Predicting judgments from such\ndocuments and extracting their explanation becomes a challenging task, more so\non documents with no structural annotation. We define this problem as \"scarce\nannotated legal documents\" and explore their lack of structural information and\ntheir long lengths with a deep learning-based classification framework which we\ncall MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment\nprediction. Specifically, we divide a document into parts to extract their\nembeddings from the last four layers of a custom fine-tuned Large Language\nModel, and try to approximate their structure through unsupervised clustering.\nWhich we use in another set of transformer encoder layers to learn the\ninter-chunk representations. We explore the adaptability of LLMs with\nmulti-billion parameters (GPT-Neo, and GPT-J) to legal texts and their\nintra-domain(legal) transfer learning capacity. Alongside this, we compare\ntheir performance with MESc and the impact of combining embeddings from their\nlast layers. For such hierarchical models, we also propose an explanation\nextraction algorithm named ORSE; Occlusion sensitivity-based Relevant Sentence\nExtractor;",
        "translated": ""
    },
    {
        "title": "Proposal for an Organic Web, The missing link between the Web and the\n  Semantic Web, Part 1",
        "url": "http://arxiv.org/abs/2309.10531v1",
        "pub_date": "2023-09-19",
        "summary": "A huge amount of information is produced in digital form. The Semantic Web\nstems from the realisation that dealing efficiently with this production\nrequires getting better at interlinking digital informational resources\ntogether. Its focus is on linking data. Linking data isn't enough. We need to\nprovide infrastructural support for linking all sorts of informational\nresources including resources whose understanding and fine interlinking\nrequires domain-specific human expertise. At times when many problems scale to\nplanetary dimensions, it is essential to scale coordination of information\nprocessing and information production, without giving up on expertise and depth\nof analysis, nor forcing languages and formalisms onto thinkers,\ndecision-makers and innovators that are only suitable to some forms of\nintelligence. This article makes a proposal in this direction and in line with\nthe idea of interlinking championed by the Semantic Web.",
        "translated": ""
    },
    {
        "title": "A Digital Forensics Case Study of the DJI Mini 3 Pro and DJI RC",
        "url": "http://arxiv.org/abs/2309.10487v1",
        "pub_date": "2023-09-19",
        "summary": "The consumer drone market is rapidly expanding with new drone models\nfeaturing unique variations of hardware and software. The rapid development of\ndrone technology and variability in drone systems can make it difficult for\ndigital forensic investigators and tools to keep pace and effectively extract\nand analyse digital evidence from drones. Furthermore, the growing popularity\nof drones and their increased use in illegal and harmful activities, such as\nsmuggling, espionage, and even terrorism, has led to an increase in the number\nof drone forensic cases for authorities to manage. To assist forensic\ninvestigators, a static digital forensic case study was conducted on two drone\ndevices recently released by Da-Jiang Innovations (DJI): the Mini 3 Pro drone,\nand its remote controller, the DJI RC. The study discovered the presence of\nseveral digital artefacts on both devices, including recorded media, flight\nlogs, and other information that could help investigators trace the drone's\nusage and identify its operator. Additionally, this paper explored several\nmethods for extracting and visualising the drone's flight history, and\nhighlights some of the potential methods used to limit, obscure, or remove key\ntypes of digital evidence.",
        "translated": ""
    },
    {
        "title": "RUEL: Retrieval-Augmented User Representation with Edge Browser Logs for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.10469v1",
        "pub_date": "2023-09-19",
        "summary": "Online recommender systems (RS) aim to match user needs with the vast amount\nof resources available on various platforms. A key challenge is to model user\npreferences accurately under the condition of data sparsity. To address this\nchallenge, some methods have leveraged external user behavior data from\nmultiple platforms to enrich user representation. However, all of these methods\nrequire a consistent user ID across platforms and ignore the information from\nsimilar users. In this study, we propose RUEL, a novel retrieval-based\nsequential recommender that can effectively incorporate external anonymous user\nbehavior data from Edge browser logs to enhance recommendation. We first\ncollect and preprocess a large volume of Edge browser logs over a one-year\nperiod and link them to target entities that correspond to candidate items in\nrecommendation datasets. We then design a contrastive learning framework with a\nmomentum encoder and a memory bank to retrieve the most relevant and diverse\nbrowsing sequences from the full browsing log based on the semantic similarity\nbetween user representations. After retrieval, we apply an item-level attentive\nselector to filter out noisy items and generate refined sequence embeddings for\nthe final predictor. RUEL is the first method that connects user browsing data\nwith typical recommendation datasets and can be generalized to various\nrecommendation scenarios and datasets. We conduct extensive experiments on four\nreal datasets for sequential recommendation tasks and demonstrate that RUEL\nsignificantly outperforms state-of-the-art baselines. We also conduct ablation\nstudies and qualitative analysis to validate the effectiveness of each\ncomponent of RUEL and provide additional insights into our method.",
        "translated": ""
    },
    {
        "title": "Reformulating Sequential Recommendation: Learning Dynamic User Interest\n  with Content-enriched Language Modeling",
        "url": "http://arxiv.org/abs/2309.10435v1",
        "pub_date": "2023-09-19",
        "summary": "Recommender systems are essential for online applications, and sequential\nrecommendation has enjoyed significant prevalence due to its expressive ability\nto capture dynamic user interests. However, previous sequential modeling\nmethods still have limitations in capturing contextual information. The primary\nreason for this issue is that language models often lack an understanding of\ndomain-specific knowledge and item-related textual content. To address this\nissue, we adopt a new sequential recommendation paradigm and propose LANCER,\nwhich leverages the semantic understanding capabilities of pre-trained language\nmodels to generate personalized recommendations. Our approach bridges the gap\nbetween language models and recommender systems, resulting in more human-like\nrecommendations. We demonstrate the effectiveness of our approach through\nexperiments on several benchmark datasets, showing promising results and\nproviding valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Deep Mutual Learning across Task Towers for Effective Multi-Task\n  Recommender Learning",
        "url": "http://arxiv.org/abs/2309.10357v1",
        "pub_date": "2023-09-19",
        "summary": "Recommender systems usually leverage multi-task learning methods to\nsimultaneously optimize several objectives because of the multi-faceted user\nbehavior data. The typical way of conducting multi-task learning is to\nestablish appropriate parameter sharing across multiple tasks at lower layers\nwhile reserving a separate task tower for each task at upper layers. Since the\ntask towers exert direct impact on the prediction results, we argue that the\narchitecture of standalone task towers is sub-optimal for promoting positive\nknowledge sharing. Accordingly, we propose the framework of Deep Mutual\nLearning across task towers, which is compatible with various backbone\nmulti-task networks. Extensive offline experiments and online AB tests are\nconducted to evaluate and verify the proposed approach's effectiveness.",
        "translated": ""
    },
    {
        "title": "Computational Approaches for App-to-App Retrieval and Design Consistency\n  Check",
        "url": "http://arxiv.org/abs/2309.10328v1",
        "pub_date": "2023-09-19",
        "summary": "Extracting semantic representations from mobile user interfaces (UI) and\nusing the representations for designers' decision-making processes have shown\nthe potential to be effective computational design support tools. Current\napproaches rely on machine learning models trained on small-sized mobile UI\ndatasets to extract semantic vectors and use screenshot-to-screenshot\ncomparison to retrieve similar-looking UIs given query screenshots. However,\nthe usability of these methods is limited because they are often not\nopen-sourced and have complex training pipelines for practitioners to follow,\nand are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval.\nTo this end, we (1) employ visual models trained with large web-scale images\nand test whether they could extract a UI representation in a zero-shot way and\noutperform existing specialized models, and (2) use mathematically founded\nmethods to enable app-to-app retrieval and design consistency analysis. Our\nexperiments show that our methods not only improve upon previous retrieval\nmodels but also enable multiple new applications.",
        "translated": ""
    },
    {
        "title": "Bravo MaRDI: A Wikibase Powered Knowledge Graph on Mathematics",
        "url": "http://arxiv.org/abs/2309.11484v1",
        "pub_date": "2023-09-20",
        "summary": "Mathematical world knowledge is a fundamental component of Wikidata. However,\nto date, no expertly curated knowledge graph has focused specifically on\ncontemporary mathematics. Addressing this gap, the Mathematical Research Data\nInitiative (MaRDI) has developed a comprehensive knowledge graph that links\nmultimodal research data in mathematics. This encompasses traditional research\ndata items like datasets, software, and publications and includes semantically\nadvanced objects such as mathematical formulas and hypotheses. This paper\ndetails the abilities of the MaRDI knowledge graph, which is based on Wikibase,\nleading up to its inaugural public release, codenamed Bravo, available on\nhttps://portal.mardi4nfdi.de.",
        "translated": ""
    },
    {
        "title": "Retrieving Supporting Evidence for Generative Question Answering",
        "url": "http://arxiv.org/abs/2309.11392v1",
        "pub_date": "2023-09-20",
        "summary": "Current large language models (LLMs) can exhibit near-human levels of\nperformance on many natural language-based tasks, including open-domain\nquestion answering. Unfortunately, at this time, they also convincingly\nhallucinate incorrect answers, so that responses to questions must be verified\nagainst external sources before they can be accepted at face value. In this\npaper, we report two simple experiments to automatically validate generated\nanswers against a corpus. We base our experiments on questions and passages\nfrom the MS MARCO (V1) test collection, and a retrieval pipeline consisting of\nsparse retrieval, dense retrieval and neural rerankers. In the first\nexperiment, we validate the generated answer in its entirety. After presenting\na question to an LLM and receiving a generated answer, we query the corpus with\nthe combination of the question + generated answer. We then present the LLM\nwith the combination of the question + generated answer + retrieved answer,\nprompting it to indicate if the generated answer can be supported by the\nretrieved answer. In the second experiment, we consider the generated answer at\na more granular level, prompting the LLM to extract a list of factual\nstatements from the answer and verifying each statement separately. We query\nthe corpus with each factual statement and then present the LLM with the\nstatement and the corresponding retrieved evidence. The LLM is prompted to\nindicate if the statement can be supported and make necessary edits using the\nretrieved material. With an accuracy of over 80%, we find that an LLM is\ncapable of verifying its generated answer when a corpus of supporting material\nis provided. However, manual assessment of a random sample of questions reveals\nthat incorrect generated answers are missed by this verification process. While\nthis verification process can reduce hallucinations, it can not entirely\neliminate them.",
        "translated": ""
    },
    {
        "title": "Long-tail Augmented Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2309.11177v1",
        "pub_date": "2023-09-20",
        "summary": "Graph Convolutional Networks (GCNs) has demonstrated promising results for\nrecommender systems, as they can effectively leverage high-order relationship.\nHowever, these methods usually encounter data sparsity issue in real-world\nscenarios. To address this issue, GCN-based recommendation methods employ\ncontrastive learning to introduce self-supervised signals. Despite their\neffectiveness, these methods lack consideration of the significant degree\ndisparity between head and tail nodes. This can lead to non-uniform\nrepresentation distribution, which is a crucial factor for the performance of\ncontrastive learning methods. To tackle the above issue, we propose a novel\nLong-tail Augmented Graph Contrastive Learning (LAGCL) method for\nrecommendation. Specifically, we introduce a learnable long-tail augmentation\napproach to enhance tail nodes by supplementing predicted neighbor information,\nand generate contrastive views based on the resulting augmented graph. To make\nthe data augmentation schema learnable, we design an auto drop module to\ngenerate pseudo-tail nodes from head nodes and a knowledge transfer module to\nreconstruct the head nodes from pseudo-tail nodes. Additionally, we employ\ngenerative adversarial networks to ensure that the distribution of the\ngenerated tail/head nodes matches that of the original tail/head nodes.\nExtensive experiments conducted on three benchmark datasets demonstrate the\nsignificant improvement in performance of our model over the state-of-the-arts.\nFurther analyses demonstrate the uniformity of learned representations and the\nsuperiority of LAGCL on long-tail performance. Code is publicly available at\nhttps://github.com/im0qianqian/LAGCL",
        "translated": ""
    },
    {
        "title": "Artificial Intelligence-Enabled Intelligent Assistant for Personalized\n  and Adaptive Learning in Higher Education",
        "url": "http://arxiv.org/abs/2309.10892v1",
        "pub_date": "2023-09-19",
        "summary": "This paper presents a novel framework, Artificial Intelligence-Enabled\nIntelligent Assistant (AIIA), for personalized and adaptive learning in higher\neducation. The AIIA system leverages advanced AI and Natural Language\nProcessing (NLP) techniques to create an interactive and engaging learning\nplatform. This platform is engineered to reduce cognitive load on learners by\nproviding easy access to information, facilitating knowledge assessment, and\ndelivering personalized learning support tailored to individual needs and\nlearning styles. The AIIA's capabilities include understanding and responding\nto student inquiries, generating quizzes and flashcards, and offering\npersonalized learning pathways. The research findings have the potential to\nsignificantly impact the design, implementation, and evaluation of AI-enabled\nVirtual Teaching Assistants (VTAs) in higher education, informing the\ndevelopment of innovative educational tools that can enhance student learning\noutcomes, engagement, and satisfaction. The paper presents the methodology,\nsystem architecture, intelligent services, and integration with Learning\nManagement Systems (LMSs) while discussing the challenges, limitations, and\nfuture directions for the development of AI-enabled intelligent assistants in\neducation.",
        "translated": ""
    },
    {
        "title": "Classifying Organizations for Food System Ontologies using Natural\n  Language Processing",
        "url": "http://arxiv.org/abs/2309.10880v1",
        "pub_date": "2023-09-19",
        "summary": "Our research explores the use of natural language processing (NLP) methods to\nautomatically classify entities for the purpose of knowledge graph population\nand integration with food system ontologies. We have created NLP models that\ncan automatically classify organizations with respect to categories associated\nwith environmental issues as well as Standard Industrial Classification (SIC)\ncodes, which are used by the U.S. government to characterize business\nactivities. As input, the NLP models are provided with text snippets retrieved\nby the Google search engine for each organization, which serves as a textual\ndescription of the organization that is used for learning. Our experimental\nresults show that NLP models can achieve reasonably good performance for these\ntwo classification tasks, and they rely on a general framework that could be\napplied to many other classification problems as well. We believe that NLP\nmodels represent a promising approach for automatically harvesting information\nto populate knowledge graphs and aligning the information with existing\nontologies through shared categories and concepts.",
        "translated": ""
    },
    {
        "title": "Improving VTE Identification through Adaptive NLP Model Selection and\n  Clinical Expert Rule-based Classifier from Radiology Reports",
        "url": "http://arxiv.org/abs/2309.12273v1",
        "pub_date": "2023-09-21",
        "summary": "Rapid and accurate identification of Venous thromboembolism (VTE), a severe\ncardiovascular condition including deep vein thrombosis (DVT) and pulmonary\nembolism (PE), is important for effective treatment. Leveraging Natural\nLanguage Processing (NLP) on radiology reports, automated methods have shown\npromising advancements in identifying VTE events from retrospective data\ncohorts or aiding clinical experts in identifying VTE events from radiology\nreports. However, effectively training Deep Learning (DL) and the NLP models is\nchallenging due to limited labeled medical text data, the complexity and\nheterogeneity of radiology reports, and data imbalance. This study proposes\nnovel method combinations of DL methods, along with data augmentation, adaptive\npre-trained NLP model selection, and a clinical expert NLP rule-based\nclassifier, to improve the accuracy of VTE identification in unstructured\n(free-text) radiology reports. Our experimental results demonstrate the model's\nefficacy, achieving an impressive 97\\% accuracy and 97\\% F1 score in predicting\nDVT, and an outstanding 98.3\\% accuracy and 98.4\\% F1 score in predicting PE.\nThese findings emphasize the model's robustness and its potential to\nsignificantly contribute to VTE research.",
        "translated": ""
    },
    {
        "title": "Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval",
        "url": "http://arxiv.org/abs/2309.12158v1",
        "pub_date": "2023-09-21",
        "summary": "A range of applications of multi-modal music information retrieval is centred\naround the problem of connecting large collections of sheet music (images) to\ncorresponding audio recordings, that is, identifying pairs of audio and score\nexcerpts that refer to the same musical content. One of the typical and most\nrecent approaches to this task employs cross-modal deep learning architectures\nto learn joint embedding spaces that link the two distinct modalities - audio\nand sheet music images. While there has been steady improvement on this front\nover the past years, a number of open problems still prevent large-scale\nemployment of this methodology. In this article we attempt to provide an\ninsightful examination of the current developments on audio-sheet music\nretrieval via deep learning methods. We first identify a set of main challenges\non the road towards robust and large-scale cross-modal music retrieval in real\nscenarios. We then highlight the steps we have taken so far to address some of\nthese challenges, documenting step-by-step improvement along several\ndimensions. We conclude by analysing the remaining challenges and present ideas\nfor solving these, in order to pave the way to a unified and robust methodology\nfor cross-modal music retrieval.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Contrastive Learning for Robust Audio-Sheet Music\n  Retrieval Systems",
        "url": "http://arxiv.org/abs/2309.12134v1",
        "pub_date": "2023-09-21",
        "summary": "Linking sheet music images to audio recordings remains a key problem for the\ndevelopment of efficient cross-modal music retrieval systems. One of the\nfundamental approaches toward this task is to learn a cross-modal embedding\nspace via deep neural networks that is able to connect short snippets of audio\nand sheet music. However, the scarcity of annotated data from real musical\ncontent affects the capability of such methods to generalize to real retrieval\nscenarios. In this work, we investigate whether we can mitigate this limitation\nwith self-supervised contrastive learning, by exposing a network to a large\namount of real music data as a pre-training step, by contrasting randomly\naugmented views of snippets of both modalities, namely audio and sheet images.\nThrough a number of experiments on synthetic and real piano data, we show that\npre-trained models are able to retrieve snippets with better precision in all\nscenarios and pre-training configurations. Encouraged by these results, we\nemploy the snippet embeddings in the higher-level task of cross-modal piece\nidentification and conduct more experiments on several retrieval\nconfigurations. In this task, we observe that the retrieval quality improves\nfrom 30% up to 100% when real music data is present. We then conclude by\narguing for the potential of self-supervised contrastive learning for\nalleviating the annotated data scarcity in multi-modal music retrieval models.",
        "translated": ""
    },
    {
        "title": "Passage Summarization with Recurrent Models for Audio-Sheet Music\n  Retrieval",
        "url": "http://arxiv.org/abs/2309.12111v1",
        "pub_date": "2023-09-21",
        "summary": "Many applications of cross-modal music retrieval are related to connecting\nsheet music images to audio recordings. A typical and recent approach to this\nis to learn, via deep neural networks, a joint embedding space that correlates\nshort fixed-size snippets of audio and sheet music by means of an appropriate\nsimilarity structure. However, two challenges that arise out of this strategy\nare the requirement of strongly aligned data to train the networks, and the\ninherent discrepancies of musical content between audio and sheet music\nsnippets caused by local and global tempo differences. In this paper, we\naddress these two shortcomings by designing a cross-modal recurrent network\nthat learns joint embeddings that can summarize longer passages of\ncorresponding audio and sheet music. The benefits of our method are that it\nonly requires weakly aligned audio-sheet music pairs, as well as that the\nrecurrent network handles the non-linearities caused by tempo variations\nbetween audio and sheet music. We conduct a number of experiments on synthetic\nand real piano data and scores, showing that our proposed recurrent method\nleads to more accurate retrieval in all possible configurations.",
        "translated": ""
    },
    {
        "title": "Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph\n  Pruning and Intent Graph for Effective Recommendations",
        "url": "http://arxiv.org/abs/2309.11741v1",
        "pub_date": "2023-09-21",
        "summary": "The recommendation of appropriate development pathways, also known as\necological civilization patterns for achieving Sustainable Development Goals\n(namely, sustainable development patterns), are of utmost importance for\npromoting ecological, economic, social, and resource sustainability in a\nspecific region. To achieve this, the recommendation process must carefully\nconsider the region's natural, environmental, resource, and economic\ncharacteristics. However, current recommendation algorithms in the field of\ncomputer science fall short in adequately addressing the spatial heterogeneity\nrelated to environment and sparsity of regional historical interaction data,\nwhich limits their effectiveness in recommending sustainable development\npatterns. To overcome these challenges, this paper proposes a method called\nUser Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the\nhigh-density linking capability of the pruned User Graph to address the issue\nof spatial heterogeneity neglect in recommendation algorithms. Secondly, we\nconstruct an Intent Graph by incorporating the intent network, which captures\nthe preferences for attributes including environmental elements of target\nregions. This approach effectively alleviates the problem of sparse historical\ninteraction data in the region. Through extensive experiments, we demonstrate\nthat UGPIG outperforms state-of-the-art recommendation algorithms like KGCN,\nKGAT, and KGIN in sustainable development pattern recommendations, with a\nmaximum improvement of 9.61% in Top-3 recommendation performance.",
        "translated": ""
    },
    {
        "title": "Candidate Set Sampling for Evaluating Top-N Recommendation",
        "url": "http://arxiv.org/abs/2309.11723v1",
        "pub_date": "2023-09-21",
        "summary": "The strategy for selecting candidate sets -- the set of items that the\nrecommendation system is expected to rank for each user -- is an important\ndecision in carrying out an offline top-$N$ recommender system evaluation. The\nset of candidates is composed of the union of the user's test items and an\narbitrary number of non-relevant items that we refer to as decoys. Previous\nstudies have aimed to understand the effect of different candidate set sizes\nand selection strategies on evaluation. In this paper, we extend this knowledge\nby studying the specific interaction of candidate set selection strategies with\npopularity bias, and use simulation to assess whether sampled candidate sets\nresult in metric estimates that are less biased with respect to the true metric\nvalues under complete data that is typically unavailable in ordinary\nexperiments.",
        "translated": ""
    },
    {
        "title": "SE-PEF: a Resource for Personalized Expert Finding",
        "url": "http://arxiv.org/abs/2309.11686v1",
        "pub_date": "2023-09-20",
        "summary": "The problem of personalization in Information Retrieval has been under study\nfor a long time. A well-known issue related to this task is the lack of\npublicly available datasets that can support a comparative evaluation of\npersonalized search systems. To contribute in this respect, this paper\nintroduces SE-PEF (StackExchange - Personalized Expert Finding), a resource\nuseful for designing and evaluating personalized models related to the task of\nExpert Finding (EF). The contributed dataset includes more than 250k queries\nand 565k answers from 3 306 experts, which are annotated with a rich set of\nfeatures modeling the social interactions among the users of a popular cQA\nplatform. The results of the preliminary experiments conducted show the\nappropriateness of SE-PEF to evaluate and to train effective EF models.",
        "translated": ""
    },
    {
        "title": "Popularity Degradation Bias in Local Music Recommendation",
        "url": "http://arxiv.org/abs/2309.11671v1",
        "pub_date": "2023-09-20",
        "summary": "In this paper, we study the effect of popularity degradation bias in the\ncontext of local music recommendations. Specifically, we examine how accurate\ntwo top-performing recommendation algorithms, Weight Relevance Matrix\nFactorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at\nrecommending artists as a function of artist popularity. We find that both\nalgorithms improve recommendation performance for more popular artists and, as\nsuch, exhibit popularity degradation bias. While both algorithms produce a\nsimilar level of performance for more popular artists, Mult-VAE shows better\nrelative performance for less popular artists. This suggests that this\nalgorithm should be preferred for local (long-tail) music artist\nrecommendation.",
        "translated": ""
    },
    {
        "title": "Leveraging Negative Signals with Self-Attention for Sequential Music\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.11623v1",
        "pub_date": "2023-09-20",
        "summary": "Music streaming services heavily rely on their recommendation engines to\ncontinuously provide content to their consumers. Sequential recommendation\nconsequently has seen considerable attention in current literature, where state\nof the art approaches focus on self-attentive models leveraging contextual\ninformation such as long and short-term user history and item features;\nhowever, most of these studies focus on long-form content domains (retail,\nmovie, etc.) rather than short-form, such as music. Additionally, many do not\nexplore incorporating negative session-level feedback during training. In this\nstudy, we investigate the use of transformer-based self-attentive architectures\nto learn implicit session-level information for sequential music\nrecommendation. We additionally propose a contrastive learning task to\nincorporate negative feedback (e.g skipped tracks) to promote positive hits and\npenalize negative hits. This task is formulated as a simple loss term that can\nbe incorporated into a variety of deep learning architectures for sequential\nrecommendation. Our experiments show that this results in consistent\nperformance gains over the baseline architectures ignoring negative user\nfeedback.",
        "translated": ""
    },
    {
        "title": "Diffusion Augmentation for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.12858v1",
        "pub_date": "2023-09-22",
        "summary": "Sequential recommendation (SRS) has become the technical foundation in many\napplications recently, which aims to recommend the next item based on the\nuser's historical interactions. However, sequential recommendation often faces\nthe problem of data sparsity, which widely exists in recommender systems.\nBesides, most users only interact with a few items, but existing SRS models\noften underperform these users. Such a problem, named the long-tail user\nproblem, is still to be resolved. Data augmentation is a distinct way to\nalleviate these two problems, but they often need fabricated training\nstrategies or are hindered by poor-quality generated interactions. To address\nthese problems, we propose a Diffusion Augmentation for Sequential\nRecommendation (DiffuASR) for a higher quality generation. The augmented\ndataset by DiffuASR can be used to train the sequential recommendation models\ndirectly, free from complex training procedures. To make the best of the\ngeneration ability of the diffusion model, we first propose a diffusion-based\npseudo sequence generation framework to fill the gap between image and sequence\ngeneration. Then, a sequential U-Net is designed to adapt the diffusion noise\nprediction model U-Net to the discrete sequence generation task. At last, we\ndevelop two guide strategies to assimilate the preference between generated and\norigin sequences. To validate the proposed DiffuASR, we conduct extensive\nexperiments on three real-world datasets with three sequential recommendation\nmodels. The experimental results illustrate the effectiveness of DiffuASR. As\nfar as we know, DiffuASR is one pioneer that introduce the diffusion model to\nthe recommendation.",
        "translated": ""
    },
    {
        "title": "Enhancing Graph Collaborative Filtering via Uniformly Co-Clustered\n  Intent Modeling",
        "url": "http://arxiv.org/abs/2309.12723v1",
        "pub_date": "2023-09-22",
        "summary": "Graph-based collaborative filtering has emerged as a powerful paradigm for\ndelivering personalized recommendations. Despite their demonstrated\neffectiveness, these methods often neglect the underlying intents of users,\nwhich constitute a pivotal facet of comprehensive user interests. Consequently,\na series of approaches have arisen to tackle this limitation by introducing\nindependent intent representations. However, these approaches fail to capture\nthe intricate relationships between intents of different users and the\ncompatibility between user intents and item properties.\n  To remedy the above issues, we propose a novel method, named uniformly\nco-clustered intent modeling. Specifically, we devise a uniformly contrastive\nintent modeling module to bring together the embeddings of users with similar\nintents and items with similar properties. This module aims to model the\nnuanced relations between intents of different users and properties of\ndifferent items, especially those unreachable to each other on the user-item\ngraph. To model the compatibility between user intents and item properties, we\ndesign the user-item co-clustering module, maximizing the mutual information of\nco-clusters of users and items. This approach is substantiated through\ntheoretical validation, establishing its efficacy in modeling compatibility to\nenhance the mutual information between user and item representations.\nComprehensive experiments on various real-world datasets verify the\neffectiveness of the proposed framework.",
        "translated": ""
    },
    {
        "title": "KuaiSim: A Comprehensive Simulator for Recommender Systems",
        "url": "http://arxiv.org/abs/2309.12645v1",
        "pub_date": "2023-09-22",
        "summary": "Reinforcement Learning (RL)-based recommender systems (RSs) have garnered\nconsiderable attention due to their ability to learn optimal recommendation\npolicies and maximize long-term user rewards. However, deploying RL models\ndirectly in online environments and generating authentic data through A/B tests\ncan pose challenges and require substantial resources. Simulators offer an\nalternative approach by providing training and evaluation environments for RS\nmodels, reducing reliance on real-world data. Existing simulators have shown\npromising results but also have limitations such as simplified user feedback,\nlacking consistency with real-world data, the challenge of simulator\nevaluation, and difficulties in migration and expansion across RSs. To address\nthese challenges, we propose KuaiSim, a comprehensive user environment that\nprovides user feedback with multi-behavior and cross-session responses. The\nresulting simulator can support three levels of recommendation problems: the\nrequest level list-wise recommendation task, the whole-session level sequential\nrecommendation task, and the cross-session level retention optimization task.\nFor each task, KuaiSim also provides evaluation protocols and baseline\nrecommendation algorithms that further serve as benchmarks for future research.\nWe also restructure existing competitive simulators on the KuaiRand Dataset and\ncompare them against KuaiSim to future assess their performance and behavioral\ndifferences. Furthermore, to showcase KuaiSim's flexibility in accommodating\ndifferent datasets, we demonstrate its versatility and robustness when\ndeploying it on the ML-1m dataset.",
        "translated": ""
    },
    {
        "title": "Modeling Spatiotemporal Periodicity and Collaborative Signal for\n  Local-Life Service Recommendation",
        "url": "http://arxiv.org/abs/2309.12565v1",
        "pub_date": "2023-09-22",
        "summary": "Online local-life service platforms provide services like nearby daily\nessentials and food delivery for hundreds of millions of users. Different from\nother types of recommender systems, local-life service recommendation has the\nfollowing characteristics: (1) spatiotemporal periodicity, which means a user's\npreferences for items vary from different locations at different times. (2)\nspatiotemporal collaborative signal, which indicates similar users have similar\npreferences at specific locations and times. However, most existing methods\neither focus on merely the spatiotemporal contexts in sequences, or model the\nuser-item interactions without spatiotemporal contexts in graphs. To address\nthis issue, we design a new method named SPCS in this paper. Specifically, we\npropose a novel spatiotemporal graph transformer (SGT) layer, which explicitly\nencodes relative spatiotemporal contexts, and aggregates the information from\nmulti-hop neighbors to unify spatiotemporal periodicity and collaborative\nsignal. With extensive experiments on both public and industrial datasets, this\npaper validates the state-of-the-art performance of SPCS.",
        "translated": ""
    },
    {
        "title": "Cluster Language Model for Improved E-Commerce Retrieval and Ranking:\n  Leveraging Query Similarity and Fine-Tuning for Personalized Results",
        "url": "http://arxiv.org/abs/2309.14323v1",
        "pub_date": "2023-09-25",
        "summary": "This paper proposes a novel method to improve the accuracy of product search\nin e-commerce by utilizing a cluster language model. The method aims to address\nthe limitations of the bi-encoder architecture while maintaining a minimal\nadditional training burden. The approach involves labeling top products for\neach query, generating semantically similar query clusters using the K-Means\nclustering algorithm, and fine-tuning a global language model into cluster\nlanguage models on individual clusters. The parameters of each cluster language\nmodel are fine-tuned to learn local manifolds in the feature space efficiently,\ncapturing the nuances of various query types within each cluster. The inference\nis performed by assigning a new query to its respective cluster and utilizing\nthe corresponding cluster language model for retrieval. The proposed method\nresults in more accurate and personalized retrieval results, offering a\nsuperior alternative to the popular bi-encoder based retrieval models in\nsemantic search.",
        "translated": ""
    },
    {
        "title": "Framework based on complex networks to model and mine patient pathways",
        "url": "http://arxiv.org/abs/2309.14208v1",
        "pub_date": "2023-09-25",
        "summary": "The automatic discovery of a model to represent the history of encounters of\na group of patients with the healthcare system -- the so-called ``pathway of\npatients'' -- is a new field of research that supports clinical and\norganisational decisions to improve the quality and efficiency of the treatment\nprovided. The pathways of patients with chronic conditions tend to vary\nsignificantly from one person to another, have repetitive tasks, and demand the\nanalysis of multiple perspectives (interventions, diagnoses, medical\nspecialities, among others) influencing the results. Therefore, modelling and\nmining those pathways is still a challenging task. In this work, we propose a\nframework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a\nnovel dissimilarity measurement to compare pathways taking the elapsed time\ninto account, and (iii) a mining method based on traditional centrality\nmeasures to discover the most relevant steps of the pathways. We evaluated the\nframework using the study cases of pregnancy and diabetes, which revealed its\nusefulness in finding clusters of similar pathways, representing them in an\neasy-to-interpret way, and highlighting the most significant patterns according\nto multiple perspectives.",
        "translated": ""
    },
    {
        "title": "Comprehensive Overview of Named Entity Recognition: Models,\n  Domain-Specific Applications and Challenges",
        "url": "http://arxiv.org/abs/2309.14084v1",
        "pub_date": "2023-09-25",
        "summary": "In the domain of Natural Language Processing (NLP), Named Entity Recognition\n(NER) stands out as a pivotal mechanism for extracting structured insights from\nunstructured text. This manuscript offers an exhaustive exploration into the\nevolving landscape of NER methodologies, blending foundational principles with\ncontemporary AI advancements. Beginning with the rudimentary concepts of NER,\nthe study spans a spectrum of techniques from traditional rule-based strategies\nto the contemporary marvels of transformer architectures, particularly\nhighlighting integrations such as BERT with LSTM and CNN. The narrative\naccentuates domain-specific NER models, tailored for intricate areas like\nfinance, legal, and healthcare, emphasizing their specialized adaptability.\nAdditionally, the research delves into cutting-edge paradigms including\nreinforcement learning, innovative constructs like E-NER, and the interplay of\nOptical Character Recognition (OCR) in augmenting NER capabilities. Grounding\nits insights in practical realms, the paper sheds light on the indispensable\nrole of NER in sectors like finance and biomedicine, addressing the unique\nchallenges they present. The conclusion outlines open challenges and avenues,\nmarking this work as a comprehensive guide for those delving into NER research\nand applications.",
        "translated": ""
    },
    {
        "title": "Diversify and Conquer: Bandits and Diversity for an Enhanced E-commerce\n  Homepage Experience",
        "url": "http://arxiv.org/abs/2309.14046v1",
        "pub_date": "2023-09-25",
        "summary": "In the realm of e-commerce, popular platforms utilize widgets to recommend\nadvertisements and products to their users. However, the prevalence of mobile\ndevice usage on these platforms introduces a unique challenge due to the\nlimited screen real estate available. Consequently, the positioning of relevant\nwidgets becomes pivotal in capturing and maintaining customer engagement. Given\nthe restricted screen size of mobile devices, widgets placed at the top of the\ninterface are more prominently displayed and thus attract greater user\nattention. Conversely, widgets positioned further down the page require users\nto scroll, resulting in reduced visibility and subsequent lower impression\nrates. Therefore it becomes imperative to place relevant widgets on top.\nHowever, selecting relevant widgets to display is a challenging task as the\nwidgets can be heterogeneous, widgets can be introduced or removed at any given\ntime from the platform. In this work, we model the vertical widget reordering\nas a contextual multi-arm bandit problem with delayed batch feedback. The\nobjective is to rank the vertical widgets in a personalized manner. We present\na two-stage ranking framework that combines contextual bandits with a diversity\nlayer to improve the overall ranking. We demonstrate its effectiveness through\noffline and online A/B results, conducted on proprietary data from Myntra, a\nmajor fashion e-commerce platform in India.",
        "translated": ""
    },
    {
        "title": "Multiple Relations Classification using Imbalanced Predictions\n  Adaptation",
        "url": "http://arxiv.org/abs/2309.13718v1",
        "pub_date": "2023-09-24",
        "summary": "The relation classification task assigns the proper semantic relation to a\npair of subject and object entities; the task plays a crucial role in various\ntext mining applications, such as knowledge graph construction and entities\ninteraction discovery in biomedical text. Current relation classification\nmodels employ additional procedures to identify multiple relations in a single\nsentence. Furthermore, they overlook the imbalanced predictions pattern. The\npattern arises from the presence of a few valid relations that need positive\nlabeling in a relatively large predefined relations set. We propose a multiple\nrelations classification model that tackles these issues through a customized\noutput architecture and by exploiting additional input features. Our findings\nsuggest that handling the imbalanced predictions leads to significant\nimprovements, even on a modest training design. The results demonstrate\nsuperiority performance on benchmark datasets commonly used in relation\nclassification. To the best of our knowledge, this work is the first that\nrecognizes the imbalanced predictions within the relation classification task.",
        "translated": ""
    },
    {
        "title": "Sparsity-regularized coded ptychography for robust and efficient\n  lensless microscopy on a chip",
        "url": "http://arxiv.org/abs/2309.13611v1",
        "pub_date": "2023-09-24",
        "summary": "In ptychographic imaging, the trade-off between the number of acquisitions\nand the resultant imaging quality presents a complex optimization problem.\nIncreasing the number of acquisitions typically yields reconstructions with\nhigher spatial resolution and finer details. Conversely, a reduction in\nmeasurement frequency often compromises the quality of the reconstructed\nimages, manifesting as increased noise and coarser details. To address this\nchallenge, we employ sparsity priors to reformulate the ptychographic\nreconstruction task as a total variation regularized optimization problem. We\nintroduce a new computational framework, termed the ptychographic proximal\ntotal-variation (PPTV) solver, designed to integrate into existing ptychography\nsettings without necessitating hardware modifications. Through comprehensive\nnumerical simulations, we validate that PPTV-driven coded ptychography is\ncapable of producing highly accurate reconstructions with a minimal set of\neight intensity measurements. Convergence analysis further substantiates the\nrobustness, stability, and computational feasibility of the proposed PPTV\nalgorithm. Experimental results obtained from optical setups unequivocally\ndemonstrate that the PPTV algorithm facilitates high-throughput,\nhigh-resolution imaging while significantly reducing the measurement burden.\nThese findings indicate that the PPTV algorithm has the potential to\nsubstantially mitigate the resource-intensive requirements traditionally\nassociated with high-quality ptychographic imaging, thereby offering a pathway\ntoward the development of more compact and efficient ptychographic microscopy\nsystems.",
        "translated": ""
    },
    {
        "title": "Related Rhythms: Recommendation System To Discover Music You May Like",
        "url": "http://arxiv.org/abs/2309.13544v1",
        "pub_date": "2023-09-24",
        "summary": "Machine Learning models are being utilized extensively to drive recommender\nsystems, which is a widely explored topic today. This is especially true of the\nmusic industry, where we are witnessing a surge in growth. Besides a large\nchunk of active users, these systems are fueled by massive amounts of data.\nThese large-scale systems yield applications that aim to provide a better user\nexperience and to keep customers actively engaged. In this paper, a distributed\nMachine Learning (ML) pipeline is delineated, which is capable of taking a\nsubset of songs as input and producing a new subset of songs identified as\nbeing similar to the inputted subset. The publicly accessible Million Songs\nDataset (MSD) enables researchers to develop and explore reasonably efficient\nsystems for audio track analysis and recommendations, without having to access\na commercialized music platform. The objective of the proposed application is\nto leverage an ML system trained to optimally recommend songs that a user might\nlike.",
        "translated": ""
    },
    {
        "title": "On the Sweet Spot of Contrastive Views for Knowledge-enhanced\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.13384v1",
        "pub_date": "2023-09-23",
        "summary": "In recommender systems, knowledge graph (KG) can offer critical information\nthat is lacking in the original user-item interaction graph (IG). Recent\nprocess has explored this direction and shows that contrastive learning is a\npromising way to integrate both. However, we observe that existing KG-enhanced\nrecommenders struggle in balancing between the two contrastive views of IG and\nKG, making them sometimes even less effective than simply applying contrastive\nlearning on IG without using KG. In this paper, we propose a new contrastive\nlearning framework for KG-enhanced recommendation. Specifically, to make full\nuse of the knowledge, we construct two separate contrastive views for KG and\nIG, and maximize their mutual information; to ease the contrastive learning on\nthe two views, we further fuse KG information into IG in a one-direction\nmanner.Extensive experimental results on three real-world datasets demonstrate\nthe effectiveness and efficiency of our method, compared to the\nstate-of-the-art. Our code is available through the anonymous\nlink:https://figshare.com/articles/conference_contribution/SimKGCL/22783382",
        "translated": ""
    },
    {
        "title": "Generative Retrieval with Semantic Tree-Structured Item Identifiers via\n  Contrastive Learning",
        "url": "http://arxiv.org/abs/2309.13375v1",
        "pub_date": "2023-09-23",
        "summary": "The retrieval phase is a vital component in recommendation systems, requiring\nthe model to be effective and efficient. Recently, generative retrieval has\nbecome an emerging paradigm for document retrieval, showing notable\nperformance. These methods enjoy merits like being end-to-end differentiable,\nsuggesting their viability in recommendation. However, these methods fall short\nin efficiency and effectiveness for large-scale recommendations. To obtain\nefficiency and effectiveness, this paper introduces a generative retrieval\nframework, namely SEATER, which learns SEmAntic Tree-structured item\nidentifiERs via contrastive learning. Specifically, we employ an\nencoder-decoder model to extract user interests from historical behaviors and\nretrieve candidates via tree-structured item identifiers. SEATER devises a\nbalanced k-ary tree structure of item identifiers, allocating semantic space to\neach token individually. This strategy maintains semantic consistency within\nthe same level, while distinct levels correlate to varying semantic\ngranularities. This structure also maintains consistent and fast inference\nspeed for all items. Considering the tree structure, SEATER learns identifier\ntokens' semantics, hierarchical relationships, and inter-token dependencies. To\nachieve this, we incorporate two contrastive learning tasks with the generation\ntask to optimize both the model and identifiers. The infoNCE loss aligns the\ntoken embeddings based on their hierarchical positions. The triplet loss ranks\nsimilar identifiers in desired orders. In this way, SEATER achieves both\nefficiency and effectiveness. Extensive experiments on three public datasets\nand an industrial dataset have demonstrated that SEATER outperforms\nstate-of-the-art models significantly.",
        "translated": ""
    },
    {
        "title": "Model-enhanced Vector Index",
        "url": "http://arxiv.org/abs/2309.13335v1",
        "pub_date": "2023-09-23",
        "summary": "Embedding-based retrieval methods construct vector indices to search for\ndocument representations that are most similar to the query representations.\nThey are widely used in document retrieval due to low latency and decent recall\nperformance. Recent research indicates that deep retrieval solutions offer\nbetter model quality, but are hindered by unacceptable serving latency and the\ninability to support document updates. In this paper, we aim to enhance the\nvector index with end-to-end deep generative models, leveraging the\ndifferentiable advantages of deep retrieval models while maintaining desirable\nserving efficiency. We propose Model-enhanced Vector Index (MEVI), a\ndifferentiable model-enhanced index empowered by a twin-tower representation\nmodel. MEVI leverages a Residual Quantization (RQ) codebook to bridge the\nsequence-to-sequence deep retrieval and embedding-based models. To\nsubstantially reduce the inference time, instead of decoding the unique\ndocument ids in long sequential steps, we first generate some semantic virtual\ncluster ids of candidate documents in a small number of steps, and then\nleverage the well-adapted embedding vectors to further perform a fine-grained\nsearch for the relevant documents in the candidate virtual clusters. We\nempirically show that our model achieves better performance on the commonly\nused academic benchmarks MSMARCO Passage and Natural Questions, with comparable\nserving latency to dense retrieval solutions.",
        "translated": ""
    },
    {
        "title": "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large\n  Language Models",
        "url": "http://arxiv.org/abs/2309.15088v1",
        "pub_date": "2023-09-26",
        "summary": "Researchers have successfully applied large language models (LLMs) such as\nChatGPT to reranking in an information retrieval context, but to date, such\nwork has mostly been built on proprietary models hidden behind opaque API\nendpoints. This approach yields experimental results that are not reproducible\nand non-deterministic, threatening the veracity of outcomes that build on such\nshaky foundations. To address this significant shortcoming, we present\nRankVicuna, the first fully open-source LLM capable of performing high-quality\nlistwise reranking in a zero-shot setting. Experimental results on the TREC\n2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness\ncomparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter\nmodel, although our effectiveness remains slightly behind reranking with GPT-4.\nWe hope our work provides the foundation for future research on reranking with\nmodern LLMs. All the code necessary to reproduce our results is available at\nhttps://github.com/castorini/rank_llm.",
        "translated": ""
    },
    {
        "title": "The Role of Document Embedding in Research Paper Recommender Systems: To\n  Breakdown or to Bolster Disciplinary Borders?",
        "url": "http://arxiv.org/abs/2309.14984v1",
        "pub_date": "2023-09-26",
        "summary": "In the extensive recommender systems literature, novelty and diversity have\nbeen identified as key properties of useful recommendations. However, these\nproperties have received limited attention in the specific sub-field of\nresearch paper recommender systems. In this work, we argue for the importance\nof offering novel and diverse research paper recommendations to scientists.\nThis approach aims to reduce siloed reading, break down filter bubbles, and\npromote interdisciplinary research. We propose a novel framework for evaluating\nthe novelty and diversity of research paper recommendations that leverages\nmethods from network analysis and natural language processing. Using this\nframework, we show that the choice of representational method within a larger\nresearch paper recommendation system can have a measurable impact on the nature\nof downstream recommendations, specifically on their novelty and diversity. We\nintroduce a novel paper embedding method, which we demonstrate offers more\ninnovative and diverse recommendations without sacrificing precision, compared\nto other state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Modeling Multi-aspect Preferences and Intents for Multi-behavioral\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2309.14938v1",
        "pub_date": "2023-09-26",
        "summary": "Multi-behavioral sequential recommendation has recently attracted increasing\nattention. However, existing methods suffer from two major limitations.\nFirstly, user preferences and intents can be described in fine-grained detail\nfrom multiple perspectives; yet, these methods fail to capture their\nmulti-aspect nature. Secondly, user behaviors may contain noises, and most\nexisting methods could not effectively deal with noises. In this paper, we\npresent an attentive recurrent model with multiple projections to capture\nMulti-Aspect preferences and INTents (MAINT in short). To extract multi-aspect\npreferences from target behaviors, we propose a multi-aspect projection\nmechanism for generating multiple preference representations from multiple\naspects. To extract multi-aspect intents from multi-typed behaviors, we propose\na behavior-enhanced LSTM and a multi-aspect refinement attention mechanism. The\nattention mechanism can filter out noises and generate multiple intent\nrepresentations from different aspects. To adaptively fuse user preferences and\nintents, we propose a multi-aspect gated fusion mechanism. Extensive\nexperiments conducted on real-world datasets have demonstrated the\neffectiveness of our model.",
        "translated": ""
    },
    {
        "title": "REFORM: Removing False Correlation in Multi-level Interaction for CTR\n  Prediction",
        "url": "http://arxiv.org/abs/2309.14891v1",
        "pub_date": "2023-09-26",
        "summary": "Click-through rate (CTR) prediction is a critical task in online advertising\nand recommendation systems, as accurate predictions are essential for user\ntargeting and personalized recommendations. Most recent cutting-edge methods\nprimarily focus on investigating complex implicit and explicit feature\ninteractions. However, these methods neglect the issue of false correlations\ncaused by confounding factors or selection bias. This problem is further\nmagnified by the complexity and redundancy of these interactions. We propose a\nCTR prediction framework that removes false correlation in multi-level feature\ninteraction, termed REFORM. The proposed REFORM framework exploits a wide range\nof multi-level high-order feature representations via a two-stream stacked\nrecurrent structure while eliminating false correlations. The framework has two\nkey components: I. The multi-level stacked recurrent (MSR) structure enables\nthe model to efficiently capture diverse nonlinear interactions from feature\nspaces of different levels, and the richer representations lead to enhanced CTR\nprediction accuracy. II. The false correlation elimination (FCE) module further\nleverages Laplacian kernel mapping and sample reweighting methods to eliminate\nfalse correlations concealed within the multi-level features, allowing the\nmodel to focus on the true causal effects. Extensive experiments based on four\nchallenging CTR datasets and our production dataset demonstrate that the\nproposed REFORM model achieves state-of-the-art performance. Codes, models and\nour dataset will be released at https://github.com/yansuoyuli/REFORM.",
        "translated": ""
    },
    {
        "title": "ALEX: Towards Effective Graph Transfer Learning with Noisy Labels",
        "url": "http://arxiv.org/abs/2309.14673v1",
        "pub_date": "2023-09-26",
        "summary": "Graph Neural Networks (GNNs) have garnered considerable interest due to their\nexceptional performance in a wide range of graph machine learning tasks.\nNevertheless, the majority of GNN-based approaches have been examined using\nwell-annotated benchmark datasets, leading to suboptimal performance in\nreal-world graph learning scenarios. To bridge this gap, the present paper\ninvestigates the problem of graph transfer learning in the presence of label\nnoise, which transfers knowledge from a noisy source graph to an unlabeled\ntarget graph. We introduce a novel technique termed Balance Alignment and\nInformation-aware Examination (ALEX) to address this challenge. ALEX first\nemploys singular value decomposition to generate different views with crucial\nstructural semantics, which help provide robust node representations using\ngraph contrastive learning. To mitigate both label shift and domain shift, we\nestimate a prior distribution to build subgraphs with balanced label\ndistributions. Building on this foundation, an adversarial domain discriminator\nis incorporated for the implicit domain alignment of complex multi-modal\ndistributions. Furthermore, we project node representations into a different\nspace, optimizing the mutual information between the projected features and\nlabels. Subsequently, the inconsistency of similarity structures is evaluated\nto identify noisy samples with potential overfitting. Comprehensive experiments\non various benchmark datasets substantiate the outstanding superiority of the\nproposed ALEX in different settings.",
        "translated": ""
    },
    {
        "title": "Tranformer-based classification of user queries for medical consultancy\n  with respect to expert specialisation",
        "url": "http://arxiv.org/abs/2309.14662v1",
        "pub_date": "2023-09-26",
        "summary": "The need for skilled medical support is growing in the era of digital\nhealthcare. This research presents an innovative strategy, utilising the RuBERT\nmodel, for categorising user inquiries in the field of medical consultation\nwith a focus on expert specialisation. By harnessing the capabilities of\ntransformers, we fine-tuned the pre-trained RuBERT model on a varied dataset,\nwhich facilitates precise correspondence between queries and particular medical\nspecialisms. Using a comprehensive dataset, we have demonstrated our approach's\nsuperior performance with an F1-score of over 92%, calculated through both\ncross-validation and the traditional split of test and train datasets. Our\napproach has shown excellent generalisation across medical domains such as\ncardiology, neurology and dermatology. This methodology provides practical\nbenefits by directing users to appropriate specialists for prompt and targeted\nmedical advice. It also enhances healthcare system efficiency, reduces\npractitioner burden, and improves patient care quality. In summary, our\nsuggested strategy facilitates the attainment of specific medical knowledge,\noffering prompt and precise advice within the digital healthcare field.",
        "translated": ""
    },
    {
        "title": "Algorithmic Collusion or Competition: the Role of Platforms' Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2309.14548v1",
        "pub_date": "2023-09-25",
        "summary": "Recent academic research has extensively examined algorithmic collusion\nresulting from the utilization of artificial intelligence (AI)-based dynamic\npricing algorithms. Nevertheless, e-commerce platforms employ recommendation\nalgorithms to allocate exposure to various products, and this important aspect\nhas been largely overlooked in previous studies on algorithmic collusion. Our\nstudy bridges this important gap in the literature and examines how\nrecommendation algorithms can determine the competitive or collusive dynamics\nof AI-based pricing algorithms. Specifically, two commonly deployed\nrecommendation algorithms are examined: (i) a recommender system that aims to\nmaximize the sellers' total profit (profit-based recommender system) and (ii) a\nrecommender system that aims to maximize the demand for products sold on the\nplatform (demand-based recommender system). We construct a repeated game\nframework that incorporates both pricing algorithms adopted by sellers and the\nplatform's recommender system. Subsequently, we conduct experiments to observe\nprice dynamics and ascertain the final equilibrium. Experimental results reveal\nthat a profit-based recommender system intensifies algorithmic collusion among\nsellers due to its congruence with sellers' profit-maximizing objectives.\nConversely, a demand-based recommender system fosters price competition among\nsellers and results in a lower price, owing to its misalignment with sellers'\ngoals. Extended analyses suggest the robustness of our findings in various\nmarket scenarios. Overall, we highlight the importance of platforms'\nrecommender systems in delineating the competitive structure of the digital\nmarketplace, providing important insights for market participants and\ncorresponding policymakers.",
        "translated": ""
    },
    {
        "title": "Temporal graph models fail to capture global temporal dynamics",
        "url": "http://arxiv.org/abs/2309.15730v1",
        "pub_date": "2023-09-27",
        "summary": "A recently released Temporal Graph Benchmark is analyzed in the context of\nDynamic Link Property Prediction. We outline our observations and propose a\ntrivial optimization-free baseline of \"recently popular nodes\" outperforming\nother methods on all medium and large-size datasets in the Temporal Graph\nBenchmark. We propose two measures based on Wasserstein distance which can\nquantify the strength of short-term and long-term global dynamics of datasets.\nBy analyzing our unexpectedly strong baseline, we show how standard negative\nsampling evaluation can be unsuitable for datasets with strong temporal\ndynamics. We also show how simple negative-sampling can lead to model\ndegeneration during training, resulting in impossible to rank, fully saturated\npredictions of temporal graph networks. We propose improved negative sampling\nschemes for both training and evaluation and prove their usefulness. We conduct\na comparison with a model trained non-contrastively without negative sampling.\nOur results provide a challenging baseline and indicate that temporal graph\nnetwork architectures need deep rethinking for usage in problems with\nsignificant global dynamics, such as social media, cryptocurrency markets or\ne-commerce. We open-source the code for baselines, measures and proposed\nnegative sampling schemes.",
        "translated": ""
    },
    {
        "title": "Cold &amp; Warm Net: Addressing Cold-Start Users in Recommender Systems",
        "url": "http://arxiv.org/abs/2309.15646v1",
        "pub_date": "2023-09-27",
        "summary": "Cold-start recommendation is one of the major challenges faced by recommender\nsystems (RS). Herein, we focus on the user cold-start problem. Recently,\nmethods utilizing side information or meta-learning have been used to model\ncold-start users. However, it is difficult to deploy these methods to\nindustrial RS. There has not been much research that pays attention to the user\ncold-start problem in the matching stage. In this paper, we propose Cold &amp; Warm\nNet based on expert models who are responsible for modeling cold-start and\nwarm-up users respectively. A gate network is applied to incorporate the\nresults from two experts. Furthermore, dynamic knowledge distillation acting as\na teacher selector is introduced to assist experts in better learning user\nrepresentation. With comprehensive mutual information, features highly relevant\nto user behavior are selected for the bias net which explicitly models user\nbehavior bias. Finally, we evaluate our Cold &amp; Warm Net on public datasets in\ncomparison to models commonly applied in the matching stage and it outperforms\nother models on all user types. The proposed model has also been deployed on an\nindustrial short video platform and achieves a significant increase in app\ndwell time and user retention rate.",
        "translated": ""
    },
    {
        "title": "Identifiability Matters: Revealing the Hidden Recoverable Condition in\n  Unbiased Learning to Rank",
        "url": "http://arxiv.org/abs/2309.15560v1",
        "pub_date": "2023-09-27",
        "summary": "The application of Unbiased Learning to Rank (ULTR) is widespread in modern\nsystems for training unbiased ranking models from biased click logs. The key is\nto explicitly model a generation process for user behavior and fit click data\nbased on examination hypothesis. Previous research found empirically that the\ntrue latent relevance can be recovered in most cases as long as the clicks are\nperfectly fitted. However, we demonstrate that this is not always achievable,\nresulting in a significant reduction in ranking performance. In this work, we\naim to answer if or when the true relevance can be recovered from click data,\nwhich is a foundation issue for ULTR field. We first define a ranking model as\nidentifiable if it can recover the true relevance up to a scaling\ntransformation, which is enough for pairwise ranking objective. Then we explore\nan equivalent condition for identifiability that can be novely expressed as a\ngraph connectivity test problem: if and only if a graph (namely identifiability\ngraph, or IG) constructed on the underlying structure of the dataset is\nconnected, we can guarantee that the relevance can be correctly recovered. When\nthe IG is not connected, there may be bad cases leading to poor ranking\nperformance. To address this issue, we propose two methods, namely node\nintervention and node merging, to modify the dataset and restore connectivity\nof the IG. Empirical results obtained on a simulation dataset and two LTR\nbenchmark datasets confirm the validity of our proposed theorems and show the\neffectiveness of our methods in mitigating data bias when the relevance model\nis unidentifiable.",
        "translated": ""
    },
    {
        "title": "Automatic Feature Fairness in Recommendation via Adversaries",
        "url": "http://arxiv.org/abs/2309.15418v1",
        "pub_date": "2023-09-27",
        "summary": "Fairness is a widely discussed topic in recommender systems, but its\npractical implementation faces challenges in defining sensitive features while\nmaintaining recommendation accuracy. We propose feature fairness as the\nfoundation to achieve equitable treatment across diverse groups defined by\nvarious feature combinations. This improves overall accuracy through balanced\nfeature generalizability. We introduce unbiased feature learning through\nadversarial training, using adversarial perturbation to enhance feature\nrepresentation. The adversaries improve model generalization for\nunder-represented features. We adapt adversaries automatically based on two\nforms of feature biases: frequency and combination variety of feature values.\nThis allows us to dynamically adjust perturbation strengths and adversarial\ntraining weights. Stronger perturbations are applied to feature values with\nfewer combination varieties to improve generalization, while higher weights for\nlow-frequency features address training imbalances. We leverage the Adaptive\nAdversarial perturbation based on the widely-applied Factorization Machine\n(AAFM) as our backbone model. In experiments, AAFM surpasses strong baselines\nin both fairness and accuracy measures. AAFM excels in providing item- and\nuser-fairness for single- and multi-feature tasks, showcasing their versatility\nand scalability. To maintain good accuracy, we find that adversarial\nperturbation must be well-managed: during training, perturbations should not\noverly persist and their strengths should decay.",
        "translated": ""
    },
    {
        "title": "Frequency and cardinality recovery from sketched data: a novel approach\n  bridging Bayesian and frequentist views",
        "url": "http://arxiv.org/abs/2309.15408v1",
        "pub_date": "2023-09-27",
        "summary": "We study how to recover the frequency of a symbol in a large discrete data\nset, using only a compressed representation, or sketch, of those data obtained\nvia random hashing. This is a classical problem in computer science, with\nvarious algorithms available, such as the count-min sketch. However, these\nalgorithms often assume that the data are fixed, leading to overly conservative\nand potentially inaccurate estimates when dealing with randomly sampled data.\nIn this paper, we consider the sketched data as a random sample from an unknown\ndistribution, and then we introduce novel estimators that improve upon existing\napproaches. Our method combines Bayesian nonparametric and classical\n(frequentist) perspectives, addressing their unique limitations to provide a\nprincipled and practical solution. Additionally, we extend our method to\naddress the related but distinct problem of cardinality recovery, which\nconsists of estimating the total number of distinct objects in the data set. We\nvalidate our method on synthetic and real data, comparing its performance to\nstate-of-the-art alternatives.",
        "translated": ""
    },
    {
        "title": "A Content-Driven Micro-Video Recommendation Dataset at Scale",
        "url": "http://arxiv.org/abs/2309.15379v1",
        "pub_date": "2023-09-27",
        "summary": "Micro-videos have recently gained immense popularity, sparking critical\nresearch in micro-video recommendation with significant implications for the\nentertainment, advertising, and e-commerce industries. However, the lack of\nlarge-scale public micro-video datasets poses a major challenge for developing\neffective recommender systems. To address this challenge, we introduce a very\nlarge micro-video recommendation dataset, named \"MicroLens\", consisting of one\nbillion user-item interaction behaviors, 34 million users, and one million\nmicro-videos. This dataset also contains various raw modality information about\nvideos, including titles, cover images, audio, and full-length videos.\nMicroLens serves as a benchmark for content-driven micro-video recommendation,\nenabling researchers to utilize various modalities of video information for\nrecommendation, rather than relying solely on item IDs or off-the-shelf video\nfeatures extracted from a pre-trained network. Our benchmarking of multiple\nrecommender models and video encoders on MicroLens has yielded valuable\ninsights into the performance of micro-video recommendation. We believe that\nthis dataset will not only benefit the recommender system community but also\npromote the development of the video understanding field. Our datasets and code\nare available at https://github.com/westlake-repl/MicroLens.",
        "translated": ""
    },
    {
        "title": "LD4MRec: Simplifying and Powering Diffusion Model for Multimedia\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.15363v1",
        "pub_date": "2023-09-27",
        "summary": "Multimedia recommendation aims to predict users' future behaviors based on\nhistorical behavioral data and item's multimodal information. However, noise\ninherent in behavioral data, arising from unintended user interactions with\nuninteresting items, detrimentally impacts recommendation performance.\nRecently, diffusion models have achieved high-quality information generation,\nin which the reverse process iteratively infers future information based on the\ncorrupted state. It meets the need of predictive tasks under noisy conditions,\nand inspires exploring their application to predicting user behaviors.\nNonetheless, several challenges must be addressed: 1) Classical diffusion\nmodels require excessive computation, which does not meet the efficiency\nrequirements of recommendation systems. 2) Existing reverse processes are\nmainly designed for continuous data, whereas behavioral information is discrete\nin nature. Therefore, an effective method is needed for the generation of\ndiscrete behavioral information.\n  To tackle the aforementioned issues, we propose a Light Diffusion model for\nMultimedia Recommendation. First, to reduce computational complexity, we\nsimplify the formula of the reverse process, enabling one-step inference\ninstead of multi-step inference. Second, to achieve effective behavioral\ninformation generation, we propose a novel Conditional neural Network. It maps\nthe discrete behavior data into a continuous latent space, and generates\nbehaviors with the guidance of collaborative signals and user multimodal\npreference. Additionally, considering that completely clean behavior data is\ninaccessible, we introduce a soft behavioral reconstruction constraint during\nmodel training, facilitating behavior prediction with noisy data. Empirical\nstudies conducted on three public datasets demonstrate the effectiveness of\nLD4MRec.",
        "translated": ""
    },
    {
        "title": "Decoding the Workplace &amp; EOR: An Employee Survey Analysis by Data\n  Science Techniques and Visualization",
        "url": "http://arxiv.org/abs/2309.16329v1",
        "pub_date": "2023-09-28",
        "summary": "This research study explores the new dynamics of employee-organi-zation\nrelationships (EOR) [6] using advanced data science methodologies and presents\nfindings through accessible visualizations. Leveraging a dataset pro-cured from\na comprehensive nationwide big employee survey, this study employs innovative\nstrategy for theoretical researcher by using our state-of-the-art\nvisual-ization. The results present insightful visualizations encapsulating\ndemographic analysis, workforce satisfaction, work environment scrutiny, and\nthe employee's view via word cloud interpretations and burnout predictions.\n  The study underscores the profound implications of data science across\nvarious management sectors, enhancing understanding of workplace dynamics and\npro-moting mutual growth and satisfaction. This multifaceted approach caters to\na diverse array of readers, from researchers in sociology and management to\nfirms seeking detailed understanding of their workforce's satisfaction,\nemphasizing on practicality and interpretability.\n  The research encourages proactive measures to improve workplace\nenviron-ments, boost employee satisfaction, and foster healthier, more\nproductive organ-izations. It serves as a resourceful tool for those committed\nto these objectives, manifesting the transformative potential of data science\nin driving insightful nar-ratives about workplace dynamics and\nemployee-organization relationships. In essence, this research unearths\nvaluable insights to aid management, HR profes-sionals, and companies",
        "translated": ""
    },
    {
        "title": "Multi-Granularity Click Confidence Learning via Self-Distillation in\n  Recommendation",
        "url": "http://arxiv.org/abs/2309.16322v1",
        "pub_date": "2023-09-28",
        "summary": "Recommendation systems rely on historical clicks to learn user interests and\nprovide appropriate items. However, current studies tend to treat clicks\nequally, which may ignore the assorted intensities of user interests in\ndifferent clicks. In this paper, we aim to achieve multi-granularity Click\nconfidence Learning via Self-Distillation in recommendation (CLSD). Due to the\nlack of supervised signals in click confidence, we first apply self-supervised\nlearning to obtain click confidence scores via a global self-distillation\nmethod. After that, we define a local confidence function to adapt confidence\nscores at the user group level, since the confidence distributions can be\nvaried among user groups. With the combination of multi-granularity confidence\nlearning, we can distinguish the quality of clicks and model user interests\nmore accurately without involving extra data and model structures. The\nsignificant improvements over different backbones on industrial offline and\nonline experiments in a real-world recommender system prove the effectiveness\nof our model. Recently, CLSD has been deployed on a large-scale recommender\nsystem, affecting over 400 million users.",
        "translated": ""
    },
    {
        "title": "Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale\n  Localization",
        "url": "http://arxiv.org/abs/2309.16034v1",
        "pub_date": "2023-09-27",
        "summary": "Advancements in nanotechnology and material science are paving the way toward\nnanoscale devices that combine sensing, computing, data and energy storage, and\nwireless communication. In precision medicine, these nanodevices show promise\nfor disease diagnostics, treatment, and monitoring from within the patients'\nbloodstreams. Assigning the location of a sensed biological event with the\nevent itself, which is the main proposition of flow-guided in-body nanoscale\nlocalization, would be immensely beneficial from the perspective of precision\nmedicine. The nanoscale nature of the nanodevices and the challenging\nenvironment that the bloodstream represents, result in current flow-guided\nlocalization approaches being constrained in their communication and\nenergy-related capabilities. The communication and energy constraints of the\nnanodevices result in different features of raw data for flow-guided\nlocalization, in turn affecting its performance. An analytical modeling of the\neffects of imperfect communication and constrained energy causing intermittent\noperation of the nanodevices on the raw data produced by the nanodevices would\nbe beneficial. Hence, we propose an analytical model of raw data for\nflow-guided localization, where the raw data is modeled as a function of\ncommunication and energy-related capabilities of the nanodevice. We evaluate\nthe model by comparing its output with the one obtained through the utilization\nof a simulator for objective evaluation of flow-guided localization, featuring\ncomparably higher level of realism. Our results across a number of scenarios\nand heterogeneous performance metrics indicate high similarity between the\nmodel and simulator-generated raw datasets.",
        "translated": ""
    },
    {
        "title": "Toward Robust Recommendation via Real-time Vicinal Defense",
        "url": "http://arxiv.org/abs/2309.17278v1",
        "pub_date": "2023-09-29",
        "summary": "Recommender systems have been shown to be vulnerable to poisoning attacks,\nwhere malicious data is injected into the dataset to cause the recommender\nsystem to provide biased recommendations. To defend against such attacks,\nvarious robust learning methods have been proposed. However, most methods are\nmodel-specific or attack-specific, making them lack generality, while other\nmethods, such as adversarial training, are oriented towards evasion attacks and\nthus have a weak defense strength in poisoning attacks.\n  In this paper, we propose a general method, Real-time Vicinal Defense (RVD),\nwhich leverages neighboring training data to fine-tune the model before making\na recommendation for each user. RVD works in the inference phase to ensure the\nrobustness of the specific sample in real-time, so there is no need to change\nthe model structure and training process, making it more practical. Extensive\nexperimental results demonstrate that RVD effectively mitigates targeted\npoisoning attacks across various models without sacrificing accuracy. Moreover,\nthe defensive effect can be further amplified when our method is combined with\nother strategies.",
        "translated": ""
    },
    {
        "title": "SAppKG: Mobile App Recommendation Using Knowledge Graph and Side\n  Information-A Secure Framework",
        "url": "http://arxiv.org/abs/2309.17115v1",
        "pub_date": "2023-09-29",
        "summary": "Due to the rapid development of technology and the widespread usage of\nsmartphones, the number of mobile applications is exponentially growing.\nFinding a suitable collection of apps that aligns with users needs and\npreferences can be challenging. However, mobile app recommender systems have\nemerged as a helpful tool in simplifying this process. But there is a drawback\nto employing app recommender systems. These systems need access to user data,\nwhich is a serious security violation. While users seek accurate opinions, they\ndo not want to compromise their privacy in the process. We address this issue\nby developing SAppKG, an end-to-end user privacy-preserving knowledge graph\narchitecture for mobile app recommendation based on knowledge graph models such\nas SAppKG-S and SAppKG-D, that utilized the interaction data and side\ninformation of app attributes. We tested the proposed model on real-world data\nfrom the Google Play app store, using precision, recall, mean absolute\nprecision, and mean reciprocal rank. We found that the proposed model improved\nresults on all four metrics. We also compared the proposed model to baseline\nmodels and found that it outperformed them on all four metrics.",
        "translated": ""
    },
    {
        "title": "Aligning the Capabilities of Large Language Models with the Context of\n  Information Retrieval via Contrastive Feedback",
        "url": "http://arxiv.org/abs/2309.17078v1",
        "pub_date": "2023-09-29",
        "summary": "Information Retrieval (IR), the process of finding information to satisfy\nuser's information needs, plays an essential role in modern people's lives.\nRecently, large language models (LLMs) have demonstrated remarkable\ncapabilities across various tasks, some of which are important for IR.\nNonetheless, LLMs frequently confront the issue of generating responses that\nlack specificity. This has limited the overall effectiveness of LLMs for IR in\nmany cases. To address these issues, we present an unsupervised alignment\nframework called Reinforcement Learning from Contrastive Feedback (RLCF), which\nempowers LLMs to generate both high-quality and context-specific responses that\nsuit the needs of IR tasks. Specifically, we construct contrastive feedback by\ncomparing each document with its similar documents, and then propose a reward\nfunction named Batched-MRR to teach LLMs to generate responses that captures\nthe fine-grained information that distinguish documents from their similar\nones. To demonstrate the effectiveness of RLCF, we conducted experiments in two\ntypical applications of LLMs in IR, i.e., data augmentation and summarization.\nThe experimental results show that RLCF can effectively improve the performance\nof LLMs in IR context.",
        "translated": ""
    },
    {
        "title": "Beyond Co-occurrence: Multi-modal Session-based Recommendation",
        "url": "http://arxiv.org/abs/2309.17037v1",
        "pub_date": "2023-09-29",
        "summary": "Session-based recommendation is devoted to characterizing preferences of\nanonymous users based on short sessions. Existing methods mostly focus on\nmining limited item co-occurrence patterns exposed by item ID within sessions,\nwhile ignoring what attracts users to engage with certain items is rich\nmulti-modal information displayed on pages. Generally, the multi-modal\ninformation can be classified into two categories: descriptive information\n(e.g., item images and description text) and numerical information (e.g.,\nprice). In this paper, we aim to improve session-based recommendation by\nmodeling the above multi-modal information holistically. There are mainly three\nissues to reveal user intent from multi-modal information: (1) How to extract\nrelevant semantics from heterogeneous descriptive information with different\nnoise? (2) How to fuse these heterogeneous descriptive information to\ncomprehensively infer user interests? (3) How to handle probabilistic influence\nof numerical information on user behaviors? To solve above issues, we propose a\nnovel multi-modal session-based recommendation (MMSBR) that models both\ndescriptive and numerical information under a unified framework. Specifically,\na pseudo-modality contrastive learning is devised to enhance the representation\nlearning of descriptive information. Afterwards, a hierarchical pivot\ntransformer is presented to fuse heterogeneous descriptive information.\nMoreover, we represent numerical information with Gaussian distribution and\ndesign a Wasserstein self-attention to handle the probabilistic influence mode.\nExtensive experiments on three real-world datasets demonstrate the\neffectiveness of the proposed MMSBR. Further analysis also proves that our\nMMSBR can alleviate the cold-start problem in SBR effectively.",
        "translated": ""
    },
    {
        "title": "Hallucination Reduction in Long Input Text Summarization",
        "url": "http://arxiv.org/abs/2309.16781v1",
        "pub_date": "2023-09-28",
        "summary": "Hallucination in text summarization refers to the phenomenon where the model\ngenerates information that is not supported by the input source document.\nHallucination poses significant obstacles to the accuracy and reliability of\nthe generated summaries. In this paper, we aim to reduce hallucinated outputs\nor hallucinations in summaries of long-form text documents. We have used the\nPubMed dataset, which contains long scientific research documents and their\nabstracts. We have incorporated the techniques of data filtering and joint\nentity and summary generation (JAENS) in the fine-tuning of the Longformer\nEncoder-Decoder (LED) model to minimize hallucinations and thereby improve the\nquality of the generated summary. We have used the following metrics to measure\nfactual consistency at the entity level: precision-source, and F1-target. Our\nexperiments show that the fine-tuned LED model performs well in generating the\npaper abstract. Data filtering techniques based on some preprocessing steps\nreduce entity-level hallucinations in the generated summaries in terms of some\nof the factual consistency metrics.",
        "translated": ""
    },
    {
        "title": "CORec-Cri: How collaborative and social technologies can help to\n  contextualize crises?",
        "url": "http://arxiv.org/abs/2310.02143v1",
        "pub_date": "2023-10-03",
        "summary": "Crisis situations can present complex and multifaceted challenges, often\nrequiring the involvement of multiple organizations and stakeholders with\nvarying areas of expertise, responsibilities, and resources. Acquiring accurate\nand timely information about impacted areas is crucial to effectively respond\nto these crises. In this paper, we investigate how collaborative and social\ntechnologies help to contextualize crises, including identifying impacted areas\nand real-time needs. To this end, we define CORec-Cri (Contextulized\nOntology-based Recommender system for crisis management) based on existing\nwork. Our motivation for this approach is two-fold: first, effective\ncollaboration among stakeholders is essential for efficient and coordinated\ncrisis response; second, social computing facilitates interaction, information\nflow, and collaboration among stakeholders. We detail the key components of our\nsystem design, highlighting its potential to support decision-making, resource\nallocation, and communication among stakeholders. Finally, we provide examples\nof how our system can be applied to contextualize crises to improve crisis\nmanagement.",
        "translated": ""
    },
    {
        "title": "Online Multimedia Verification with Computational Tools and OSINT:\n  Russia-Ukraine Conflict Case Studies",
        "url": "http://arxiv.org/abs/2310.01978v1",
        "pub_date": "2023-10-03",
        "summary": "This paper investigates the use of computational tools and Open-Source\nIntelligence (OSINT) techniques for verifying online multimedia content, with a\nspecific focus on real-world cases from the Russia-Ukraine conflict. Over a\nnine-month period from April to December 2022, we examine verification\nworkflows, tools, and case studies published by \\faktiskbar. Our study\nshowcases the effectiveness of diverse resources, including AI tools,\ngeolocation tools, internet archives, and social media monitoring platforms, in\nenabling journalists and fact-checkers to efficiently process and corroborate\nevidence, ensuring the dissemination of accurate information. This research\nunderscores the vital role of computational tools and OSINT techniques in\npromoting evidence-based reporting and combatting misinformation. We also touch\non the current limitations of available tools and prospects for future\ndevelopments in multimedia verification.",
        "translated": ""
    },
    {
        "title": "DANI: Fast Diffusion Aware Network Inference with Preserving Topological\n  Structure Property",
        "url": "http://arxiv.org/abs/2310.01696v1",
        "pub_date": "2023-10-02",
        "summary": "The fast growth of social networks and their data access limitations in\nrecent years has led to increasing difficulty in obtaining the complete\ntopology of these networks. However, diffusion information over these networks\nis available, and many algorithms have been proposed to infer the underlying\nnetworks using this information. The previously proposed algorithms only focus\non inferring more links and ignore preserving the critical topological\ncharacteristics of the underlying social networks. In this paper, we propose a\nnovel method called DANI to infer the underlying network while preserving its\nstructural properties. It is based on the Markov transition matrix derived from\ntime series cascades, as well as the node-node similarity that can be observed\nin the cascade behavior from a structural point of view. In addition, the\npresented method has linear time complexity (increases linearly with the number\nof nodes, number of cascades, and square of the average length of cascades),\nand its distributed version in the MapReduce framework is also scalable. We\napplied the proposed approach to both real and synthetic networks. The\nexperimental results showed that DANI has higher accuracy and lower run time\nwhile maintaining structural properties, including modular structure, degree\ndistribution, connected components, density, and clustering coefficients, than\nwell-known network inference methods.",
        "translated": ""
    },
    {
        "title": "Towards Efficient and Effective Adaptation of Large Language Models for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2310.01612v1",
        "pub_date": "2023-10-02",
        "summary": "In recent years, with large language models (LLMs) achieving state-of-the-art\nperformance in context understanding, increasing efforts have been dedicated to\ndeveloping LLM-enhanced sequential recommendation (SR) methods. Considering\nthat most existing LLMs are not specifically optimized for recommendation\ntasks, adapting them for SR becomes a critical step in LLM-enhanced SR methods.\nThough numerous adaptation methods have been developed, it still remains a\nsignificant challenge to adapt LLMs for SR both efficiently and effectively. To\naddress this challenge, in this paper, we introduce a novel side sequential\nnetwork adaptation method, denoted as SSNA, for LLM enhanced SR. SSNA features\nthree key designs to allow both efficient and effective LLM adaptation. First,\nSSNA learns adapters separate from LLMs, while fixing all the pre-trained\nparameters within LLMs to allow efficient adaptation. In addition, SSNA adapts\nthe top-a layers of LLMs jointly, and integrates adapters sequentially for\nenhanced effectiveness (i.e., recommendation performance). We compare SSNA\nagainst five state-of-the-art baseline methods on five benchmark datasets using\nthree LLMs. The experimental results demonstrate that SSNA significantly\noutperforms all the baseline methods in terms of recommendation performance,\nand achieves substantial improvement over the best-performing baseline methods\nat both run-time and memory efficiency during training. Our analysis shows the\neffectiveness of integrating adapters in a sequential manner. Our parameter\nstudy demonstrates the effectiveness of jointly adapting the top-a layers of\nLLMs.",
        "translated": ""
    },
    {
        "title": "Causality-informed Rapid Post-hurricane Building Damage Detection in\n  Large Scale from InSAR Imagery",
        "url": "http://arxiv.org/abs/2310.01565v1",
        "pub_date": "2023-10-02",
        "summary": "Timely and accurate assessment of hurricane-induced building damage is\ncrucial for effective post-hurricane response and recovery efforts. Recently,\nremote sensing technologies provide large-scale optical or Interferometric\nSynthetic Aperture Radar (InSAR) imagery data immediately after a disastrous\nevent, which can be readily used to conduct rapid building damage assessment.\nCompared to optical satellite imageries, the Synthetic Aperture Radar can\npenetrate cloud cover and provide more complete spatial coverage of damaged\nzones in various weather conditions. However, these InSAR imageries often\ncontain highly noisy and mixed signals induced by co-occurring or co-located\nbuilding damage, flood, flood/wind-induced vegetation changes, as well as\nanthropogenic activities, making it challenging to extract accurate building\ndamage information. In this paper, we introduced an approach for rapid\npost-hurricane building damage detection from InSAR imagery. This approach\nencoded complex causal dependencies among wind, flood, building damage, and\nInSAR imagery using a holistic causal Bayesian network. Based on the causal\nBayesian network, we further jointly inferred the large-scale unobserved\nbuilding damage by fusing the information from InSAR imagery with prior\nphysical models of flood and wind, without the need for ground truth labels.\nFurthermore, we validated our estimation results in a real-world devastating\nhurricane -- the 2022 Hurricane Ian. We gathered and annotated building damage\nground truth data in Lee County, Florida, and compared the introduced method's\nestimation results with the ground truth and benchmarked it against\nstate-of-the-art models to assess the effectiveness of our proposed method.\nResults show that our method achieves rapid and accurate detection of building\ndamage, with significantly reduced processing time compared to traditional\nmanual inspection methods.",
        "translated": ""
    },
    {
        "title": "Replicating Relevance-Ranked Synonym Discovery in a New Language and\n  Domain",
        "url": "http://arxiv.org/abs/2310.01507v1",
        "pub_date": "2023-10-02",
        "summary": "Domain-specific synonyms occur in many specialized search tasks, such as when\nsearching medical documents, legal documents, and software engineering\nartifacts. We replicate prior work on ranking domain-specific synonyms in the\nconsumer health domain by applying the approach to a new language and domain:\nidentifying Swedish language synonyms in the building construction domain. We\nchose this setting because identifying synonyms in this domain is helpful for\ndownstream systems, where different users may query for documents (e.g.,\nengineering requirements) using different terminology. We consider two new\nfeatures inspired by the change in language and methodological advances since\nthe prior work's publication. An evaluation using data from the building\nconstruction domain supports the finding from the prior work that synonym\ndiscovery is best approached as a learning to rank task in which a human editor\nviews ranked synonym candidates in order to construct a domain-specific\nthesaurus. We additionally find that FastText embeddings alone provide a strong\nbaseline, though they do not perform as well as the strongest learning to rank\nmethod. Finally, we analyze the performance of individual features and the\ndifferences in the domains.",
        "translated": ""
    },
    {
        "title": "LEEC: A Legal Element Extraction Dataset with an Extensive\n  Domain-Specific Label System",
        "url": "http://arxiv.org/abs/2310.01271v1",
        "pub_date": "2023-10-02",
        "summary": "As a pivotal task in natural language processing, element extraction has\ngained significance in the legal domain. Extracting legal elements from\njudicial documents helps enhance interpretative and analytical capacities of\nlegal cases, and thereby facilitating a wide array of downstream applications\nin various domains of law. Yet existing element extraction datasets are limited\nby their restricted access to legal knowledge and insufficient coverage of\nlabels. To address this shortfall, we introduce a more comprehensive,\nlarge-scale criminal element extraction dataset, comprising 15,831 judicial\ndocuments and 159 labels. This dataset was constructed through two main steps:\nFirst, designing the label system by our team of legal experts based on prior\nlegal research which identified critical factors driving and processes\ngenerating sentencing outcomes in criminal cases; Second, employing the legal\nknowledge to annotate judicial documents according to the label system and\nannotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents\nthe most extensive and domain-specific legal element extraction dataset for the\nChinese legal system. Leveraging the annotated data, we employed various SOTA\nmodels that validates the applicability of LEEC for Document Event Extraction\n(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .",
        "translated": ""
    },
    {
        "title": "NewsRecLib: A PyTorch-Lightning Library for Neural News Recommendation",
        "url": "http://arxiv.org/abs/2310.01146v1",
        "pub_date": "2023-10-02",
        "summary": "NewsRecLib is an open-source library based on Pytorch-Lightning and Hydra\ndeveloped for training and evaluating neural news recommendation models. The\nforemost goals of NewsRecLib are to promote reproducible research and rigorous\nexperimental evaluation by (i) providing a unified and highly configurable\nframework for exhaustive experimental studies and (ii) enabling a thorough\nanalysis of the performance contribution of different model architecture\ncomponents and training regimes. NewsRecLib is highly modular, allows\nspecifying experiments in a single configuration file, and includes extensive\nlogging facilities. Moreover, NewsRecLib provides out-of-the-box\nimplementations of several prominent neural models, training methods, standard\nevaluation benchmarks, and evaluation metrics for news recommendation.",
        "translated": ""
    },
    {
        "title": "Dataset Condensation for Recommendation",
        "url": "http://arxiv.org/abs/2310.01038v1",
        "pub_date": "2023-10-02",
        "summary": "Training recommendation models on large datasets often requires significant\ntime and computational resources. Consequently, an emergent imperative has\narisen to construct informative, smaller-scale datasets for efficiently\ntraining. Dataset compression techniques explored in other domains show\npotential possibility to address this problem, via sampling a subset or\nsynthesizing a small dataset. However, applying existing approaches to condense\nrecommendation datasets is impractical due to following challenges: (i)\nsampling-based methods are inadequate in addressing the long-tailed\ndistribution problem; (ii) synthesizing-based methods are not applicable due to\ndiscreteness of interactions and large size of recommendation datasets; (iii)\nneither of them fail to address the specific issue in recommendation of false\nnegative items, where items with potential user interest are incorrectly\nsampled as negatives owing to insufficient exposure.\n  To bridge this gap, we investigate dataset condensation for recommendation,\nwhere discrete interactions are continualized with probabilistic\nre-parameterization. To avoid catastrophically expensive computations, we adopt\na one-step update strategy for inner model training and introducing policy\ngradient estimation for outer dataset synthesis. To mitigate amplification of\nlong-tailed problem, we compensate long-tailed users in the condensed dataset.\nFurthermore, we propose to utilize a proxy model to identify false negative\nitems. Theoretical analysis regarding the convergence property is provided.\nExtensive experiments on multiple datasets demonstrate the efficacy of our\nmethod. In particular, we reduce the dataset size by 75% while approximating\nover 98% of the original performance on Dianping and over 90% on other\ndatasets.",
        "translated": ""
    },
    {
        "title": "Organized Event Participant Prediction Enhanced by Social Media\n  Retweeting Data",
        "url": "http://arxiv.org/abs/2310.00896v1",
        "pub_date": "2023-10-02",
        "summary": "Nowadays, many platforms on the Web offer organized events, allowing users to\nbe organizers or participants. For such platforms, it is beneficial to predict\npotential event participants. Existing work on this problem tends to borrow\nrecommendation techniques. However, compared to e-commerce items and purchases,\nevents and participation are usually of a much smaller frequency, and the data\nmay be insufficient to learn an accurate model. In this paper, we propose to\nutilize social media retweeting activity data to enhance the learning of event\nparticipant prediction models. We create a joint knowledge graph to bridge the\nsocial media and the target domain, assuming that event descriptions and tweets\nare written in the same language. Furthermore, we propose a learning model that\nutilizes retweeting information for the target domain prediction more\neffectively. We conduct comprehensive experiments in two scenarios with\nreal-world data. In each scenario, we set up training data of different sizes,\nas well as warm and cold test cases. The evaluation results show that our\napproach consistently outperforms several baseline models, especially with the\nwarm test cases, and when target domain data is limited.",
        "translated": ""
    },
    {
        "title": "Retrieval meets Long Context Large Language Models",
        "url": "http://arxiv.org/abs/2310.03025v1",
        "pub_date": "2023-10-04",
        "summary": "Extending the context window of large language models (LLMs) is getting\npopular recently, while the solution of augmenting LLMs with retrieval has\nexisted for years. The natural questions are: i) Retrieval-augmentation versus\nlong context window, which one is better for downstream tasks? ii) Can both\nmethods be combined to get the best of both worlds? In this work, we answer\nthese questions by studying both solutions using two state-of-the-art\npretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps\nsurprisingly, we find that LLM with 4K context window using simple\nretrieval-augmentation at generation can achieve comparable performance to\nfinetuned LLM with 16K context window via positional interpolation on long\ncontext tasks, while taking much less computation. More importantly, we\ndemonstrate that retrieval can significantly improve the performance of LLMs\nregardless of their extended context window sizes. Our best model,\nretrieval-augmented LLaMA2-70B with 32K context window, outperforms\nGPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long\ncontext tasks including question answering and query-based summarization. It\nalso outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while\nbeing much faster at generation. Our study provides general insights on the\nchoice of retrieval-augmentation versus long context extension of LLM for\npractitioners.",
        "translated": ""
    },
    {
        "title": "Potential Factors Leading to Popularity Unfairness in Recommender\n  Systems: A User-Centered Analysis",
        "url": "http://arxiv.org/abs/2310.02961v1",
        "pub_date": "2023-10-04",
        "summary": "Popularity bias is a well-known issue in recommender systems where few\npopular items are over-represented in the input data, while majority of other\nless popular items are under-represented. This disparate representation often\nleads to bias in exposure given to the items in the recommendation results.\nExtensive research examined this bias from item perspective and attempted to\nmitigate it by enhancing the recommendation of less popular items. However, a\nrecent research has revealed the impact of this bias on users. Users with\ndifferent degree of tolerance toward popular items are not fairly served by the\nrecommendation system: users interested in less popular items receive more\npopular items in their recommendations, while users interested in popular items\nare recommended what they want. This is mainly due to the popularity bias that\npopular items are over-recommended. In this paper, we aim at investigating the\nfactors leading to this user-side unfairness of popularity bias in recommender\nsystems. In particular, we investigate two factors: 1) the relationship between\nthis unfairness and users' interest toward items' categories (e.g., movie\ngenres), 2) the relationship between this unfairness and the diversity of the\npopularity group in users' profile (the degree to which the user is interested\nin items with different degree of popularity). Experiments on a movie\nrecommendation dataset using multiple recommendation algorithms show that these\ntwo factors are significantly correlated with the degree of popularity\nunfairness in the recommendation results.",
        "translated": ""
    },
    {
        "title": "Auto-FP: An Experimental Study of Automated Feature Preprocessing for\n  Tabular Data",
        "url": "http://arxiv.org/abs/2310.02540v1",
        "pub_date": "2023-10-04",
        "summary": "Classical machine learning models, such as linear models and tree-based\nmodels, are widely used in industry. These models are sensitive to data\ndistribution, thus feature preprocessing, which transforms features from one\ndistribution to another, is a crucial step to ensure good model quality.\nManually constructing a feature preprocessing pipeline is challenging because\ndata scientists need to make difficult decisions about which preprocessors to\nselect and in which order to compose them. In this paper, we study how to\nautomate feature preprocessing (Auto-FP) for tabular data. Due to the large\nsearch space, a brute-force solution is prohibitively expensive. To address\nthis challenge, we interestingly observe that Auto-FP can be modelled as either\na hyperparameter optimization (HPO) or a neural architecture search (NAS)\nproblem. This observation enables us to extend a variety of HPO and NAS\nalgorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation\nand analysis of 15 algorithms on 45 public ML datasets. Overall,\nevolution-based algorithms show the leading average ranking. Surprisingly, the\nrandom search turns out to be a strong baseline. Many surrogate-model-based and\nbandit-based search algorithms, which achieve good performance for HPO and NAS,\ndo not outperform random search for Auto-FP. We analyze the reasons for our\nfindings and conduct a bottleneck analysis to identify the opportunities to\nimprove these algorithms. Furthermore, we explore how to extend Auto-FP to\nsupport parameter search and compare two ways to achieve this goal. In the end,\nwe evaluate Auto-FP in an AutoML context and discuss the limitations of popular\nAutoML tools. To the best of our knowledge, this is the first study on\nautomated feature preprocessing. We hope our work can inspire researchers to\ndevelop new algorithms tailored for Auto-FP.",
        "translated": ""
    },
    {
        "title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics\n  of Uncertainty and Prediction Error in Musical Improvisation",
        "url": "http://arxiv.org/abs/2310.02518v1",
        "pub_date": "2023-10-04",
        "summary": "Musical improvisation, much like spontaneous speech, reveals intricate facets\nof the improviser's state of mind and emotional character. However, the\nspecific musical components that reveal such individuality remain largely\nunexplored. Within the framework of brain's statistical learning and predictive\nprocessing, this study examined the temporal dynamics of uncertainty and\nsurprise (prediction error) in a piece of musical improvisation. This study\nemployed the HBSL model to analyze a corpus of 456 Jazz improvisations,\nspanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated\ndistinctive temporal patterns of surprise and uncertainty, especially in pitch\nand pitch-rhythm sequences, revealing era-specific features from the early 20th\nto the 21st centuries. Conversely, rhythm sequences exhibited a consistent\ndegree of uncertainty across eras. Further, the acoustic properties remain\nunchanged across different periods. These findings highlight the importance of\nhow temporal dynamics of surprise and uncertainty in improvisational music\nchange over periods, profoundly influencing the distinctive methodologies\nartists adopt for improvisation in each era. Further, it is suggested that the\ndevelopment of improvisational music can be attributed to the brain's adaptive\nstatistical learning mechanisms, which constantly refine internal models to\nmirror the cultural and emotional nuances of their respective epochs. This\nstudy unravels the evolutionary trajectory of improvisational music and\nhighlights the nuanced shifts artists employ to resonate with the cultural and\nemotional landscapes of their times.",
        "translated": ""
    },
    {
        "title": "Linear Recurrent Units for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2310.02367v1",
        "pub_date": "2023-10-03",
        "summary": "State-of-the-art sequential recommendation relies heavily on\nself-attention-based recommender models. Yet such models are computationally\nexpensive and often too slow for real-time recommendation. Furthermore, the\nself-attention operation is performed at a sequence-level, thereby making\nlow-cost incremental inference challenging. Inspired by recent advances in\nefficient language modeling, we propose linear recurrent units for sequential\nrecommendation (LRURec). Similar to recurrent neural networks, LRURec offers\nrapid inference and can achieve incremental inference on sequential inputs. By\ndecomposing the linear recurrence operation and designing recursive\nparallelization in our framework, LRURec provides the additional benefits of\nreduced model size and parallelizable training. Moreover, we optimize the\narchitecture of LRURec by implementing a series of modifications to address the\nlack of non-linearity and improve training dynamics. To validate the\neffectiveness of our proposed LRURec, we conduct extensive experiments on\nmultiple real-world datasets and compare its performance against\nstate-of-the-art sequential recommenders. Experimental results demonstrate the\neffectiveness of LRURec, which consistently outperforms baselines by a\nsignificant margin. Results also highlight the efficiency of LRURec with our\nparallelized training paradigm and fast inference on long sequences, showing\nits potential to further enhance user experience in sequential recommendation.",
        "translated": ""
    },
    {
        "title": "Beyond-Accuracy: A Review on Diversity, Serendipity and Fairness in\n  Recommender Systems Based on Graph Neural Networks",
        "url": "http://arxiv.org/abs/2310.02294v1",
        "pub_date": "2023-10-03",
        "summary": "By providing personalized suggestions to users, recommender systems have\nbecome essential to numerous online platforms. Collaborative filtering,\nparticularly graph-based approaches using Graph Neural Networks (GNNs), have\ndemonstrated great results in terms of recommendation accuracy. However,\naccuracy may not always be the most important criterion for evaluating\nrecommender systems' performance, since beyond-accuracy aspects such as\nrecommendation diversity, serendipity, and fairness can strongly influence user\nengagement and satisfaction. This review paper focuses on addressing these\ndimensions in GNN-based recommender systems, going beyond the conventional\naccuracy-centric perspective. We begin by reviewing recent developments in\napproaches that improve not only the accuracy-diversity trade-off but also\npromote serendipity and fairness in GNN-based recommender systems. We discuss\ndifferent stages of model development including data preprocessing, graph\nconstruction, embedding initialization, propagation layers, embedding fusion,\nscore computation, and training methodologies. Furthermore, we present a look\ninto the practical difficulties encountered in assuring diversity, serendipity,\nand fairness, while retaining high accuracy. Finally, we discuss potential\nfuture research directions for developing more robust GNN-based recommender\nsystems that go beyond the unidimensional perspective of focusing solely on\naccuracy. This review aims to provide researchers and practitioners with an\nin-depth understanding of the multifaceted issues that arise when designing\nGNN-based recommender systems, setting our work apart by offering a\ncomprehensive exploration of beyond-accuracy dimensions.",
        "translated": ""
    },
    {
        "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving\n  Pipelines",
        "url": "http://arxiv.org/abs/2310.03714v1",
        "pub_date": "2023-10-05",
        "summary": "The ML community is rapidly exploring techniques for prompting language\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\n\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward\na more systematic approach for developing and optimizing LM pipelines, we\nintroduce DSPy, a programming model that abstracts LM pipelines as text\ntransformation graphs, i.e. imperative computational graphs where LMs are\ninvoked through declarative modules. DSPy modules are parameterized, meaning\nthey can learn (by creating and collecting demonstrations) how to apply\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\nmetric. We conduct two case studies, showing that succinct DSPy programs can\nexpress and optimize sophisticated LM pipelines that reason about math word\nproblems, tackle multi-hop retrieval, answer complex questions, and control\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\nprompting (generally by over 25% and 65%, respectively) and pipelines with\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\nof that, DSPy programs compiled to open and relatively small LMs like\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\nhttps://github.com/stanfordnlp/dspy",
        "translated": ""
    },
    {
        "title": "FASER: Binary Code Similarity Search through the use of Intermediate\n  Representations",
        "url": "http://arxiv.org/abs/2310.03605v1",
        "pub_date": "2023-10-05",
        "summary": "Being able to identify functions of interest in cross-architecture software\nis useful whether you are analysing for malware, securing the software supply\nchain or conducting vulnerability research. Cross-Architecture Binary Code\nSimilarity Search has been explored in numerous studies and has used a wide\nrange of different data sources to achieve its goals. The data sources\ntypically used draw on common structures derived from binaries such as function\ncontrol flow graphs or binary level call graphs, the output of the disassembly\nprocess or the outputs of a dynamic analysis approach. One data source which\nhas received less attention is binary intermediate representations. Binary\nIntermediate representations possess two interesting properties: they are cross\narchitecture by their very nature and encode the semantics of a function\nexplicitly to support downstream usage. Within this paper we propose Function\nas a String Encoded Representation (FASER) which combines long document\ntransformers with the use of intermediate representations to create a model\ncapable of cross architecture function search without the need for manual\nfeature engineering, pre-training or a dynamic analysis step. We compare our\napproach against a series of baseline approaches for two tasks; A general\nfunction search task and a targeted vulnerability search task. Our approach\ndemonstrates strong performance across both tasks, performing better than all\nbaseline approaches.",
        "translated": ""
    },
    {
        "title": "TPDR: A Novel Two-Step Transformer-based Product and Class Description\n  Match and Retrieval Method",
        "url": "http://arxiv.org/abs/2310.03491v1",
        "pub_date": "2023-10-05",
        "summary": "There is a niche of companies responsible for intermediating the purchase of\nlarge batches of varied products for other companies, for which the main\nchallenge is to perform product description standardization, i.e., matching an\nitem described by a client with a product described in a catalog. The problem\nis complex since the client's product description may be: (1) potentially\nnoisy; (2) short and uninformative (e.g., missing information about model and\nsize); and (3) cross-language. In this paper, we formalize this problem as a\nranking task: given an initial client product specification (query), return the\nmost appropriate standardized descriptions (response). In this paper, we\npropose TPDR, a two-step Transformer-based Product and Class Description\nRetrieval method that is able to explore the semantic correspondence between IS\nand SD, by exploiting attention mechanisms and contrastive learning. First,\nTPDR employs the transformers as two encoders sharing the embedding vector\nspace: one for encoding the IS and another for the SD, in which corresponding\npairs (IS, SD) must be close in the vector space. Closeness is further enforced\nby a contrastive learning mechanism leveraging a specialized loss function.\nTPDR also exploits a (second) re-ranking step based on syntactic features that\nare very important for the exact matching (model, dimension) of certain\nproducts that may have been neglected by the transformers. To evaluate our\nproposal, we consider 11 datasets from a real company, covering different\napplication contexts. Our solution was able to retrieve the correct\nstandardized product before the 5th ranking position in 71% of the cases and\nits correct category in the first position in 80% of the situations. Moreover,\nthe effectiveness gains over purely syntactic or semantic baselines reach up to\n3.7 times, solving cases that none of the approaches in isolation can do by\nthemselves.",
        "translated": ""
    },
    {
        "title": "Personalized Transformer-based Ranking for e-Commerce at Yandex",
        "url": "http://arxiv.org/abs/2310.03481v1",
        "pub_date": "2023-10-05",
        "summary": "Personalizing the user experience with high-quality recommendations based on\nuser activities is vital for e-commerce platforms. This is particularly\nimportant in scenarios where the user's intent is not explicit, such as on the\nhomepage. Recently, personalized embedding-based systems have significantly\nimproved the quality of recommendations and search results in the e-commerce\ndomain. However, most of these works focus on enhancing the retrieval stage.\n  In this paper, we demonstrate that features produced by retrieval-focused\ndeep learning models are sub-optimal for ranking stage in e-commerce\nrecommendations. To address this issue, we propose a two-stage training process\nthat fine-tunes two-tower models to achieve optimal ranking performance. We\nprovide a detailed description of our transformer-based two-tower model\narchitecture, which is specifically designed for personalization in e-commerce.\n  Additionally, we introduce a novel technique for debiasing context in offline\nmodels and report significant improvements in ranking performance when using\nweb-search queries for e-commerce recommendations. Our model has been\nsuccessfully deployed at Yandex and has delivered strong performance in online\nA/B testing.",
        "translated": ""
    },
    {
        "title": "Amazon Books Rating prediction &amp; Recommendation Model",
        "url": "http://arxiv.org/abs/2310.03200v1",
        "pub_date": "2023-10-04",
        "summary": "This paper uses the dataset of Amazon to predict the books ratings listed on\nAmazon website. As part of this project, we predicted the ratings of the books,\nand also built a recommendation cluster. This recommendation cluster provides\nthe recommended books based on the column's values from dataset, for instance,\ncategory, description, author, price, reviews etc. This paper provides a flow\nof handling big data files, data engineering, building models and providing\npredictions. The models predict book ratings column using various PySpark\nMachine Learning APIs. Additionally, we used hyper-parameters and parameters\ntuning. Also, Cross Validation and TrainValidationSplit were used for\ngeneralization. Finally, we performed a comparison between Binary\nClassification and Multiclass Classification in their accuracies. We converted\nour label from multiclass to binary to see if we could find any difference\nbetween the two classifications. As a result, we found out that we get higher\naccuracy in binary classification than in multiclass classification.",
        "translated": ""
    },
    {
        "title": "Impedance Leakage Vulnerability and its Utilization in\n  Reverse-engineering Embedded Software",
        "url": "http://arxiv.org/abs/2310.03175v1",
        "pub_date": "2023-10-04",
        "summary": "Discovering new vulnerabilities and implementing security and privacy\nmeasures are important to protect systems and data against physical attacks.\nOne such vulnerability is impedance, an inherent property of a device that can\nbe exploited to leak information through an unintended side channel, thereby\nposing significant security and privacy risks. Unlike traditional\nvulnerabilities, impedance is often overlooked or narrowly explored, as it is\ntypically treated as a fixed value at a specific frequency in research and\ndesign endeavors. Moreover, impedance has never been explored as a source of\ninformation leakage. This paper demonstrates that the impedance of an embedded\ndevice is not constant and directly relates to the programs executed on the\ndevice. We define this phenomenon as impedance leakage and use this as a side\nchannel to extract software instructions from protected memory. Our experiment\non the ATmega328P microcontroller and the Artix 7 FPGA indicates that the\nimpedance side channel can detect software instructions with 96.1% and 92.6%\naccuracy, respectively. Furthermore, we explore the dual nature of the\nimpedance side channel, highlighting the potential for beneficial purposes and\nthe associated risk of intellectual property theft. Finally, potential\ncountermeasures that specifically address impedance leakage are discussed.",
        "translated": ""
    },
    {
        "title": "Policy-Gradient Training of Language Models for Ranking",
        "url": "http://arxiv.org/abs/2310.04407v1",
        "pub_date": "2023-10-06",
        "summary": "Text retrieval plays a crucial role in incorporating factual knowledge for\ndecision making into language processing pipelines, ranging from chat-based web\nsearch to question answering systems. Current state-of-the-art text retrieval\nmodels leverage pre-trained large language models (LLMs) to achieve competitive\nperformance, but training LLM-based retrievers via typical contrastive losses\nrequires intricate heuristics, including selecting hard negatives and using\nadditional supervision as learning signals. This reliance on heuristics stems\nfrom the fact that the contrastive loss itself is heuristic and does not\ndirectly optimize the downstream metrics of decision quality at the end of the\nprocessing pipeline. To address this issue, we introduce Neural PG-RANK, a\nnovel training algorithm that learns to rank by instantiating a LLM as a\nPlackett-Luce ranking policy. Neural PG-RANK provides a principled method for\nend-to-end training of retrieval models as part of larger decision systems via\npolicy gradient, with little reliance on complex heuristics, and it effectively\nunifies the training objective with downstream decision-making quality. We\nconduct extensive experiments on various text retrieval benchmarks. The results\ndemonstrate that when the training objective aligns with the evaluation setup,\nNeural PG-RANK yields remarkable in-domain performance improvement, with\nsubstantial out-of-domain generalization to some critical datasets employed in\ndownstream question answering tasks.",
        "translated": ""
    },
    {
        "title": "On the Embedding Collapse when Scaling up Recommendation Models",
        "url": "http://arxiv.org/abs/2310.04400v1",
        "pub_date": "2023-10-06",
        "summary": "Recent advances in deep foundation models have led to a promising trend of\ndeveloping large recommendation models to leverage vast amounts of available\ndata. However, we experiment to scale up existing recommendation models and\nobserve that the enlarged models do not improve satisfactorily. In this\ncontext, we investigate the embedding layers of enlarged models and identify a\nphenomenon of embedding collapse, which ultimately hinders scalability, wherein\nthe embedding matrix tends to reside in a low-dimensional subspace. Through\nempirical and theoretical analysis, we demonstrate that the feature interaction\nmodule specific to recommendation models has a two-sided effect. On the one\nhand, the interaction restricts embedding learning when interacting with\ncollapsed embeddings, exacerbating the collapse issue. On the other hand,\nfeature interaction is crucial in mitigating the fitting of spurious features,\nthereby improving scalability. Based on this analysis, we propose a simple yet\neffective multi-embedding design incorporating embedding-set-specific\ninteraction modules to capture diverse patterns and reduce collapse. Extensive\nexperiments demonstrate that this proposed design provides consistent\nscalability for various recommendation models.",
        "translated": ""
    },
    {
        "title": "Workload-aware and Learned Z-Indexes",
        "url": "http://arxiv.org/abs/2310.04268v1",
        "pub_date": "2023-10-06",
        "summary": "In this paper, a learned and workload-aware variant of a Z-index, which\njointly optimizes storage layout and search structures, as a viable solution\nfor the above challenges of spatial indexing. Specifically, we first formulate\na cost function to measure the performance of a Z-index on a dataset for a\nrange-query workload. Then, we optimize the Z-index structure by minimizing the\ncost function through adaptive partitioning and ordering for index\nconstruction. Moreover, we design a novel page-skipping mechanism to improve\nits query performance by reducing access to irrelevant data pages. Our\nextensive experiments show that our index improves range query time by 40% on\naverage over the baselines, while always performing better or comparably to\nstate-of-the-art spatial indexes. Additionally, our index maintains good point\nquery performance while providing favourable construction time and index size\ntradeoffs.",
        "translated": ""
    },
    {
        "title": "Lending Interaction Wings to Recommender Systems with Conversational\n  Agents",
        "url": "http://arxiv.org/abs/2310.04230v1",
        "pub_date": "2023-10-06",
        "summary": "Recommender systems trained on offline historical user behaviors are\nembracing conversational techniques to online query user preference. Unlike\nprior conversational recommendation approaches that systemically combine\nconversational and recommender parts through a reinforcement learning\nframework, we propose CORE, a new offline-training and online-checking paradigm\nthat bridges a COnversational agent and REcommender systems via a unified\nuncertainty minimization framework. It can benefit any recommendation platform\nin a plug-and-play style. Here, CORE treats a recommender system as an offline\nrelevance score estimator to produce an estimated relevance score for each\nitem; while a conversational agent is regarded as an online relevance score\nchecker to check these estimated scores in each session. We define uncertainty\nas the summation of unchecked relevance scores. In this regard, the\nconversational agent acts to minimize uncertainty via querying either\nattributes or items. Based on the uncertainty minimization framework, we derive\nthe expected certainty gain of querying each attribute and item, and develop a\nnovel online decision tree algorithm to decide what to query at each turn.\nExperimental results on 8 industrial datasets show that CORE could be\nseamlessly employed on 9 popular recommendation approaches. We further\ndemonstrate that our conversational agent could communicate as a human if\nempowered by a pre-trained large language model.",
        "translated": ""
    },
    {
        "title": "Keyword Augmented Retrieval: Novel framework for Information Retrieval\n  integrated with speech interface",
        "url": "http://arxiv.org/abs/2310.04205v1",
        "pub_date": "2023-10-06",
        "summary": "Retrieving answers in a quick and low cost manner without hallucinations from\na combination of structured and unstructured data using Language models is a\nmajor hurdle which prevents employment of Language models in knowledge\nretrieval automation. This becomes accentuated when one wants to integrate a\nspeech interface. Besides, for commercial search and chatbot applications,\ncomplete reliance on commercial large language models (LLMs) like GPT 3.5 etc.\ncan be very costly. In this work, authors have addressed this problem by first\ndeveloping a keyword based search framework which augments discovery of the\ncontext to be provided to the large language model. The keywords in turn are\ngenerated by LLM and cached for comparison with keywords generated by LLM\nagainst the query raised. This significantly reduces time and cost to find the\ncontext within documents. Once the context is set, LLM uses that to provide\nanswers based on a prompt tailored for Q&amp;A. This research work demonstrates\nthat use of keywords in context identification reduces the overall inference\ntime and cost of information retrieval. Given this reduction in inference time\nand cost with the keyword augmented retrieval framework, a speech based\ninterface for user input and response readout was integrated. This allowed a\nseamless interaction with the language model.",
        "translated": ""
    },
    {
        "title": "Searching COVID-19 clinical research using graphical abstracts",
        "url": "http://arxiv.org/abs/2310.04094v1",
        "pub_date": "2023-10-06",
        "summary": "Objective. Graphical abstracts are small graphs of concepts that visually\nsummarize the main findings of scientific articles. While graphical abstracts\nare customarily used in scientific publications to anticipate and summarize\ntheir main results, we propose them as a means for expressing graph searches\nover existing literature. Materials and methods. We consider the COVID-19 Open\nResearch Dataset (CORD-19), a corpus of more than one million abstracts; each\nof them is described as a graph of co-occurring ontological terms, selected\nfrom the Unified Medical Language System (UMLS) and the Ontology of Coronavirus\nInfectious Disease (CIDO). Graphical abstracts are also expressed as graphs of\nontological terms, possibly augmented by utility terms describing their\ninteractions (e.g., \"associated with\", \"increases\", \"induces\"). We build a\nco-occurrence network of concepts mentioned in the corpus; we then identify the\nbest matches of graphical abstracts on the network. We exploit graph database\ntechnology and shortest-path queries. Results. We build a large co-occurrence\nnetwork, consisting of 128,249 entities and 47,198,965 relationships. A\nwell-designed interface allows users to explore the network by formulating or\nadapting queries in the form of an abstract; it produces a bibliography of\npublications, globally ranked; each publication is further associated with the\nspecific parts of the abstract that it explains, thereby allowing the user to\nunderstand each aspect of the matching. Discussion and Conclusion. Our approach\nsupports the process of scientific hypothesis formulation and evidence search;\nit can be reapplied to any scientific domain, although our mastering of UMLS\nmakes it most suited to clinical domains.",
        "translated": ""
    },
    {
        "title": "AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term\n  User Engagement",
        "url": "http://arxiv.org/abs/2310.03984v1",
        "pub_date": "2023-10-06",
        "summary": "Growing attention has been paid to Reinforcement Learning (RL) algorithms\nwhen optimizing long-term user engagement in sequential recommendation tasks.\nOne challenge in large-scale online recommendation systems is the constant and\ncomplicated changes in users' behavior patterns, such as interaction rates and\nretention tendencies. When formulated as a Markov Decision Process (MDP), the\ndynamics and reward functions of the recommendation system are continuously\naffected by these changes. Existing RL algorithms for recommendation systems\nwill suffer from distribution shift and struggle to adapt in such an MDP. In\nthis paper, we introduce a novel paradigm called Adaptive Sequential\nRecommendation (AdaRec) to address this issue. AdaRec proposes a new\ndistance-based representation loss to extract latent information from users'\ninteraction trajectories. Such information reflects how RL policy fits to\ncurrent user behavior patterns, and helps the policy to identify subtle changes\nin the recommendation system. To make rapid adaptation to these changes, AdaRec\nencourages exploration with the idea of optimism under uncertainty. The\nexploration is further guarded by zero-order action optimization to ensure\nstable recommendation quality in complicated environments. We conduct extensive\nempirical analyses in both simulator-based and live sequential recommendation\ntasks, where AdaRec exhibits superior long-term performance compared to all\nbaseline algorithms.",
        "translated": ""
    },
    {
        "title": "An Efficient Content-based Time Series Retrieval System",
        "url": "http://arxiv.org/abs/2310.03919v1",
        "pub_date": "2023-10-05",
        "summary": "A Content-based Time Series Retrieval (CTSR) system is an information\nretrieval system for users to interact with time series emerged from multiple\ndomains, such as finance, healthcare, and manufacturing. For example, users\nseeking to learn more about the source of a time series can submit the time\nseries as a query to the CTSR system and retrieve a list of relevant time\nseries with associated metadata. By analyzing the retrieved metadata, users can\ngather more information about the source of the time series. Because the CTSR\nsystem is required to work with time series data from diverse domains, it needs\na high-capacity model to effectively measure the similarity between different\ntime series. On top of that, the model within the CTSR system has to compute\nthe similarity scores in an efficient manner as the users interact with the\nsystem in real-time. In this paper, we propose an effective and efficient CTSR\nmodel that outperforms alternative models, while still providing reasonable\ninference runtimes. To demonstrate the capability of the proposed method in\nsolving business problems, we compare it against alternative models using our\nin-house transaction data. Our findings reveal that the proposed model is the\nmost suitable solution compared to others for our transaction data problem.",
        "translated": ""
    },
    {
        "title": "Living Lab Evaluation for Life and Social Sciences Search Platforms --\n  LiLAS at CLEF 2021",
        "url": "http://arxiv.org/abs/2310.03859v1",
        "pub_date": "2023-10-05",
        "summary": "Meta-evaluation studies of system performances in controlled offline\nevaluation campaigns, like TREC and CLEF, show a need for innovation in\nevaluating IR-systems. The field of academic search is no exception to this.\nThis might be related to the fact that relevance in academic search is\nmultilayered and therefore the aspect of user-centric evaluation is becoming\nmore and more important. The Living Labs for Academic Search (LiLAS) lab aims\nto strengthen the concept of user-centric living labs for the domain of\nacademic search by allowing participants to evaluate their retrieval approaches\nin two real-world academic search systems from the life sciences and the social\nsciences. To this end, we provide participants with metadata on the systems'\ncontent as well as candidate lists with the task to rank the most relevant\ncandidate to the top. Using the STELLA-infrastructure, we allow participants to\neasily integrate their approaches into the real-world systems and provide the\npossibility to compare different approaches at the same time.",
        "translated": ""
    },
    {
        "title": "Accurate Cold-start Bundle Recommendation via Popularity-based\n  Coalescence and Curriculum Heating",
        "url": "http://arxiv.org/abs/2310.03813v1",
        "pub_date": "2023-10-05",
        "summary": "How can we accurately recommend cold-start bundles to users? The cold-start\nproblem in bundle recommendation is critical in practical scenarios since new\nbundles are continuously created for various marketing purposes. Despite its\nimportance, no previous studies have addressed cold-start bundle\nrecommendation. Moreover, existing methods for cold-start item recommendation\noverly rely on historical information, even for unpopular bundles, failing to\ntackle the primary challenge of the highly skewed distribution of bundle\ninteractions. In this work, we propose CoHeat (Popularity-based Coalescence and\nCurriculum Heating), an accurate approach for the cold-start bundle\nrecommendation. CoHeat tackles the highly skewed distribution of bundle\ninteractions by incorporating both historical and affiliation information based\non the bundle's popularity when estimating the user-bundle relationship.\nFurthermore, CoHeat effectively learns latent representations by exploiting\ncurriculum learning and contrastive learning. CoHeat demonstrates superior\nperformance in cold-start bundle recommendation, achieving up to 193% higher\nnDCG@20 compared to the best competitor.",
        "translated": ""
    },
    {
        "title": "Sequential Tag Recommendation",
        "url": "http://arxiv.org/abs/2310.05423v1",
        "pub_date": "2023-10-09",
        "summary": "With the development of Internet technology and the expansion of social\nnetworks, online platforms have become an important way for people to obtain\ninformation. The introduction of tags facilitates information categorization\nand retrieval. Meanwhile, the development of tag recommendation systems not\nonly enables users to input tags more efficiently, but also improves the\nquality of tags. However, current tag recommendation methods only consider the\ncontent of the current post and do not take into account the influence of user\npreferences. Since the main body of tag recommendation is the user, it is very\nnecessary to obtain the user's tagging habits. Therefore, this paper proposes a\ntag recommendation algorithm (MLP4STR) based on the dynamic preference of\nuser's behavioral sequence, which models the user's historical post information\nand historical tag information to obtain the user's dynamic interest changes. A\npure MLP structure across feature dimensions is used in sequence modeling to\nmodel the interaction between tag content and post content to fully extract the\nuser's interests. Finally tag recommendation is performed.",
        "translated": ""
    },
    {
        "title": "Augmented Embeddings for Custom Retrievals",
        "url": "http://arxiv.org/abs/2310.05380v1",
        "pub_date": "2023-10-09",
        "summary": "Information retrieval involves selecting artifacts from a corpus that are\nmost relevant to a given search query. The flavor of retrieval typically used\nin classical applications can be termed as homogeneous and relaxed, where\nqueries and corpus elements are both natural language (NL) utterances\n(homogeneous) and the goal is to pick most relevant elements from the corpus in\nthe Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed).\nRecently, retrieval is being used extensively in preparing prompts for large\nlanguage models (LLMs) to enable LLMs to perform targeted tasks. These new\napplications of retrieval are often heterogeneous and strict -- the queries and\nthe corpus contain different kinds of entities, such as NL and code, and there\nis a need for improving retrieval at Top-K for small values of K, such as K=1\nor 3 or 5. Current dense retrieval techniques based on pretrained embeddings\nprovide a general-purpose and powerful approach for retrieval, but they are\noblivious to task-specific notions of similarity of heterogeneous artifacts. We\nintroduce Adapted Dense Retrieval, a mechanism to transform embeddings to\nenable improved task-specific, heterogeneous and strict retrieval. Adapted\nDense Retrieval works by learning a low-rank residual adaptation of the\npretrained black-box embedding. We empirically validate our approach by showing\nimprovements over the state-of-the-art general-purpose embeddings-based\nbaseline.",
        "translated": ""
    },
    {
        "title": "A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and\n  Locations in the Healthcare Domain",
        "url": "http://arxiv.org/abs/2310.05258v1",
        "pub_date": "2023-10-08",
        "summary": "Efficiently finding doctors and locations is an important search problem for\npatients in the healthcare domain, for which traditional information retrieval\nmethods tend not to work optimally. In the last ten years, knowledge graphs\n(KGs) have emerged as a powerful way to combine the benefits of gleaning\ninsights from semi-structured data using semantic modeling, natural language\nprocessing techniques like information extraction, and robust querying using\nstructured query languages like SPARQL and Cypher. In this short paper, we\npresent a KG-based search engine architecture for robustly finding doctors and\nlocations in the healthcare domain. Early results demonstrate that our approach\ncan lead to significantly higher coverage for complex queries without degrading\nquality.",
        "translated": ""
    },
    {
        "title": "GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient\n  Partially Relevant Video Retrieval",
        "url": "http://arxiv.org/abs/2310.05195v1",
        "pub_date": "2023-10-08",
        "summary": "Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a \\textbf{G}aussian-\\textbf{M}ixture-\\textbf{M}odel based\nTrans\\textbf{former} which models clip representations implicitly. During frame\ninteractions, we incorporate Gaussian-Mixture-Model constraints to focus each\nframe on its adjacent frames instead of the whole video. Then generated\nrepresentations will contain multi-scale clip information, achieving implicit\nclip modeling. In addition, PRVR methods ignore semantic differences between\ntext queries relevant to the same video, leading to a sparse embedding space.\nWe propose a query diverse loss to distinguish these text queries, making the\nembedding space more intensive and contain more semantic information. Extensive\nexperiments on three large-scale video datasets (\\ie, TVR, ActivityNet\nCaptions, and Charades-STA) demonstrate the superiority and efficiency of\nGMMFormer.",
        "translated": ""
    },
    {
        "title": "From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for\n  Conversational Exploratory Search",
        "url": "http://arxiv.org/abs/2310.05150v1",
        "pub_date": "2023-10-08",
        "summary": "Exploratory search is an open-ended information retrieval process that aims\nat discovering knowledge about a topic or domain rather than searching for a\nspecific answer or piece of information. Conversational interfaces are\nparticularly suitable for supporting exploratory search, allowing users to\nrefine queries and examine search results through interactive dialogues. In\naddition to conversational search interfaces, knowledge graphs are also useful\nin supporting information exploration due to their rich semantic representation\nof data items. In this study, we demonstrate the synergistic effects of\ncombining knowledge graphs and conversational interfaces for exploratory\nsearch, bridging the gap between structured and unstructured information\nretrieval. To this end, we propose a knowledge-driven dialogue system for\nexploring news articles by asking natural language questions and using the\ngraph structure to navigate between related topics. Based on a user study with\n54 participants, we empirically evaluate the effectiveness of the graph-based\nexploratory search and discuss design implications for developing such systems.",
        "translated": ""
    },
    {
        "title": "CARLG: Leveraging Contextual Clues and Role Correlations for Improving\n  Document-level Event Argument Extraction",
        "url": "http://arxiv.org/abs/2310.05116v1",
        "pub_date": "2023-10-08",
        "summary": "Document-level event argument extraction (EAE) is a crucial but challenging\nsubtask in information extraction. Most existing approaches focus on the\ninteraction between arguments and event triggers, ignoring two critical points:\nthe information of contextual clues and the semantic correlations among\nargument roles. In this paper, we propose the CARLG model, which consists of\ntwo modules: Contextual Clues Aggregation (CCA) and Role-based Latent\nInformation Guidance (RLIG), effectively leveraging contextual clues and role\ncorrelations for improving document-level EAE. The CCA module adaptively\ncaptures and integrates contextual clues by utilizing context attention weights\nfrom a pre-trained encoder. The RLIG module captures semantic correlations\nthrough role-interactive encoding and provides valuable information guidance\nwith latent role representation. Notably, our CCA and RLIG modules are compact,\ntransplantable and efficient, which introduce no more than 1% new parameters\nand can be easily equipped on other span-base methods with significant\nperformance boost. Extensive experiments on the RAMS, WikiEvents, and MLEE\ndatasets demonstrate the superiority of the proposed CARLG model. It\noutperforms previous state-of-the-art approaches by 1.26 F1, 1.22 F1, and 1.98\nF1, respectively, while reducing the inference time by 31%. Furthermore, we\nprovide detailed experimental analyses based on the performance gains and\nillustrate the interpretability of our model.",
        "translated": ""
    },
    {
        "title": "A framework to generate sparsity-inducing regularizers for enhanced\n  low-rank matrix completion",
        "url": "http://arxiv.org/abs/2310.04954v1",
        "pub_date": "2023-10-08",
        "summary": "Applying half-quadratic optimization to loss functions can yield the\ncorresponding regularizers, while these regularizers are usually not\nsparsity-inducing regularizers (SIRs). To solve this problem, we devise a\nframework to generate an SIR with closed-form proximity operator. Besides, we\nspecify our framework using several commonly-used loss functions, and produce\nthe corresponding SIRs, which are then adopted as nonconvex rank surrogates for\nlow-rank matrix completion. Furthermore, algorithms based on the alternating\ndirection method of multipliers are developed. Extensive numerical results show\nthe effectiveness of our methods in terms of recovery performance and runtime.",
        "translated": ""
    },
    {
        "title": "Commercialized Generative AI: A Critical Study of the Feasibility and\n  Ethics of Generating Native Advertising Using Large Language Models in\n  Conversational Web Search",
        "url": "http://arxiv.org/abs/2310.04892v1",
        "pub_date": "2023-10-07",
        "summary": "How will generative AI pay for itself? Unless charging users for access,\nselling advertising is the only alternative. Especially in the multi-billion\ndollar web search market with ads as the main source of revenue, the\nintroduction of a subscription model seems unlikely. The recent disruption of\nsearch by generative large language models could thus ultimately be accompanied\nby generated ads. Our concern is that the commercialization of generative AI in\ngeneral and large language models in particular could lead to native\nadvertising in the form of quite subtle brand or product placements. In web\nsearch, the evolution of search engine results pages (SERPs) from traditional\nlists of ``ten blue links'' (lists SERPs) to generated text with web page\nreferences (text SERPs) may further blur the line between advertising-based and\norganic search results, making it difficult for users to distinguish between\nthe two, depending on how advertising is integrated and disclosed. To raise\nawareness of this potential development, we conduct a pilot study analyzing the\ncapabilities of current large language models to blend ads with organic search\nresults. Although the models still struggle to subtly frame ads in an unrelated\ncontext, their potential is evident when integrating ads into related topics\nwhich calls for further investigation.",
        "translated": ""
    },
    {
        "title": "Hybrid Recommendation System using Graph Neural Network and BERT\n  Embeddings",
        "url": "http://arxiv.org/abs/2310.04878v1",
        "pub_date": "2023-10-07",
        "summary": "Recommender systems have emerged as a crucial component of the modern web\necosystem. The effectiveness and accuracy of such systems are critical for\nproviding users with personalized recommendations that meet their specific\ninterests and needs. In this paper, we introduce a novel model that utilizes a\nGraph Neural Network (GNN) in conjunction with sentence transformer embeddings\nto predict anime recommendations for different users. Our model employs the\ntask of link prediction to create a recommendation system that considers both\nthe features of anime and user interactions with different anime. The\nhybridization of the GNN and transformer embeddings enables us to capture both\ninter-level and intra-level features of anime data.Our model not only\nrecommends anime to users but also predicts the rating a specific user would\ngive to an anime. We utilize the GraphSAGE network for model building and\nweighted root mean square error (RMSE) to evaluate the performance of the\nmodel. Our approach has the potential to significantly enhance the accuracy and\neffectiveness of anime recommendation systems and can be extended to other\ndomains that require personalized recommendations.",
        "translated": ""
    },
    {
        "title": "ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding",
        "url": "http://arxiv.org/abs/2310.04865v1",
        "pub_date": "2023-10-07",
        "summary": "Developing text mining approaches to mine aspects from customer reviews has\nbeen well-studied due to its importance in understanding customer needs and\nproduct attributes. In contrast, it remains unclear how to predict the future\nemerging aspects of a new product that currently has little review information.\nThis task, which we named product aspect forecasting, is critical for\nrecommending new products, but also challenging because of the missing reviews.\nHere, we propose ForeSeer, a novel textual mining and product embedding\napproach progressively trained on temporal product graphs for this novel\nproduct aspect forecasting task. ForeSeer transfers reviews from similar\nproducts on a large product graph and exploits these reviews to predict aspects\nthat might emerge in future reviews. A key novelty of our method is to jointly\nprovide review, product, and aspect embeddings that are both time-sensitive and\nless affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer\non a real-world product review system containing 11,536,382 reviews and 11,000\nproducts over 3 years. We observe that ForeSeer substantially outperformed\nexisting approaches with at least 49.1\\% AUPRC improvement under the real\nsetting where aspect associations are not given. ForeSeer further improves\nfuture link prediction on the product graph and the review aspect association\nprediction. Collectively, Foreseer offers a novel framework for review\nforecasting by effectively integrating review text, product network, and\ntemporal information, opening up new avenues for online shopping recommendation\nand e-commerce applications.",
        "translated": ""
    },
    {
        "title": "Efficient Retrieval of Images with Irregular Patterns using\n  Morphological Image Analysis: Applications to Industrial and Healthcare\n  datasets",
        "url": "http://arxiv.org/abs/2310.06566v1",
        "pub_date": "2023-10-10",
        "summary": "Image retrieval is the process of searching and retrieving images from a\ndatabase based on their visual content and features. Recently, much attention\nhas been directed towards the retrieval of irregular patterns within industrial\nor medical images by extracting features from the images, such as deep\nfeatures, colour-based features, shape-based features and local features. This\nhas applications across a spectrum of industries, including fault inspection,\ndisease diagnosis, and maintenance prediction. This paper proposes an image\nretrieval framework to search for images containing similar irregular patterns\nby extracting a set of morphological features (DefChars) from images; the\ndatasets employed in this paper contain wind turbine blade images with defects,\nchest computerised tomography scans with COVID-19 infection, heatsink images\nwith defects, and lake ice images. The proposed framework was evaluated with\ndifferent feature extraction methods (DefChars, resized raw image, local binary\npattern, and scale-invariant feature transforms) and distance metrics to\ndetermine the most efficient parameters in terms of retrieval performance\nacross datasets. The retrieval results show that the proposed framework using\nthe DefChars and the Manhattan distance metric achieves a mean average\nprecision of 80% and a low standard deviation of 0.09 across classes of\nirregular patterns, outperforming alternative feature-metric combinations\nacross all datasets. Furthermore, the low standard deviation between each class\nhighlights DefChars' capability for a reliable image retrieval task, even in\nthe presence of class imbalances or small-sized datasets.",
        "translated": ""
    },
    {
        "title": "A Multi-facet Paradigm to Bridge Large Language Model and Recommendation",
        "url": "http://arxiv.org/abs/2310.06491v1",
        "pub_date": "2023-10-10",
        "summary": "Large Language Models (LLMs) have garnered considerable attention in\nrecommender systems. To achieve LLM-based recommendation, item indexing and\ngeneration grounding are two essential steps, bridging between recommendation\nitems and natural language. Item indexing assigns a unique identifier to\nrepresent each item in natural language, and generation grounding grounds the\ngenerated token sequences to in-corpus items. However, previous works suffer\nfrom inherent limitations in the two steps. For item indexing, existing\nID-based identifiers (e.g., numeric IDs) and description-based identifiers\n(e.g., titles) often compromise semantic richness or uniqueness. Moreover,\ngeneration grounding might inadvertently produce out-of-corpus identifiers.\nWorse still, autoregressive generation heavily relies on the initial token's\nquality. To combat these issues, we propose a novel multi-facet paradigm,\nnamely TransRec, to bridge the LLMs to recommendation. Specifically, TransRec\nemploys multi-facet identifiers that incorporate ID, title, and attribute,\nachieving both distinctiveness and semantics. Additionally, we introduce a\nspecialized data structure for TransRec to guarantee the in-corpus identifier\ngeneration and adopt substring indexing to encourage LLMs to generate from any\nposition. We implement TransRec on two backbone LLMs, i.e., BART-large and\nLLaMA-7B. Empirical results on three real-world datasets under diverse settings\n(e.g., full training and few-shot training with warm- and cold-start testings)\nattest to the superiority of TransRec.",
        "translated": ""
    },
    {
        "title": "Topological RANSAC for instance verification and retrieval without\n  fine-tuning",
        "url": "http://arxiv.org/abs/2310.06486v1",
        "pub_date": "2023-10-10",
        "summary": "This paper presents an innovative approach to enhancing explainable image\nretrieval, particularly in situations where a fine-tuning set is unavailable.\nThe widely-used SPatial verification (SP) method, despite its efficacy, relies\non a spatial model and the hypothesis-testing strategy for instance\nrecognition, leading to inherent limitations, including the assumption of\nplanar structures and neglect of topological relations among features. To\naddress these shortcomings, we introduce a pioneering technique that replaces\nthe spatial model with a topological one within the RANSAC process. We propose\nbio-inspired saccade and fovea functions to verify the topological consistency\namong features, effectively circumventing the issues associated with SP's\nspatial model. Our experimental results demonstrate that our method\nsignificantly outperforms SP, achieving state-of-the-art performance in\nnon-fine-tuning retrieval. Furthermore, our approach can enhance performance\nwhen used in conjunction with fine-tuned features. Importantly, our method\nretains high explainability and is lightweight, offering a practical and\nadaptable solution for a variety of real-world applications.",
        "translated": ""
    },
    {
        "title": "Query-dominant User Interest Network for Large-Scale Search Ranking",
        "url": "http://arxiv.org/abs/2310.06444v1",
        "pub_date": "2023-10-10",
        "summary": "Historical behaviors have shown great effect and potential in various\nprediction tasks, including recommendation and information retrieval. The\noverall historical behaviors are various but noisy while search behaviors are\nalways sparse. Most existing approaches in personalized search ranking adopt\nthe sparse search behaviors to learn representation with bottleneck, which do\nnot sufficiently exploit the crucial long-term interest. In fact, there is no\ndoubt that user long-term interest is various but noisy for instant search, and\nhow to exploit it well still remains an open problem.\n  To tackle this problem, in this work, we propose a novel model named\nQuery-dominant user Interest Network (QIN), including two cascade units to\nfilter the raw user behaviors and reweigh the behavior subsequences.\nSpecifically, we propose a relevance search unit (RSU), which aims to search a\nsubsequence relevant to the query first and then search the sub-subsequences\nrelevant to the target item. These items are then fed into an attention unit\ncalled Fused Attention Unit (FAU). It should be able to calculate attention\nscores from the ID field and attribute field separately, and then adaptively\nfuse the item embedding and content embedding based on the user engagement of\npast period. Extensive experiments and ablation studies on real-world datasets\ndemonstrate the superiority of our model over state-of-the-art methods. The QIN\nnow has been successfully deployed on Kuaishou search, an online video search\nplatform, and obtained 7.6% improvement on CTR.",
        "translated": ""
    },
    {
        "title": "Harnessing Administrative Data Inventories to Create a Reliable\n  Transnational Reference Database for Crop Type Monitoring",
        "url": "http://arxiv.org/abs/2310.06393v1",
        "pub_date": "2023-10-10",
        "summary": "With leaps in machine learning techniques and their applicationon Earth\nobservation challenges has unlocked unprecedented performance across the\ndomain. While the further development of these methods was previously limited\nby the availability and volume of sensor data and computing resources, the lack\nof adequate reference data is now constituting new bottlenecks. Since creating\nsuch ground-truth information is an expensive and error-prone task, new ways\nmust be devised to source reliable, high-quality reference data on large\nscales. As an example, we showcase E URO C ROPS, a reference dataset for crop\ntype classification that aggregates and harmonizes administrative data surveyed\nin different countries with the goal of transnational interoperability.",
        "translated": ""
    },
    {
        "title": "P5: Plug-and-Play Persona Prompting for Personalized Response Selection",
        "url": "http://arxiv.org/abs/2310.06390v1",
        "pub_date": "2023-10-10",
        "summary": "The use of persona-grounded retrieval-based chatbots is crucial for\npersonalized conversations, but there are several challenges that need to be\naddressed. 1) In general, collecting persona-grounded corpus is very expensive.\n2) The chatbot system does not always respond in consideration of persona at\nreal applications. To address these challenges, we propose a plug-and-play\npersona prompting method. Our system can function as a standard open-domain\nchatbot if persona information is not available. We demonstrate that this\napproach performs well in the zero-shot setting, which reduces the dependence\non persona-ground training data. This makes it easier to expand the system to\nother languages without the need to build a persona-grounded corpus.\nAdditionally, our model can be fine-tuned for even better performance. In our\nexperiments, the zero-shot model improved the standard model by 7.71 and 1.04\npoints in the original persona and revised persona, respectively. The\nfine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39\npoints in the original persona and revised persona, respectively. To the best\nof our knowledge, this is the first attempt to solve the problem of\npersonalized response selection using prompt sequences. Our code is available\non github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.",
        "translated": ""
    },
    {
        "title": "MuseChat: A Conversational Music Recommendation System for Videos",
        "url": "http://arxiv.org/abs/2310.06282v1",
        "pub_date": "2023-10-10",
        "summary": "We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.",
        "translated": ""
    },
    {
        "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
        "url": "http://arxiv.org/abs/2310.07713v1",
        "pub_date": "2023-10-11",
        "summary": "Pretraining auto-regressive large language models (LLMs) with retrieval\ndemonstrates better perplexity and factual accuracy by leveraging external\ndatabases. However, the size of existing pretrained retrieval-augmented LLM is\nstill limited (e.g., Retro has 7.5B parameters), which limits the effectiveness\nof instruction tuning and zero-shot generalization. In this work, we introduce\nRetro 48B, the largest LLM pretrained with retrieval before instruction tuning.\nSpecifically, we continue to pretrain the 43B GPT model on additional 100\nbillion tokens using the Retro augmentation method by retrieving from 1.2\ntrillion tokens. The obtained foundation model, Retro 48B, largely outperforms\nthe original 43B GPT in terms of perplexity. After instruction tuning on Retro,\nInstructRetro demonstrates significant improvement over the instruction tuned\nGPT on zero-shot question answering (QA) tasks. Specifically, the average\nimprovement of InstructRetro is 7% over its GPT counterpart across 8 short-form\nQA tasks, and 10% over GPT across 4 challenging long-form QA tasks.\nSurprisingly, we find that one can ablate the encoder from InstructRetro\narchitecture and directly use its decoder backbone, while achieving comparable\nresults. We hypothesize that pretraining with retrieval makes its decoder good\nat incorporating context for QA. Our results highlights the promising direction\nto obtain a better GPT decoder for QA through continued pretraining with\nretrieval before instruction tuning.",
        "translated": ""
    },
    {
        "title": "Retrieve Anything To Augment Large Language Models",
        "url": "http://arxiv.org/abs/2310.07554v1",
        "pub_date": "2023-10-11",
        "summary": "Large language models (LLMs) face significant challenges stemming from the\ninherent limitations in knowledge, memory, alignment, and action. These\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\nfrom the external world, such as knowledge base, memory store, demonstration\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\nbridging the gap between LLMs and the external assistance. However,\nconventional methods encounter two pressing issues. On one hand, the\ngeneral-purpose retrievers are not properly optimized for the retrieval\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\nrequired versatility, hindering their performance across the diverse retrieval\naugmentation scenarios.\n  In this work, we present a novel approach, the LLM Embedder, which\ncomprehensively support the diverse needs of LLMs' retrieval augmentation with\none unified embedding model. Training such an unified model is non-trivial, as\nvarious retrieval tasks aim to capture distinct semantic relationships, often\nsubject to mutual interference. To address this challenge, we systematically\noptimize our training methodology. This includes reward formulation based on\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\nfine-tuning with explicit instructions, and the use of homogeneous in-batch\nnegative sampling. These optimization strategies contribute to the outstanding\nempirical performance of the LLM-Embedder. Notably, it yields remarkable\nenhancements in retrieval augmentation for LLMs, surpassing both\ngeneral-purpose and task-specific retrievers in various evaluation scenarios.\nThis project is made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.",
        "translated": ""
    },
    {
        "title": "GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized\n  Adaptive Testing",
        "url": "http://arxiv.org/abs/2310.07477v1",
        "pub_date": "2023-10-11",
        "summary": "Computerized Adaptive Testing(CAT) refers to an online system that adaptively\nselects the best-suited question for students with various abilities based on\ntheir historical response records. Most CAT methods only focus on the quality\nobjective of predicting the student ability accurately, but neglect concept\ndiversity or question exposure control, which are important considerations in\nensuring the performance and validity of CAT. Besides, the students' response\nrecords contain valuable relational information between questions and knowledge\nconcepts. The previous methods ignore this relational information, resulting in\nthe selection of sub-optimal test questions. To address these challenges, we\npropose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly,\nthree objectives, namely quality, diversity and novelty, are introduced into\nthe Scalarized Multi-Objective Reinforcement Learning framework of CAT, which\nrespectively correspond to improving the prediction accuracy, increasing the\nconcept diversity and reducing the question exposure. We use an Actor-Critic\nRecommender to select questions and optimize three objectives simultaneously by\nthe scalarization function. Secondly, we utilize the graph neural network to\nlearn relation-aware embeddings of questions and concepts. These embeddings are\nable to aggregate neighborhood information in the relation graphs between\nquestions and concepts. We conduct experiments on three real-world educational\ndatasets, and show that GMOCAT not only outperforms the state-of-the-art\nmethods in the ability prediction, but also achieve superior performance in\nimproving the concept diversity and alleviating the question exposure. Our code\nis available at https://github.com/justarter/GMOCAT.",
        "translated": ""
    },
    {
        "title": "Preliminary Results of a Scientometric Analysis of the German\n  Information Retrieval Community 2020-2023",
        "url": "http://arxiv.org/abs/2310.07346v1",
        "pub_date": "2023-10-11",
        "summary": "The German Information Retrieval community is located in two different\nsub-fields: Information and computer science. There are no current studies that\ninvestigate these communities on a scientometric level. Available studies only\nfocus on the information scientific part of the community. We generated a data\nset of 401 recent IR-related publications extracted from six core IR\nconferences from a mainly computer scientific background. We analyze this data\nset at the institutional and researcher level. The data set is publicly\nreleased, and we also demonstrate a mapping use case.",
        "translated": ""
    },
    {
        "title": "A Completely Locale-independent Session-based Recommender System by\n  Leveraging Trained Model",
        "url": "http://arxiv.org/abs/2310.07281v1",
        "pub_date": "2023-10-11",
        "summary": "In this paper, we propose a solution that won the 10th prize in the KDD Cup\n2023 Challenge Task 2 (Next Product Recommendation for Underrepresented\nLanguages/Locales). Our approach involves two steps: (i) Identify candidate\nitem sets based on co-visitation, and (ii) Re-ranking the items using LightGBM\nwith locale-independent features, including session-based features and product\nsimilarity. The experiment demonstrated that the locale-independent model\nperformed consistently well across different test locales, and performed even\nbetter when incorporating data from other locales into the training.",
        "translated": ""
    },
    {
        "title": "Validating Synthetic Usage Data in Living Lab Environments",
        "url": "http://arxiv.org/abs/2310.07142v1",
        "pub_date": "2023-10-11",
        "summary": "Evaluating retrieval performance without editorial relevance judgments is\nchallenging, but instead, user interactions can be used as relevance signals.\nLiving labs offer a way for small-scale platforms to validate information\nretrieval systems with real users. If enough user interaction data are\navailable, click models can be parameterized from historical sessions to\nevaluate systems before exposing users to experimental rankings. However,\ninteraction data are sparse in living labs, and little is studied about how\nclick models can be validated for reliable user simulations when click data are\navailable in moderate amounts.\n  This work introduces an evaluation approach for validating synthetic usage\ndata generated by click models in data-sparse human-in-the-loop environments\nlike living labs. We ground our methodology on the click model's estimates\nabout a system ranking compared to a reference ranking for which the relative\nperformance is known. Our experiments compare different click models and their\nreliability and robustness as more session log data becomes available. In our\nsetup, simple click models can reliably determine the relative system\nperformance with already 20 logged sessions for 50 queries. In contrast, more\ncomplex click models require more session data for reliable estimates, but they\nare a better choice in simulated interleaving experiments when enough session\ndata are available. While it is easier for click models to distinguish between\nmore diverse systems, it is harder to reproduce the system ranking based on the\nsame retrieval algorithm with different interpolation weights. Our setup is\nentirely open, and we share the code to reproduce the experiments.",
        "translated": ""
    },
    {
        "title": "AE-smnsMLC: Multi-Label Classification with Semantic Matching and\n  Negative Label Sampling for Product Attribute Value Extraction",
        "url": "http://arxiv.org/abs/2310.07137v1",
        "pub_date": "2023-10-11",
        "summary": "Product attribute value extraction plays an important role for many\nreal-world applications in e-Commerce such as product search and\nrecommendation. Previous methods treat it as a sequence labeling task that\nneeds more annotation for position of values in the product text. This limits\ntheir application to real-world scenario in which only attribute values are\nweakly-annotated for each product without their position. Moreover, these\nmethods only use product text (i.e., product title and description) and do not\nconsider the semantic connection between the multiple attribute values of a\ngiven product and its text, which can help attribute value extraction. In this\npaper, we reformulate this task as a multi-label classification task that can\nbe applied for real-world scenario in which only annotation of attribute values\nis available to train models (i.e., annotation of positional information of\nattribute values is not available). We propose a classification model with\nsemantic matching and negative label sampling for attribute value extraction.\nSemantic matching aims to capture semantic interactions between attribute\nvalues of a given product and its text. Negative label sampling aims to enhance\nthe model's ability of distinguishing similar values belonging to the same\nattribute. Experimental results on three subsets of a large real-world\ne-Commerce dataset demonstrate the effectiveness and superiority of our\nproposed model.",
        "translated": ""
    },
    {
        "title": "Answer Candidate Type Selection: Text-to-Text Language Model for Closed\n  Book Question Answering Meets Knowledge Graphs",
        "url": "http://arxiv.org/abs/2310.07008v1",
        "pub_date": "2023-10-10",
        "summary": "Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield\npromising results in the Knowledge Graph Question Answering (KGQA) task.\nHowever, the capacity of the models is limited and the quality decreases for\nquestions with less popular entities. In this paper, we present a novel\napproach which works on top of the pre-trained Text-to-Text QA system to\naddress this issue. Our simple yet effective method performs filtering and\nre-ranking of generated candidates based on their types derived from Wikidata\n\"instance_of\" property.",
        "translated": ""
    },
    {
        "title": "A Comparative Study of Transformer-based Neural Text Representation\n  Techniques on Bug Triaging",
        "url": "http://arxiv.org/abs/2310.06913v1",
        "pub_date": "2023-10-10",
        "summary": "Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n  Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.",
        "translated": ""
    },
    {
        "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval",
        "url": "http://arxiv.org/abs/2310.08319v1",
        "pub_date": "2023-10-12",
        "summary": "The effectiveness of multi-stage text retrieval has been solidly demonstrated\nsince before the era of pre-trained language models. However, most existing\nstudies utilize models that predate recent advances in large language models\n(LLMs). This study seeks to explore potential improvements that\nstate-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning\nthe latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise\nreranker (RankLLaMA) for both passage retrieval and document retrieval using\nthe MS MARCO datasets. Our findings demonstrate that the effectiveness of large\nlanguage models indeed surpasses that of smaller models. Additionally, since\nLLMs can inherently handle longer contexts, they can represent entire documents\nholistically, obviating the need for traditional segmenting and pooling\nstrategies. Furthermore, evaluations on BEIR demonstrate that our\nRepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model\ncheckpoints from this study are available on HuggingFace.",
        "translated": ""
    },
    {
        "title": "On Using GUI Interaction Data to Improve Text Retrieval-based Bug\n  Localization",
        "url": "http://arxiv.org/abs/2310.08083v1",
        "pub_date": "2023-10-12",
        "summary": "One of the most important tasks related to managing bug reports is localizing\nthe fault so that a fix can be applied. As such, prior work has aimed to\nautomate this task of bug localization by formulating it as an information\nretrieval problem, where potentially buggy files are retrieved and ranked\naccording to their textual similarity with a given bug report. However, there\nis often a notable semantic gap between the information contained in bug\nreports and identifiers or natural language contained within source code files.\nFor user-facing software, there is currently a key source of information that\ncould aid in bug localization, but has not been thoroughly investigated -\ninformation from the GUI.\n  We investigate the hypothesis that, for end user-facing applications,\nconnecting information in a bug report with information from the GUI, and using\nthis to aid in retrieving potentially buggy files, can improve upon existing\ntechniques for bug localization. To examine this phenomenon, we conduct a\ncomprehensive empirical study that augments four baseline techniques for bug\nlocalization with GUI interaction information from a reproduction scenario to\n(i) filter out potentially irrelevant files, (ii) boost potentially relevant\nfiles, and (iii) reformulate text-retrieval queries. To carry out our study, we\nsource the current largest dataset of fully-localized and reproducible real\nbugs for Android apps, with corresponding bug reports, consisting of 80 bug\nreports from 39 popular open-source apps. Our results illustrate that\naugmenting traditional techniques with GUI information leads to a marked\nincrease in effectiveness across multiple metrics, including a relative\nincrease in Hits@10 of 13-18%. Additionally, through further analysis, we find\nthat our studied augmentations largely complement existing techniques.",
        "translated": ""
    },
    {
        "title": "Rethinking Negative Pairs in Code Search",
        "url": "http://arxiv.org/abs/2310.08069v1",
        "pub_date": "2023-10-12",
        "summary": "Recently, contrastive learning has become a key component in fine-tuning code\nsearch models for software development efficiency and effectiveness. It pulls\ntogether positive code snippets while pushing negative samples away given\nsearch queries. Among contrastive learning, InfoNCE is the most widely used\nloss function due to its better performance. However, the following problems in\nnegative samples of InfoNCE may deteriorate its representation learning: 1) The\nexistence of false negative samples in large code corpora due to duplications.\n2). The failure to explicitly differentiate between the potential relevance of\nnegative samples. As an example, a bubble sorting algorithm example is less\n``negative'' than a file saving function for the quick sorting algorithm query.\nIn this paper, we tackle the above problems by proposing a simple yet effective\nSoft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss\nfunction, we apply three methods to estimate the weights of negative pairs and\nshow that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.\nTheoretically, we analyze the effects of Soft-InfoNCE on controlling the\ndistribution of learnt code representations and on deducing a more precise\nmutual information estimation. We furthermore discuss the superiority of\nproposed loss functions with other design alternatives. Extensive experiments\ndemonstrate the effectiveness of Soft-InfoNCE and weights estimation methods\nunder state-of-the-art code search models on a large-scale public dataset\nconsisting of six programming languages. Source code is available at\n\\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.",
        "translated": ""
    },
    {
        "title": "Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain\n  Models",
        "url": "http://arxiv.org/abs/2310.08039v1",
        "pub_date": "2023-10-12",
        "summary": "Industrial systems such as recommender systems and online advertising, have\nbeen widely equipped with multi-stage architectures, which are divided into\nseveral cascaded modules, including matching, pre-ranking, ranking and\nre-ranking. As a critical bridge between matching and ranking, existing\npre-ranking approaches mainly endure sample selection bias (SSB) problem owing\nto ignoring the entire-chain data dependence, resulting in sub-optimal\nperformances. In this paper, we rethink pre-ranking system from the perspective\nof the entire sample space, and propose Entire-chain Cross-domain Models (ECM),\nwhich leverage samples from the whole cascaded stages to effectively alleviate\nSSB problem. Besides, we design a fine-grained neural structure named ECMM to\nfurther improve the pre-ranking accuracy. Specifically, we propose a\ncross-domain multi-tower neural network to comprehensively predict for each\nstage result, and introduce the sub-networking routing strategy with $L0$\nregularization to reduce computational costs. Evaluations on real-world\nlarge-scale traffic logs demonstrate that our pre-ranking models outperform\nSOTA methods while time consumption is maintained within an acceptable level,\nwhich achieves better trade-off between efficiency and effectiveness.",
        "translated": ""
    },
    {
        "title": "Continual Learning via Manifold Expansion Replay",
        "url": "http://arxiv.org/abs/2310.08038v1",
        "pub_date": "2023-10-12",
        "summary": "In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.",
        "translated": ""
    },
    {
        "title": "Multi-View Variational Autoencoder for Missing Value Imputation in\n  Untargeted Metabolomics",
        "url": "http://arxiv.org/abs/2310.07990v1",
        "pub_date": "2023-10-12",
        "summary": "Background: Missing data is a common challenge in mass spectrometry-based\nmetabolomics, which can lead to biased and incomplete analyses. The integration\nof whole-genome sequencing (WGS) data with metabolomics data has emerged as a\npromising approach to enhance the accuracy of data imputation in metabolomics\nstudies. Method: In this study, we propose a novel method that leverages the\ninformation from WGS data and reference metabolites to impute unknown\nmetabolites. Our approach utilizes a multi-view variational autoencoder to\njointly model the burden score, polygenetic risk score (PGS), and linkage\ndisequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature\nextraction and missing metabolomics data imputation. By learning the latent\nrepresentations of both omics data, our method can effectively impute missing\nmetabolomics values based on genomic information. Results: We evaluate the\nperformance of our method on empirical metabolomics datasets with missing\nvalues and demonstrate its superiority compared to conventional imputation\ntechniques. Using 35 template metabolites derived burden scores, PGS and\nLD-pruned SNPs, the proposed methods achieved r2-scores &gt; 0.01 for 71.55% of\nmetabolites. Conclusion: The integration of WGS data in metabolomics imputation\nnot only improves data completeness but also enhances downstream analyses,\npaving the way for more comprehensive and accurate investigations of metabolic\npathways and disease associations. Our findings offer valuable insights into\nthe potential benefits of utilizing WGS data for metabolomics data imputation\nand underscore the importance of leveraging multi-modal data integration in\nprecision medicine research.",
        "translated": ""
    },
    {
        "title": "Refined Mechanism Design for Approximately Structured Priors via Active\n  Regression",
        "url": "http://arxiv.org/abs/2310.07874v1",
        "pub_date": "2023-10-11",
        "summary": "We consider the problem of a revenue-maximizing seller with a large number of\nitems $m$ for sale to $n$ strategic bidders, whose valuations are drawn\nindependently from high-dimensional, unknown prior distributions. It is\nwell-known that optimal and even approximately-optimal mechanisms for this\nsetting are notoriously difficult to characterize or compute, and, even when\nthey can be found, are often rife with various counter-intuitive properties. In\nthis paper, following a model introduced recently by Cai and\nDaskalakis~\\cite{cai2022recommender}, we consider the case that bidders' prior\ndistributions can be well-approximated by a topic model. We design an active\nlearning component, responsible for interacting with the bidders and outputting\nlow-dimensional approximations of their types, and a mechanism design\ncomponent, responsible for robustifying mechanisms for the low-dimensional\nmodel to work for the approximate types of the former component. On the active\nlearning front, we cast our problem in the framework of Randomized Linear\nAlgebra (RLA) for regression problems, allowing us to import several\nbreakthrough results from that line of research, and adapt them to our setting.\nOn the mechanism design front, we remove many restrictive assumptions of prior\nwork on the type of access needed to the underlying distributions and the\nassociated mechanisms. To the best of our knowledge, our work is the first to\nformulate connections between mechanism design, and RLA for active learning of\nregression problems, opening the door for further applications of randomized\nlinear algebra primitives to mechanism design.",
        "translated": ""
    },
    {
        "title": "Language Models As Semantic Indexers",
        "url": "http://arxiv.org/abs/2310.07815v1",
        "pub_date": "2023-10-11",
        "summary": "Semantic identifier (ID) is an important concept in information retrieval\nthat aims to preserve the semantics of objects such as documents and items\ninside their IDs. Previous studies typically adopt a two-stage pipeline to\nlearn semantic IDs by first procuring embeddings using off-the-shelf text\nencoders and then deriving IDs based on the embeddings. However, each step\nintroduces potential information loss and there is usually an inherent mismatch\nbetween the distribution of embeddings within the latent space produced by text\nencoders and the anticipated distribution required for semantic indexing.\nNevertheless, it is non-trivial to design a method that can learn the\ndocument's semantic representations and its hierarchical structure\nsimultaneously, given that semantic IDs are discrete and sequentially\nstructured, and the semantic supervision is deficient. In this paper, we\nintroduce LMINDEXER, a self-supervised framework to learn semantic IDs with a\ngenerative language model. We tackle the challenge of sequential discrete ID by\nintroducing a semantic indexer capable of generating neural sequential discrete\nrepresentations with progressive training and contrastive learning. In response\nto the semantic supervision deficiency, we propose to train the model with a\nself-supervised document reconstruction objective. The learned semantic indexer\ncan facilitate various downstream tasks, such as recommendation and retrieval.\nWe conduct experiments on three tasks including recommendation, product search,\nand document retrieval on five datasets from various domains, where LMINDEXER\noutperforms competitive baselines significantly and consistently.",
        "translated": ""
    },
    {
        "title": "Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble\n  Sampling",
        "url": "http://arxiv.org/abs/2310.07786v1",
        "pub_date": "2023-10-11",
        "summary": "Real-world applications of contextual bandits often exhibit non-stationarity\ndue to seasonality, serendipity, and evolving social trends. While a number of\nnon-stationary contextual bandit learning algorithms have been proposed in the\nliterature, they excessively explore due to a lack of prioritization for\ninformation of enduring value, or are designed in ways that do not scale in\nmodern applications with high-dimensional user-specific features and large\naction set, or both. In this paper, we introduce a novel non-stationary\ncontextual bandit algorithm that addresses these concerns. It combines a\nscalable, deep-neural-network-based architecture with a carefully designed\nexploration mechanism that strategically prioritizes collecting information\nwith the most lasting value in a non-stationary environment. Through empirical\nevaluations on two real-world recommendation datasets, which exhibit pronounced\nnon-stationarity, we demonstrate that our approach significantly outperforms\nthe state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "ClickPrompt: CTR Models are Strong Prompt Generators for Adapting\n  Language Models to CTR Prediction",
        "url": "http://arxiv.org/abs/2310.09234v1",
        "pub_date": "2023-10-13",
        "summary": "Click-through rate (CTR) prediction has become increasingly indispensable for\nvarious Internet applications. Traditional CTR models convert the multi-field\ncategorical data into ID features via one-hot encoding, and extract the\ncollaborative signals among features. Such a paradigm suffers from the problem\nof semantic information loss. Another line of research explores the potential\nof pretrained language models (PLMs) for CTR prediction by converting input\ndata into textual sentences through hard prompt templates. Although semantic\nsignals are preserved, they generally fail to capture the collaborative\ninformation (e.g., feature interactions, pure ID features), not to mention the\nunacceptable inference overhead brought by the huge model size. In this paper,\nwe aim to model both the semantic knowledge and collaborative knowledge for\naccurate CTR estimation, and meanwhile address the inference inefficiency\nissue. To benefit from both worlds and close their gaps, we propose a novel\nmodel-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models\nto generate interaction-aware soft prompts for PLMs. We design a\nprompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM\nhas to recover the masked tokens based on the language context, as well as the\nsoft prompts generated by CTR model. The collaborative and semantic knowledge\nfrom ID and textual features would be explicitly aligned and interacted via the\nprompt interface. Then, we can either tune the CTR model with PLM for superior\nperformance, or solely tune the CTR model without PLM for inference efficiency.\nExperiments on four real-world datasets validate the effectiveness of\nClickPrompt compared with existing baselines.",
        "translated": ""
    },
    {
        "title": "AgentCF: Collaborative Learning with Autonomous Language Agents for\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2310.09233v1",
        "pub_date": "2023-10-13",
        "summary": "Recently, there has been an emergence of employing LLM-powered agents as\nbelievable human proxies, based on their remarkable decision-making capability.\nHowever, existing studies mainly focus on simulating human dialogue. Human\nnon-verbal behaviors, such as item clicking in recommender systems, although\nimplicitly exhibiting user preferences and could enhance the modeling of users,\nhave not been deeply explored. The main reasons lie in the gap between language\nmodeling and behavior modeling, as well as the incomprehension of LLMs about\nuser-item relations.\n  To address this issue, we propose AgentCF for simulating user-item\ninteractions in recommender systems through agent-based collaborative\nfiltering. We creatively consider not only users but also items as agents, and\ndevelop a collaborative learning approach that optimizes both kinds of agents\ntogether. Specifically, at each time step, we first prompt the user and item\nagents to interact autonomously. Then, based on the disparities between the\nagents' decisions and real-world interaction records, user and item agents are\nprompted to reflect on and adjust the misleading simulations collaboratively,\nthereby modeling their two-sided relations. The optimized agents can also\npropagate their preferences to other agents in subsequent interactions,\nimplicitly capturing the collaborative filtering idea. Overall, the optimized\nagents exhibit diverse interaction behaviors within our framework, including\nuser-item, user-user, item-item, and collective interactions. The results show\nthat these agents can demonstrate personalized behaviors akin to those of\nreal-world individuals, sparking the development of next-generation user\nbehavior simulation.",
        "translated": ""
    },
    {
        "title": "EHI: End-to-end Learning of Hierarchical Index for Efficient Dense\n  Retrieval",
        "url": "http://arxiv.org/abs/2310.08891v1",
        "pub_date": "2023-10-13",
        "summary": "Dense embedding-based retrieval is now the industry standard for semantic\nsearch and ranking problems, like obtaining relevant web documents for a given\nquery. Such techniques use a two-stage process: (a) contrastive learning to\ntrain a dual encoder to embed both the query and documents and (b) approximate\nnearest neighbor search (ANNS) for finding similar documents for a given query.\nThese two stages are disjoint; the learned embeddings might be ill-suited for\nthe ANNS method and vice-versa, leading to suboptimal performance. In this\nwork, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns\nboth the embeddings and the ANNS structure to optimize retrieval performance.\nEHI uses a standard dual encoder model for embedding queries and documents\nwhile learning an inverted file index (IVF) style tree structure for efficient\nANNS. To ensure stable and efficient learning of discrete tree-based ANNS\nstructure, EHI introduces the notion of dense path embedding that captures the\nposition of a query/document in the tree. We demonstrate the effectiveness of\nEHI on several benchmarks, including de-facto industry standard MS MARCO (Dev\nset and TREC DL19) datasets. For example, with the same compute budget, EHI\noutperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and\nby 4.2% (nDCG@10) on TREC DL19 benchmarks.",
        "translated": ""
    },
    {
        "title": "Question Answering for Electronic Health Records: A Scoping Review of\n  datasets and models",
        "url": "http://arxiv.org/abs/2310.08759v1",
        "pub_date": "2023-10-12",
        "summary": "Question Answering (QA) systems on patient-related data can assist both\nclinicians and patients. They can, for example, assist clinicians in\ndecision-making and enable patients to have a better understanding of their\nmedical history. Significant amounts of patient data are stored in Electronic\nHealth Records (EHRs), making EHR QA an important research area. In EHR QA, the\nanswer is obtained from the medical record of the patient. Because of the\ndifferences in data format and modality, this differs greatly from other\nmedical QA tasks that employ medical websites or scientific papers to retrieve\nanswers, making it critical to research EHR question answering. This study\naimed to provide a methodological review of existing works on QA over EHRs. We\nsearched for articles from January 1st, 2005 to September 30th, 2023 in four\ndigital sources including Google Scholar, ACL Anthology, ACM Digital Library,\nand PubMed to collect relevant publications on EHR QA. 4111 papers were\nidentified for our study, and after screening based on our inclusion criteria,\nwe obtained a total of 47 papers for further study. Out of the 47 papers, 25\npapers were about EHR QA datasets, and 37 papers were about EHR QA models. It\nwas observed that QA on EHRs is relatively new and unexplored. Most of the\nworks are fairly recent. Also, it was observed that emrQA is by far the most\npopular EHR QA dataset, both in terms of citations and usage in other papers.\nFurthermore, we identified the different models used in EHR QA along with the\nevaluation metrics used for these models.",
        "translated": ""
    },
    {
        "title": "Individual Variation Affects Outbreak Magnitude and Predictability in an\n  Extended Multi-Pathogen SIR Model of Pigeons Vising Dairy Farms",
        "url": "http://arxiv.org/abs/2310.08613v1",
        "pub_date": "2023-10-12",
        "summary": "Zoonotic disease transmission between animals and humans is a growing risk\nand the agricultural context acts as a likely point of transition, with\nindividual heterogeneity acting as an important contributor. Thus,\nunderstanding the dynamics of disease spread in the wildlife-livestock\ninterface is crucial for mitigating these risks of transmission. Specifically,\nthe interactions between pigeons and in-door cows at dairy farms can lead to\nsignificant disease transmission and economic losses for farmers; putting\nlivestock, adjacent human populations, and other wildlife species at risk. In\nthis paper, we propose a novel spatio-temporal multi-pathogen model with\ncontinuous spatial movement. The model expands on the\nSusceptible-Exposed-Infected-Recovered-Dead (SEIRD) framework and accounts for\nboth within-species and cross-species transmission of pathogens, as well as the\nexploration-exploitation movement dynamics of pigeons, which play a critical\nrole in the spread of infection agents. In addition to model formulation, we\nalso implement it as an agent-based simulation approach and use empirical field\ndata to investigate different biologically realistic scenarios, evaluating the\neffect of various parameters on the epidemic spread. Namely, in agreement with\ntheoretical expectations, the model predicts that the heterogeneity of the\npigeons' movement dynamics can drastically affect both the magnitude and\nstability of outbreaks. In addition, joint infection by multiple pathogens can\nhave an interactive effect unobservable in single-pathogen SIR models,\nreflecting a non-intuitive inhibition of the outbreak. Our findings highlight\nthe impact of heterogeneity in host behavior on their pathogens and allow\nrealistic predictions of outbreak dynamics in the multi-pathogen\nwildlife-livestock interface with consequences to zoonotic diseases in various\nsystems.",
        "translated": ""
    },
    {
        "title": "Type-aware Decoding via Explicitly Aggregating Event Information for\n  Document-level Event Extraction",
        "url": "http://arxiv.org/abs/2310.10487v1",
        "pub_date": "2023-10-16",
        "summary": "Document-level event extraction (DEE) faces two main challenges:\narguments-scattering and multi-event. Although previous methods attempt to\naddress these challenges, they overlook the interference of event-unrelated\nsentences during event detection and neglect the mutual interference of\ndifferent event roles during argument extraction. Therefore, this paper\nproposes a novel Schema-based Explicitly Aggregating~(SEA) model to address\nthese limitations. SEA aggregates event information into event type and role\nrepresentations, enabling the decoding of event records based on specific\ntype-aware representations. By detecting each event based on its event type\nrepresentation, SEA mitigates the interference caused by event-unrelated\ninformation. Furthermore, SEA extracts arguments for each role based on its\nrole-aware representations, reducing mutual interference between different\nroles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEA\noutperforms the SOTA methods.",
        "translated": ""
    },
    {
        "title": "DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource\n  Event Extraction",
        "url": "http://arxiv.org/abs/2310.10481v1",
        "pub_date": "2023-10-16",
        "summary": "Most current Event Extraction (EE) methods focus on the high-resource\nscenario, which requires a large amount of annotated data and can hardly be\napplied to low-resource domains. To address EE more effectively with limited\nresources, we propose the Demonstration-enhanced Schema-guided Generation\n(DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we\npropose the demonstration-based learning paradigm for EE to fully use the\nannotated data, which transforms them into demonstrations to illustrate the\nextraction process and help the model learn effectively. Secondly, we formulate\nEE as a natural language generation task guided by schema-based prompts,\nthereby leveraging label semantics and promoting knowledge transfer in\nlow-resource scenarios. We conduct extensive experiments under in-domain and\ndomain adaptation low-resource settings on three datasets, and study the\nrobustness of DemoSG. The results show that DemoSG significantly outperforms\ncurrent methods in low-resource scenarios.",
        "translated": ""
    },
    {
        "title": "BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework\n  for Music-Dance Retrieval",
        "url": "http://arxiv.org/abs/2310.10300v1",
        "pub_date": "2023-10-16",
        "summary": "Dance and music are closely related forms of expression, with mutual\nretrieval between dance videos and music being a fundamental task in various\nfields like education, art, and sports. However, existing methods often suffer\nfrom unnatural generation effects or fail to fully explore the correlation\nbetween music and dance. To overcome these challenges, we propose BeatDance, a\nnovel beat-based model-agnostic contrastive learning framework. BeatDance\nincorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat\nBlender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval\nperformance by utilizing the alignment between music beats and dance movements.\nWe also introduce the Music-Dance (MD) dataset, a large-scale collection of\nover 10,000 music-dance video pairs for training and testing. Experimental\nresults on the MD dataset demonstrate the superiority of our method over\nexisting baselines, achieving state-of-the-art performance. The code and\ndataset will be made public available upon acceptance.",
        "translated": ""
    },
    {
        "title": "Rethinking Financial Service Promotion With Hybrid Recommender Systems\n  at PicPay",
        "url": "http://arxiv.org/abs/2310.10268v1",
        "pub_date": "2023-10-16",
        "summary": "The fintech PicPay offers a wide range of financial services to its 30\nmillion monthly active users, with more than 50 thousand items recommended in\nthe PicPay mobile app. In this scenario, promoting specific items that are\nstrategic to the company can be very challenging. In this work, we present a\nSwitching Hybrid Recommender System that combines two algorithms to effectively\npromote items without negatively impacting the user's experience. The results\nof our A/B tests show an uplift of up to 3.2\\% when compared to a default\nrecommendation strategy.",
        "translated": ""
    },
    {
        "title": "An Interpretable Deep-Learning Framework for Predicting Hospital\n  Readmissions From Electronic Health Records",
        "url": "http://arxiv.org/abs/2310.10187v1",
        "pub_date": "2023-10-16",
        "summary": "With the increasing availability of patients' data, modern medicine is\nshifting towards prospective healthcare. Electronic health records contain a\nvariety of information useful for clinical patient description and can be\nexploited for the construction of predictive models, given that similar medical\nhistories will likely lead to similar progressions. One example is unplanned\nhospital readmission prediction, an essential task for reducing hospital costs\nand improving patient health. Despite predictive models showing very good\nperformances especially with deep-learning models, they are often criticized\nfor the poor interpretability of their results, a fundamental characteristic in\nthe medical field, where incorrect predictions might have serious consequences\nfor the patient health. In this paper we propose a novel, interpretable\ndeep-learning framework for predicting unplanned hospital readmissions,\nsupported by NLP findings on word embeddings and by neural-network models\n(ConvLSTM) for better handling temporal data. We validate our system on the two\npredictive tasks of hospital readmission within 30 and 180 days, using\nreal-world data. In addition, we introduce and test a model-dependent technique\nto make the representation of results easily interpretable by the medical\nstaff. Our solution achieves better performances compared to traditional models\nbased on machine learning, while providing at the same time more interpretable\nresults.",
        "translated": ""
    },
    {
        "title": "DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy\n  Slot Filling Task",
        "url": "http://arxiv.org/abs/2310.10169v1",
        "pub_date": "2023-10-16",
        "summary": "Recently, prompt-based generative frameworks have shown impressive\ncapabilities in sequence labeling tasks. However, in practical dialogue\nscenarios, relying solely on simplistic templates and traditional corpora\npresents a challenge for these methods in generalizing to unknown input\nperturbations. To address this gap, we propose a multi-task demonstration based\ngenerative framework for noisy slot filling, named DemoNSF. Specifically, we\nintroduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask\n(RM), and hybrid discrimination (HD), to implicitly capture semantic structural\ninformation of input perturbations at different granularities. In the\ndownstream main task, we design a noisy demonstration construction strategy for\nthe generative framework, which explicitly incorporates task-specific\ninformation and perturbed distribution during training and inference.\nExperiments on two benchmarks demonstrate that DemoNSF outperforms all baseline\nmethods and achieves strong generalization. Further analysis provides empirical\nguidance for the practical application of generative frameworks. Our code is\nreleased at https://github.com/dongguanting/Demo-NSF.",
        "translated": ""
    },
    {
        "title": "DNA: Denoised Neighborhood Aggregation for Fine-grained Category\n  Discovery",
        "url": "http://arxiv.org/abs/2310.10151v1",
        "pub_date": "2023-10-16",
        "summary": "Discovering fine-grained categories from coarsely labeled data is a practical\nand challenging task, which can bridge the gap between the demand for\nfine-grained analysis and the high annotation cost. Previous works mainly focus\non instance-level discrimination to learn low-level features, but ignore\nsemantic similarities between data, which may prevent these models learning\ncompact cluster representations. In this paper, we propose Denoised\nNeighborhood Aggregation (DNA), a self-supervised framework that encodes\nsemantic structures of data into the embedding space. Specifically, we retrieve\nk-nearest neighbors of a query as its positive keys to capture semantic\nsimilarities between data and then aggregate information from the neighbors to\nlearn compact cluster representations, which can make fine-grained categories\nmore separatable. However, the retrieved neighbors can be noisy and contain\nmany false-positive keys, which can degrade the quality of learned embeddings.\nTo cope with this challenge, we propose three principles to filter out these\nfalse neighbors for better representation learning. Furthermore, we\ntheoretically justify that the learning objective of our framework is\nequivalent to a clustering loss, which can capture semantic similarities\nbetween data to form compact fine-grained clusters. Extensive experiments on\nthree benchmark datasets show that our method can retrieve more accurate\nneighbors (21.31% accuracy improvement) and outperform state-of-the-art models\nby a large margin (average 9.96% improvement on three metrics). Our code and\ndata are available at https://github.com/Lackel/DNA.",
        "translated": ""
    },
    {
        "title": "On Generative Agents in Recommendation",
        "url": "http://arxiv.org/abs/2310.10108v1",
        "pub_date": "2023-10-16",
        "summary": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie\nrecommendation simulator, leveraging LLM-empowered generative agents equipped\nwith user profile, memory, and actions modules specifically tailored for the\nrecommender system. In particular, these agents' profile modules are\ninitialized using the MovieLens dataset, capturing users' unique tastes and\nsocial traits; memory modules log both factual and emotional memories and are\nintegrated with an emotion-driven reflection mechanism; action modules support\na wide variety of behaviors, spanning both taste-driven and emotion-driven\nactions. Each agent interacts with personalized movie recommendations in a\npage-by-page manner, relying on a pre-implemented collaborative filtering-based\nrecommendation algorithm. We delve into both the capabilities and limitations\nof Agent4Rec, aiming to explore an essential research question: to what extent\ncan LLM-empowered generative agents faithfully simulate the behavior of real,\nautonomous humans in recommender systems? Extensive and multi-faceted\nevaluations of Agent4Rec highlight both the alignment and deviation between\nagents and user-personalized preferences. Beyond mere performance comparison,\nwe explore insightful experiments, such as emulating the filter bubble effect\nand discovering the underlying causal relationships in recommendation tasks.\nOur codes are available at https://github.com/LehengTHU/Agent4Rec.",
        "translated": ""
    },
    {
        "title": "Dual-Scale Interest Extraction Framework with Self-Supervision for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2310.10025v1",
        "pub_date": "2023-10-16",
        "summary": "In the sequential recommendation task, the recommender generally learns\nmultiple embeddings from a user's historical behaviors, to catch the diverse\ninterests of the user. Nevertheless, the existing approaches just extract each\ninterest independently for the corresponding sub-sequence while ignoring the\nglobal correlation of the entire interaction sequence, which may fail to\ncapture the user's inherent preference for the potential interests\ngeneralization and unavoidably make the recommended items homogeneous with the\nhistorical behaviors. In this paper, we propose a novel Dual-Scale Interest\nExtraction framework (DSIE) to precisely estimate the user's current interests.\nSpecifically, DSIE explicitly models the user's inherent preference with\ncontrastive learning by attending over his/her entire interaction sequence at\nthe global scale and catches the user's diverse interests in a fine granularity\nat the local scale. Moreover, we develop a novel interest aggregation module to\nintegrate the multi-interests according to the inherent preference to generate\nthe user's current interests for the next-item prediction. Experiments\nconducted on three real-world benchmark datasets demonstrate that DSIE\noutperforms the state-of-the-art models in terms of recommendation preciseness\nand novelty.",
        "translated": ""
    },
    {
        "title": "Farzi Data: Autoregressive Data Distillation",
        "url": "http://arxiv.org/abs/2310.09983v1",
        "pub_date": "2023-10-15",
        "summary": "We study data distillation for auto-regressive machine learning tasks, where\nthe input and output have a strict left-to-right causal structure. More\nspecifically, we propose Farzi, which summarizes an event sequence dataset into\na small number of synthetic sequences -- Farzi Data -- which are optimized to\nmaintain (if not improve) model performance compared to training on the full\ndataset. Under the hood, Farzi conducts memory-efficient data distillation by\n(i) deriving efficient reverse-mode differentiation of the Adam optimizer by\nleveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional\ndiscrete event-space into a latent-space which provably promotes implicit\nregularization. Empirically, for sequential recommendation and language\nmodeling tasks, we are able to achieve 98-120% of downstream full-data\nperformance when training state-of-the-art models on Farzi Data of size as\nlittle as 0.1% of the original dataset. Notably, being able to train better\nmodels with significantly less data sheds light on the design of future large\nauto-regressive models, and opens up new opportunities to further scale up\nmodel and data sizes.",
        "translated": ""
    },
    {
        "title": "On Coherence-based Predictors for Dense Query Performance Prediction",
        "url": "http://arxiv.org/abs/2310.11405v1",
        "pub_date": "2023-10-17",
        "summary": "Query Performance Prediction (QPP) estimates the effectiveness of a search\nengine's results in response to a query without relevance judgments.\nTraditionally, post-retrieval predictors have focused upon either the\ndistribution of the retrieval scores, or the coherence of the top-ranked\ndocuments using traditional bag-of-words index representations. More recently,\nBERT-based models using dense embedded document representations have been used\nto create new predictors, but mostly applied to predict the performance of\nrankings created by BM25. Instead, we aim to predict the effectiveness of\nrankings created by single-representation dense retrieval models (ANCE &amp;\nTCT-ColBERT). Therefore, we propose a number of variants of existing\nunsupervised coherence-based predictors that employ neural embedding\nrepresentations. In our experiments on the TREC Deep Learning Track datasets,\nwe demonstrate improved accuracy upon dense retrieval (up to 92% compared to\nsparse variants for TCT-ColBERT and 188% for ANCE). Going deeper, we select the\nmost representative and best performing predictors to study the importance of\ndifferences among predictors and query types on query performance. Using\nexisting distribution-based evaluation QPP measures and a particular type of\nlinear mixed models, we find that query types further significantly influence\nquery performance (and are up to 35% responsible for the unstable performance\nof QPP predictors), and that this sensitivity is unique to dense retrieval\nmodels. Our approach introduces a new setting for obtaining richer information\non query differences in dense QPP that can explain potential unstable\nperformance of existing predictors and outlines the unique characteristics of\ndifferent query types on dense retrieval models.",
        "translated": ""
    },
    {
        "title": "Graph Neural Networks for Recommendation: Reproducibility, Graph\n  Topology, and Node Representation",
        "url": "http://arxiv.org/abs/2310.11270v1",
        "pub_date": "2023-10-17",
        "summary": "Graph neural networks (GNNs) have gained prominence in recommendation systems\nin recent years. By representing the user-item matrix as a bipartite and\nundirected graph, GNNs have demonstrated their potential to capture short- and\nlong-distance user-item interactions, thereby learning more accurate preference\npatterns than traditional recommendation approaches. In contrast to previous\ntutorials on the same topic, this tutorial aims to present and examine three\nkey aspects that characterize GNNs for recommendation: (i) the reproducibility\nof state-of-the-art approaches, (ii) the potential impact of graph topological\ncharacteristics on the performance of these models, and (iii) strategies for\nlearning node representations when training features from scratch or utilizing\npre-trained embeddings as additional item information (e.g., multimodal\nfeatures). The goal is to provide three novel theoretical and practical\nperspectives on the field, currently subject to debate in graph learning but\nlong been overlooked in the context of recommendation systems.",
        "translated": ""
    },
    {
        "title": "MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.11088v1",
        "pub_date": "2023-10-17",
        "summary": "It is a long-standing challenge in modern recommender systems to effectively\nmake recommendations for new users, namely the cold-start problem. Cross-Domain\nRecommendation (CDR) has been proposed to address this challenge, but current\nways to represent users' interests across systems are still severely limited.\nWe introduce Personal Knowledge Graph (PKG) as a domain-invariant interest\nrepresentation, and propose a novel CDR paradigm named MeKB-Rec. We first link\nusers and entities in a knowledge base to construct a PKG of users' interests,\nnamed MeKB. Then we learn a semantic representation of MeKB for the\ncross-domain recommendation. To efficiently utilize limited training data in\nCDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into\nunderstanding users' interests. Beyond most existing systems, our approach\nbuilds a semantic mapping across domains which breaks the requirement for\nin-domain user behaviors, enabling zero-shot recommendations for new users in a\nlow-resource domain. We experiment MeKB-Rec on well-established public CDR\ndatasets, and demonstrate that the new formulation % is more powerful than\nprevious approaches, achieves a new state-of-the-art that significantly\nimproves HR@10 and NDCG@10 metrics over best previous approaches by 24\\%--91\\%,\nwith a 105\\% improvement for HR@10 of zero-shot users with no behavior in the\ntarget domain. We deploy MeKB-Rec in WeiXin recommendation scenarios and\nachieve significant gains in core online metrics. MeKB-Rec is now serving\nhundreds of millions of users in real-world products.",
        "translated": ""
    },
    {
        "title": "Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation",
        "url": "http://arxiv.org/abs/2310.11049v1",
        "pub_date": "2023-10-17",
        "summary": "This paper describes our submission to the SemEval-2023 for Task 6 on\nLegalEval: Understanding Legal Texts. Our submission concentrated on three\nsubtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment\nPrediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation\n(CJPE) for Task-C2. We conducted various experiments on these subtasks and\npresented the results in detail, including data statistics and methodology. It\nis worth noting that legal tasks, such as those tackled in this research, have\nbeen gaining importance due to the increasing need to automate legal analysis\nand support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$,\nand 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the\nleaderboard.",
        "translated": ""
    },
    {
        "title": "If the Sources Could Talk: Evaluating Large Language Models for Research\n  Assistance in History",
        "url": "http://arxiv.org/abs/2310.10808v1",
        "pub_date": "2023-10-16",
        "summary": "The recent advent of powerful Large-Language Models (LLM) provides a new\nconversational form of inquiry into historical memory (or, training data, in\nthis case). We show that by augmenting such LLMs with vector embeddings from\nhighly specialized academic sources, a conversational methodology can be made\naccessible to historians and other researchers in the Humanities. Concretely,\nwe evaluate and demonstrate how LLMs have the ability of assisting researchers\nwhile they examine a customized corpora of different types of documents,\nincluding, but not exclusive to: (1). primary sources, (2). secondary sources\nwritten by experts, and (3). the combination of these two. Compared to\nestablished search interfaces for digital catalogues, such as metadata and\nfull-text search, we evaluate the richer conversational style of LLMs on the\nperformance of two main types of tasks: (1). question-answering, and (2).\nextraction and organization of data. We demonstrate that LLMs semantic\nretrieval and reasoning abilities on problem-specific tasks can be applied to\nlarge textual archives that have not been part of the its training data.\nTherefore, LLMs can be augmented with sources relevant to specific research\nprojects, and can be queried privately by researchers.",
        "translated": ""
    },
    {
        "title": "Automated Attribute Extraction from Legal Proceedings",
        "url": "http://arxiv.org/abs/2310.12131v1",
        "pub_date": "2023-10-18",
        "summary": "The escalating number of pending cases is a growing concern world-wide.\nRecent advancements in digitization have opened up possibilities for leveraging\nartificial intelligence (AI) tools in the processing of legal documents.\nAdopting a structured representation for legal documents, as opposed to a mere\nbag-of-words flat text representation, can significantly enhance processing\ncapabilities. With the aim of achieving this objective, we put forward a set of\ndiverse attributes for criminal case proceedings. We use a state-of-the-art\nsequence labeling framework to automatically extract attributes from the legal\ndocuments. Moreover, we demonstrate the efficacy of the extracted attributes in\na downstream task, namely legal judgment prediction.",
        "translated": ""
    },
    {
        "title": "Unveiling the Siren's Song: Towards Reliable Fact-Conflicting\n  Hallucination Detection",
        "url": "http://arxiv.org/abs/2310.12086v1",
        "pub_date": "2023-10-18",
        "summary": "Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread\nattention owing to their myriad of practical applications, yet their adoption\nhas been constrained by issues of fact-conflicting hallucinations across web\nplatforms. The assessment of factuality in text, produced by LLMs, remains\ninadequately explored, extending not only to the judgment of vanilla facts but\nalso encompassing the evaluation of factual errors emerging in complex\ninferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a\nfact-conflicting hallucination detection benchmark meticulously designed for\nLLMs. Functioning as a pivotal tool in evaluating factuality within\n\"Query-Respons\" contexts, our benchmark assimilates a large-scale dataset,\nencapsulating a broad spectrum of factuality patterns, such as vanilla,\nmulti-hops, comparison, and set-operation patterns. A distinctive feature of\nour benchmark is its incorporation of fact-based chains of evidence, thereby\nfacilitating comprehensive and conducive factual reasoning throughout the\nassessment process. We evaluate multiple LLMs, demonstrating the effectiveness\nof the benchmark and current methods fall short of faithfully detecting factual\nerrors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective\nconsiderations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming\nto yield more credible detection through the amalgamation of predictive results\nand evidence. The benchmark dataset and source code will be made available in\nhttps://github.com/zjunlp/FactCHD.",
        "translated": ""
    },
    {
        "title": "Simulating Users in Interactive Web Table Retrieval",
        "url": "http://arxiv.org/abs/2310.11931v1",
        "pub_date": "2023-10-18",
        "summary": "Considering the multimodal signals of search items is beneficial for\nretrieval effectiveness. Especially in web table retrieval (WTR) experiments,\naccounting for multimodal properties of tables boosts effectiveness. However,\nit still remains an open question how the single modalities affect user\nexperience in particular. Previous work analyzed WTR performance in ad-hoc\nretrieval benchmarks, which neglects interactive search behavior and limits the\nconclusion about the implications for real-world user environments.\n  To this end, this work presents an in-depth evaluation of simulated\ninteractive WTR search sessions as a more cost-efficient and reproducible\nalternative to real user studies. As a first of its kind, we introduce\ninteractive query reformulation strategies based on Doc2Query, incorporating\ncognitive states of simulated user knowledge. Our evaluations include two\nperspectives on user effectiveness by considering different cost paradigms,\nnamely query-wise and time-oriented measures of effort. Our multi-perspective\nevaluation scheme reveals new insights about query strategies, the impact of\nmodalities, and different user types in simulated WTR search sessions.",
        "translated": ""
    },
    {
        "title": "CIR at the NTCIR-17 ULTRE-2 Task",
        "url": "http://arxiv.org/abs/2310.11852v1",
        "pub_date": "2023-10-18",
        "summary": "The Chinese academy of sciences Information Retrieval team (CIR) has\nparticipated in the NTCIR-17 ULTRE-2 task. This paper describes our approaches\nand reports our results on the ULTRE-2 task. We recognize the issue of false\nnegatives in the Baidu search data in this competition is very severe, much\nmore severe than position bias. Hence, we adopt the Dual Learning Algorithm\n(DLA) to address the position bias and use it as an auxiliary model to study\nhow to alleviate the false negative issue. We approach the problem from two\nperspectives: 1) correcting the labels for non-clicked items by a relevance\njudgment model trained from DLA, and learn a new ranker that is initialized\nfrom DLA; 2) including random documents as true negatives and documents that\nhave partial matching as hard negatives. Both methods can enhance the model\nperformance and our best method has achieved nDCG@10 of 0.5355, which is 2.66%\nbetter than the best score from the organizer.",
        "translated": ""
    },
    {
        "title": "DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing\n  in Multi-task Learning",
        "url": "http://arxiv.org/abs/2310.11777v1",
        "pub_date": "2023-10-18",
        "summary": "In recent years, DL has developed rapidly, and personalized services are\nexploring using DL algorithms to improve the performance of the recommendation\nsystem. For personalized services, a successful recommendation consists of two\nparts: attracting users to click the item and users being willing to consume\nthe item. If both tasks need to be predicted at the same time, traditional\nrecommendation systems generally train two independent models. This approach is\ncumbersome and does not effectively model the relationship between the two\nsubtasks of \"click-consumption\". Therefore, in order to improve the success\nrate of recommendation and reduce computational costs, researchers are trying\nto model multi-task learning.\n  At present, existing multi-task learning models generally adopt hard\nparameter sharing or soft parameter sharing architecture, but these two\narchitectures each have certain problems. Therefore, in this work, we propose a\nnovel recommendation model based on real recommendation scenarios, Deep Cross\nnetwork based on RNN for partial parameter sharing (DCRNN). The model has three\ninnovations: 1) It adopts the idea of cross network and uses RNN network to\ncross-process the features, thereby effectively improves the expressive ability\nof the model; 2) It innovatively proposes the structure of partial parameter\nsharing; 3) It can effectively capture the potential correlation between\ndifferent tasks to optimize the efficiency and methods for learning different\ntasks.",
        "translated": ""
    },
    {
        "title": "From Relevance to Utility: Evidence Retrieval with Feedback for Fact\n  Verification",
        "url": "http://arxiv.org/abs/2310.11675v1",
        "pub_date": "2023-10-18",
        "summary": "Retrieval-enhanced methods have become a primary approach in fact\nverification (FV); it requires reasoning over multiple retrieved pieces of\nevidence to verify the integrity of a claim. To retrieve evidence, existing\nwork often employs off-the-shelf retrieval models whose design is based on the\nprobability ranking principle. We argue that, rather than relevance, for FV we\nneed to focus on the utility that a claim verifier derives from the retrieved\nevidence. We introduce the feedback-based evidence retriever(FER) that\noptimizes the evidence retrieval process by incorporating feedback from the\nclaim verifier. As a feedback signal we use the divergence in utility between\nhow effectively the verifier utilizes the retrieved evidence and the\nground-truth evidence to produce the final claim label. Empirical studies\ndemonstrate the superiority of FER over prevailing baselines.",
        "translated": ""
    },
    {
        "title": "VKIE: The Application of Key Information Extraction on Video Text",
        "url": "http://arxiv.org/abs/2310.11650v1",
        "pub_date": "2023-10-18",
        "summary": "Extracting structured information from videos is critical for numerous\ndownstream applications in the industry. In this paper, we define a significant\ntask of extracting hierarchical key information from visual texts on videos. To\nfulfill this task, we decouples it into four subtasks and introduce two\nimplementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially\ncompletes the four subtasks in continuous stages, while UniVKIE is improved by\nunifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage\nmultimodal information from vision, text, and coordinates for feature\nrepresentation. Extensive experiments on one well-defined dataset demonstrate\nthat our solutions can achieve remarkable performance and efficient inference\nspeed. The code and dataset will be publicly available.",
        "translated": ""
    },
    {
        "title": "Open Information Extraction: A Review of Baseline Techniques,\n  Approaches, and Applications",
        "url": "http://arxiv.org/abs/2310.11644v1",
        "pub_date": "2023-10-18",
        "summary": "With the abundant amount of available online and offline text data, there\narises a crucial need to extract the relation between phrases and summarize the\nmain content of each document in a few words. For this purpose, there have been\nmany studies recently in Open Information Extraction (OIE). OIE improves upon\nrelation extraction techniques by analyzing relations across different domains\nand avoids requiring hand-labeling pre-specified relations in sentences. This\npaper surveys recent approaches of OIE and its applications on Knowledge Graph\n(KG), text summarization, and Question Answering (QA). Moreover, the paper\ndescribes OIE basis methods in relation extraction. It briefly discusses the\nmain approaches and the pros and cons of each method. Finally, it gives an\noverview about challenges, open issues, and future work opportunities for OIE,\nrelation extraction, and OIE applications.",
        "translated": ""
    },
    {
        "title": "Unified Browsing Models for Linear and Grid Layouts",
        "url": "http://arxiv.org/abs/2310.12524v1",
        "pub_date": "2023-10-19",
        "summary": "Many information access systems operationalize their results in terms of\nrankings, which are then displayed to users in various ranking layouts such as\nlinear lists or grids. User interaction with a retrieved item is highly\ndependent on the item's position in the layout, and users do not provide\nsimilar attention to every position in ranking (under any layout model). User\nattention is an important component in the evaluation process of ranking, due\nto its use in effectiveness metrics that estimate utility as well as fairness\nmetrics that evaluate ranking based on social and ethical concerns. These\nmetrics take user browsing behavior into account in their measurement\nstrategies to estimate the attention the user is likely to provide to each item\nin ranking. Research on understanding user browsing behavior has proposed\nseveral user browsing models, and further observed that user browsing behavior\ndiffers with different ranking layouts. However, the underlying concepts of\nthese browsing models are often similar, including varying components and\nparameter settings. We seek to leverage that similarity to represent multiple\nbrowsing models in a generalized, configurable framework which can be further\nextended to more complex ranking scenarios. In this paper, we describe a\nprobabilistic user browsing model for linear rankings, show how they can be\nconfigured to yield models commonly used in current evaluation practice, and\ngeneralize this model to also account for browsing behaviors in grid-based\nlayouts. This model provides configurable framework for estimating the\nattention that results from user browsing activity for a range of IR evaluation\nand measurement applications in multiple formats, and also identifies\nparameters that need to be estimated through user studies to provide realistic\nevaluation beyond ranked lists.",
        "translated": ""
    },
    {
        "title": "Auto Search Indexer for End-to-End Document Retrieval",
        "url": "http://arxiv.org/abs/2310.12455v1",
        "pub_date": "2023-10-19",
        "summary": "Generative retrieval, which is a new advanced paradigm for document\nretrieval, has recently attracted research interests, since it encodes all\ndocuments into the model and directly generates the retrieved documents.\nHowever, its power is still underutilized since it heavily relies on the\n\"preprocessed\" document identifiers (docids), thus limiting its retrieval\nperformance and ability to retrieve new documents. In this paper, we propose a\nnovel fully end-to-end retrieval paradigm. It can not only end-to-end learn the\nbest docids for existing and new documents automatically via a semantic\nindexing module, but also perform end-to-end document retrieval via an\nencoder-decoder-based generative model, namely Auto Search Indexer (ASI).\nBesides, we design a reparameterization mechanism to combine the above two\nmodules into a joint optimization framework. Extensive experimental results\ndemonstrate the superiority of our model over advanced baselines on both public\nand industrial datasets and also verify the ability to deal with new documents.",
        "translated": ""
    },
    {
        "title": "Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy\n  Searcher",
        "url": "http://arxiv.org/abs/2310.12443v1",
        "pub_date": "2023-10-19",
        "summary": "The advent of Large Language Models (LLMs) has shown the potential to improve\nrelevance and provide direct answers in web searches. However, challenges arise\nin validating the reliability of generated results and the credibility of\ncontributing sources, due to the limitations of traditional information\nretrieval algorithms and the LLM hallucination problem. Aiming to create a\n\"PageRank\" for the LLM era, we strive to transform LLM into a relevant,\nresponsible, and trustworthy searcher. We propose a novel generative retrieval\nframework leveraging the knowledge of LLMs to foster a direct link between\nqueries and online sources. This framework consists of three core modules:\nGenerator, Validator, and Optimizer, each focusing on generating trustworthy\nonline sources, verifying source reliability, and refining unreliable sources,\nrespectively. Extensive experiments and evaluations highlight our method's\nsuperior relevance, responsibility, and trustfulness against various SOTA\nmethods.",
        "translated": ""
    },
    {
        "title": "On Identifying Points of Semantic Shift Across Domains",
        "url": "http://arxiv.org/abs/2310.12369v1",
        "pub_date": "2023-10-18",
        "summary": "The semantics used for particular terms in an academic field organically\nevolve over time. Tracking this evolution through inspection of published\nliterature has either been from the perspective of Linguistic scholars or has\nconcentrated the focus of term evolution within a single domain of study. In\nthis paper, we performed a case study to identify semantic evolution across\ndifferent domains and identify examples of inter-domain semantic shifts. We\ninitially used keywords as the basis of our search and executed an iterative\nprocess of following citations to find the initial mention of the concepts in\nthe field. We found that a select set of keywords like ``semaphore'',\n``polymorphism'', and ``ontology'' were mentioned within Computer Science\nliterature and tracked the seminal study that borrowed those terms from\noriginal fields by citations. We marked these events as semantic evolution\npoints. Through this manual investigation method, we can identify term\nevolution across different academic fields. This study reports our initial\nfindings that will seed future automated and computational methods of\nincorporating concepts from additional academic fields.",
        "translated": ""
    },
    {
        "title": "Retrieve-Cluster-Summarize: An Alternative to End-to-End Training for\n  Query-specific Article Generation",
        "url": "http://arxiv.org/abs/2310.12361v1",
        "pub_date": "2023-10-18",
        "summary": "Query-specific article generation is the task of, given a search query,\ngenerate a single article that gives an overview of the topic. We envision such\narticles as an alternative to presenting a ranking of search results. While\ngenerative Large Language Models (LLMs) like chatGPT also address this task,\nthey are known to hallucinate new information, their models are secret, hard to\nanalyze and control. Some generative LLMs provide supporting references, yet\nthese are often unrelated to the generated content. As an alternative, we\npropose to study article generation systems that integrate document retrieval,\nquery-specific clustering, and summarization. By design, such models can\nprovide actual citations as provenance for their generated text. In particular,\nwe contribute an evaluation framework that allows to separately trains and\nevaluate each of these three components before combining them into one system.\nWe experimentally demonstrate that a system comprised of the best-performing\nindividual components also obtains the best F-1 overall system quality.",
        "translated": ""
    },
    {
        "title": "Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language",
        "url": "http://arxiv.org/abs/2310.13540v1",
        "pub_date": "2023-10-20",
        "summary": "With the thriving of pre-trained language model (PLM) widely verified in\nvarious of NLP tasks, pioneer efforts attempt to explore the possible\ncooperation of the general textual information in PLM with the personalized\nbehavioral information in user historical behavior sequences to enhance\nsequential recommendation (SR). However, despite the commonalities of input\nformat and task goal, there are huge gaps between the behavioral and textual\ninformation, which obstruct thoroughly modeling SR as language modeling via\nPLM. To bridge the gap, we propose a novel Unified pre-trained language model\nenhanced sequential recommendation (UPSR), aiming to build a unified\npre-trained recommendation model for multi-domain recommendation tasks. We\nformally design five key indicators, namely naturalness, domain consistency,\ninformativeness, noise &amp; ambiguity, and text length, to guide the text-&gt;item\nadaptation and behavior sequence-&gt;text sequence adaptation differently for\npre-training and fine-tuning stages, which are essential but under-explored by\nprevious works. In experiments, we conduct extensive evaluations on seven\ndatasets with both tuning and zero-shot settings and achieve the overall best\nperformance. Comprehensive model analyses also provide valuable insights for\nbehavior modeling via PLM, shedding light on large pre-trained recommendation\nmodels. The source codes will be released in the future.",
        "translated": ""
    },
    {
        "title": "Robust Training for Conversational Question Answering Models with\n  Reinforced Reformulation Generation",
        "url": "http://arxiv.org/abs/2310.13505v1",
        "pub_date": "2023-10-20",
        "summary": "Models for conversational question answering (ConvQA) over knowledge graphs\n(KGs) are usually trained and tested on benchmarks of gold QA pairs. This\nimplies that training is limited to surface forms seen in the respective\ndatasets, and evaluation is on a small set of held-out questions. Through our\nproposed framework REIGN, we take several steps to remedy this restricted\nlearning setup. First, we systematically generate reformulations of training\nquestions to increase robustness of models to surface form variations. This is\na particularly challenging problem, given the incomplete nature of such\nquestions. Second, we guide ConvQA models towards higher performance by feeding\nit only those reformulations that help improve their answering quality, using\ndeep reinforcement learning. Third, we demonstrate the viability of training\nmajor model components on one benchmark and applying them zero-shot to another.\nFinally, for a rigorous evaluation of robustness for trained models, we use and\nrelease large numbers of diverse reformulations generated by prompting GPT for\nbenchmark test sets (resulting in 20x increase in sizes). Our findings show\nthat ConvQA models with robust training via reformulations, significantly\noutperform those with standard training from gold QA pairs only.",
        "translated": ""
    },
    {
        "title": "Two-Stage Triplet Loss Training with Curriculum Augmentation for\n  Audio-Visual Retrieval",
        "url": "http://arxiv.org/abs/2310.13451v1",
        "pub_date": "2023-10-20",
        "summary": "The cross-modal retrieval model leverages the potential of triple loss\noptimization to learn robust embedding spaces. However, existing methods often\ntrain these models in a singular pass, overlooking the distinction between\nsemi-hard and hard triples in the optimization process. The oversight of not\ndistinguishing between semi-hard and hard triples leads to suboptimal model\nperformance. In this paper, we introduce a novel approach rooted in curriculum\nlearning to address this problem. We propose a two-stage training paradigm that\nguides the model's learning process from semi-hard to hard triplets. In the\nfirst stage, the model is trained with a set of semi-hard triplets, starting\nfrom a low-loss base. Subsequently, in the second stage, we augment the\nembeddings using an interpolation technique. This process identifies potential\nhard negatives, alleviating issues arising from high-loss functions due to a\nscarcity of hard triples. Our approach then applies hard triplet mining in the\naugmented embedding space to further optimize the model. Extensive experimental\nresults conducted on two audio-visual datasets show a significant improvement\nof approximately 9.8% in terms of average Mean Average Precision (MAP) over the\ncurrent state-of-the-art method, MSNSCA, for the Audio-Visual Cross-Modal\nRetrieval (AV-CMR) task on the AVE dataset, indicating the effectiveness of our\nproposed method.",
        "translated": ""
    },
    {
        "title": "Music Augmentation and Denoising For Peak-Based Audio Fingerprinting",
        "url": "http://arxiv.org/abs/2310.13388v1",
        "pub_date": "2023-10-20",
        "summary": "Audio fingerprinting is a well-established solution for song identification\nfrom short recording excerpts. Popular methods rely on the extraction of sparse\nrepresentations, generally spectral peaks, and have proven to be accurate,\nfast, and scalable to large collections. However, real-world applications of\naudio identification often happen in noisy environments, which can cause these\nsystems to fail. In this work, we tackle this problem by introducing and\nreleasing a new audio augmentation pipeline that adds noise to music snippets\nin a realistic way, by stochastically mimicking real-world scenarios. We then\npropose and release a deep learning model that removes noisy components from\nspectrograms in order to improve peak-based fingerprinting systems' accuracy.\nWe show that the addition of our model improves the identification performance\nof commonly used audio fingerprinting systems, even under noisy conditions.",
        "translated": ""
    },
    {
        "title": "Towards Multi-Subsession Conversational Recommendation",
        "url": "http://arxiv.org/abs/2310.13365v1",
        "pub_date": "2023-10-20",
        "summary": "Conversational recommendation systems (CRS) could acquire dynamic user\npreferences towards desired items through multi-round interactive dialogue.\nPrevious CRS mainly focuses on the single conversation (subsession) that user\nquits after a successful recommendation, neglecting the common scenario where\nuser has multiple conversations (multi-subsession) over a short period.\nTherefore, we propose a novel conversational recommendation scenario named\nMulti-Subsession Multi-round Conversational Recommendation (MSMCR), where user\nwould still resort to CRS after several subsessions and might preserve vague\ninterests, and system would proactively ask attributes to activate user\ninterests in the current subsession. To fill the gap in this new CRS scenario,\nwe devise a novel framework called Multi-Subsession Conversational Recommender\nwith Activation Attributes (MSCAA). Specifically, we first develop a\ncontext-aware recommendation module, comprehensively modeling user interests\nfrom historical interactions, previous subsessions, and feedback in the current\nsubsession. Furthermore, an attribute selection policy module is proposed to\nlearn a flexible strategy for asking appropriate attributes to elicit user\ninterests. Finally, we design a conversation policy module to manage the above\ntwo modules to decide actions between asking and recommending. Extensive\nexperiments on four datasets verify the effectiveness of our MSCAA framework\nfor the MSMCR setting.",
        "translated": ""
    },
    {
        "title": "Motif-Based Prompt Learning for Universal Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2310.13303v1",
        "pub_date": "2023-10-20",
        "summary": "Cross-Domain Recommendation (CDR) stands as a pivotal technology addressing\nissues of data sparsity and cold start by transferring general knowledge from\nthe source to the target domain. However, existing CDR models suffer\nlimitations in adaptability across various scenarios due to their inherent\ncomplexity. To tackle this challenge, recent advancements introduce universal\nCDR models that leverage shared embeddings to capture general knowledge across\ndomains and transfer it through \"Multi-task Learning\" or \"Pre-train, Fine-tune\"\nparadigms. However, these models often overlook the broader structural topology\nthat spans domains and fail to align training objectives, potentially leading\nto negative transfer. To address these issues, we propose a motif-based prompt\nlearning framework, MOP, which introduces motif-based shared embeddings to\nencapsulate generalized domain knowledge, catering to both intra-domain and\ninter-domain CDR tasks. Specifically, we devise three typical motifs:\nbutterfly, triangle, and random walk, and encode them through a Motif-based\nEncoder to obtain motif-based shared embeddings. Moreover, we train MOP under\nthe \"Pre-training \\&amp; Prompt Tuning\" paradigm. By unifying pre-training and\nrecommendation tasks as a common motif-based similarity learning task and\nintegrating adaptable prompt parameters to guide the model in downstream\nrecommendation tasks, MOP excels in transferring domain knowledge effectively.\nExperimental results on four distinct CDR tasks demonstrate the effectiveness\nof MOP than the state-of-the-art models.",
        "translated": ""
    },
    {
        "title": "VR PreM+: An Immersive Pre-learning Branching Visualization System for\n  Museum Tours",
        "url": "http://arxiv.org/abs/2310.13294v1",
        "pub_date": "2023-10-20",
        "summary": "We present VR PreM+, an innovative VR system designed to enhance web\nexploration beyond traditional computer screens. Unlike static 2D displays, VR\nPreM+ leverages 3D environments to create an immersive pre-learning experience.\nUsing keyword-based information retrieval allows users to manage and connect\nvarious content sources in a dynamic 3D space, improving communication and data\ncomparison. We conducted preliminary and user studies that demonstrated\nefficient information retrieval, increased user engagement, and a greater sense\nof presence. These findings yielded three design guidelines for future VR\ninformation systems: display, interaction, and user-centric design. VR PreM+\nbridges the gap between traditional web browsing and immersive VR, offering an\ninteractive and comprehensive approach to information acquisition. It holds\npromise for research, education, and beyond.",
        "translated": ""
    },
    {
        "title": "Unified Pretraining for Recommendation via Task Hypergraphs",
        "url": "http://arxiv.org/abs/2310.13286v1",
        "pub_date": "2023-10-20",
        "summary": "Although pretraining has garnered significant attention and popularity in\nrecent years, its application in graph-based recommender systems is relatively\nlimited. It is challenging to exploit prior knowledge by pretraining in widely\nused ID-dependent datasets. On one hand, user-item interaction history in one\ndataset can hardly be transferred to other datasets through pretraining, where\nIDs are different. On the other hand, pretraining and finetuning on the same\ndataset leads to a high risk of overfitting. In this paper, we propose a novel\nmultitask pretraining framework named Unified Pretraining for Recommendation\nvia Task Hypergraphs. For a unified learning pattern to handle diverse\nrequirements and nuances of various pretext tasks, we design task hypergraphs\nto generalize pretext tasks to hyperedge prediction. A novel transitional\nattention layer is devised to discriminatively learn the relevance between each\npretext task and recommendation. Experimental results on three benchmark\ndatasets verify the superiority of UPRTH. Additional detailed investigations\nare conducted to demonstrate the effectiveness of the proposed framework.",
        "translated": ""
    },
    {
        "title": "An Exploratory Study on Simulated Annealing for Feature Selection in\n  Learning-to-Rank",
        "url": "http://arxiv.org/abs/2310.13269v1",
        "pub_date": "2023-10-20",
        "summary": "Learning-to-rank is an applied domain of supervised machine learning. As\nfeature selection has been found to be effective for improving the accuracy of\nlearning models in general, it is intriguing to investigate this process for\nlearning-to-rank domain. In this study, we investigate the use of a popular\nmeta-heuristic approach called simulated annealing for this task. Under the\ngeneral framework of simulated annealing, we explore various neighborhood\nselection strategies and temperature cooling schemes. We further introduce a\nnew hyper-parameter called the progress parameter that can effectively be used\nto traverse the search space. Our algorithms are evaluated on five publicly\nbenchmark datasets of learning-to-rank. For a better validation, we also\ncompare the simulated annealing-based feature selection algorithm with another\neffective meta-heuristic algorithm, namely local beam search. Extensive\nexperimental results shows the efficacy of our proposed models.",
        "translated": ""
    },
    {
        "title": "A Data-Centric Multi-Objective Learning Framework for Responsible\n  Recommendation Systems",
        "url": "http://arxiv.org/abs/2310.13260v1",
        "pub_date": "2023-10-20",
        "summary": "Recommendation systems effectively guide users in locating their desired\ninformation within extensive content repositories. Generally, a recommendation\nmodel is optimized to enhance accuracy metrics from a user utility standpoint,\nsuch as click-through rate or matching relevance. However, a responsible\nindustrial recommendation system must address not only user utility\n(responsibility to users) but also other objectives, including increasing\nplatform revenue (responsibility to platforms), ensuring fairness\n(responsibility to content creators), and maintaining unbiasedness\n(responsibility to long-term healthy development). Multi-objective learning is\na potent approach for achieving responsible recommendation systems.\nNevertheless, current methods encounter two challenges: difficulty in scaling\nto heterogeneous objectives within a unified framework, and inadequate\ncontrollability over objective priority during optimization, leading to\nuncontrollable solutions.\n  In this paper, we present a data-centric optimization framework, MoRec, which\nunifies the learning of diverse objectives. MoRec is a tri-level framework: the\nouter level manages the balance between different objectives, utilizing a\nproportional-integral-derivative (PID)-based controller to ensure a preset\nregularization on the primary objective. The middle level transforms\nobjective-aware optimization into data sampling weights using sign gradients.\nThe inner level employs a standard optimizer to update model parameters with\nthe sampled data. Consequently, MoRec can flexibly support various objectives\nwhile maintaining the original model intact. Comprehensive experiments on two\npublic datasets and one industrial dataset showcase the effectiveness,\ncontrollability, flexibility, and Pareto efficiency of MoRec, making it highly\nsuitable for real-world implementation.",
        "translated": ""
    },
    {
        "title": "Budgeted Embedding Table For Recommender Systems",
        "url": "http://arxiv.org/abs/2310.14884v1",
        "pub_date": "2023-10-23",
        "summary": "At the heart of contemporary recommender systems (RSs) are latent factor\nmodels that provide quality recommendation experience to users. These models\nuse embedding vectors, which are typically of a uniform and fixed size, to\nrepresent users and items. As the number of users and items continues to grow,\nthis design becomes inefficient and hard to scale. Recent lightweight embedding\nmethods have enabled different users and items to have diverse embedding sizes,\nbut are commonly subject to two major drawbacks. Firstly, they limit the\nembedding size search to optimizing a heuristic balancing the recommendation\nquality and the memory complexity, where the trade-off coefficient needs to be\nmanually tuned for every memory budget requested. The implicitly enforced\nmemory complexity term can even fail to cap the parameter usage, making the\nresultant embedding table fail to meet the memory budget strictly. Secondly,\nmost solutions, especially reinforcement learning based ones derive and\noptimize the embedding size for each each user/item on an instance-by-instance\nbasis, which impedes the search efficiency. In this paper, we propose Budgeted\nEmbedding Table (BET), a novel method that generates table-level actions (i.e.,\nembedding sizes for all users and items) that is guaranteed to meet\npre-specified memory budgets. Furthermore, by leveraging a set-based action\nformulation and engaging set representation learning, we present an innovative\naction search strategy powered by an action fitness predictor that efficiently\nevaluates each table-level action. Experiments have shown state-of-the-art\nperformance on two real-world datasets when BET is paired with three popular\nrecommender models under different memory budgets.",
        "translated": ""
    },
    {
        "title": "DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye\n  Movement for Machine Reading",
        "url": "http://arxiv.org/abs/2310.14802v1",
        "pub_date": "2023-10-23",
        "summary": "The use of visually-rich documents (VRDs) in various fields has created a\ndemand for Document AI models that can read and comprehend documents like\nhumans, which requires the overcoming of technical, linguistic, and cognitive\nbarriers. Unfortunately, the lack of appropriate datasets has significantly\nhindered advancements in the field. To address this issue, we introduce\n\\textsc{DocTrack}, a VRD dataset really aligned with human eye-movement\ninformation using eye-tracking technology. This dataset can be used to\ninvestigate the challenges mentioned above. Additionally, we explore the impact\nof human reading order on document understanding tasks and examine what would\nhappen if a machine reads in the same order as a human. Our results suggest\nthat although Document AI models have made significant progress, they still\nhave a long way to go before they can read VRDs as accurately, continuously,\nand flexibly as humans do. These findings have potential implications for\nfuture research and development of Document AI models. The data is available at\n\\url{https://github.com/hint-lab/doctrack}.",
        "translated": ""
    },
    {
        "title": "Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue",
        "url": "http://arxiv.org/abs/2310.14626v1",
        "pub_date": "2023-10-23",
        "summary": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.",
        "translated": ""
    },
    {
        "title": "Large Search Model: Redefining Search Stack in the Era of LLMs",
        "url": "http://arxiv.org/abs/2310.14587v1",
        "pub_date": "2023-10-23",
        "summary": "Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.",
        "translated": ""
    },
    {
        "title": "CorefPrompt: Prompt-based Event Coreference Resolution by Measuring\n  Event Type and Argument Compatibilities",
        "url": "http://arxiv.org/abs/2310.14512v1",
        "pub_date": "2023-10-23",
        "summary": "Event coreference resolution (ECR) aims to group event mentions referring to\nthe same real-world event into clusters. Most previous studies adopt the\n\"encoding first, then scoring\" framework, making the coreference judgment rely\non event encoding. Furthermore, current methods struggle to leverage\nhuman-summarized ECR rules, e.g., coreferential events should have the same\nevent type, to guide the model. To address these two issues, we propose a\nprompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM\n(masked language model) task. This allows for simultaneous event modeling and\ncoreference discrimination within a single template, with a fully shared\ncontext. In addition, we introduce two auxiliary prompt tasks, event-type\ncompatibility and argument compatibility, to explicitly demonstrate the\nreasoning process of ECR, which helps the model make final predictions.\nExperimental results show that our method CorefPrompt performs well in a\nstate-of-the-art (SOTA) benchmark.",
        "translated": ""
    },
    {
        "title": "\"Why Should I Review This Paper?\" Unifying Semantic, Topic, and Citation\n  Factors for Paper-Reviewer Matching",
        "url": "http://arxiv.org/abs/2310.14483v1",
        "pub_date": "2023-10-23",
        "summary": "As many academic conferences are overwhelmed by a rapidly increasing number\nof paper submissions, automatically finding appropriate reviewers for each\nsubmission becomes a more urgent need than ever. Various factors have been\nconsidered by previous attempts on this task to measure the expertise relevance\nbetween a paper and a reviewer, including whether the paper is semantically\nclose to, shares topics with, and cites previous papers of the reviewer.\nHowever, the majority of previous studies take only one of these factors into\naccount, leading to an incomprehensive evaluation of paper-reviewer relevance.\nTo bridge this gap, in this paper, we propose a unified model for\npaper-reviewer matching that jointly captures semantic, topic, and citation\nfactors. In the unified model, a contextualized language model backbone is\nshared by all factors to learn common knowledge, while instruction tuning is\nintroduced to characterize the uniqueness of each factor by producing\nfactor-aware paper embeddings. Experiments on four datasets (one of which is\nnewly contributed by us) across different fields, including machine learning,\ncomputer vision, information retrieval, and data mining, consistently validate\nthe effectiveness of our proposed UniPR model in comparison with\nstate-of-the-art paper-reviewer matching methods and scientific pre-trained\nlanguage models.",
        "translated": ""
    },
    {
        "title": "PaRaDe: Passage Ranking using Demonstrations with Large Language Models",
        "url": "http://arxiv.org/abs/2310.14408v1",
        "pub_date": "2023-10-22",
        "summary": "Recent studies show that large language models (LLMs) can be instructed to\neffectively perform zero-shot passage re-ranking, in which the results of a\nfirst stage retrieval method, such as BM25, are rated and reordered to improve\nrelevance. In this work, we improve LLM-based re-ranking by algorithmically\nselecting few-shot demonstrations to include in the prompt. Our analysis\ninvestigates the conditions where demonstrations are most helpful, and shows\nthat adding even one demonstration is significantly beneficial. We propose a\nnovel demonstration selection strategy based on difficulty rather than the\ncommonly used semantic similarity. Furthermore, we find that demonstrations\nhelpful for ranking are also effective at question generation. We hope our work\nwill spur more principled research into question generation and passage\nranking.",
        "translated": ""
    },
    {
        "title": "Offline Metrics for Evaluating Explanation Goals in Recommender Systems",
        "url": "http://arxiv.org/abs/2310.14379v1",
        "pub_date": "2023-10-22",
        "summary": "Explanations are crucial for improving users' transparency, persuasiveness,\nengagement, and trust in Recommender Systems (RSs). However, evaluating the\neffectiveness of explanation algorithms regarding those goals remains\nchallenging due to existing offline metrics' limitations. This paper introduces\nnew metrics for the evaluation and validation of explanation algorithms based\non the items and properties used to form the sentence of an explanation.\nTowards validating the metrics, the results of three state-of-the-art post-hoc\nexplanation algorithms were evaluated for six RSs, comparing the offline\nmetrics results with those of an online user study. The findings show the\nproposed offline metrics can effectively measure the performance of explanation\nalgorithms and highlight a trade-off between the goals of transparency and\ntrust, which are related to popular properties, and the goals of engagement and\npersuasiveness, which are associated with the diversification of properties\ndisplayed to users. Furthermore, the study contributes to the development of\nmore robust evaluation methods for explanation algorithms in RSs.",
        "translated": ""
    },
    {
        "title": "Intent Contrastive Learning with Cross Subsequences for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.14318v1",
        "pub_date": "2023-10-22",
        "summary": "The user purchase behaviors are mainly influenced by their intentions (e.g.,\nbuying clothes for decoration, buying brushes for painting, etc.). Modeling a\nuser's latent intention can significantly improve the performance of\nrecommendations. Previous works model users' intentions by considering the\npredefined label in auxiliary information or introducing stochastic data\naugmentation to learn purposes in the latent space. However, the auxiliary\ninformation is sparse and not always available for recommender systems, and\nintroducing stochastic data augmentation may introduce noise and thus change\nthe intentions hidden in the sequence. Therefore, leveraging user intentions\nfor sequential recommendation (SR) can be challenging because they are\nfrequently varied and unobserved. In this paper, Intent contrastive learning\nwith Cross Subsequences for sequential Recommendation (ICSRec) is proposed to\nmodel users' latent intentions. Specifically, ICSRec first segments a user's\nsequential behaviors into multiple subsequences by using a dynamic sliding\noperation and takes these subsequences into the encoder to generate the\nrepresentations for the user's intentions. To tackle the problem of no explicit\nlabels for purposes, ICSRec assumes different subsequences with the same target\nitem may represent the same intention and proposes a coarse-grain intent\ncontrastive learning to push these subsequences closer. Then, fine-grain intent\ncontrastive learning is mentioned to capture the fine-grain intentions of\nsubsequences in sequential behaviors. Extensive experiments conducted on four\nreal-world datasets demonstrate the superior performance of the proposed ICSRec\nmodel compared with baseline methods.",
        "translated": ""
    },
    {
        "title": "One Model for All: Large Language Models are Domain-Agnostic\n  Recommendation Systems",
        "url": "http://arxiv.org/abs/2310.14304v1",
        "pub_date": "2023-10-22",
        "summary": "The purpose of sequential recommendation is to utilize the interaction\nhistory of a user and predict the next item that the user is most likely to\ninteract with. While data sparsity and cold start are two challenges that most\nrecommender systems are still facing, many efforts are devoted to utilizing\ndata from other domains, called cross-domain methods. However, general\ncross-domain methods explore the relationship between two domains by designing\ncomplex model architecture, making it difficult to scale to multiple domains\nand utilize more data. Moreover, existing recommendation systems use IDs to\nrepresent item, which carry less transferable signals in cross-domain\nscenarios, and user cross-domain behaviors are also sparse, making it\nchallenging to learn item relationship from different domains. These problems\nhinder the application of multi-domain methods to sequential recommendation.\nRecently, large language models (LLMs) exhibit outstanding performance in world\nknowledge learning from text corpora and general-purpose question answering.\nInspired by these successes, we propose a simple but effective framework for\ndomain-agnostic recommendation by exploiting the pre-trained LLMs (namely\nLLM-Rec). We mix the user's behavior across different domains, and then\nconcatenate the title information of these items into a sentence and model the\nuser's behaviors with a pre-trained language model. We expect that by mixing\nthe user's behaviors across different domains, we can exploit the common\nknowledge encoded in the pre-trained language model to alleviate the problems\nof data sparsity and cold start problems. Furthermore, we are curious about\nwhether the latest technical advances in nature language processing (NLP) can\ntransfer to the recommendation scenarios.",
        "translated": ""
    },
    {
        "title": "Representation Learning with Large Language Models for Recommendation",
        "url": "http://arxiv.org/abs/2310.15950v1",
        "pub_date": "2023-10-24",
        "summary": "Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.",
        "translated": ""
    },
    {
        "title": "Topology-aware Debiased Self-supervised Graph Learning for\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.15858v1",
        "pub_date": "2023-10-24",
        "summary": "In recommendation, graph-based Collaborative Filtering (CF) methods mitigate\nthe data sparsity by introducing Graph Contrastive Learning (GCL). However, the\nrandom negative sampling strategy in these GCL-based CF models neglects the\nsemantic structure of users (items), which not only introduces false negatives\n(negatives that are similar to anchor user (item)) but also ignores the\npotential positive samples. To tackle the above issues, we propose\nTopology-aware Debiased Self-supervised Graph Learning (TDSGL) for\nrecommendation, which constructs contrastive pairs according to the semantic\nsimilarity between users (items). Specifically, since the original user-item\ninteraction data commendably reflects the purchasing intent of users and\ncertain characteristics of items, we calculate the semantic similarity between\nusers (items) on interaction data. Then, given a user (item), we construct its\nnegative pairs by selecting users (items) which embed different semantic\nstructures to ensure the semantic difference between the given user (item) and\nits negatives. Moreover, for a user (item), we design a feature extraction\nmodule that converts other semantically similar users (items) into an auxiliary\npositive sample to acquire a more informative representation. Experimental\nresults show that the proposed model outperforms the state-of-the-art models\nsignificantly on three public datasets. Our model implementation codes are\navailable at https://github.com/malajikuai/TDSGL.",
        "translated": ""
    },
    {
        "title": "A statistical significance testing approach for measuring term\n  burstiness with applications to domain-specific terminology extraction",
        "url": "http://arxiv.org/abs/2310.15790v1",
        "pub_date": "2023-10-24",
        "summary": "Domain-specific terminology extraction is an important task in text analysis.\nA term in a corpus is said to be \"bursty\" when its occurrences are concentrated\nin few out of many documents. Being content rich, bursty terms are highly\nsuited for subject matter characterization, and serve as natural candidates for\nidentifying with technical terminology. Multiple measures of term burstiness\nhave been proposed in the literature. However, the statistical significance\ntesting paradigm has remained underexplored in text analysis, including in\nrelation to term burstiness. To test these waters, we propose as our main\ncontribution a multinomial language model-based exact test of statistical\nsignificance for term burstiness. Due to its prohibitive computational cost, we\nadvance a heuristic formula designed to serve as a proxy for test P-values. As\na complementary theoretical contribution, we derive a previously unreported\nrelationship connecting the inverse document frequency and inverse collection\nfrequency (two foundational quantities in text analysis) under the multinomial\nlanguage model. The relation is used in the evaluation of our heuristic. Using\nthe GENIA Term corpus benchmark, we compare our approach against established\nmethods, demonstrating our heuristic's potential in identifying domain-specific\ntechnical terms. We hope this demonstration of statistical significance testing\nin text analysis serves as a springboard for future research.",
        "translated": ""
    },
    {
        "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for\n  Inference Cost Reduction",
        "url": "http://arxiv.org/abs/2310.15556v1",
        "pub_date": "2023-10-24",
        "summary": "Since ChatGPT released its API for public use, the number of applications\nbuilt on top of commercial large language models (LLMs) increase exponentially.\nOne popular usage of such models is leveraging its in-context learning ability\nand generating responses given user queries leveraging knowledge obtained by\nretrieval augmentation. One problem of deploying commercial retrieval-augmented\nLLMs is the cost due to the additionally retrieved context that largely\nincreases the input token size of the LLMs. To mitigate this, we propose a\ntoken compression scheme that includes two methods: summarization compression\nand semantic compression. The first method applies a T5-based model that is\nfine-tuned by datasets generated using self-instruct containing samples with\nvarying lengths and reduce token size by doing summarization. The second method\nfurther compresses the token size by removing words with lower impact on the\nsemantic. In order to adequately evaluate the effectiveness of the proposed\nmethods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)\nfocusing on food recommendation for women around pregnancy period or infants.\nOur summarization compression can reduce 65% of the retrieval token size with\nfurther 0.3% improvement on the accuracy; semantic compression provides a more\nflexible way to trade-off the token size with performance, for which we can\nreduce the token size by 20% with only 1.6% of accuracy drop.",
        "translated": ""
    },
    {
        "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2310.15511v1",
        "pub_date": "2023-10-24",
        "summary": "We study the ability of state-of-the art models to answer constraint\nsatisfaction queries for information retrieval (e.g., 'a list of ice cream\nshops in San Diego'). In the past, such queries were considered to be tasks\nthat could only be solved via web-search or knowledge bases. More recently,\nlarge language models (LLMs) have demonstrated initial emergent abilities in\nthis task. However, many current retrieval benchmarks are either saturated or\ndo not measure constraint satisfaction. Motivated by rising concerns around\nfactual incorrectness and hallucinations of LLMs, we present KITAB, a new\ndataset for measuring constraint satisfaction abilities of language models.\nKITAB consists of book-related data across more than 600 authors and 13,000\nqueries, and also offers an associated dynamic data collection and constraint\nverification approach for acquiring similar test data for other authors. Our\nextended experiments on GPT4 and GPT3.5 characterize and decouple common\nfailure modes across dimensions such as information popularity, constraint\ntypes, and context availability. Results show that in the absence of context,\nmodels exhibit severe limitations as measured by irrelevant information,\nfactual errors, and incompleteness, many of which exacerbate as information\npopularity decreases. While context availability mitigates irrelevant\ninformation, it is not helpful for satisfying constraints, identifying\nfundamental barriers to constraint satisfaction. We open source our\ncontributions to foster further research on improving constraint satisfaction\nabilities of future models.",
        "translated": ""
    },
    {
        "title": "Robust Representation Learning for Unified Online Top-K Recommendation",
        "url": "http://arxiv.org/abs/2310.15492v1",
        "pub_date": "2023-10-24",
        "summary": "In large-scale industrial e-commerce, the efficiency of an online\nrecommendation system is crucial in delivering highly relevant item/content\nadvertising that caters to diverse business scenarios. However, most existing\nstudies focus solely on item advertising, neglecting the significance of\ncontent advertising. This oversight results in inconsistencies within the\nmulti-entity structure and unfair retrieval. Furthermore, the challenge of\nretrieving top-k advertisements from multi-entity advertisements across\ndifferent domains adds to the complexity. Recent research proves that\nuser-entity behaviors within different domains exhibit characteristics of\ndifferentiation and homogeneity. Therefore, the multi-domain matching models\ntypically rely on the hybrid-experts framework with domain-invariant and\ndomain-specific representations. Unfortunately, most approaches primarily focus\non optimizing the combination mode of different experts, failing to address the\ninherent difficulty in optimizing the expert modules themselves. The existence\nof redundant information across different domains introduces interference and\ncompetition among experts, while the distinct learning objectives of each\ndomain lead to varying optimization challenges among experts. To tackle these\nissues, we propose robust representation learning for the unified online top-k\nrecommendation. Our approach constructs unified modeling in entity space to\nensure data fairness. The robust representation learning employs domain\nadversarial learning and multi-view wasserstein distribution learning to learn\nrobust representations. Moreover, the proposed method balances conflicting\nobjectives through the homoscedastic uncertainty weights and orthogonality\nconstraints. Various experiments validate the effectiveness and rationality of\nour proposed method, which has been successfully deployed online to serve real\nbusiness scenarios.",
        "translated": ""
    },
    {
        "title": "Off-Policy Evaluation for Large Action Spaces via Policy Convolution",
        "url": "http://arxiv.org/abs/2310.15433v1",
        "pub_date": "2023-10-24",
        "summary": "Developing accurate off-policy estimators is crucial for both evaluating and\noptimizing for new policies. The main challenge in off-policy estimation is the\ndistribution shift between the logging policy that generates data and the\ntarget policy that we aim to evaluate. Typically, techniques for correcting\ndistribution shift involve some form of importance sampling. This approach\nresults in unbiased value estimation but often comes with the trade-off of high\nvariance, even in the simpler case of one-step contextual bandits. Furthermore,\nimportance sampling relies on the common support assumption, which becomes\nimpractical when the action space is large. To address these challenges, we\nintroduce the Policy Convolution (PC) family of estimators. These methods\nleverage latent structure within actions -- made available through action\nembeddings -- to strategically convolve the logging and target policies. This\nconvolution introduces a unique bias-variance trade-off, which can be\ncontrolled by adjusting the amount of convolution. Our experiments on synthetic\nand benchmark datasets demonstrate remarkable mean squared error (MSE)\nimprovements when using PC, especially when either the action space or policy\nmismatch becomes large, with gains of up to 5 - 6 orders of magnitude over\nexisting estimators.",
        "translated": ""
    },
    {
        "title": "Towards Hybrid-grained Feature Interaction Selection for Deep Sparse\n  Network",
        "url": "http://arxiv.org/abs/2310.15342v1",
        "pub_date": "2023-10-23",
        "summary": "Deep sparse networks are widely investigated as a neural network architecture\nfor prediction tasks with high-dimensional sparse features, with which feature\ninteraction selection is a critical component. While previous methods primarily\nfocus on how to search feature interaction in a coarse-grained space, less\nattention has been given to a finer granularity. In this work, we introduce a\nhybrid-grained feature interaction selection approach that targets both feature\nfield and feature value for deep sparse networks. To explore such expansive\nspace, we propose a decomposed space which is calculated on the fly. We then\ndevelop a selection algorithm called OptFeature, which efficiently selects the\nfeature interaction from both the feature field and the feature value\nsimultaneously. Results from experiments on three large real-world benchmark\ndatasets demonstrate that OptFeature performs well in terms of accuracy and\nefficiency. Additional studies support the feasibility of our method.",
        "translated": ""
    },
    {
        "title": "Triple Simplex Matrix Completion for Expense Forecasting",
        "url": "http://arxiv.org/abs/2310.15275v1",
        "pub_date": "2023-10-23",
        "summary": "Forecasting project expenses is a crucial step for businesses to avoid budget\noverruns and project failures. Traditionally, this has been done by financial\nanalysts or data science techniques such as time-series analysis. However,\nthese approaches can be uncertain and produce results that differ from the\nplanned budget, especially at the start of a project with limited data points.\nThis paper proposes a constrained non-negative matrix completion model that\npredicts expenses by learning the likelihood of the project correlating with\ncertain expense patterns in the latent space. The model is constrained on three\nprobability simplexes, two of which are on the factor matrices and the third on\nthe missing entries. Additionally, the predicted expense values are guaranteed\nto meet the budget constraint without the need of post-processing. An inexact\nalternating optimization algorithm is developed to solve the associated\noptimization problem and is proven to converge to a stationary point. Results\nfrom two real datasets demonstrate the effectiveness of the proposed method in\ncomparison to state-of-the-art algorithms.",
        "translated": ""
    },
    {
        "title": "Improving Conversational Recommendation Systems via Bias Analysis and\n  Language-Model-Enhanced Data Augmentation",
        "url": "http://arxiv.org/abs/2310.16738v1",
        "pub_date": "2023-10-25",
        "summary": "Conversational Recommendation System (CRS) is a rapidly growing research area\nthat has gained significant attention alongside advancements in language\nmodelling techniques. However, the current state of conversational\nrecommendation faces numerous challenges due to its relative novelty and\nlimited existing contributions. In this study, we delve into benchmark datasets\nfor developing CRS models and address potential biases arising from the\nfeedback loop inherent in multi-turn interactions, including selection bias and\nmultiple popularity bias variants. Drawing inspiration from the success of\ngenerative data via using language models and data augmentation techniques, we\npresent two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model\nperformance while mitigating biases. Through extensive experiments on ReDial\nand TG-ReDial benchmark datasets, we show a consistent improvement of CRS\ntechniques with our data augmentation approaches and offer additional insights\non addressing multiple newly formulated biases.",
        "translated": ""
    },
    {
        "title": "Exploring Large Language Models for Code Explanation",
        "url": "http://arxiv.org/abs/2310.16673v1",
        "pub_date": "2023-10-25",
        "summary": "Automating code documentation through explanatory text can prove highly\nbeneficial in code understanding. Large Language Models (LLMs) have made\nremarkable strides in Natural Language Processing, especially within software\nengineering tasks such as code generation and code summarization. This study\nspecifically delves into the task of generating natural-language summaries for\ncode snippets, using various LLMs. The findings indicate that Code LLMs\noutperform their generic counterparts, and zero-shot methods yield superior\nresults when dealing with datasets with dissimilar distributions between\ntraining and testing sets.",
        "translated": ""
    },
    {
        "title": "Distributionally Robust Unsupervised Dense Retrieval Training on Web\n  Graphs",
        "url": "http://arxiv.org/abs/2310.16605v1",
        "pub_date": "2023-10-25",
        "summary": "This paper introduces Web-DRO, an unsupervised dense retrieval model, which\nclusters documents based on web structures and reweights the groups during\ncontrastive training. Specifically, we first leverage web graph links and\ncontrastively train an embedding model for clustering anchor-document pairs.\nThen we use Group Distributional Robust Optimization to reweight different\nclusters of anchor-document pairs, which guides the model to assign more\nweights to the group with higher contrastive loss and pay more attention to the\nworst case during training. Our experiments on MS MARCO and BEIR show that our\nmodel, Web-DRO, significantly improves the retrieval effectiveness in\nunsupervised scenarios. A comparison of clustering techniques shows that\ntraining on the web graph combining URL information reaches optimal performance\non clustering. Further analysis confirms that group weights are stable and\nvalid, indicating consistent model preferences as well as effective\nup-weighting of valuable groups and down-weighting of uninformative ones. The\ncode of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "translated": ""
    },
    {
        "title": "Model-enhanced Contrastive Reinforcement Learning for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.16566v1",
        "pub_date": "2023-10-25",
        "summary": "Reinforcement learning (RL) has been widely applied in recommendation systems\ndue to its potential in optimizing the long-term engagement of users. From the\nperspective of RL, recommendation can be formulated as a Markov decision\nprocess (MDP), where recommendation system (agent) can interact with users\n(environment) and acquire feedback (reward signals).However, it is impractical\nto conduct online interactions with the concern on user experience and\nimplementation complexity, and we can only train RL recommenders with offline\ndatasets containing limited reward signals and state transitions. Therefore,\nthe data sparsity issue of reward signals and state transitions is very severe,\nwhile it has long been overlooked by existing RL recommenders.Worse still, RL\nmethods learn through the trial-and-error mode, but negative feedback cannot be\nobtained in implicit feedback recommendation tasks, which aggravates the\noverestimation problem of offline RL recommender. To address these challenges,\nwe propose a novel RL recommender named model-enhanced contrastive\nreinforcement learning (MCRL). On the one hand, we learn a value function to\nestimate the long-term engagement of users, together with a conservative value\nlearning mechanism to alleviate the overestimation problem.On the other hand,\nwe construct some positive and negative state-action pairs to model the reward\nfunction and state transition function with contrastive learning to exploit the\ninternal structure information of MDP. Experiments demonstrate that the\nproposed method significantly outperforms existing offline RL and\nself-supervised RL methods with different representative backbone networks on\ntwo real-world datasets.",
        "translated": ""
    },
    {
        "title": "Faithful Path Language Modelling for Explainable Recommendation over\n  Knowledge Graph",
        "url": "http://arxiv.org/abs/2310.16452v1",
        "pub_date": "2023-10-25",
        "summary": "Path reasoning methods over knowledge graphs have gained popularity for their\npotential to improve transparency in recommender systems. However, the\nresulting models still rely on pre-trained knowledge graph embeddings, fail to\nfully exploit the interdependence between entities and relations in the KG for\nrecommendation, and may generate inaccurate explanations. In this paper, we\nintroduce PEARLM, a novel approach that efficiently captures user behaviour and\nproduct-side knowledge through language modelling. With our approach, knowledge\ngraph embeddings are directly learned from paths over the KG by the language\nmodel, which also unifies entities and relations in the same optimisation\nspace. Constraints on the sequence decoding additionally guarantee path\nfaithfulness with respect to the KG. Experiments on two datasets show the\neffectiveness of our approach compared to state-of-the-art baselines. Source\ncode and datasets: AVAILABLE AFTER GETTING ACCEPTED.",
        "translated": ""
    },
    {
        "title": "Multiple Key-value Strategy in Recommendation Systems Incorporating\n  Large Language Model",
        "url": "http://arxiv.org/abs/2310.16409v1",
        "pub_date": "2023-10-25",
        "summary": "Recommendation system (RS) plays significant roles in matching users\ninformation needs for Internet applications, and it usually utilizes the\nvanilla neural network as the backbone to handle embedding details. Recently,\nthe large language model (LLM) has exhibited emergent abilities and achieved\ngreat breakthroughs both in the CV and NLP communities. Thus, it is logical to\nincorporate RS with LLM better, which has become an emerging research\ndirection. Although some existing works have made their contributions to this\nissue, they mainly consider the single key situation (e.g. historical\ninteractions), especially in sequential recommendation. The situation of\nmultiple key-value data is simply neglected. This significant scenario is\nmainstream in real practical applications, where the information of users (e.g.\nage, occupation, etc) and items (e.g. title, category, etc) has more than one\nkey. Therefore, we aim to implement sequential recommendations based on\nmultiple key-value data by incorporating RS with LLM. In particular, we\ninstruct tuning a prevalent open-source LLM (Llama 7B) in order to inject\ndomain knowledge of RS into the pre-trained LLM. Since we adopt multiple\nkey-value strategies, LLM is hard to learn well among these keys. Thus the\ngeneral and innovative shuffle and mask strategies, as an innovative manner of\ndata argument, are designed. To demonstrate the effectiveness of our approach,\nextensive experiments are conducted on the popular and suitable dataset\nMovieLens which contains multiple keys-value. The experimental results\ndemonstrate that our approach can nicely and effectively complete this\nchallenging issue.",
        "translated": ""
    },
    {
        "title": "URL-BERT: Training Webpage Representations via Social Media Engagements",
        "url": "http://arxiv.org/abs/2310.16303v1",
        "pub_date": "2023-10-25",
        "summary": "Understanding and representing webpages is crucial to online social networks\nwhere users may share and engage with URLs. Common language model (LM) encoders\nsuch as BERT can be used to understand and represent the textual content of\nwebpages. However, these representations may not model thematic information of\nweb domains and URLs or accurately capture their appeal to social media users.\nIn this work, we introduce a new pre-training objective that can be used to\nadapt LMs to understand URLs and webpages. Our proposed framework consists of\ntwo steps: (1) scalable graph embeddings to learn shallow representations of\nURLs based on user engagement on social media and (2) a contrastive objective\nthat aligns LM representations with the aforementioned graph-based\nrepresentation. We apply our framework to the multilingual version of BERT to\nobtain the model URL-BERT. We experimentally demonstrate that our continued\npre-training approach improves webpage understanding on a variety of tasks and\nTwitter internal and external benchmarks.",
        "translated": ""
    },
    {
        "title": "Context-aware feature attribution through argumentation",
        "url": "http://arxiv.org/abs/2310.16157v1",
        "pub_date": "2023-10-24",
        "summary": "Feature attribution is a fundamental task in both machine learning and data\nanalysis, which involves determining the contribution of individual features or\nvariables to a model's output. This process helps identify the most important\nfeatures for predicting an outcome. The history of feature attribution methods\ncan be traced back to General Additive Models (GAMs), which extend linear\nregression models by incorporating non-linear relationships between dependent\nand independent variables. In recent years, gradient-based methods and\nsurrogate models have been applied to unravel complex Artificial Intelligence\n(AI) systems, but these methods have limitations. GAMs tend to achieve lower\naccuracy, gradient-based methods can be difficult to interpret, and surrogate\nmodels often suffer from stability and fidelity issues. Furthermore, most\nexisting methods do not consider users' contexts, which can significantly\ninfluence their preferences. To address these limitations and advance the\ncurrent state-of-the-art, we define a novel feature attribution framework\ncalled Context-Aware Feature Attribution Through Argumentation (CA-FATA). Our\nframework harnesses the power of argumentation by treating each feature as an\nargument that can either support, attack or neutralize a prediction.\nAdditionally, CA-FATA formulates feature attribution as an argumentation\nprocedure, and each computation has explicit semantics, which makes it\ninherently interpretable. CA-FATA also easily integrates side information, such\nas users' contexts, resulting in more accurate predictions.",
        "translated": ""
    },
    {
        "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model\n  System for Answering Medical Questions using Scientific Literature",
        "url": "http://arxiv.org/abs/2310.16146v1",
        "pub_date": "2023-10-24",
        "summary": "The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.",
        "translated": ""
    },
    {
        "title": "Context-aware explainable recommendations over knowledge graphs",
        "url": "http://arxiv.org/abs/2310.16141v1",
        "pub_date": "2023-10-24",
        "summary": "Knowledge graphs contain rich semantic relationships related to items and\nincorporating such semantic relationships into recommender systems helps to\nexplore the latent connections of items, thus improving the accuracy of\nprediction and enhancing the explainability of recommendations. However, such\nexplainability is not adapted to users' contexts, which can significantly\ninfluence their preferences. In this work, we propose CA-KGCN (Context-Aware\nKnowledge Graph Convolutional Network), an end-to-end framework that can model\nusers' preferences adapted to their contexts and can incorporate rich semantic\nrelationships in the knowledge graph related to items. This framework captures\nusers' attention to different factors: contexts and features of items. More\nspecifically, the framework can model users' preferences adapted to their\ncontexts and provide explanations adapted to the given context. Experiments on\nthree real-world datasets show the effectiveness of our framework: modeling\nusers' preferences adapted to their contexts and explaining the recommendations\ngenerated.",
        "translated": ""
    },
    {
        "title": "LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset",
        "url": "http://arxiv.org/abs/2310.17609v1",
        "pub_date": "2023-10-26",
        "summary": "As an important component of intelligent legal systems, legal case retrieval\nplays a critical role in ensuring judicial justice and fairness. However, the\ndevelopment of legal case retrieval technologies in the Chinese legal system is\nrestricted by three problems in existing datasets: limited data size, narrow\ndefinitions of legal relevance, and naive candidate pooling strategies used in\ndata sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale\nLegal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192\ncandidates extracted from 4.3 million criminal case documents. To the best of\nour knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval\ndatasets, providing extensive coverage of criminal charges. Additionally, we\nenrich the existing relevance criteria by considering three key aspects:\ncharacterization, penalty, procedure. This comprehensive criteria enriches the\ndataset and may provides a more holistic perspective. Furthermore, we propose a\ntwo-level candidate set pooling strategy that effectively identify potential\ncandidates for each query case. It's important to note that all cases in the\ndataset have been annotated by multiple legal experts specializing in criminal\nlaw. Their expertise ensures the accuracy and reliability of the annotations.\nWe evaluate several state-of-the-art retrieval models at LeCaRDv2,\ndemonstrating that there is still significant room for improvement in legal\ncase retrieval. The details of LeCaRDv2 can be found at the anonymous website\nhttps://github.com/anonymous1113243/LeCaRDv2.",
        "translated": ""
    },
    {
        "title": "LightLM: A Lightweight Deep and Narrow Language Model for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.17488v1",
        "pub_date": "2023-10-26",
        "summary": "This paper presents LightLM, a lightweight Transformer-based language model\nfor generative recommendation. While Transformer-based generative modeling has\ngained importance in various AI sub-fields such as NLP and vision, generative\nrecommendation is still in its infancy due to its unique demand on personalized\ngenerative modeling. Existing works on generative recommendation often use\nNLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are\nheavy-weight and are not specifically designed for recommendation tasks.\nLightLM tackles the issue by introducing a light-weight deep and narrow\nTransformer architecture, which is specifically tailored for direct generation\nof recommendation items. This structure is especially apt for straightforward\ngenerative recommendation and stems from the observation that language model\ndoes not have to be too wide for this task, as the input predominantly consists\nof short tokens that are well-suited for the model's capacity. We also show\nthat our devised user and item ID indexing methods, i.e., Spectral\nCollaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables\nthe deep and narrow Transformer architecture to outperform large-scale language\nmodels for recommendation. Besides, to address the hallucination problem of\ngenerating items as output, we propose the constrained generation process for\ngenerative recommenders. Experiments on real-world datasets show that LightLM\noutperforms various competitive baselines in terms of both recommendation\naccuracy and efficiency. The code can be found at\nhttps://github.com/dongyuanjushi/LightLM.",
        "translated": ""
    },
    {
        "title": "FMMRec: Fairness-aware Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2310.17373v1",
        "pub_date": "2023-10-26",
        "summary": "Recently, multimodal recommendations have gained increasing attention for\neffectively addressing the data sparsity problem by incorporating\nmodality-based representations. Although multimodal recommendations excel in\naccuracy, the introduction of different modalities (e.g., images, text, and\naudio) may expose more users' sensitive information (e.g., gender and age) to\nrecommender systems, resulting in potentially more serious unfairness issues.\nDespite many efforts on fairness, existing fairness-aware methods are either\nincompatible with multimodal scenarios, or lead to suboptimal fairness\nperformance due to neglecting sensitive information of multimodal content. To\nachieve counterfactual fairness in multimodal recommendations, we propose a\nnovel fairness-aware multimodal recommendation approach (dubbed as FMMRec) to\ndisentangle the sensitive and non-sensitive information from modal\nrepresentations and leverage the disentangled modal representations to guide\nfairer representation learning. Specifically, we first disentangle biased and\nfiltered modal representations by maximizing and minimizing their sensitive\nattribute prediction ability respectively. With the disentangled modal\nrepresentations, we mine the modality-based unfair and fair (corresponding to\nbiased and filtered) user-user structures for enhancing explicit user\nrepresentation with the biased and filtered neighbors from the corresponding\nstructures, followed by adversarially filtering out sensitive information.\nExperiments on two real-world public datasets demonstrate the superiority of\nour FMMRec relative to the state-of-the-art baselines. Our source code is\navailable at https://anonymous.4open.science/r/FMMRec.",
        "translated": ""
    },
    {
        "title": "Exploring the Potential of Generative AI for the World Wide Web",
        "url": "http://arxiv.org/abs/2310.17370v1",
        "pub_date": "2023-10-26",
        "summary": "Generative Artificial Intelligence (AI) is a cutting-edge technology capable\nof producing text, images, and various media content leveraging generative\nmodels and user prompts. Between 2022 and 2023, generative AI surged in\npopularity with a plethora of applications spanning from AI-powered movies to\nchatbots. In this paper, we delve into the potential of generative AI within\nthe realm of the World Wide Web, specifically focusing on image generation. Web\ndevelopers already harness generative AI to help crafting text and images,\nwhile Web browsers might use it in the future to locally generate images for\ntasks like repairing broken webpages, conserving bandwidth, and enhancing\nprivacy. To explore this research area, we have developed WebDiffusion, a tool\nthat allows to simulate a Web powered by stable diffusion, a popular\ntext-to-image model, from both a client and server perspective. WebDiffusion\nfurther supports crowdsourcing of user opinions, which we use to evaluate the\nquality and accuracy of 409 AI-generated images sourced from 60 webpages. Our\nfindings suggest that generative AI is already capable of producing pertinent\nand high-quality Web images, even without requiring Web designers to manually\ninput prompts, just by leveraging contextual information available within the\nwebpages. However, we acknowledge that direct in-browser image generation\nremains a challenge, as only highly powerful GPUs, such as the A40 and A100,\ncan (partially) compete with classic image downloads. Nevertheless, this\napproach could be valuable for a subset of the images, for example when fixing\nbroken webpages or handling highly private content.",
        "translated": ""
    },
    {
        "title": "On Surgical Fine-tuning for Language Encoders",
        "url": "http://arxiv.org/abs/2310.17041v1",
        "pub_date": "2023-10-25",
        "summary": "Fine-tuning all the layers of a pre-trained neural language encoder (either\nusing all the parameters or using parameter-efficient methods) is often the\nde-facto way of adapting it to a new task. We show evidence that for different\ndownstream language tasks, fine-tuning only a subset of layers is sufficient to\nobtain performance that is close to and often better than fine-tuning all the\nlayers in the language encoder. We propose an efficient metric based on the\ndiagonal of the Fisher information matrix (FIM score), to select the candidate\nlayers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE\ntasks and across distinct language encoders, that this metric can effectively\nselect layers leading to a strong downstream performance. Our work highlights\nthat task-specific information corresponding to a given downstream task is\noften localized within a few layers, and tuning only those is sufficient for\nstrong performance. Additionally, we demonstrate the robustness of the FIM\nscore to rank layers in a manner that remains constant during the optimization\nprocess.",
        "translated": ""
    },
    {
        "title": "The Word2vec Graph Model for Author Attribution and Genre Detection in\n  Literary Analysis",
        "url": "http://arxiv.org/abs/2310.16972v1",
        "pub_date": "2023-10-25",
        "summary": "Analyzing the writing styles of authors and articles is a key to supporting\nvarious literary analyses such as author attribution and genre detection. Over\nthe years, rich sets of features that include stylometry, bag-of-words, n-grams\nhave been widely used to perform such analysis. However, the effectiveness of\nthese features largely depends on the linguistic aspects of a particular\nlanguage and datasets specific characteristics. Consequently, techniques based\non these feature sets cannot give desired results across domains. In this\npaper, we propose a novel Word2vec graph based modeling of a document that can\nrightly capture both context and style of the document. By using these Word2vec\ngraph based features, we perform classification to perform author attribution\nand genre detection tasks. Our detailed experimental study with a comprehensive\nset of literary writings shows the effectiveness of this method over\ntraditional feature based approaches. Our code and data are publicly available\nat https://cutt.ly/svLjSgk",
        "translated": ""
    },
    {
        "title": "Text2Bundle: Towards Personalized Query-based Bundle Generation",
        "url": "http://arxiv.org/abs/2310.18004v1",
        "pub_date": "2023-10-27",
        "summary": "Bundle generation aims to provide a bundle of items for the user, and has\nbeen widely studied and applied on online service platforms. Existing bundle\ngeneration methods mainly utilized user's preference from historical\ninteractions in common recommendation paradigm, and ignored the potential\ntextual query which is user's current explicit intention. There can be a\nscenario in which a user proactively queries a bundle with some natural\nlanguage description, the system should be able to generate a bundle that\nexactly matches the user's intention through the user's query and preferences.\nIn this work, we define this user-friendly scenario as Query-based Bundle\nGeneration task and propose a novel framework Text2Bundle that leverages both\nthe user's short-term interests from the query and the user's long-term\npreferences from the historical interactions. Our framework consists of three\nmodules: (1) a query interest extractor that mines the user's fine-grained\ninterests from the query; (2) a unified state encoder that learns the current\nbundle context state and the user's preferences based on historical interaction\nand current query; and (3) a bundle generator that generates personalized and\ncomplementary bundles using a reinforcement learning with specifically designed\nrewards. We conduct extensive experiments on three real-world datasets and\ndemonstrate the effectiveness of our framework compared with several\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Chain-of-Choice Hierarchical Policy Learning for Conversational\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.17922v1",
        "pub_date": "2023-10-27",
        "summary": "Conversational Recommender Systems (CRS) illuminate user preferences via\nmulti-round interactive dialogues, ultimately navigating towards precise and\nsatisfactory recommendations. However, contemporary CRS are limited to\ninquiring binary or multi-choice questions based on a single attribute type\n(e.g., color) per round, which causes excessive rounds of interaction and\ndiminishes the user's experience. To address this, we propose a more realistic\nand efficient conversational recommendation problem setting, called\nMulti-Type-Attribute Multi-round Conversational Recommendation (MTAMCR), which\nenables CRS to inquire about multi-choice questions covering multiple types of\nattributes in each round, thereby improving interactive efficiency. Moreover,\nby formulating MTAMCR as a hierarchical reinforcement learning task, we propose\na Chain-of-Choice Hierarchical Policy Learning (CoCHPL) framework to enhance\nboth the questioning efficiency and recommendation effectiveness in MTAMCR.\nSpecifically, a long-term policy over options (i.e., ask or recommend)\ndetermines the action type, while two short-term intra-option policies\nsequentially generate the chain of attributes or items through multi-step\nreasoning and selection, optimizing the diversity and interdependence of\nquestioning attributes. Finally, extensive experiments on four benchmarks\ndemonstrate the superior performance of CoCHPL over prevailing state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "Ranking with Slot Constraints",
        "url": "http://arxiv.org/abs/2310.17870v1",
        "pub_date": "2023-10-27",
        "summary": "We introduce the problem of ranking with slot constraints, which can be used\nto model a wide range of application problems -- from college admission with\nlimited slots for different majors, to composing a stratified cohort of\neligible participants in a medical trial. We show that the conventional\nProbability Ranking Principle (PRP) can be highly sub-optimal for\nslot-constrained ranking problems, and we devise a new ranking algorithm,\ncalled MatchRank. The goal of MatchRank is to produce rankings that maximize\nthe number of filled slots if candidates are evaluated by a human decision\nmaker in the order of the ranking. In this way, MatchRank generalizes the PRP,\nand it subsumes the PRP as a special case when there are no slot constraints.\nOur theoretical analysis shows that MatchRank has a strong approximation\nguarantee without any independence assumptions between slots or candidates.\nFurthermore, we show how MatchRank can be implemented efficiently. Beyond the\ntheoretical guarantees, empirical evaluations show that MatchRank can provide\nsubstantial improvements over a range of synthetic and real-world tasks.",
        "translated": ""
    },
    {
        "title": "GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value\n  in Similar Item Recommendation",
        "url": "http://arxiv.org/abs/2310.17732v1",
        "pub_date": "2023-10-26",
        "summary": "Similar item recommendation is a critical task in the e-Commerce industry,\nwhich helps customers explore similar and relevant alternatives based on their\ninterested products. Despite the traditional machine learning models, Graph\nNeural Networks (GNNs), by design, can understand complex relations like\nsimilarity between products. However, in contrast to their wide usage in\nretrieval tasks and their focus on optimizing the relevance, the current GNN\narchitectures are not tailored toward maximizing revenue-related objectives\nsuch as Gross Merchandise Value (GMV), which is one of the major business\nmetrics for e-Commerce companies. In addition, defining accurate edge relations\nin GNNs is non-trivial in large-scale e-Commerce systems, due to the\nheterogeneity nature of the item-item relationships. This work aims to address\nthese issues by designing a new GNN architecture called GNN-GMVO (Graph Neural\nNetwork - Gross Merchandise Value Optimizer). This model directly optimizes GMV\nwhile considering the complex relations between items. In addition, we propose\na customized edge construction method to tailor the model toward similar item\nrecommendation task and alleviate the noisy and complex item-item relations. In\nour comprehensive experiments on three real-world datasets, we show higher\nprediction performance and expected GMV for top ranked items recommended by our\nmodel when compared with selected state-of-the-art benchmark models.",
        "translated": ""
    },
    {
        "title": "Poisoning Retrieval Corpora by Injecting Adversarial Passages",
        "url": "http://arxiv.org/abs/2310.19156v1",
        "pub_date": "2023-10-29",
        "summary": "Dense retrievers have achieved state-of-the-art performance in various\ninformation retrieval tasks, but to what extent can they be safely deployed in\nreal-world applications? In this work, we propose a novel attack for dense\nretrieval systems in which a malicious user generates a small number of\nadversarial passages by perturbing discrete tokens to maximize similarity with\na provided set of training queries. When these adversarial passages are\ninserted into a large retrieval corpus, we show that this attack is highly\neffective in fooling these systems to retrieve them for queries that were not\nseen by the attacker. More surprisingly, these adversarial passages can\ndirectly generalize to out-of-domain queries and corpora with a high success\nattack rate -- for instance, we find that 50 generated passages optimized on\nNatural Questions can mislead &gt;94% of questions posed in financial documents or\nonline forums. We also benchmark and compare a range of state-of-the-art dense\nretrievers, both unsupervised and supervised. Although different systems\nexhibit varying levels of vulnerability, we show they can all be successfully\nattacked by injecting up to 500 passages, a small fraction compared to a\nretrieval corpus of millions of passages.",
        "translated": ""
    },
    {
        "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query\n  Expansion",
        "url": "http://arxiv.org/abs/2310.19056v1",
        "pub_date": "2023-10-29",
        "summary": "Query expansion is a commonly-used technique in many search systems to better\nrepresent users' information needs with additional query terms. Existing\nstudies for this task usually propose to expand a query with retrieved or\ngenerated contextual documents. However, both types of methods have clear\nlimitations. For retrieval-based methods, the documents retrieved with the\noriginal query might not be accurate enough to reveal the search intent,\nespecially when the query is brief or ambiguous. For generation-based methods,\nexisting models can hardly be trained or aligned on a particular corpus, due to\nthe lack of corpus-specific labeled data. In this paper, we propose a novel\nLarge Language Model (LLM) based mutual verification framework for query\nexpansion, which alleviates the aforementioned limitations. Specifically, we\nfirst design a query-query-document generation pipeline, which can effectively\nleverage the contextual knowledge encoded in LLMs to generate sub-queries and\ncorresponding documents from multiple perspectives. Next, we employ a mutual\nverification method for both generated and retrieved contextual documents,\nwhere 1) retrieved documents are filtered with the external contextual\nknowledge in generated documents, and 2) generated documents are filtered with\nthe corpus-specific knowledge in retrieved documents. Overall, the proposed\nmethod allows retrieved and generated documents to complement each other to\nfinalize a better query expansion. We conduct extensive experiments on three\ninformation retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.\nThe results demonstrate that our method outperforms other baselines\nsignificantly.",
        "translated": ""
    },
    {
        "title": "A Multimodal Ecological Civilization Pattern Recommendation Method Based\n  on Large Language Models and Knowledge Graph",
        "url": "http://arxiv.org/abs/2310.18951v1",
        "pub_date": "2023-10-29",
        "summary": "The Ecological Civilization Pattern Recommendation System (ECPRS) aims to\nrecommend suitable ecological civilization patterns for target regions,\npromoting sustainable development and reducing regional disparities. However,\nthe current representative recommendation methods are not suitable for\nrecommending ecological civilization patterns in a geographical context. There\nare two reasons for this. Firstly, regions have spatial heterogeneity, and the\n(ECPRS)needs to consider factors like climate, topography, vegetation, etc., to\nrecommend civilization patterns adapted to specific ecological environments,\nensuring the feasibility and practicality of the recommendations. Secondly, the\nabstract features of the ecological civilization patterns in the real world\nhave not been fully utilized., resulting in poor richness in their embedding\nrepresentations and consequently, lower performance of the recommendation\nsystem. Considering these limitations, we propose the ECPR-MML method.\nInitially, based on the novel method UGPIG, we construct a knowledge graph to\nextract regional representations incorporating spatial heterogeneity features.\nFollowing that, inspired by the significant progress made by Large Language\nModels (LLMs) in the field of Natural Language Processing (NLP), we employ\nLarge LLMs to generate multimodal features for ecological civilization patterns\nin the form of text and images. We extract and integrate these multimodal\nfeatures to obtain semantically rich representations of ecological\ncivilization. Through extensive experiments, we validate the performance of our\nECPR-MML model. Our results show that F1@5 is 2.11% higher compared to\nstate-of-the-art models, 2.02% higher than NGCF, and 1.16% higher than UGPIG.\nFurthermore, multimodal data can indeed enhance recommendation performance.\nHowever, the data generated by LLM is not as effective as real data to a\ncertain extent.",
        "translated": ""
    },
    {
        "title": "The diminishing state of shared reality on US television news",
        "url": "http://arxiv.org/abs/2310.18863v1",
        "pub_date": "2023-10-29",
        "summary": "The potential for a large, diverse population to coexist peacefully is\nthought to depend on the existence of a ``shared reality:'' a public sphere in\nwhich participants are exposed to similar facts about similar topics. A\ngeneration ago, broadcast television news was widely considered to serve this\nfunction; however, since the rise of cable news in the 1990s, critics and\nscholars have worried that the corresponding fragmentation and segregation of\naudiences along partisan lines has caused this shared reality to be lost. Here\nwe examine this concern using a unique combination of data sets tracking the\nproduction (since 2012) and consumption (since 2016) of television news content\non the three largest cable and broadcast networks respectively. With regard to\nproduction, we find strong evidence for the ``loss of shared reality\nhypothesis:'' while broadcast continues to cover similar topics with similar\nlanguage, cable news networks have become increasingly distinct, both from\nbroadcast news and each other, diverging both in terms of content and language.\nWith regard to consumption, we find more mixed evidence: while broadcast news\nhas indeed declined in popularity, it remains the dominant source of news for\nroughly 50\\% more Americans than does cable; moreover, its decline, while\nsomewhat attributable to cable, appears driven more by a shift away from news\nconsumption altogether than a growth in cable consumption. We conclude that\nshared reality on US television news is indeed diminishing, but is more robust\nthan previously thought and is declining for somewhat different reasons.",
        "translated": ""
    },
    {
        "title": "Leveraging Multimodal Features and Item-level User Feedback for Bundle\n  Construction",
        "url": "http://arxiv.org/abs/2310.18770v1",
        "pub_date": "2023-10-28",
        "summary": "Automatic bundle construction is a crucial prerequisite step in various\nbundle-aware online services. Previous approaches are mostly designed to model\nthe bundling strategy of existing bundles. However, it is hard to acquire\nlarge-scale well-curated bundle dataset, especially for those platforms that\nhave not offered bundle services before. Even for platforms with mature bundle\nservices, there are still many items that are included in few or even zero\nbundles, which give rise to sparsity and cold-start challenges in the bundle\nconstruction models. To tackle these issues, we target at leveraging multimodal\nfeatures, item-level user feedback signals, and the bundle composition\ninformation, to achieve a comprehensive formulation of bundle construction.\nNevertheless, such formulation poses two new technical challenges: 1) how to\nlearn effective representations by optimally unifying multiple features, and 2)\nhow to address the problems of modality missing, noise, and sparsity problems\ninduced by the incomplete query bundles. In this work, to address these\ntechnical challenges, we propose a Contrastive Learning-enhanced Hierarchical\nEncoder method (CLHE). Specifically, we use self-attention modules to combine\nthe multimodal and multi-item features, and then leverage both item- and\nbundle-level contrastive learning to enhance the representation learning, thus\nto counter the modality missing, noise, and sparsity problems. Extensive\nexperiments on four datasets in two application domains demonstrate that our\nmethod outperforms a list of SOTA methods. The code and dataset are available\nat https://github.com/Xiaohao-Liu/CLHE.",
        "translated": ""
    },
    {
        "title": "Empowering Collaborative Filtering with Principled Adversarial\n  Contrastive Loss",
        "url": "http://arxiv.org/abs/2310.18700v1",
        "pub_date": "2023-10-28",
        "summary": "Contrastive Learning (CL) has achieved impressive performance in\nself-supervised learning tasks, showing superior generalization ability.\nInspired by the success, adopting CL into collaborative filtering (CF) is\nprevailing in semi-supervised top-K recommendations. The basic idea is to\nroutinely conduct heuristic-based data augmentation and apply contrastive\nlosses (e.g., InfoNCE) on the augmented views. Yet, some CF-tailored challenges\nmake this adoption suboptimal, such as the issue of out-of-distribution, the\nrisk of false negatives, and the nature of top-K evaluation. They necessitate\nthe CL-based CF scheme to focus more on mining hard negatives and\ndistinguishing false negatives from the vast unlabeled user-item interactions,\nfor informative contrast signals. Worse still, there is limited understanding\nof contrastive loss in CF methods, especially w.r.t. its generalization\nability. To bridge the gap, we delve into the reasons underpinning the success\nof contrastive loss in CF, and propose a principled Adversarial InfoNCE loss\n(AdvInfoNCE), which is a variant of InfoNCE, specially tailored for CF methods.\nAdvInfoNCE adaptively explores and assigns hardness to each negative instance\nin an adversarial fashion and further utilizes a fine-grained hardness-aware\nranking criterion to empower the recommender's generalization ability. Training\nCF models with AdvInfoNCE, we validate the effectiveness of AdvInfoNCE on both\nsynthetic and real-world benchmark datasets, thus showing its generalization\nability to mitigate out-of-distribution problems. Given the theoretical\nguarantees and empirical superiority of AdvInfoNCE over most contrastive loss\nfunctions, we advocate its adoption as a standard loss in recommender systems,\nparticularly for the out-of-distribution tasks. Codes are available at\nhttps://github.com/LehengTHU/AdvInfoNCE.",
        "translated": ""
    },
    {
        "title": "Dense Retrieval as Indirect Supervision for Large-space Decision Making",
        "url": "http://arxiv.org/abs/2310.18619v1",
        "pub_date": "2023-10-28",
        "summary": "Many discriminative natural language understanding (NLU) tasks have large\nlabel spaces. Learning such a process of large-space decision making is\nparticularly challenging due to the lack of training instances per label and\nthe difficulty of selection among many fine-grained labels. Inspired by dense\nretrieval methods for passage finding in open-domain QA, we propose a\nreformulation of large-space discriminative NLU tasks as a learning-to-retrieve\ntask, leading to a novel solution named Dense Decision Retrieval (DDR ).\nInstead of predicting fine-grained decisions as logits, DDR adopts a\ndual-encoder architecture that learns to predict by retrieving from a decision\nthesaurus. This approach not only leverages rich indirect supervision signals\nfrom easy-to-consume learning resources for dense retrieval, it also leads to\nenhanced prediction generalizability with a semantically meaningful\nrepresentation of the large decision space. When evaluated on tasks with\ndecision spaces ranging from hundreds to hundred-thousand scales, DDR\noutperforms strong baselines greatly by 27.54% in P@1 on two extreme\nmulti-label classification tasks, 1.17% in F1 score ultra-fine entity typing,\nand 1.26% in accuracy on three few-shot intent classification tasks on average.\nCode and resources are available at https://github.com/luka-group/DDR",
        "translated": ""
    },
    {
        "title": "Embedding in Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2310.18608v1",
        "pub_date": "2023-10-28",
        "summary": "Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that coverts the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors and can enhance\nthe recommendation performance. Applying embedding techniques captures complex\nentity relationships and has spurred substantial research. In this survey, we\nprovide an overview of the recent literature on embedding techniques in\nrecommender systems. This survey covers embedding methods like collaborative\nfiltering, self-supervised learning, and graph-based techniques. Collaborative\nfiltering generates embeddings capturing user-item preferences, excelling in\nsparse data. Self-supervised methods leverage contrastive or generative\nlearning for various tasks. Graph-based techniques like node2vec exploit\ncomplex relationships in network-rich environments. Addressing the scalability\nchallenges inherent to embedding methods, our survey delves into innovative\ndirections within the field of recommendation systems. These directions aim to\nenhance performance and reduce computational complexity, paving the way for\nimproved recommender systems. Among these innovative approaches, we will\nintroduce Auto Machine Learning (AutoML), hash techniques, and quantization\ntechniques in this survey. We discuss various architectures and techniques and\nhighlight the challenges and future directions in these aspects. This survey\naims to provide a comprehensive overview of the state-of-the-art in this\nrapidly evolving field and serve as a useful resource for researchers and\npractitioners working in the area of recommender systems.",
        "translated": ""
    },
    {
        "title": "GATSY: Graph Attention Network for Music Artist Similarity",
        "url": "http://arxiv.org/abs/2311.00635v1",
        "pub_date": "2023-11-01",
        "summary": "The artist similarity quest has become a crucial subject in social and\nscientific contexts. Modern research solutions facilitate music discovery\naccording to user tastes. However, defining similarity among artists may\ninvolve several aspects, even related to a subjective perspective, and it often\naffects a recommendation. This paper presents GATSY, a recommendation system\nbuilt upon graph attention networks and driven by a clusterized embedding of\nartists. The proposed framework takes advantage of a graph topology of the\ninput data to achieve outstanding performance results without relying heavily\non hand-crafted features. This flexibility allows us to introduce fictitious\nartists in a music dataset, create bridges to previously unrelated artists, and\nget recommendations conditioned by possibly heterogeneous sources. Experimental\nresults prove the effectiveness of the proposed method with respect to\nstate-of-the-art solutions.",
        "translated": ""
    },
    {
        "title": "A Collaborative Filtering-Based Two Stage Model with Item Dependency for\n  Course Recommendation",
        "url": "http://arxiv.org/abs/2311.00612v1",
        "pub_date": "2023-11-01",
        "summary": "Recommender systems have been studied for decades with numerous promising\nmodels been proposed. Among them, Collaborative Filtering (CF) models are\narguably the most successful one due to its high accuracy in recommendation and\nelimination of privacy-concerned personal meta-data from training. This paper\nextends the usage of CF-based model to the task of course recommendation. We\npoint out several challenges in applying the existing CF-models to build a\ncourse recommendation engine, including the lack of rating and meta-data, the\nimbalance of course registration distribution, and the demand of course\ndependency modeling. We then propose several ideas to address these challenges.\nEventually, we combine a two-stage CF model regularized by course dependency\nwith a graph-based recommender based on course-transition network, to achieve\nAUC as high as 0.97 with a real-world dataset.",
        "translated": ""
    },
    {
        "title": "Bayes-enhanced Multi-view Attention Networks for Robust POI\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.00491v1",
        "pub_date": "2023-11-01",
        "summary": "POI recommendation is practically important to facilitate various\nLocation-Based Social Network services, and has attracted rising research\nattention recently. Existing works generally assume the available POI check-ins\nreported by users are the ground-truth depiction of user behaviors. However, in\nreal application scenarios, the check-in data can be rather unreliable due to\nboth subjective and objective causes including positioning error and user\nprivacy concerns, leading to significant negative impacts on the performance of\nthe POI recommendation. To this end, we investigate a novel problem of robust\nPOI recommendation by considering the uncertainty factors of the user\ncheck-ins, and proposes a Bayes-enhanced Multi-view Attention Network.\nSpecifically, we construct personal POI transition graph, the semantic-based\nPOI graph and distance-based POI graph to comprehensively model the\ndependencies among the POIs. As the personal POI transition graph is usually\nsparse and sensitive to noise, we design a Bayes-enhanced spatial dependency\nlearning module for data augmentation from the local view. A Bayesian posterior\nguided graph augmentation approach is adopted to generate a new graph with\ncollaborative signals to increase the data diversity. Then both the original\nand the augmented graphs are used for POI representation learning to counteract\nthe data uncertainty issue. Next, the POI representations of the three view\ngraphs are input into the proposed multi-view attention-based user preference\nlearning module. By incorporating the semantic and distance correlations of\nPOIs, the user preference can be effectively refined and finally robust\nrecommendation results are achieved. The results of extensive experiments show\nthat BayMAN significantly outperforms the state-of-the-art methods in POI\nrecommendation when the available check-ins are incomplete and noisy.",
        "translated": ""
    },
    {
        "title": "LLMRec: Large Language Models with Graph Augmentation for Recommendation",
        "url": "http://arxiv.org/abs/2311.00423v1",
        "pub_date": "2023-11-01",
        "summary": "The problem of data sparsity has long been a challenge in recommendation\nsystems, and previous studies have attempted to address this issue by\nincorporating side information. However, this approach often introduces side\neffects such as noise, availability issues, and low data quality, which in turn\nhinder the accurate modeling of user preferences and adversely impact\nrecommendation performance. In light of the recent advancements in large\nlanguage models (LLMs), which possess extensive knowledge bases and strong\nreasoning capabilities, we propose a novel framework called LLMRec that\nenhances recommender systems by employing three simple yet effective LLM-based\ngraph augmentation strategies. Our approach leverages the rich content\navailable within online platforms (e.g., Netflix, MovieLens) to augment the\ninteraction graph in three ways: (i) reinforcing user-item interaction egde,\n(ii) enhancing the understanding of item node attributes, and (iii) conducting\nuser node profiling, intuitively from the natural language perspective. By\nemploying these strategies, we address the challenges posed by sparse implicit\nfeedback and low-quality side information in recommenders. Besides, to ensure\nthe quality of the augmentation, we develop a denoised data robustification\nmechanism that includes techniques of noisy implicit feedback pruning and\nMAE-based feature enhancement that help refine the augmented data and improve\nits reliability. Furthermore, we provide theoretical analysis to support the\neffectiveness of LLMRec and clarify the benefits of our method in facilitating\nmodel optimization. Experimental results on benchmark datasets demonstrate the\nsuperiority of our LLM-based augmentation approach over state-of-the-art\ntechniques. To ensure reproducibility, we have made our code and augmented data\npublicly available at: https://github.com/HKUDS/LLMRec.git",
        "translated": ""
    },
    {
        "title": "Towards Automatic Sampling of User Behaviors for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2311.00388v1",
        "pub_date": "2023-11-01",
        "summary": "Sequential recommender systems (SRS) have gained widespread popularity in\nrecommendation due to their ability to effectively capture dynamic user\npreferences. One default setting in the current SRS is to uniformly consider\neach historical behavior as a positive interaction. Actually, this setting has\nthe potential to yield sub-optimal performance, as each item makes a distinct\ncontribution to the user's interest. For example, purchased items should be\ngiven more importance than clicked ones. Hence, we propose a general automatic\nsampling framework, named AutoSAM, to non-uniformly treat historical behaviors.\nSpecifically, AutoSAM augments the standard sequential recommendation\narchitecture with an additional sampler layer to adaptively learn the skew\ndistribution of the raw input, and then sample informative sub-sets to build\nmore generalizable SRS. To overcome the challenges of non-differentiable\nsampling actions and also introduce multiple decision factors for sampling, we\nfurther introduce a novel reinforcement learning based method to guide the\ntraining of the sampler. We theoretically design multi-objective sampling\nrewards including Future Prediction and Sequence Perplexity, and then optimize\nthe whole framework in an end-to-end manner by combining the policy gradient.\nWe conduct extensive experiments on benchmark recommender models and four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nthe proposed approach. We will make our code publicly available after the\nacceptance.",
        "translated": ""
    },
    {
        "title": "Caseformer: Pre-training for Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2311.00333v1",
        "pub_date": "2023-11-01",
        "summary": "Legal case retrieval aims to help legal workers find relevant cases related\nto their cases at hand, which is important for the guarantee of fairness and\njustice in legal judgments. While recent advances in neural retrieval methods\nhave significantly improved the performance of open-domain retrieval tasks\n(e.g., Web search), their advantages have not been observed in legal case\nretrieval due to their thirst for annotated data. As annotating large-scale\ntraining data in legal domains is prohibitive due to the need for domain\nexpertise, traditional search techniques based on lexical matching such as\nTF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval\nsystems. While previous studies have designed several pre-training methods for\nIR models in open-domain tasks, these methods are usually suboptimal in legal\ncase retrieval because they cannot understand and capture the key knowledge and\ndata structures in the legal corpus. To this end, we propose a novel\npre-training framework named Caseformer that enables the pre-trained models to\nlearn legal knowledge and domain-specific relevance information in legal case\nretrieval without any human-labeled data. Through three unsupervised learning\ntasks, Caseformer is able to capture the special language, document structure,\nand relevance patterns of legal case documents, making it a strong backbone for\ndownstream legal case retrieval tasks. Experimental results show that our model\nhas achieved state-of-the-art performance in both zero-shot and full-data\nfine-tuning settings. Also, experiments on both Chinese and English legal\ndatasets demonstrate that the effectiveness of Caseformer is\nlanguage-independent in legal case retrieval.",
        "translated": ""
    },
    {
        "title": "Federated Topic Model and Model Pruning Based on Variational Autoencoder",
        "url": "http://arxiv.org/abs/2311.00314v1",
        "pub_date": "2023-11-01",
        "summary": "Topic modeling has emerged as a valuable tool for discovering patterns and\ntopics within large collections of documents. However, when cross-analysis\ninvolves multiple parties, data privacy becomes a critical concern. Federated\ntopic modeling has been developed to address this issue, allowing multiple\nparties to jointly train models while protecting pri-vacy. However, there are\ncommunication and performance challenges in the federated sce-nario. In order\nto solve the above problems, this paper proposes a method to establish a\nfederated topic model while ensuring the privacy of each node, and use neural\nnetwork model pruning to accelerate the model, where the client periodically\nsends the model neu-ron cumulative gradients and model weights to the server,\nand the server prunes the model. To address different requirements, two\ndifferent methods are proposed to determine the model pruning rate. The first\nmethod involves slow pruning throughout the entire model training process,\nwhich has limited acceleration effect on the model training process, but can\nensure that the pruned model achieves higher accuracy. This can significantly\nreduce the model inference time during the inference process. The second\nstrategy is to quickly reach the target pruning rate in the early stage of\nmodel training in order to accelerate the model training speed, and then\ncontinue to train the model with a smaller model size after reaching the target\npruning rate. This approach may lose more useful information but can complete\nthe model training faster. Experimental results show that the federated topic\nmodel pruning based on the variational autoencoder proposed in this paper can\ngreatly accelerate the model training speed while ensuring the model's\nperformance.",
        "translated": ""
    },
    {
        "title": "DistDNAS: Search Efficient Feature Interactions within 2 Hours",
        "url": "http://arxiv.org/abs/2311.00231v1",
        "pub_date": "2023-11-01",
        "summary": "Search efficiency and serving efficiency are two major axes in building\nfeature interactions and expediting the model development process in\nrecommender systems. On large-scale benchmarks, searching for the optimal\nfeature interaction design requires extensive cost due to the sequential\nworkflow on the large volume of data. In addition, fusing interactions of\nvarious sources, orders, and mathematical operations introduces potential\nconflicts and additional redundancy toward recommender models, leading to\nsub-optimal trade-offs in performance and serving cost. In this paper, we\npresent DistDNAS as a neat solution to brew swift and efficient feature\ninteraction design. DistDNAS proposes a supernet to incorporate interaction\nmodules of varying orders and types as a search space. To optimize search\nefficiency, DistDNAS distributes the search and aggregates the choice of\noptimal interaction modules on varying data dates, achieving over 25x speed-up\nand reducing search cost from 2 days to 2 hours. To optimize serving\nefficiency, DistDNAS introduces a differentiable cost-aware loss to penalize\nthe selection of redundant interaction modules, enhancing the efficiency of\ndiscovered feature interactions in serving. We extensively evaluate the best\nmodels crafted by DistDNAS on a 1TB Criteo Terabyte dataset. Experimental\nevaluations demonstrate 0.001 AUC improvement and 60% FLOPs saving over current\nstate-of-the-art CTR models.",
        "translated": ""
    },
    {
        "title": "Farthest Greedy Path Sampling for Two-shot Recommender Search",
        "url": "http://arxiv.org/abs/2310.20705v1",
        "pub_date": "2023-10-31",
        "summary": "Weight-sharing Neural Architecture Search (WS-NAS) provides an efficient\nmechanism for developing end-to-end deep recommender models. However, in\ncomplex search spaces, distinguishing between superior and inferior\narchitectures (or paths) is challenging. This challenge is compounded by the\nlimited coverage of the supernet and the co-adaptation of subnet weights, which\nrestricts the exploration and exploitation capabilities inherent to\nweight-sharing mechanisms. To address these challenges, we introduce Farthest\nGreedy Path Sampling (FGPS), a new path sampling strategy that balances path\nquality and diversity. FGPS enhances path diversity to facilitate more\ncomprehensive supernet exploration, while emphasizing path quality to ensure\nthe effective identification and utilization of promising architectures. By\nincorporating FGPS into a Two-shot NAS (TS-NAS) framework, we derive\nhigh-performance architectures. Evaluations on three Click-Through Rate (CTR)\nprediction benchmarks demonstrate that our approach consistently achieves\nsuperior results, outperforming both manually designed and most NAS-based\nmodels.",
        "translated": ""
    },
    {
        "title": "Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding",
        "url": "http://arxiv.org/abs/2310.20588v1",
        "pub_date": "2023-10-31",
        "summary": "In the era of the Internet of Things (IoT), the retrieval of relevant medical\ninformation has become essential for efficient clinical decision-making. This\npaper introduces MedFusionRank, a novel approach to zero-shot medical\ninformation retrieval (MIR) that combines the strengths of pre-trained language\nmodels and statistical methods while addressing their limitations. The proposed\napproach leverages a pre-trained BERT-style model to extract compact yet\ninformative keywords. These keywords are then enriched with domain knowledge by\nlinking them to conceptual entities within a medical knowledge graph.\nExperimental evaluations on medical datasets demonstrate MedFusion Rank's\nsuperior performance over existing methods, with promising results with a\nvariety of evaluation metrics. MedFusionRank demonstrates efficacy in\nretrieving relevant information, even from short or single-term queries.",
        "translated": ""
    },
    {
        "title": "Collaborative Large Language Model for Recommender Systems",
        "url": "http://arxiv.org/abs/2311.01343v1",
        "pub_date": "2023-11-02",
        "summary": "Recently, there is a growing interest in developing next-generation\nrecommender systems (RSs) based on pretrained large language models (LLMs),\nfully utilizing their encoded knowledge and reasoning ability. However, the\nsemantic gap between natural language and recommendation tasks is still not\nwell addressed, leading to multiple issues such as spuriously-correlated\nuser/item descriptors, ineffective language modeling on user/item contents, and\ninefficient recommendations via auto-regression, etc. In this paper, we propose\nCLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and\nID paradigm of RS, aiming to address the above challenges simultaneously. We\nfirst extend the vocabulary of pretrained LLMs with user/item ID tokens to\nfaithfully model the user/item collaborative and content semantics.\nAccordingly, in the pretraining stage, a novel soft+hard prompting strategy is\nproposed to effectively learn user/item collaborative/content token embeddings\nvia language modeling on RS-specific corpora established from user-item\ninteractions and user/item features, where each document is split into a prompt\nconsisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and\na main text consisting of homogeneous item tokens or vocab tokens that\nfacilitates stable and effective language modeling. In addition, a novel mutual\nregularization strategy is introduced to encourage the CLLM4Rec to capture\nrecommendation-oriented information from user/item contents. Finally, we\npropose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where\nan item prediction head with multinomial likelihood is added to the pretrained\nCLLM4Rec backbone to predict hold-out items based on the soft+hard prompts\nestablished from masked user-item interaction history, where recommendations of\nmultiple items can be generated efficiently.",
        "translated": ""
    },
    {
        "title": "Recommendations by Concise User Profiles from Review Text",
        "url": "http://arxiv.org/abs/2311.01314v1",
        "pub_date": "2023-11-02",
        "summary": "Recommender systems are most successful for popular items and users with\nample interactions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of supporting users who have very sparse interactions but\npost informative review texts. Our experimental studies address two book\ncommunities with these characteristics. We design a framework with\nTransformer-based representation learning, covering user-item interactions,\nitem content, and user-provided reviews. To overcome interaction sparseness, we\ndevise techniques for selecting the most informative cues to construct concise\nuser profiles. Comprehensive experiments, with datasets from Amazon and\nGoodreads, show that judicious selection of text snippets achieves the best\nperformance, even in comparison to ChatGPT-generated user profiles.",
        "translated": ""
    },
    {
        "title": "VM-Rec: A Variational Mapping Approach for Cold-start User\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.01304v1",
        "pub_date": "2023-11-02",
        "summary": "The cold-start problem is a common challenge for most recommender systems.\nWith extremely limited interactions of cold-start users, conventional\nrecommender models often struggle to generate embeddings with sufficient\nexpressivity. Moreover, the absence of auxiliary content information of users\nexacerbates the presence of challenges, rendering most cold-start methods\ndifficult to apply. To address this issue, our motivation is based on the\nobservation that if a model can generate expressive embeddings for existing\nusers with relatively more interactions, who were also initially cold-start\nusers, then we can establish a mapping from few initial interactions to\nexpressive embeddings, simulating the process of generating embeddings for\ncold-start users. Based on this motivation, we propose a Variational Mapping\napproach for cold-start user Recommendation (VM-Rec). Firstly, we generate a\npersonalized mapping function for cold-start users based on their initial\ninteractions, and parameters of the function are generated from a variational\ndistribution. For the sake of interpretability and computational efficiency, we\nmodel the personalized mapping function as a sparse linear model, where each\nparameter indicates the association to a specific existing user. Consequently,\nwe use this mapping function to map the embeddings of existing users to an\nembedding of the cold-start user in the same space. The resulting embedding has\nsimilar expressivity to that of existing users and can be directly integrated\ninto a pre-trained recommender model to predict click through rates or ranking\nscores. We evaluate our method based on three widely used recommender models as\npre-trained base recommender models, outperforming four popular cold-start\nmethods on two datasets under the same base model.",
        "translated": ""
    },
    {
        "title": "Efficient Neural Ranking using Forward Indexes and Lightweight Encoders",
        "url": "http://arxiv.org/abs/2311.01263v1",
        "pub_date": "2023-11-02",
        "summary": "Dual-encoder-based dense retrieval models have become the standard in IR.\nThey employ large Transformer-based language models, which are notoriously\ninefficient in terms of resources and latency. We propose Fast-Forward indexes\n-- vector forward indexes which exploit the semantic matching capabilities of\ndual-encoder models for efficient and effective re-ranking. Our framework\nenables re-ranking at very high retrieval depths and combines the merits of\nboth lexical and semantic matching via score interpolation. Furthermore, in\norder to mitigate the limitations of dual-encoders, we tackle two main\nchallenges: Firstly, we improve computational efficiency by either\npre-computing representations, avoiding unnecessary computations altogether, or\nreducing the complexity of encoders. This allows us to considerably improve\nranking efficiency and latency. Secondly, we optimize the memory footprint and\nmaintenance cost of indexes; we propose two complementary techniques to reduce\nthe index size and show that, by dynamically dropping irrelevant document\ntokens, the index maintenance efficiency can be improved substantially. We\nperform evaluation to show the effectiveness and efficiency of Fast-Forward\nindexes -- our method has low latency and achieves competitive results without\nthe need for hardware acceleration, such as GPUs.",
        "translated": ""
    },
    {
        "title": "Navigating Complex Search Tasks with AI Copilots",
        "url": "http://arxiv.org/abs/2311.01235v1",
        "pub_date": "2023-11-02",
        "summary": "As many of us in the information retrieval (IR) research community know and\nappreciate, search is far from being a solved problem. Millions of people\nstruggle with tasks on search engines every day. Often, their struggles relate\nto the intrinsic complexity of their task and the failure of search systems to\nfully understand the task and serve relevant results. The task motivates the\nsearch, creating the gap/problematic situation that searchers attempt to\nbridge/resolve and drives search behavior as they work through different task\nfacets. Complex search tasks require more than support for rudimentary fact\nfinding or re-finding. Research on methods to support complex tasks includes\nwork on generating query and website suggestions, personalizing and\ncontextualizing search, and developing new search experiences, including those\nthat span time and space. The recent emergence of generative artificial\nintelligence (AI) and the arrival of assistive agents, or copilots, based on\nthis technology, has the potential to offer further assistance to searchers,\nespecially those engaged in complex tasks. There are profound implications from\nthese advances for the design of intelligent systems and for the future of\nsearch itself. This article, based on a keynote by the author at the 2023 ACM\nSIGIR Conference, explores these issues and charts a course toward new horizons\nin information access guided by AI copilots.",
        "translated": ""
    },
    {
        "title": "Bi-Preference Learning Heterogeneous Hypergraph Networks for\n  Session-based Recommendation",
        "url": "http://arxiv.org/abs/2311.01125v1",
        "pub_date": "2023-11-02",
        "summary": "Session-based recommendation intends to predict next purchased items based on\nanonymous behavior sequences. Numerous economic studies have revealed that item\nprice is a key factor influencing user purchase decisions. Unfortunately,\nexisting methods for session-based recommendation only aim at capturing user\ninterest preference, while ignoring user price preference. Actually, there are\nprimarily two challenges preventing us from accessing price preference.\nFirstly, the price preference is highly associated to various item features\n(i.e., category and brand), which asks us to mine price preference from\nheterogeneous information. Secondly, price preference and interest preference\nare interdependent and collectively determine user choice, necessitating that\nwe jointly consider both price and interest preference for intent modeling. To\nhandle above challenges, we propose a novel approach Bi-Preference Learning\nHeterogeneous Hypergraph Networks (BiPNet) for session-based recommendation.\nSpecifically, the customized heterogeneous hypergraph networks with a\ntriple-level convolution are devised to capture user price and interest\npreference from heterogeneous features of items. Besides, we develop a\nBi-Preference Learning schema to explore mutual relations between price and\ninterest preference and collectively learn these two preferences under the\nmulti-task learning architecture. Extensive experiments on multiple public\ndatasets confirm the superiority of BiPNet over competitive baselines.\nAdditional research also supports the notion that the price is crucial for the\ntask.",
        "translated": ""
    },
    {
        "title": "Collaboration and Transition: Distilling Item Transitions into\n  Multi-Query Self-Attention for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2311.01056v1",
        "pub_date": "2023-11-02",
        "summary": "Modern recommender systems employ various sequential modules such as\nself-attention to learn dynamic user interests. However, these methods are less\neffective in capturing collaborative and transitional signals within user\ninteraction sequences. First, the self-attention architecture uses the\nembedding of a single item as the attention query, which is inherently\nchallenging to capture collaborative signals. Second, these methods typically\nfollow an auto-regressive framework, which is unable to learn global item\ntransition patterns. To overcome these limitations, we propose a new method\ncalled Multi-Query Self-Attention with Transition-Aware Embedding Distillation\n(MQSA-TED). First, we propose an $L$-query self-attention module that employs\nflexible window sizes for attention queries to capture collaborative signals.\nIn addition, we introduce a multi-query self-attention method that balances the\nbias-variance trade-off in modeling user preferences by combining long and\nshort-query self-attentions. Second, we develop a transition-aware embedding\ndistillation module that distills global item-to-item transition patterns into\nitem embeddings, which enables the model to memorize and leverage transitional\nsignals and serves as a calibrator for collaborative signals. Experimental\nresults on four real-world datasets show the superiority of our proposed method\nover state-of-the-art sequential recommendation methods.",
        "translated": ""
    },
    {
        "title": "Evaluation Measures of Individual Item Fairness for Recommender Systems:\n  A Critical Study",
        "url": "http://arxiv.org/abs/2311.01013v1",
        "pub_date": "2023-11-02",
        "summary": "Fairness is an emerging and challenging topic in recommender systems. In\nrecent years, various ways of evaluating and therefore improving fairness have\nemerged. In this study, we examine existing evaluation measures of fairness in\nrecommender systems. Specifically, we focus solely on exposure-based fairness\nmeasures of individual items that aim to quantify the disparity in how\nindividual items are recommended to users, separate from item relevance to\nusers. We gather all such measures and we critically analyse their theoretical\nproperties. We identify a series of limitations in each of them, which\ncollectively may render the affected measures hard or impossible to interpret,\nto compute, or to use for comparing recommendations. We resolve these\nlimitations by redefining or correcting the affected measures, or we argue why\ncertain limitations cannot be resolved. We further perform a comprehensive\nempirical analysis of both the original and our corrected versions of these\nfairness measures, using real-world and synthetic datasets. Our analysis\nprovides novel insights into the relationship between measures based on\ndifferent fairness concepts, and different levels of measure sensitivity and\nstrictness. We conclude with practical suggestions of which fairness measures\nshould be used and when. Our code is publicly available. To our knowledge, this\nis the first critical comparison of individual item fairness measures in\nrecommender systems.",
        "translated": ""
    },
    {
        "title": "Research Team Identification Based on Representation Learning of\n  Academic Heterogeneous Information Network",
        "url": "http://arxiv.org/abs/2311.00922v1",
        "pub_date": "2023-11-02",
        "summary": "Academic networks in the real world can usually be described by heterogeneous\ninformation networks composed of multi-type nodes and relationships. Some\nexisting research on representation learning for homogeneous information\nnetworks lacks the ability to explore heterogeneous information networks in\nheterogeneous information networks. It cannot be applied to heterogeneous\ninformation networks. Aiming at the practical needs of effectively identifying\nand discovering scientific research teams from the academic heterogeneous\ninformation network composed of massive and complex scientific and\ntechnological big data, this paper proposes a scientific research team\nidentification method based on representation learning of academic\nheterogeneous information networks. The attention mechanism at node level and\nmeta-path level learns low-dimensional, dense and real-valued vector\nrepresentations on the basis of retaining the rich topological information of\nnodes in the network and the semantic information based on meta-paths, and\nrealizes effective identification and discovery of scientific research teams\nand important team members in academic heterogeneous information networks based\non maximizing node influence. Experimental results show that our proposed\nmethod outperforms the comparative methods.",
        "translated": ""
    },
    {
        "title": "Enhancing search engine precision and user experience through\n  sentiment-based polysemy resolution",
        "url": "http://arxiv.org/abs/2311.01895v1",
        "pub_date": "2023-11-03",
        "summary": "With the proliferation of digital content and the need for efficient\ninformation retrieval, this study's insights can be applied to various domains,\nincluding news services, e-commerce, and digital marketing, to provide users\nwith more meaningful and tailored experiences. The study addresses the common\nproblem of polysemy in search engines, where the same keyword may have multiple\nmeanings. It proposes a solution to this issue by embedding a smart search\nfunction into the search engine, which can differentiate between different\nmeanings based on sentiment. The study leverages sentiment analysis, a powerful\nnatural language processing (NLP) technique, to classify and categorize news\narticles based on their emotional tone. This can provide more insightful and\nnuanced search results. The article reports an impressive accuracy rate of 85%\nfor the proposed smart search function, which outperforms conventional search\nengines. This indicates the effectiveness of the sentiment-based approach. The\nresearch explores multiple sentiment analysis models, including Sentistrength\nand Valence Aware Dictionary for Sentiment Reasoning (VADER), to determine the\nbest-performing approach. The findings can be applied to enhance search\nengines, making them more capable of understanding the context and intent\nbehind users 'queries. This can lead to better search results that are more\naligned with what users are looking for. The proposed smart search function can\nimprove the user experience by reducing the need to sift through irrelevant\nsearch results. This is particularly important in an age where information\noverload is common.",
        "translated": ""
    },
    {
        "title": "Multi-EuP: The Multilingual European Parliament Dataset for Analysis of\n  Bias in Information Retrieval",
        "url": "http://arxiv.org/abs/2311.01870v1",
        "pub_date": "2023-11-03",
        "summary": "We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K\nmulti-lingual documents collected from the European Parliament, spanning 24\nlanguages. This dataset is designed to investigate fairness in a multilingual\ninformation retrieval (IR) context to analyze both language and demographic\nbias in a ranking context. It boasts an authentic multilingual corpus,\nfeaturing topics translated into all 24 languages, as well as cross-lingual\nrelevance judgments. Furthermore, it offers rich demographic information\nassociated with its documents, facilitating the study of demographic bias. We\nreport the effectiveness of Multi-EuP for benchmarking both monolingual and\nmultilingual IR. We also conduct a preliminary experiment on language bias\ncaused by the choice of tokenization strategy.",
        "translated": ""
    },
    {
        "title": "SortNet: Learning To Rank By a Neural-Based Sorting Algorithm",
        "url": "http://arxiv.org/abs/2311.01864v1",
        "pub_date": "2023-11-03",
        "summary": "The problem of relevance ranking consists of sorting a set of objects with\nrespect to a given criterion. Since users may prefer different relevance\ncriteria, the ranking algorithms should be adaptable to the user needs. Two\nmain approaches exist in literature for the task of learning to rank: 1) a\nscore function, learned by examples, which evaluates the properties of each\nobject yielding an absolute relevance value that can be used to order the\nobjects or 2) a pairwise approach, where a \"preference function\" is learned\nusing pairs of objects to define which one has to be ranked first. In this\npaper, we present SortNet, an adaptive ranking algorithm which orders objects\nusing a neural network as a comparator. The neural network training set\nprovides examples of the desired ordering between pairs of items and it is\nconstructed by an iterative procedure which, at each iteration, adds the most\ninformative training examples. Moreover, the comparator adopts a connectionist\narchitecture that is particularly suited for implementing a preference\nfunction. We also prove that such an architecture has the universal\napproximation property and can implement a wide class of functions. Finally,\nthe proposed algorithm is evaluated on the LETOR dataset showing promising\nperformances in comparison with other state of the art algorithms.",
        "translated": ""
    },
    {
        "title": "Universal Multi-modal Multi-domain Pre-trained Recommendation",
        "url": "http://arxiv.org/abs/2311.01831v1",
        "pub_date": "2023-11-03",
        "summary": "There is a rapidly-growing research interest in modeling user preferences via\npre-training multi-domain interactions for recommender systems. However,\nExisting pre-trained multi-domain recommendations mostly select the item texts\nto be bridges across domains, and simply explore the user behaviors in target\ndomains. Hence, they ignore other informative multi-modal item contents (e.g.,\nvisual information), and also lack of thorough consideration of user behaviors\nfrom all interactive domains. To address these issues, in this paper, we\npropose to pre-train universal multi-modal item content presentation for\nmulti-domain recommendation, called UniM^2Rec, which could smoothly learn the\nmulti-modal item content presentations and the multi-modal user preferences\nfrom all domains. With the pre-trained multi-domain recommendation model,\nUniM^2Rec could be efficiently and effectively transferred to new target\ndomains in practice. Extensive experiments conducted on five real-world\ndatasets in target domains demonstrate the superiority of the proposed method\nover existing competitive methods, especially for the real-world recommendation\nscenarios that usually struggle with seriously missing or noisy item contents.",
        "translated": ""
    },
    {
        "title": "Unbiased Offline Evaluation for Learning to Rank with Business Rules",
        "url": "http://arxiv.org/abs/2311.01828v1",
        "pub_date": "2023-11-03",
        "summary": "For industrial learning-to-rank (LTR) systems, it is common that the output\nof a ranking model is modified, either as a results of post-processing logic\nthat enforces business requirements, or as a result of unforeseen design flaws\nor bugs present in real-world production systems. This poses a challenge for\ndeploying off-policy learning and evaluation methods, as these often rely on\nthe assumption that rankings implied by the model's scores coincide with\ndisplayed items to the users. Further requirements for reliable offline\nevaluation are proper randomization and correct estimation of the propensities\nof displaying each item in any given position of the ranking, which are also\nimpacted by the aforementioned post-processing. We investigate empirically how\nthese scenarios impair off-policy evaluation for learning-to-rank models. We\nthen propose a novel correction method based on the Birkhoff-von-Neumann\ndecomposition that is robust to this type of post-processing. We obtain more\naccurate off-policy estimates in offline experiments, overcoming the problem of\npost-processed rankings. To the best of our knowledge this is the first study\non the impact of real-world business rules on offline evaluation of LTR models.",
        "translated": ""
    },
    {
        "title": "Epidemic Decision-making System Based Federated Reinforcement Learning",
        "url": "http://arxiv.org/abs/2311.01749v1",
        "pub_date": "2023-11-03",
        "summary": "Epidemic decision-making can effectively help the government to\ncomprehensively consider public security and economic development to respond to\npublic health and safety emergencies. Epidemic decision-making can effectively\nhelp the government to comprehensively consider public security and economic\ndevelopment to respond to public health and safety emergencies. Some studies\nhave shown that intensive learning can effectively help the government to make\nepidemic decision, thus achieving the balance between health security and\neconomic development. Some studies have shown that intensive learning can\neffectively help the government to make epidemic decision, thus achieving the\nbalance between health security and economic development. However, epidemic\ndata often has the characteristics of limited samples and high privacy.\nHowever, epidemic data often has the characteristics of limited samples and\nhigh privacy. This model can combine the epidemic situation data of various\nprovinces for cooperative training to use as an enhanced learning model for\nepidemic situation decision, while protecting the privacy of data. The\nexperiment shows that the enhanced federated learning can obtain more optimized\nperformance and return than the enhanced learning, and the enhanced federated\nlearning can also accelerate the training convergence speed of the training\nmodel. accelerate the training convergence speed of the client. At the same\ntime, through the experimental comparison, A2C is the most suitable\nreinforcement learning model for the epidemic situation decision-making.\nlearning model for the epidemic situation decision-making scenario, followed by\nthe PPO model, and the performance of DDPG is unsatisfactory.",
        "translated": ""
    },
    {
        "title": "Plot Retrieval as an Assessment of Abstract Semantic Association",
        "url": "http://arxiv.org/abs/2311.01666v1",
        "pub_date": "2023-11-03",
        "summary": "Retrieving relevant plots from the book for a query is a critical task, which\ncan improve the reading experience and efficiency of readers. Readers usually\nonly give an abstract and vague description as the query based on their own\nunderstanding, summaries, or speculations of the plot, which requires the\nretrieval model to have a strong ability to estimate the abstract semantic\nassociations between the query and candidate plots. However, existing\ninformation retrieval (IR) datasets cannot reflect this ability well. In this\npaper, we propose Plot Retrieval, a labeled dataset to train and evaluate the\nperformance of IR models on the novel task Plot Retrieval. Text pairs in Plot\nRetrieval have less word overlap and more abstract semantic association, which\ncan reflect the ability of the IR models to estimate the abstract semantic\nassociation, rather than just traditional lexical or semantic matching.\nExtensive experiments across various lexical retrieval, sparse retrieval, dense\nretrieval, and cross-encoder methods compared with human studies on Plot\nRetrieval show current IR models still struggle in capturing abstract semantic\nassociation between texts. Plot Retrieval can be the benchmark for further\nresearch on the semantic association modeling ability of IR models.",
        "translated": ""
    },
    {
        "title": "Instruction Distillation Makes Large Language Models Efficient Zero-shot\n  Rankers",
        "url": "http://arxiv.org/abs/2311.01555v1",
        "pub_date": "2023-11-02",
        "summary": "Recent studies have demonstrated the great potential of Large Language Models\n(LLMs) serving as zero-shot relevance rankers. The typical approach involves\nmaking comparisons between pairs or lists of documents. Although effective,\nthese listwise and pairwise methods are not efficient and also heavily rely on\nintricate prompt engineering. To tackle this problem, we introduce a novel\ninstruction distillation method. The key idea is to distill the pairwise\nranking ability of open-sourced LLMs to a simpler but more efficient pointwise\nranking. Specifically, given the same LLM, we first rank documents using the\neffective pairwise approach with complex instructions, and then distill the\nteacher predictions to the pointwise approach with simpler instructions.\nEvaluation results on the BEIR, TREC, and ReDial datasets demonstrate that\ninstruction distillation can improve efficiency by 10 to 100x and also enhance\nthe ranking performance of LLMs. Furthermore, our approach surpasses the\nperformance of existing supervised methods like monoT5 and is on par with the\nstate-of-the-art zero-shot methods. The code to reproduce our results is\navailable at www.github.com/sunnweiwei/RankGPT.",
        "translated": ""
    },
    {
        "title": "A Foundation Model for Music Informatics",
        "url": "http://arxiv.org/abs/2311.03318v1",
        "pub_date": "2023-11-06",
        "summary": "This paper investigates foundation models tailored for music informatics, a\ndomain currently challenged by the scarcity of labeled data and generalization\nissues. To this end, we conduct an in-depth comparative study among various\nfoundation model variants, examining key determinants such as model\narchitectures, tokenization methods, temporal resolution, data, and model\nscalability. This research aims to bridge the existing knowledge gap by\nelucidating how these individual factors contribute to the success of\nfoundation models in music informatics. Employing a careful evaluation\nframework, we assess the performance of these models across diverse downstream\ntasks in music information retrieval, with a particular focus on token-level\nand sequence-level classification. Our results reveal that our model\ndemonstrates robust performance, surpassing existing models in specific key\nmetrics. These findings contribute to the understanding of self-supervised\nlearning in music informatics and pave the way for developing more effective\nand versatile foundation models in the field. A pretrained version of our model\nis publicly available to foster reproducibility and future research.",
        "translated": ""
    },
    {
        "title": "Improving Collaborative Filtering Recommendation via Graph Learning",
        "url": "http://arxiv.org/abs/2311.03316v1",
        "pub_date": "2023-11-06",
        "summary": "Recommendation systems are designed to provide personalized predictions for\nitems that are most appealing to individual customers. Among various types of\nrecommendation algorithms, k-nearest neighbor based collaborative filtering\nalgorithm attracts tremendous attention and are widely used in practice.\nHowever, the k-nearest neighbor scheme can only capture the local relationship\namong users and the uniform neighborhood size is also not suitable to represent\nthe underlying data structure. In this paper, we leverage emerging graph signal\nprocessing (GSP) theory to construct sparse yet high quality graph to enhance\nthe solution quality and efficiency of collaborative filtering algorithm.\nExperimental results show that our method outperforms k-NN based collaborative\nfiltering algorithm by a large margin on the benchmark data set.",
        "translated": ""
    },
    {
        "title": "Injecting Categorical Labels and Syntactic Information into Biomedical\n  NER",
        "url": "http://arxiv.org/abs/2311.03113v1",
        "pub_date": "2023-11-06",
        "summary": "We present a simple approach to improve biomedical named entity recognition\n(NER) by injecting categorical labels and Part-of-speech (POS) information into\nthe model. We use two approaches, in the first approach, we first train a\nsequence-level classifier to classify the sentences into categories to obtain\nthe sentence-level tags (categorical labels). The sequence classifier is\nmodeled as an entailment problem by modifying the labels as a natural language\ntemplate. This helps to improve the accuracy of the classifier. Further, this\nlabel information is injected into the NER model. In this paper, we demonstrate\neffective ways to represent and inject these labels and POS attributes into the\nNER model. In the second approach, we jointly learn the categorical labels and\nNER labels. Here we also inject the POS tags into the model to increase the\nsyntactic context of the model. Experiments on three benchmark datasets show\nthat incorporating categorical label information with syntactic context is\nquite useful and outperforms baseline BERT-based models.",
        "translated": ""
    },
    {
        "title": "GLEN: Generative Retrieval via Lexical Index Learning",
        "url": "http://arxiv.org/abs/2311.03057v1",
        "pub_date": "2023-11-06",
        "summary": "Generative retrieval shed light on a new paradigm of document retrieval,\naiming to directly generate the identifier of a relevant document for a query.\nWhile it takes advantage of bypassing the construction of auxiliary index\nstructures, existing studies face two significant challenges: (i) the\ndiscrepancy between the knowledge of pre-trained language models and\nidentifiers and (ii) the gap between training and inference that poses\ndifficulty in learning to rank. To overcome these challenges, we propose a\nnovel generative retrieval method, namely Generative retrieval via LExical\niNdex learning (GLEN). For training, GLEN effectively exploits a dynamic\nlexical identifier using a two-phase index learning strategy, enabling it to\nlearn meaningful lexical identifiers and relevance signals between queries and\ndocuments. For inference, GLEN utilizes collision-free inference, using\nidentifier weights to rank documents without additional overhead. Experimental\nresults prove that GLEN achieves state-of-the-art or competitive performance\nagainst existing generative retrieval methods on various benchmark datasets,\ne.g., NQ320k, MS MARCO, and BEIR. The code is available at\nhttps://github.com/skleee/GLEN.",
        "translated": ""
    },
    {
        "title": "Retrieval-Augmented Code Generation for Universal Information Extraction",
        "url": "http://arxiv.org/abs/2311.02962v1",
        "pub_date": "2023-11-06",
        "summary": "Information Extraction (IE) aims to extract structural knowledge (e.g.,\nentities, relations, events) from natural language texts, which brings\nchallenges to existing methods due to task-specific schemas and complex text\nexpressions. Code, as a typical kind of formalized language, is capable of\ndescribing structural knowledge under various schemas in a universal way. On\nthe other hand, Large Language Models (LLMs) trained on both codes and texts\nhave demonstrated powerful capabilities of transforming texts into codes, which\nprovides a feasible solution to IE tasks. Therefore, in this paper, we propose\na universal retrieval-augmented code generation framework based on LLMs, called\nCode4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define\ntask-specific schemas of various structural knowledge in a universal way. By so\ndoing, extracting knowledge under these schemas can be transformed into\ngenerating codes that instantiate the predefined Python classes with the\ninformation in texts. To generate these codes more precisely, Code4UIE adopts\nthe in-context learning mechanism to instruct LLMs with examples. In order to\nobtain appropriate examples for different tasks, Code4UIE explores several\nexample retrieval strategies, which can retrieve examples semantically similar\nto the given texts. Extensive experiments on five representative IE tasks\nacross nine datasets demonstrate the effectiveness of the Code4UIE framework.",
        "translated": ""
    },
    {
        "title": "Contrastive Multi-Level Graph Neural Networks for Session-based\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.02938v1",
        "pub_date": "2023-11-06",
        "summary": "Session-based recommendation (SBR) aims to predict the next item at a certain\ntime point based on anonymous user behavior sequences. Existing methods\ntypically model session representation based on simple item transition\ninformation. However, since session-based data consists of limited users'\nshort-term interactions, modeling session representation by capturing fixed\nitem transition information from a single dimension suffers from data sparsity.\nIn this paper, we propose a novel contrastive multi-level graph neural networks\n(CM-GNN) to better exploit complex and high-order item transition information.\nSpecifically, CM-GNN applies local-level graph convolutional network (L-GCN)\nand global-level network (G-GCN) on the current session and all the sessions\nrespectively, to effectively capture pairwise relations over all the sessions\nby aggregation strategy. Meanwhile, CM-GNN applies hyper-level graph\nconvolutional network (H-GCN) to capture high-order information among all the\nitem transitions. CM-GNN further introduces an attention-based fusion module to\nlearn pairwise relation-based session representation by fusing the item\nrepresentations generated by L-GCN and G-GCN. CM-GNN averages the item\nrepresentations obtained by H-GCN to obtain high-order relation-based session\nrepresentation. Moreover, to convert the high-order item transition information\ninto the pairwise relation-based session representation, CM-GNN maximizes the\nmutual information between the representations derived from the fusion module\nand the average pool layer by contrastive learning paradigm. We conduct\nextensive experiments on multiple widely used benchmark datasets to validate\nthe efficacy of the proposed method. The encouraging results demonstrate that\nour proposed method outperforms the state-of-the-art SBR techniques.",
        "translated": ""
    },
    {
        "title": "CAME: Competitively Learning a Mixture-of-Experts Model for First-stage\n  Retrieval",
        "url": "http://arxiv.org/abs/2311.02834v1",
        "pub_date": "2023-11-06",
        "summary": "The first-stage retrieval aims to retrieve a subset of candidate documents\nfrom a huge collection both effectively and efficiently. Since various matching\npatterns can exist between queries and relevant documents, previous work tries\nto combine multiple retrieval models to find as many relevant results as\npossible. The constructed ensembles, whether learned independently or jointly,\ndo not care which component model is more suitable to an instance during\ntraining. Thus, they cannot fully exploit the capabilities of different types\nof retrieval models in identifying diverse relevance patterns. Motivated by\nthis observation, in this paper, we propose a Mixture-of-Experts (MoE) model\nconsisting of representative matching experts and a novel competitive learning\nmechanism to let the experts develop and enhance their expertise during\ntraining. Specifically, our MoE model shares the bottom layers to learn common\nsemantic representations and uses differently structured upper layers to\nrepresent various types of retrieval experts. Our competitive learning\nmechanism has two stages: (1) a standardized learning stage to train the\nexperts equally to develop their capabilities to conduct relevance matching;\n(2) a specialized learning stage where the experts compete with each other on\nevery training instance and get rewards and updates according to their\nperformance to enhance their expertise on certain types of samples.\nExperimental results on three retrieval benchmark datasets show that our method\nsignificantly outperforms the state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "APGL4SR: A Generic Framework with Adaptive and Personalized Global\n  Collaborative Information in Sequential Recommendation",
        "url": "http://arxiv.org/abs/2311.02816v1",
        "pub_date": "2023-11-06",
        "summary": "The sequential recommendation system has been widely studied for its\npromising effectiveness in capturing dynamic preferences buried in users'\nsequential behaviors. Despite the considerable achievements, existing methods\nusually focus on intra-sequence modeling while overlooking exploiting global\ncollaborative information by inter-sequence modeling, resulting in inferior\nrecommendation performance. Therefore, previous works attempt to tackle this\nproblem with a global collaborative item graph constructed by pre-defined\nrules. However, these methods neglect two crucial properties when capturing\nglobal collaborative information, i.e., adaptiveness and personalization,\nyielding sub-optimal user representations. To this end, we propose a\ngraph-driven framework, named Adaptive and Personalized Graph Learning for\nSequential Recommendation (APGL4SR), that incorporates adaptive and\npersonalized global collaborative information into sequential recommendation\nsystems. Specifically, we first learn an adaptive global graph among all items\nand capture global collaborative information with it in a self-supervised\nfashion, whose computational burden can be further alleviated by the proposed\nSVD-based accelerator. Furthermore, based on the graph, we propose to extract\nand utilize personalized item correlations in the form of relative positional\nencoding, which is a highly compatible manner of personalizing the utilization\nof global collaborative information. Finally, the entire framework is optimized\nin a multi-task learning paradigm, thus each part of APGL4SR can be mutually\nreinforced. As a generic framework, APGL4SR can outperform other baselines with\nsignificant margins. The code is available at\nhttps://github.com/Graph-Team/APGL4SR.",
        "translated": ""
    },
    {
        "title": "CausalCite: A Causal Formulation of Paper Citations",
        "url": "http://arxiv.org/abs/2311.02790v1",
        "pub_date": "2023-11-05",
        "summary": "Evaluating the significance of a paper is pivotal yet challenging for the\nscientific community. While the citation count is the most commonly used proxy\nfor this purpose, they are widely criticized for failing to accurately reflect\na paper's true impact. In this work, we propose a causal inference method,\nTextMatch, which adapts the traditional matching framework to high-dimensional\ntext embeddings. Specifically, we encode each paper using the text embeddings\nby large language models (LLMs), extract similar samples by cosine similarity,\nand synthesize a counterfactual sample by the weighted average of similar\npapers according to their similarity values. We apply the resulting metric,\ncalled CausalCite, as a causal formulation of paper citations. We show its\neffectiveness on various criteria, such as high correlation with paper impact\nas reported by scientific experts on a previous dataset of 1K papers,\n(test-of-time) awards for past papers, and its stability across various\nsub-fields of AI. We also provide a set of findings that can serve as suggested\nways for future researchers to use our metric for a better understanding of a\npaper's quality. Our code and data are at\nhttps://github.com/causalNLP/causal-cite.",
        "translated": ""
    },
    {
        "title": "CIRCLE: Multi-Turn Query Clarifications with Reinforcement Learning",
        "url": "http://arxiv.org/abs/2311.02737v1",
        "pub_date": "2023-11-05",
        "summary": "Users often have trouble formulating their information needs into words on\nthe first try when searching online. This can lead to frustration, as they may\nhave to reformulate their queries when retrieved information is not relevant.\nThis can be due to a lack of familiarity with the specific terminology related\nto their search topic, or because queries are ambiguous and related to multiple\ntopics. Most modern search engines have interactive features that suggest\nclarifications or similar queries based on what others have searched for.\nHowever, the proposed models are either based on a single interaction or\nevaluated on search logs, hindering the naturalness of the interactions. In\nthis paper, we introduce CIRCLE, a generative model for multi-turn query\nClarifications wIth ReinforCement LEarning that leverages multi-turn\ninteractions through a user simulation framework. Our model aims at generating\na diverse set of query clarifications using a pretrained language model\nfine-tuned using reinforcement learning. We evaluate it against well\nestablished google suggestions using a user simulation framework.",
        "translated": ""
    },
    {
        "title": "Deep Hashing via Householder Quantization",
        "url": "http://arxiv.org/abs/2311.04207v1",
        "pub_date": "2023-11-07",
        "summary": "Hashing is at the heart of large-scale image similarity search, and recent\nmethods have been substantially improved through deep learning techniques. Such\nalgorithms typically learn continuous embeddings of the data. To avoid a\nsubsequent costly binarization step, a common solution is to employ loss\nfunctions that combine a similarity learning term (to ensure similar images are\ngrouped to nearby embeddings) and a quantization penalty term (to ensure that\nthe embedding entries are close to binarized entries, e.g., -1 or 1). Still,\nthe interaction between these two terms can make learning harder and the\nembeddings worse. We propose an alternative quantization strategy that\ndecomposes the learning problem in two stages: first, perform similarity\nlearning over the embedding space with no quantization; second, find an optimal\northogonal transformation of the embeddings so each coordinate of the embedding\nis close to its sign, and then quantize the transformed embedding through the\nsign function. In the second step, we parametrize orthogonal transformations\nusing Householder matrices to efficiently leverage stochastic gradient descent.\nSince similarity measures are usually invariant under orthogonal\ntransformations, this quantization strategy comes at no cost in terms of\nperformance. The resulting algorithm is unsupervised, fast, hyperparameter-free\nand can be run on top of any existing deep hashing or metric learning\nalgorithm. We provide extensive experimental results showing that this approach\nleads to state-of-the-art performance on widely used image datasets, and,\nunlike other quantization strategies, brings consistent improvements in\nperformance to existing deep hashing algorithms.",
        "translated": ""
    },
    {
        "title": "Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary\n  Case Study",
        "url": "http://arxiv.org/abs/2311.04199v1",
        "pub_date": "2023-11-07",
        "summary": "Large Multimodal Models (LMMs) have demonstrated impressive performance\nacross various vision and language tasks, yet their potential applications in\nrecommendation tasks with visual assistance remain unexplored. To bridge this\ngap, we present a preliminary case study investigating the recommendation\ncapabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a\nseries of qualitative test samples spanning multiple domains and employ these\nsamples to assess the quality of GPT-4V's responses within recommendation\nscenarios. Evaluation results on these test samples prove that GPT-4V has\nremarkable zero-shot recommendation abilities across diverse domains, thanks to\nits robust visual-text comprehension capabilities and extensive general\nknowledge. However, we have also identified some limitations in using GPT-4V\nfor recommendations, including a tendency to provide similar responses when\ngiven similar inputs. This report concludes with an in-depth discussion of the\nchallenges and research opportunities associated with utilizing GPT-4V in\nrecommendation scenarios. Our objective is to explore the potential of\nextending LMMs from vision and language tasks to recommendation tasks. We hope\nto inspire further research into next-generation multimodal generative\nrecommendation models, which can enhance user experiences by offering greater\ndiversity and interactivity. All images and prompts used in this report will be\naccessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.",
        "translated": ""
    },
    {
        "title": "Proceedings of the 5th International Workshop on Reading Music Systems",
        "url": "http://arxiv.org/abs/2311.04091v1",
        "pub_date": "2023-11-07",
        "summary": "The International Workshop on Reading Music Systems (WoRMS) is a workshop\nthat tries to connect researchers who develop systems for reading music, such\nas in the field of Optical Music Recognition, with other researchers and\npractitioners that could benefit from such systems, like librarians or\nmusicologists. The relevant topics of interest for the workshop include, but\nare not limited to: Music reading systems; Optical music recognition; Datasets\nand performance evaluation; Image processing on music scores; Writer\nidentification; Authoring, editing, storing and presentation systems for music\nscores; Multi-modal systems; Novel input-methods for music to produce written\nmusic; Web-based Music Information Retrieval services; Applications and\nprojects; Use-cases related to written music.\n  These are the proceedings of the 5th International Workshop on Reading Music\nSystems, held in Milan, Italy on Nov. 4th 2023.",
        "translated": ""
    },
    {
        "title": "The Music Meta Ontology: a flexible semantic model for the\n  interoperability of music metadata",
        "url": "http://arxiv.org/abs/2311.03942v1",
        "pub_date": "2023-11-07",
        "summary": "The semantic description of music metadata is a key requirement for the\ncreation of music datasets that can be aligned, integrated, and accessed for\ninformation retrieval and knowledge discovery. It is nonetheless an open\nchallenge due to the complexity of musical concepts arising from different\ngenres, styles, and periods -- standing to benefit from a lingua franca to\naccommodate various stakeholders (musicologists, librarians, data engineers,\netc.). To initiate this transition, we introduce the Music Meta ontology, a\nrich and flexible semantic model to describe music metadata related to artists,\ncompositions, performances, recordings, and links. We follow eXtreme Design\nmethodologies and best practices for data engineering, to reflect the\nperspectives and the requirements of various stakeholders into the design of\nthe model, while leveraging ontology design patterns and accounting for\nprovenance at different levels (claims, links). After presenting the main\nfeatures of Music Meta, we provide a first evaluation of the model, alignments\nto other schema (Music Ontology, DOREMUS, Wikidata), and support for data\ntransformation.",
        "translated": ""
    },
    {
        "title": "OLaLa: Ontology Matching with Large Language Models",
        "url": "http://arxiv.org/abs/2311.03837v1",
        "pub_date": "2023-11-07",
        "summary": "Ontology (and more generally: Knowledge Graph) Matching is a challenging task\nwhere information in natural language is one of the most important signals to\nprocess. With the rise of Large Language Models, it is possible to incorporate\nthis knowledge in a better way into the matching pipeline. A number of\ndecisions still need to be taken, e.g., how to generate a prompt that is useful\nto the model, how information in the KG can be formulated in prompts, which\nLarge Language Model to choose, how to provide existing correspondences to the\nmodel, how to generate candidates, etc. In this paper, we present a prototype\nthat explores these questions by applying zero-shot and few-shot prompting with\nmultiple open Large Language Models to different tasks of the Ontology\nAlignment Evaluation Initiative (OAEI). We show that with only a handful of\nexamples and a well-designed prompt, it is possible to achieve results that are\nen par with supervised matching systems which use a much larger portion of the\nground truth.",
        "translated": ""
    },
    {
        "title": "Bridging the Information Gap Between Domain-Specific Model and General\n  LLM for Personalized Recommendation",
        "url": "http://arxiv.org/abs/2311.03778v1",
        "pub_date": "2023-11-07",
        "summary": "Generative large language models(LLMs) are proficient in solving general\nproblems but often struggle to handle domain-specific tasks. This is because\nmost of domain-specific tasks, such as personalized recommendation, rely on\ntask-related information for optimal performance. Current methods attempt to\nsupplement task-related information to LLMs by designing appropriate prompts or\nemploying supervised fine-tuning techniques. Nevertheless, these methods\nencounter the certain issue that information such as community behavior pattern\nin RS domain is challenging to express in natural language, which limits the\ncapability of LLMs to surpass state-of-the-art domain-specific models. On the\nother hand, domain-specific models for personalized recommendation which mainly\nrely on user interactions are susceptible to data sparsity due to their limited\ncommon knowledge capabilities. To address these issues, we proposes a method to\nbridge the information gap between the domain-specific models and the general\nlarge language models. Specifically, we propose an information sharing module\nwhich serves as an information storage mechanism and also acts as a bridge for\ncollaborative training between the LLMs and domain-specific models. By doing\nso, we can improve the performance of LLM-based recommendation with the help of\nuser behavior pattern information mined by domain-specific models. On the other\nhand, the recommendation performance of domain-specific models can also be\nimproved with the help of common knowledge learned by LLMs. Experimental\nresults on three real-world datasets have demonstrated the effectiveness of the\nproposed method.",
        "translated": ""
    },
    {
        "title": "Large Language Model based Long-tail Query Rewriting in Taobao Search",
        "url": "http://arxiv.org/abs/2311.03758v1",
        "pub_date": "2023-11-07",
        "summary": "In the realm of e-commerce search, the significance of semantic matching\ncannot be overstated, as it directly impacts both user experience and company\nrevenue. Query rewriting serves as an important technique to bridge semantic\ngaps inherent in the semantic matching process. However, existing query\nrewriting methods often struggle to effectively optimize long-tail queries and\nalleviate the phenomenon of \\textit{``\\nothing''} caused by semantic gap. In\nthis paper, we present \\textbf{\\method}, a comprehensive framework that\n\\textbf{B}ridges the s\\textbf{E}mantic gap for long-tail \\textbf{QUE}ries.\n\\method comprises three stages: multi-instruction supervised fine tuning (SFT),\noffline feedback, and objective alignment. Specifically, we first construct a\nrewriting dataset based on rejection sampling, and mix it with multiple\nauxiliary tasks data to fine tune our large language model (LLM) in a\nsupervised fashion during the first stage. Subsequently, with the well-trained\nLLM, we employ beam search to generate multiple candidate rewrites, which would\nbe fed into Taobao offline system to simulate the retrieval process and obtain\nthe partial order. Leveraging the partial order of candidate rewrites, we\nintroduce a contrastive learning method to highlight the distinctions between\nrewrites and align the model with the Taobao online objectives. Offline\nexperiments prove the effectiveness of our method in enhancing retrieval\nperformance. Online A/B tests reveal that our method can significantly boost\ngross merchandise volume (GMV), number of transaction (\\#Trans) and unique\nvisitor (UV) for long-tail queries. \\method has been deployed on Taobao, one of\nmost popular online shopping platforms in China, since October 2023.",
        "translated": ""
    },
    {
        "title": "Towards Automated Negative Sampling in Implicit Recommendation",
        "url": "http://arxiv.org/abs/2311.03526v1",
        "pub_date": "2023-11-06",
        "summary": "Negative sampling methods are vital in implicit recommendation models as they\nallow us to obtain negative instances from massive unlabeled data. Most\nexisting approaches focus on sampling hard negative samples in various ways.\nThese studies are orthogonal to the recommendation model and implicit datasets.\nHowever, such an idea contradicts the common belief in AutoML that the model\nand dataset should be matched. Empirical experiments suggest that the\nbest-performing negative sampler depends on the implicit dataset and the\nspecific recommendation model. Hence, we propose a hypothesis that the negative\nsampler should align with the capacity of the recommendation models as well as\nthe statistics of the datasets to achieve optimal performance. A mismatch\nbetween these three would result in sub-optimal outcomes. An intuitive idea to\naddress the mismatch problem is to exhaustively select the best-performing\nnegative sampler given the model and dataset. However, such an approach is\ncomputationally expensive and time-consuming, leaving the problem unsolved. In\nthis work, we propose the AutoSample framework that adaptively selects the\nbest-performing negative sampler among candidates. Specifically, we propose a\nloss-to-instance approximation to transform the negative sampler search task\ninto the learning task over a weighted sum, enabling end-to-end training of the\nmodel. We also designed an adaptive search algorithm to extensively and\nefficiently explore the search space. A specific initialization approach is\nalso obtained to better utilize the obtained model parameters during the search\nstage, which is similar to curriculum learning and leads to better performance\nand less computation resource consumption. We evaluate the proposed framework\non four benchmarks over three models. Extensive experiments demonstrate the\neffectiveness and efficiency of our proposed framework.",
        "translated": ""
    },
    {
        "title": "Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems",
        "url": "http://arxiv.org/abs/2311.03488v1",
        "pub_date": "2023-11-06",
        "summary": "While recommender systems have become an integral component of the Web\nexperience, their heavy reliance on user data raises privacy and security\nconcerns. Substituting user data with synthetic data can address these\nconcerns, but accurately replicating these real-world datasets has been a\nnotoriously challenging problem. Recent advancements in generative AI have\ndemonstrated the impressive capabilities of diffusion models in generating\nrealistic data across various domains. In this work we introduce a Score-based\nDiffusion Recommendation Model (SDRM), which captures the intricate patterns of\nreal-world datasets required for training highly accurate recommender systems.\nSDRM allows for the generation of synthetic data that can replace existing\ndatasets to preserve user privacy, or augment existing datasets to address\nexcessive data sparsity. Our method outperforms competing baselines such as\ngenerative adversarial networks, variational autoencoders, and recently\nproposed diffusion models in synthesizing various datasets to replace or\naugment the original data by an average improvement of 4.30% in Recall@$n$ and\n4.65% in NDCG@$n$.",
        "translated": ""
    },
    {
        "title": "Towards Open-world Cross-Domain Sequential Recommendation: A\n  Model-Agnostic Contrastive Denoising Approach",
        "url": "http://arxiv.org/abs/2311.04760v1",
        "pub_date": "2023-11-08",
        "summary": "Cross-domain sequential recommendation (CDSR) aims to address the data\nsparsity problems that exist in traditional sequential recommendation (SR)\nsystems.\n  The existing approaches aim to design a specific cross-domain unit that can\ntransfer and propagate information across multiple domains by relying on\noverlapping users with abundant behaviors. However, in real-world recommender\nsystems, CDSR scenarios usually consist of a majority of long-tailed users with\nsparse behaviors and cold-start users who only exist in one domain. This leads\nto a drop in the performance of existing CDSR methods in the real-world\nindustry platform. Therefore, improving the consistency and effectiveness of\nmodels in open-world CDSR scenarios is crucial for constructing CDSR models\n(\\textit{1st} CH). Recently, some SR approaches have utilized auxiliary\nbehaviors to complement the information for long-tailed users. However, these\nmulti-behavior SR methods cannot deliver promising performance in CDSR, as they\noverlook the semantic gap between target and auxiliary behaviors, as well as\nuser interest deviation across domains (\\textit{2nd} CH).",
        "translated": ""
    },
    {
        "title": "Evaluating Generative Ad Hoc Information Retrieval",
        "url": "http://arxiv.org/abs/2311.04694v1",
        "pub_date": "2023-11-08",
        "summary": "Recent advances in large language models have enabled the development of\nviable generative information retrieval systems. A generative retrieval system\nreturns a grounded generated text in response to an information need instead of\nthe traditional document ranking. Quantifying the utility of these types of\nresponses is essential for evaluating generative retrieval systems. As the\nestablished evaluation methodology for ranking-based ad hoc retrieval may seem\nunsuitable for generative retrieval, new approaches for reliable, repeatable,\nand reproducible experimentation are required. In this paper, we survey the\nrelevant information retrieval and natural language processing literature,\nidentify search tasks and system architectures in generative retrieval, develop\na corresponding user model, and study its operationalization. This theoretical\nanalysis provides a foundation and new insights for the evaluation of\ngenerative ad hoc retrieval systems.",
        "translated": ""
    },
    {
        "title": "Towards Deeper, Lighter and Interpretable Cross Network for CTR\n  Prediction",
        "url": "http://arxiv.org/abs/2311.04635v1",
        "pub_date": "2023-11-08",
        "summary": "Click Through Rate (CTR) prediction plays an essential role in recommender\nsystems and online advertising. It is crucial to effectively model feature\ninteractions to improve the prediction performance of CTR models. However,\nexisting methods face three significant challenges. First, while most methods\ncan automatically capture high-order feature interactions, their performance\ntends to diminish as the order of feature interactions increases. Second,\nexisting methods lack the ability to provide convincing interpretations of the\nprediction results, especially for high-order feature interactions, which\nlimits the trustworthiness of their predictions. Third, many methods suffer\nfrom the presence of redundant parameters, particularly in the embedding layer.\nThis paper proposes a novel method called Gated Deep Cross Network (GDCN) and a\nField-level Dimension Optimization (FDO) approach to address these challenges.\nAs the core structure of GDCN, Gated Cross Network (GCN) captures explicit\nhigh-order feature interactions and dynamically filters important interactions\nwith an information gate in each order. Additionally, we use the FDO approach\nto learn condensed dimensions for each field based on their importance.\nComprehensive experiments on five datasets demonstrate the effectiveness,\nsuperiority and interpretability of GDCN. Moreover, we verify the effectiveness\nof FDO in learning various dimensions and reducing model parameters. The code\nis available on \\url{https://github.com/anonctr/GDCN}.",
        "translated": ""
    },
    {
        "title": "A Comprehensive Summarization and Evaluation of Feature Refinement\n  Modules for CTR Prediction",
        "url": "http://arxiv.org/abs/2311.04625v1",
        "pub_date": "2023-11-08",
        "summary": "Click-through rate (CTR) prediction is widely used in academia and industry.\nMost CTR tasks fall into a feature embedding \\&amp; feature interaction paradigm,\nwhere the accuracy of CTR prediction is mainly improved by designing practical\nfeature interaction structures. However, recent studies have argued that the\nfixed feature embedding learned only through the embedding layer limits the\nperformance of existing CTR models. Some works apply extra modules on top of\nthe embedding layer to dynamically refine feature representations in different\ninstances, making it effective and easy to integrate with existing CTR methods.\nDespite the promising results, there is a lack of a systematic review and\nsummarization of this new promising direction on the CTR task. To fill this\ngap, we comprehensively summarize and define a new module, namely\n\\textbf{feature refinement} (FR) module, that can be applied between feature\nembedding and interaction layers. We extract 14 FR modules from previous works,\nincluding instances where the FR module was proposed but not clearly defined or\nexplained. We fully assess the effectiveness and compatibility of existing FR\nmodules through comprehensive and extensive experiments with over 200 augmented\nmodels and over 4,000 runs for more than 15,000 GPU hours. The results offer\ninsightful guidelines for researchers, and all benchmarking code and\nexperimental results are open-sourced. In addition, we present a new\narchitecture of assigning independent FR modules to separate sub-networks for\nparallel CTR models, as opposed to the conventional method of inserting a\nshared FR module on top of the embedding layer. Our approach is also supported\nby comprehensive experiments demonstrating its effectiveness.",
        "translated": ""
    },
    {
        "title": "Rethinking Cross-Domain Sequential Recommendation under Open-World\n  Assumptions",
        "url": "http://arxiv.org/abs/2311.04590v1",
        "pub_date": "2023-11-08",
        "summary": "Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data\nsparsity and cold-start problems present in Single-Domain Sequential\nRecommendation (SDSR). Existing CDSR works design their elaborate structures\nrelying on overlapping users to propagate the cross-domain information.\nHowever, current CDSR methods make closed-world assumptions, assuming fully\noverlapping users across multiple domains and that the data distribution\nremains unchanged from the training environment to the test environment. As a\nresult, these methods typically result in lower performance on online\nreal-world platforms due to the data distribution shifts. To address these\nchallenges under open-world assumptions, we design an \\textbf{A}daptive\n\\textbf{M}ulti-\\textbf{I}nterest \\textbf{D}ebiasing framework for cross-domain\nsequential recommendation (\\textbf{AMID}), which consists of a multi-interest\ninformation module (\\textbf{MIM}) and a doubly robust estimator (\\textbf{DRE}).\nOur framework is adaptive for open-world environments and can improve the model\nof most off-the-shelf single-domain sequential backbone models for CDSR. Our\nMIM establishes interest groups that consider both overlapping and\nnon-overlapping users, allowing us to effectively explore user intent and\nexplicit interest. To alleviate biases across multiple domains, we developed\nthe DRE for the CDSR methods. We also provide a theoretical analysis that\ndemonstrates the superiority of our proposed estimator in terms of bias and\ntail bound, compared to the IPS estimator used in previous work.",
        "translated": ""
    },
    {
        "title": "From Input to Output: A Multi-layer Knowledge Distillation Framework for\n  Compressing Recommendation Models",
        "url": "http://arxiv.org/abs/2311.04549v1",
        "pub_date": "2023-11-08",
        "summary": "To reduce the size of recommendation models, there have been many studies on\ncompressing recommendation models using knowledge distillation. In this paper,\nwe decompose recommendation models into three layers, i.e., the input layer,\nthe intermediate layer, and the output layer, and address deficiencies layer by\nlayer. First, previous methods focus only on two layers, neglecting the input\nlayer. Second, in the intermediate layer, existing methods ignore the\ninconsistency of user preferences induced by the projectors. Third, in the\noutput layer, existing methods use only hard labels rather than soft labels\nfrom the teacher. To address these deficiencies, we propose\n\\textbf{M}ulti-layer \\textbf{K}nowledge \\textbf{D}istillation (MKD), which\nconsists of three components: 1) Distillation with Neighbor-based Knowledge\n(NKD) utilizes the teacher's knowledge about entities with similar\ncharacteristics in the input layer to enable the student to learn robust\nrepresentations. 2) Distillation with Consistent Preference (CPD) reduces the\ninconsistency of user preferences caused by projectors in the intermediate\nlayer by two regularization terms. 3) Distillation with Soft Labels (SLD)\nconstructs soft labels in the output layer by considering the predictions of\nboth the teacher and the student. Our extensive experiments show that MKD even\noutperforms the teacher with one-tenth of the model size.",
        "translated": ""
    },
    {
        "title": "FEIR: Quantifying and Reducing Envy and Inferiority for Fair\n  Recommendation of Limited Resources",
        "url": "http://arxiv.org/abs/2311.04542v1",
        "pub_date": "2023-11-08",
        "summary": "In settings such as e-recruitment and online dating, recommendation involves\ndistributing limited opportunities, calling for novel approaches to quantify\nand enforce fairness. We introduce \\emph{inferiority}, a novel (un)fairness\nmeasure quantifying a user's competitive disadvantage for their recommended\nitems. Inferiority complements \\emph{envy}, a fairness notion measuring\npreference for others' recommendations. We combine inferiority and envy with\n\\emph{utility}, an accuracy-related measure of aggregated relevancy scores.\nSince these measures are non-differentiable, we reformulate them using a\nprobabilistic interpretation of recommender systems, yielding differentiable\nversions. We combine these loss functions in a multi-objective optimization\nproblem called \\texttt{FEIR} (Fairness through Envy and Inferiority Reduction),\napplied as post-processing for standard recommender systems. Experiments on\nsynthetic and real-world data demonstrate that our approach improves trade-offs\nbetween inferiority, envy, and utility compared to naive recommendations and\nthe baseline methods.",
        "translated": ""
    },
    {
        "title": "Bandit Learning to Rank with Position-Based Click Models: Personalized\n  and Equal Treatments",
        "url": "http://arxiv.org/abs/2311.04528v1",
        "pub_date": "2023-11-08",
        "summary": "Online learning to rank (ONL2R) is a foundational problem for recommender\nsystems and has received increasing attention in recent years. Among the\nexisting approaches for ONL2R, a natural modeling architecture is the\nmulti-armed bandit framework coupled with the position-based click model.\nHowever, developing efficient online learning policies for MAB-based ONL2R with\nposition-based click models is highly challenging due to the combinatorial\nnature of the problem, and partial observability in the position-based click\nmodel. To date, results in MAB-based ONL2R with position-based click models\nremain rather limited, which motivates us to fill this gap in this work. Our\nmain contributions in this work are threefold: i) We propose the first general\nMAB framework that captures all key ingredients of ONL2R with position-based\nclick models. Our model considers personalized and equal treatments in ONL2R\nranking recommendations, both of which are widely used in practice; ii) Based\non the above analytical framework, we develop two unified greed- and UCB-based\npolicies called GreedyRank and UCBRank, each of which can be applied to\npersonalized and equal ranking treatments; and iii) We show that both\nGreedyRank and UCBRank enjoy $O(\\sqrt{t}\\ln t)$ and $O(\\sqrt{t\\ln t})$ anytime\nsublinear regret for personalized and equal treatment, respectively. For the\nfundamentally hard equal ranking treatment, we identify classes of collective\nutility functions and their associated sufficient conditions under which\n$O(\\sqrt{t}\\ln t)$ and $O(\\sqrt{t\\ln t})$ anytime sublinear regrets are still\nachievable for GreedyRank and UCBRank, respectively. Our numerical experiments\nalso verify our theoretical results and demonstrate the efficiency of\nGreedyRank and UCBRank in seeking the optimal action under various problem\nsettings.",
        "translated": ""
    },
    {
        "title": "Twitter Sentiment Analysis of Covid Vacciness",
        "url": "http://arxiv.org/abs/2311.04479v1",
        "pub_date": "2023-11-08",
        "summary": "In this paper, we look at a database of tweets sorted by various keywords\nthat could indicate the users sentiment towards covid vaccines. With social\nmedia becoming such a prevalent source of opinion, sorting and ranking tweets\nthat hold important information such as opinions on covid vaccines is of utmost\nimportance. Two different ranking scales were used, and ranking a tweet in this\nway could represent the difference between an opinion being lost and an opinion\nbeing featured on the site, which affects the decisions and behavior of people,\nand why researchers were interested in it. Using natural language processing\ntechniques, our aim is to determine and categorize opinions about covid\nvaccines with the highest accuracy possible.",
        "translated": ""
    },
    {
        "title": "Automated Annotation of Scientific Texts for ML-based Keyphrase\n  Extraction and Validation",
        "url": "http://arxiv.org/abs/2311.05042v1",
        "pub_date": "2023-11-08",
        "summary": "Advanced omics technologies and facilities generate a wealth of valuable data\ndaily; however, the data often lacks the essential metadata required for\nresearchers to find and search them effectively. The lack of metadata poses a\nsignificant challenge in the utilization of these datasets. Machine\nlearning-based metadata extraction techniques have emerged as a potentially\nviable approach to automatically annotating scientific datasets with the\nmetadata necessary for enabling effective search. Text labeling, usually\nperformed manually, plays a crucial role in validating machine-extracted\nmetadata. However, manual labeling is time-consuming; thus, there is an need to\ndevelop automated text labeling techniques in order to accelerate the process\nof scientific innovation. This need is particularly urgent in fields such as\nenvironmental genomics and microbiome science, which have historically received\nless attention in terms of metadata curation and creation of gold-standard text\nmining datasets.\n  In this paper, we present two novel automated text labeling approaches for\nthe validation of ML-generated metadata for unlabeled texts, with specific\napplications in environmental genomics. Our techniques show the potential of\ntwo new ways to leverage existing information about the unlabeled texts and the\nscientific domain. The first technique exploits relationships between different\ntypes of data sources related to the same research study, such as publications\nand proposals. The second technique takes advantage of domain-specific\ncontrolled vocabularies or ontologies. In this paper, we detail applying these\napproaches for ML-generated metadata validation. Our results show that the\nproposed label assignment approaches can generate both generic and\nhighly-specific text labels for the unlabeled texts, with up to 44% of the\nlabels matching with those suggested by a ML keyword extraction algorithm.",
        "translated": ""
    },
    {
        "title": "Towards Effective Paraphrasing for Information Disguise",
        "url": "http://arxiv.org/abs/2311.05018v1",
        "pub_date": "2023-11-08",
        "summary": "Information Disguise (ID), a part of computational ethics in Natural Language\nProcessing (NLP), is concerned with best practices of textual paraphrasing to\nprevent the non-consensual use of authors' posts on the Internet. Research on\nID becomes important when authors' written online communication pertains to\nsensitive domains, e.g., mental health. Over time, researchers have utilized\nAI-based automated word spinners (e.g., SpinRewriter, WordAI) for paraphrasing\ncontent. However, these tools fail to satisfy the purpose of ID as their\nparaphrased content still leads to the source when queried on search engines.\nThere is limited prior work on judging the effectiveness of paraphrasing\nmethods for ID on search engines or their proxies, neural retriever (NeurIR)\nmodels. We propose a framework where, for a given sentence from an author's\npost, we perform iterative perturbation on the sentence in the direction of\nparaphrasing with an attempt to confuse the search mechanism of a NeurIR system\nwhen the sentence is queried on it. Our experiments involve the subreddit\n'r/AmItheAsshole' as the source of public content and Dense Passage Retriever\nas a NeurIR system-based proxy for search engines. Our work introduces a novel\nmethod of phrase-importance rankings using perplexity scores and involves\nmulti-level phrase substitutions via beam search. Our multi-phrase substitution\nscheme succeeds in disguising sentences 82% of the time and hence takes an\nessential step towards enabling researchers to disguise sensitive content\neffectively before making it public. We also release the code of our approach.",
        "translated": ""
    },
    {
        "title": "Augmenting Ad-Hoc IR Dataset for Interactive Conversational Search",
        "url": "http://arxiv.org/abs/2311.06119v1",
        "pub_date": "2023-11-10",
        "summary": "A peculiarity of conversational search systems is that they involve\nmixed-initiatives such as system-generated query clarifying questions.\nEvaluating those systems at a large scale on the end task of IR is very\nchallenging, requiring adequate datasets containing such interactions. However,\ncurrent datasets only focus on either traditional ad-hoc IR tasks or query\nclarification tasks, the latter being usually seen as a reformulation task from\nthe initial query. The only two datasets known to us that contain both document\nrelevance judgments and the associated clarification interactions are Qulac and\nClariQ. Both are based on the TREC Web Track 2009-12 collection, but cover a\nvery limited number of topics (237 topics), far from being enough for training\nand testing conversational IR models. To fill the gap, we propose a methodology\nto automatically build large-scale conversational IR datasets from ad-hoc IR\ndatasets in order to facilitate explorations on conversational IR. Our\nmethodology is based on two processes: 1) generating query clarification\ninteractions through query clarification and answer generators, and 2)\naugmenting ad-hoc IR datasets with simulated interactions. In this paper, we\nfocus on MsMarco and augment it with query clarification and answer\nsimulations. We perform a thorough evaluation showing the quality and the\nrelevance of the generated interactions for each initial query. This paper\nshows the feasibility and utility of augmenting ad-hoc IR datasets for\nconversational IR.",
        "translated": ""
    },
    {
        "title": "Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval",
        "url": "http://arxiv.org/abs/2311.06067v1",
        "pub_date": "2023-11-10",
        "summary": "In recent years, hashing methods have been popular in the large-scale media\nsearch for low storage and strong representation capabilities. To describe\nobjects with similar overall appearance but subtle differences, more and more\nstudies focus on hashing-based fine-grained image retrieval. Existing hashing\nnetworks usually generate both local and global features through attention\nguidance on the same deep activation tensor, which limits the diversity of\nfeature representations. To handle this limitation, we substitute convolutional\ndescriptors for attention-guided features and propose an Attributes Grouping\nand Mining Hashing (AGMH), which groups and embeds the category-specific visual\nattributes in multiple descriptors to generate a comprehensive feature\nrepresentation for efficient fine-grained image retrieval. Specifically, an\nAttention Dispersion Loss (ADL) is designed to force the descriptors to attend\nto various local regions and capture diverse subtle details. Moreover, we\npropose a Stepwise Interactive External Attention (SIEA) to mine critical\nattributes in each descriptor and construct correlations between fine-grained\nattributes and objects. The attention mechanism is dedicated to learning\ndiscrete attributes, which will not cost additional computations in hash codes\ngeneration. Finally, the compact binary codes are learned by preserving\npairwise similarities. Experimental results demonstrate that AGMH consistently\nyields the best performance against state-of-the-art methods on fine-grained\nbenchmark datasets.",
        "translated": ""
    },
    {
        "title": "ID Embedding as Subtle Features of Content and Structure for Multimodal\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.05956v1",
        "pub_date": "2023-11-10",
        "summary": "Multimodal recommendation aims to model user and item representations\ncomprehensively with the involvement of multimedia content for effective\nrecommendations. Existing research has shown that it is beneficial for\nrecommendation performance to combine (user- and item-) ID embeddings with\nmultimodal salient features, indicating the value of IDs. However, there is a\nlack of a thorough analysis of the ID embeddings in terms of feature semantics\nin the literature. In this paper, we revisit the value of ID embeddings for\nmultimodal recommendation and conduct a thorough study regarding its semantics,\nwhich we recognize as subtle features of content and structures. Then, we\npropose a novel recommendation model by incorporating ID embeddings to enhance\nthe semantic features of both content and structures. Specifically, we put\nforward a hierarchical attention mechanism to incorporate ID embeddings in\nmodality fusing, coupled with contrastive learning, to enhance content\nrepresentations. Meanwhile, we propose a lightweight graph convolutional\nnetwork for each modality to amalgamate neighborhood and ID embeddings for\nimproving structural representations. Finally, the content and structure\nrepresentations are combined to form the ultimate item embedding for\nrecommendation. Extensive experiments on three real-world datasets (Baby,\nSports, and Clothing) demonstrate the superiority of our method over\nstate-of-the-art multimodal recommendation methods and the effectiveness of\nfine-grained ID embeddings.",
        "translated": ""
    },
    {
        "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented\n  Generation and Soft-Prompting for Non-Specialist LLM Users",
        "url": "http://arxiv.org/abs/2311.05903v1",
        "pub_date": "2023-11-10",
        "summary": "Research into methods for improving the performance of large language models\n(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and\nsoft-prompting has tended to focus on the use of highly technical or high-cost\ntechniques, making many of the newly discovered approaches comparatively\ninaccessible to non-technical users. In this paper we tested an unmodified\nversion of GPT 3.5, a fine-tuned version, and the same unmodified model when\ngiven access to a vectorised RAG database, both in isolation and in combination\nwith a basic, non-algorithmic soft prompt. In each case we tested the model's\nability to answer a set of 100 questions relating primarily to events that\noccurred after September 2021 (the point at which GPT 3.5's training data set\nends). We found that if commercial platforms are used and default settings are\napplied with no iteration in order to establish a baseline set of outputs, a\nfine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach\nout-performed both. The application of a soft prompt significantly improved the\nperformance of each approach.",
        "translated": ""
    },
    {
        "title": "Citation Recommendation on Scholarly Legal Articles",
        "url": "http://arxiv.org/abs/2311.05902v1",
        "pub_date": "2023-11-10",
        "summary": "Citation recommendation is the task of finding appropriate citations based on\na given piece of text. The proposed datasets for this task consist mainly of\nseveral scientific fields, lacking some core ones, such as law. Furthermore,\ncitation recommendation is used within the legal domain to identify supporting\narguments, utilizing non-scholarly legal articles. In order to alleviate the\nlimitations of existing studies, we gather the first scholarly legal dataset\nfor the task of citation recommendation. Also, we conduct experiments with\nstate-of-the-art models and compare their performance on this dataset. The\nstudy suggests that, while BM25 is a strong benchmark for the legal citation\nrecommendation task, the most effective method involves implementing a two-step\nprocess that entails pre-fetching with BM25+, followed by re-ranking with\nSciNCL, which enhances the performance of the baseline from 0.26 to 0.30\nMAP@10. Moreover, fine-tuning leads to considerable performance increases in\npre-trained models, which shows the importance of including legal articles in\nthe training data of these models.",
        "translated": ""
    },
    {
        "title": "Hiformer: Heterogeneous Feature Interactions Learning with Transformers\n  for Recommender Systems",
        "url": "http://arxiv.org/abs/2311.05884v1",
        "pub_date": "2023-11-10",
        "summary": "Learning feature interaction is the critical backbone to building recommender\nsystems. In web-scale applications, learning feature interaction is extremely\nchallenging due to the sparse and large input feature space; meanwhile,\nmanually crafting effective feature interactions is infeasible because of the\nexponential solution space. We propose to leverage a Transformer-based\narchitecture with attention layers to automatically capture feature\ninteractions. Transformer architectures have witnessed great success in many\ndomains, such as natural language processing and computer vision. However,\nthere has not been much adoption of Transformer architecture for feature\ninteraction modeling in industry. We aim at closing the gap. We identify two\nkey challenges for applying the vanilla Transformer architecture to web-scale\nrecommender systems: (1) Transformer architecture fails to capture the\nheterogeneous feature interactions in the self-attention layer; (2) The serving\nlatency of Transformer architecture might be too high to be deployed in\nweb-scale recommender systems. We first propose a heterogeneous self-attention\nlayer, which is a simple yet effective modification to the self-attention layer\nin Transformer, to take into account the heterogeneity of feature interactions.\nWe then introduce \\textsc{Hiformer} (\\textbf{H}eterogeneous\n\\textbf{I}nteraction Trans\\textbf{former}) to further improve the model\nexpressiveness. With low-rank approximation and model pruning, \\hiformer enjoys\nfast inference for online deployment. Extensive offline experiment results\ncorroborates the effectiveness and efficiency of the \\textsc{Hiformer} model.\nWe have successfully deployed the \\textsc{Hiformer} model to a real world large\nscale App ranking model at Google Play, with significant improvement in key\nengagement metrics (up to +2.66\\%).",
        "translated": ""
    },
    {
        "title": "DPR: An Algorithm Mitigate Bias Accumulation in Recommendation feedback\n  loops",
        "url": "http://arxiv.org/abs/2311.05864v1",
        "pub_date": "2023-11-10",
        "summary": "Recommendation models trained on the user feedback collected from deployed\nrecommendation systems are commonly biased. User feedback is considerably\naffected by the exposure mechanism, as users only provide feedback on the items\nexposed to them and passively ignore the unexposed items, thus producing\nnumerous false negative samples. Inevitably, biases caused by such user\nfeedback are inherited by new models and amplified via feedback loops.\nMoreover, the presence of false negative samples makes negative sampling\ndifficult and introduces spurious information in the user preference modeling\nprocess of the model. Recent work has investigated the negative impact of\nfeedback loops and unknown exposure mechanisms on recommendation quality and\nuser experience, essentially treating them as independent factors and ignoring\ntheir cross-effects. To address these issues, we deeply analyze the data\nexposure mechanism from the perspective of data iteration and feedback loops\nwith the Missing Not At Random (\\textbf{MNAR}) assumption, theoretically\ndemonstrating the existence of an available stabilization factor in the\ntransformation of the exposure mechanism under the feedback loops. We further\npropose Dynamic Personalized Ranking (\\textbf{DPR}), an unbiased algorithm that\nuses dynamic re-weighting to mitigate the cross-effects of exposure mechanisms\nand feedback loops without additional information. Furthermore, we design a\nplugin named Universal Anti-False Negative (\\textbf{UFN}) to mitigate the\nnegative impact of the false negative problem. We demonstrate theoretically\nthat our approach mitigates the negative effects of feedback loops and unknown\nexposure mechanisms. Experimental results on real-world datasets demonstrate\nthat models using DPR can better handle bias accumulation and the universality\nof UFN in mainstream loss methods.",
        "translated": ""
    },
    {
        "title": "Exploring Fine-tuning ChatGPT for News Recommendation",
        "url": "http://arxiv.org/abs/2311.05850v1",
        "pub_date": "2023-11-10",
        "summary": "News recommendation systems (RS) play a pivotal role in the current digital\nage, shaping how individuals access and engage with information. The fusion of\nnatural language processing (NLP) and RS, spurred by the rise of large language\nmodels such as the GPT and T5 series, blurs the boundaries between these\ndomains, making a tendency to treat RS as a language task. ChatGPT, renowned\nfor its user-friendly interface and increasing popularity, has become a\nprominent choice for a wide range of NLP tasks. While previous studies have\nexplored ChatGPT on recommendation tasks, this study breaks new ground by\ninvestigating its fine-tuning capability, particularly within the news domain.\nIn this study, we design two distinct prompts: one designed to treat news RS as\nthe ranking task and another tailored for the rating task. We evaluate\nChatGPT's performance in news recommendation by eliciting direct responses\nthrough the formulation of these two tasks. More importantly, we unravel the\npivotal role of fine-tuning data quality in enhancing ChatGPT's personalized\nrecommendation capabilities, and illustrates its potential in addressing the\nlongstanding challenge of the \"cold item\" problem in RS. Our experiments,\nconducted using the Microsoft News dataset (MIND), reveal significant\nimprovements achieved by ChatGPT after fine-tuning, especially in scenarios\nwhere a user's topic interests remain consistent, treating news RS as a ranking\ntask. This study illuminates the transformative potential of fine-tuning\nChatGPT as a means to advance news RS, offering more effective news consumption\nexperiences.",
        "translated": ""
    },
    {
        "title": "Leveraging LLMs for Synthesizing Training Data Across Many Languages in\n  Multilingual Dense Retrieval",
        "url": "http://arxiv.org/abs/2311.05800v1",
        "pub_date": "2023-11-10",
        "summary": "Dense retrieval models have predominantly been studied for English, where\nmodels have shown great success, due to the availability of human-labeled\ntraining pairs. However, there has been limited success for multilingual\nretrieval so far, as training data is uneven or scarcely available across\nmultiple languages. Synthetic training data generation is promising (e.g.,\nInPars or Promptagator), but has been investigated only for English. Therefore,\nto study model capabilities across both cross-lingual and monolingual retrieval\ntasks, we develop SWIM-IR, a synthetic retrieval training dataset containing 33\n(high to very-low resource) languages for training multilingual dense retrieval\nmodels without requiring any human supervision. To construct SWIM-IR, we\npropose SAP (summarize-then-ask prompting), where the large language model\n(LLM) generates a textual summary prior to the query generation step. SAP\nassists the LLM in generating informative queries in the target language. Using\nSWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval\nmodels and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve\n(cross-lingual), XTREME-UP (cross-lingual) and MIRACL (monolingual). Our\nmodels, called SWIM-X, are competitive with human-supervised dense retrieval\nmodels, e.g., mContriever, finding that SWIM-IR can cheaply substitute for\nexpensive human-labeled retrieval training data.",
        "translated": ""
    },
    {
        "title": "BIDRN: A Method of Bidirectional Recurrent Neural Network for Sentiment\n  Analysis",
        "url": "http://arxiv.org/abs/2311.07296v1",
        "pub_date": "2023-11-13",
        "summary": "Text mining research has grown in importance in recent years due to the\ntremendous increase in the volume of unstructured textual data. This has\nresulted in immense potential as well as obstacles in the sector, which may be\nefficiently addressed with adequate analytical and study methods. Deep\nBidirectional Recurrent Neural Networks are used in this study to analyze\nsentiment. The method is categorized as sentiment polarity analysis because it\nmay generate a dataset with sentiment labels. This dataset can be used to train\nand evaluate sentiment analysis models capable of extracting impartial\nopinions. This paper describes the Sentiment Analysis-Deep Bidirectional\nRecurrent Neural Networks (SA-BDRNN) Scheme, which seeks to overcome the\nchallenges and maximize the potential of text mining in the context of Big\nData. The current study proposes a SA-DBRNN Scheme that attempts to give a\nsystematic framework for sentiment analysis in the context of student input on\ninstitution choice. The purpose of this study is to compare the effectiveness\nof the proposed SA- DBRNN Scheme to existing frameworks to establish a robust\ndeep neural network that might serve as an adequate classification model in the\nfield of sentiment analysis.",
        "translated": ""
    },
    {
        "title": "Understanding the Influence of Data Characteristics on the Performance\n  of Point-of-Interest Recommendation Algorithms",
        "url": "http://arxiv.org/abs/2311.07229v1",
        "pub_date": "2023-11-13",
        "summary": "The performance of recommendation algorithms is closely tied to key\ncharacteristics of the data sets they use, such as sparsity, popularity bias,\nand preference distributions. In this paper, we conduct a comprehensive\nexplanatory analysis to shed light on the impact of a broad range of data\ncharacteristics within the point-of-interest (POI) recommendation domain. To\naccomplish this, we extend prior methodologies used to characterize traditional\nrecommendation problems by introducing new explanatory variables specifically\nrelevant to POI recommendation. We subdivide a POI recommendation data set on\nNew York City into domain-driven subsamples to measure the effect of varying\nthese characteristics on different state-of-the-art POI recommendation\nalgorithms in terms of accuracy, novelty, and item exposure. Our findings,\nobtained through the application of an explanatory framework employing\nmultiple-regression models, reveal that the relevant independent variables\nencompass all categories of data characteristics and account for as much as\n$R^2 = $ 85-90\\% of the accuracy and item exposure achieved by the algorithms.\nOur study reaffirms the pivotal role of prominent data characteristics, such as\ndensity, popularity bias, and the distribution of check-ins in POI\nrecommendation. Additionally, we unveil novel factors, such as the proximity of\nuser activity to the city center and the duration of user activity. In summary,\nour work reveals why certain POI recommendation algorithms excel in specific\nrecommendation problems and, conversely, offers practical insights into which\ndata characteristics should be modified (or explicitly recognized) to achieve\nbetter performance.",
        "translated": ""
    },
    {
        "title": "On Elastic Language Models",
        "url": "http://arxiv.org/abs/2311.07204v1",
        "pub_date": "2023-11-13",
        "summary": "Large-scale pretrained language models have achieved compelling performance\nin a wide range of language understanding and information retrieval tasks.\nKnowledge distillation offers an opportunity to compress a large language model\nto a small one, in order to reach a reasonable latency-performance tradeoff.\nHowever, for scenarios where the number of requests (e.g., queries submitted to\na search engine) is highly variant, the static tradeoff attained by the\ncompressed language model might not always fit. Once a model is assigned with a\nstatic tradeoff, it could be inadequate in that the latency is too high when\nthe number of requests is large or the performance is too low when the number\nof requests is small. To this end, we propose an elastic language model\n(ElasticLM) that elastically adjusts the tradeoff according to the request\nstream. The basic idea is to introduce a compute elasticity to the compressed\nlanguage model, so that the tradeoff could vary on-the-fly along scalable and\ncontrollable compute. Specifically, we impose an elastic structure to enable\nElasticLM with compute elasticity and design an elastic optimization to learn\nElasticLM under compute elasticity. To serve ElasticLM, we apply an elastic\nschedule. Considering the specificity of information retrieval, we adapt\nElasticLM to dense retrieval and reranking and present ElasticDenser and\nElasticRanker respectively. Offline evaluation is conducted on a language\nunderstanding benchmark GLUE; and several information retrieval tasks including\nNatural Question, Trivia QA, and MS MARCO. The results show that ElasticLM\nalong with ElasticDenser and ElasticRanker can perform correctly and\ncompetitively compared with an array of static baselines. Furthermore, online\nsimulation with concurrency is also carried out. The results demonstrate that\nElasticLM can provide elastic tradeoffs with respect to varying request stream.",
        "translated": ""
    },
    {
        "title": "Do LLMs Implicitly Exhibit User Discrimination in Recommendation? An\n  Empirical Study",
        "url": "http://arxiv.org/abs/2311.07054v1",
        "pub_date": "2023-11-13",
        "summary": "Recently, Large Language Models (LLMs) have enhanced user interaction,\nenabling seamless information retrieval and recommendations. However, concerns\nemerge as these LLMs have shown tendencies to display discrimination related to\nusers' sensitive characteristics (such as gender), leading to explicit user\nunfairness. Furthermore, our analysis uncovers a more discreet variant of bias\nin LLMs, defined as implicit user unfairness, wherein these models demonstrate\ndiscriminatory recommendation behaviors based solely on non-sensitive user\ndetails, like usernames or email addresses. This subtle form of unfairness,\nwhile more pervasive, poses a significant threat to the ethical integrity and\nrights of minority user groups. To comprehensively explore implicit user\nunfairness, our analysis unfolds in three key steps: (1) We uncover the reasons\nfor this implicit user unfairness: LLMs can infer users' sensitive attributes\nfrom non-sensitive attributes (e.g. user names) due to their extensive world\nknowledge. (2) Our findings expose that the magnitude of implicit user\nunfairness within LLMs surpasses the level of explicit user unfairness observed\nin traditional recommender models, signifying a more alarming issue of\nunfairness, i.e. some non-sensitive features of users like names may result in\nmore serious discrimination phenomena. (3) We analyze the long-term effect of\nimplicit user unfairness, identifying that it will reinforce information\nbubbles at an accelerated rate compared to traditional RS. We emphasize the\nneed to identify and mitigate implicit user unfairness, aiming to avert the\npotential human-LLMs recommendation systems deterioration.",
        "translated": ""
    },
    {
        "title": "CL-Flow:Strengthening the Normalizing Flows by Contrastive Learning for\n  Better Anomaly Detection",
        "url": "http://arxiv.org/abs/2311.06794v1",
        "pub_date": "2023-11-12",
        "summary": "In the anomaly detection field, the scarcity of anomalous samples has\ndirected the current research emphasis towards unsupervised anomaly detection.\nWhile these unsupervised anomaly detection methods offer convenience, they also\noverlook the crucial prior information embedded within anomalous samples.\nMoreover, among numerous deep learning methods, supervised methods generally\nexhibit superior performance compared to unsupervised methods. Considering the\nreasons mentioned above, we propose a self-supervised anomaly detection\napproach that combines contrastive learning with 2D-Flow to achieve more\nprecise detection outcomes and expedited inference processes. On one hand, we\nintroduce a novel approach to anomaly synthesis, yielding anomalous samples in\naccordance with authentic industrial scenarios, alongside their surrogate\nannotations. On the other hand, having obtained a substantial number of\nanomalous samples, we enhance the 2D-Flow framework by incorporating\ncontrastive learning, leveraging diverse proxy tasks to fine-tune the network.\nOur approach enables the network to learn more precise mapping relationships\nfrom self-generated labels while retaining the lightweight characteristics of\nthe 2D-Flow. Compared to mainstream unsupervised approaches, our\nself-supervised method demonstrates superior detection accuracy, fewer\nadditional model parameters, and faster inference speed. Furthermore, the\nentire training and inference process is end-to-end. Our approach showcases new\nstate-of-the-art results, achieving a performance of 99.6\\% in image-level\nAUROC on the MVTecAD dataset and 96.8\\% in image-level AUROC on the BTAD\ndataset.",
        "translated": ""
    },
    {
        "title": "Alleviating Behavior Data Imbalance for Multi-Behavior Graph\n  Collaborative Filtering",
        "url": "http://arxiv.org/abs/2311.06777v1",
        "pub_date": "2023-11-12",
        "summary": "Graph collaborative filtering, which learns user and item representations\nthrough message propagation over the user-item interaction graph, has been\nshown to effectively enhance recommendation performance. However, most current\ngraph collaborative filtering models mainly construct the interaction graph on\na single behavior domain (e.g. click), even though users exhibit various types\nof behaviors on real-world platforms, including actions like click, cart, and\npurchase. Furthermore, due to variations in user engagement, there exists an\nimbalance in the scale of different types of behaviors. For instance, users may\nclick and view multiple items but only make selective purchases from a small\nsubset of them. How to alleviate the behavior imbalance problem and utilize\ninformation from the multiple behavior graphs concurrently to improve the\ntarget behavior conversion (e.g. purchase) remains underexplored. To this end,\nwe propose IMGCF, a simple but effective model to alleviate behavior data\nimbalance for multi-behavior graph collaborative filtering. Specifically, IMGCF\nutilizes a multi-task learning framework for collaborative filtering on\nmulti-behavior graphs. Then, to mitigate the data imbalance issue, IMGCF\nimproves representation learning on the sparse behavior by leveraging\nrepresentations learned from the behavior domain with abundant data volumes.\nExperiments on two widely-used multi-behavior datasets demonstrate the\neffectiveness of IMGCF.",
        "translated": ""
    },
    {
        "title": "What factors influence the popularity of user-generated text in the\n  creative domain? A case study of book reviews",
        "url": "http://arxiv.org/abs/2311.06714v1",
        "pub_date": "2023-11-12",
        "summary": "This study investigates a range of psychological, lexical, semantic, and\nreadability features of book reviews to elucidate the factors underlying their\nperceived popularity. To this end, we conduct statistical analyses of various\nfeatures, including the types and frequency of opinion and emotion-conveying\nterms, connectives, character mentions, word uniqueness, commonness, and\nsentence structure, among others. Additionally, we utilize two readability\ntests to explore whether reading ease is positively associated with review\npopularity. Finally, we employ traditional machine learning classifiers and\ntransformer-based fine-tuned language models with n-gram features to\nautomatically determine review popularity. Our findings indicate that, with the\nexception of a few features (e.g., review length, emotions, and word\nuniqueness), most attributes do not exhibit significant differences between\npopular and non-popular review groups. Furthermore, the poor performance of\nmachine learning classifiers using the word n-gram feature highlights the\nchallenges associated with determining popularity in creative domains. Overall,\nour study provides insights into the factors underlying review popularity and\nhighlights the need for further research in this area, particularly in the\ncreative realm.",
        "translated": ""
    },
    {
        "title": "Metric Optimization and Mainstream Bias Mitigation in Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2311.06689v1",
        "pub_date": "2023-11-11",
        "summary": "The first part of this thesis focuses on maximizing the overall\nrecommendation accuracy. This accuracy is usually evaluated with some\nuser-oriented metric tailored to the recommendation scenario, but because\nrecommendation is usually treated as a machine learning problem, recommendation\nmodels are trained to maximize some other generic criteria that does not\nnecessarily align with the criteria ultimately captured by the user-oriented\nevaluation metric. Recent research aims at bridging this gap between training\nand evaluation via direct ranking optimization, but still assumes that the\nmetric used for evaluation should also be the metric used for training. We\nchallenge this assumption, mainly because some metrics are more informative\nthan others. Indeed, we show that models trained via the optimization of a loss\ninspired by Rank-Biased Precision (RBP) tend to yield higher accuracy, even\nwhen accuracy is measured with metrics other than RBP. However, the superiority\nof this RBP-inspired loss stems from further benefiting users who are already\nwell-served, rather than helping those who are not.\n  This observation inspires the second part of this thesis, where our focus\nturns to helping non-mainstream users. These are users who are difficult to\nrecommend to either because there is not enough data to model them, or because\nthey have niche taste and thus few similar users to look at when recommending\nin a collaborative way. These differences in mainstreamness introduce a bias\nreflected in an accuracy gap between users or user groups, which we try to\nnarrow.",
        "translated": ""
    },
    {
        "title": "An Empirical Study of Using ChatGPT for Fact Verification Task",
        "url": "http://arxiv.org/abs/2311.06592v1",
        "pub_date": "2023-11-11",
        "summary": "ChatGPT has recently emerged as a powerful tool for performing diverse NLP\ntasks. However, ChatGPT has been criticized for generating nonfactual\nresponses, raising concerns about its usability for sensitive tasks like fact\nverification. This study investigates three key research questions: (1) Can\nChatGPT be used for fact verification tasks? (2) What are different prompts\nperformance using ChatGPT for fact verification tasks? (3) For the\nbest-performing prompt, what common mistakes does ChatGPT make? Specifically,\nthis study focuses on conducting a comprehensive and systematic analysis by\ndesigning and comparing the performance of three different prompts for fact\nverification tasks on the benchmark FEVER dataset using ChatGPT.",
        "translated": ""
    },
    {
        "title": "Mitigating Pooling Bias in E-commerce Search via False Negative\n  Estimation",
        "url": "http://arxiv.org/abs/2311.06444v1",
        "pub_date": "2023-11-11",
        "summary": "Efficient and accurate product relevance assessment is critical for user\nexperiences and business success. Training a proficient relevance assessment\nmodel requires high-quality query-product pairs, often obtained through\nnegative sampling strategies. Unfortunately, current methods introduce pooling\nbias by mistakenly sampling false negatives, diminishing performance and\nbusiness impact. To address this, we present Bias-mitigating Hard Negative\nSampling (BHNS), a novel negative sampling strategy tailored to identify and\nadjust for false negatives, building upon our original False Negative\nEstimation algorithm. Our experiments in the Instacart search setting confirm\nBHNS as effective for practical e-commerce use. Furthermore, comparative\nanalyses on public dataset showcase its domain-agnostic potential for diverse\napplications.",
        "translated": ""
    },
    {
        "title": "Retrieve and Copy: Scaling ASR Personalization to Large Catalogs",
        "url": "http://arxiv.org/abs/2311.08402v1",
        "pub_date": "2023-11-14",
        "summary": "Personalization of automatic speech recognition (ASR) models is a widely\nstudied topic because of its many practical applications. Most recently,\nattention-based contextual biasing techniques are used to improve the\nrecognition of rare words and domain specific entities. However, due to\nperformance constraints, the biasing is often limited to a few thousand\nentities, restricting real-world usability. To address this, we first propose a\n\"Retrieve and Copy\" mechanism to improve latency while retaining the accuracy\neven when scaled to a large catalog. We also propose a training strategy to\novercome the degradation in recall at such scale due to an increased number of\nconfusing entities. Overall, our approach achieves up to 6% more Word Error\nRate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a\nstrong baseline. Our method also allows for large catalog sizes of up to 20K\nwithout significantly affecting WER and F1-scores, while achieving at least 20%\ninference speedup per acoustic frame.",
        "translated": ""
    },
    {
        "title": "ChoralSynth: Synthetic Dataset of Choral Singing",
        "url": "http://arxiv.org/abs/2311.08350v1",
        "pub_date": "2023-11-14",
        "summary": "Choral singing, a widely practiced form of ensemble singing, lacks\ncomprehensive datasets in the realm of Music Information Retrieval (MIR)\nresearch, due to challenges arising from the requirement to curate multitrack\nrecordings. To address this, we devised a novel methodology, leveraging\nstate-of-the-art synthesizers to create and curate quality renditions. The\nscores were sourced from Choral Public Domain Library(CPDL). This work is done\nin collaboration with a diverse team of musicians, software engineers and\nresearchers. The resulting dataset, complete with its associated metadata, and\nmethodology is released as part of this work, opening up new avenues for\nexploration and advancement in the field of singing voice research.",
        "translated": ""
    },
    {
        "title": "Inverse Learning with Extremely Sparse Feedback for Recommendation",
        "url": "http://arxiv.org/abs/2311.08302v1",
        "pub_date": "2023-11-14",
        "summary": "Modern personalized recommendation services often rely on user feedback,\neither explicit or implicit, to improve the quality of services. Explicit\nfeedback refers to behaviors like ratings, while implicit feedback refers to\nbehaviors like user clicks. However, in the scenario of full-screen video\nviewing experiences like Tiktok and Reels, the click action is absent,\nresulting in unclear feedback from users, hence introducing noises in modeling\ntraining. Existing approaches on de-noising recommendation mainly focus on\npositive instances while ignoring the noise in a large amount of sampled\nnegative feedback. In this paper, we propose a meta-learning method to annotate\nthe unlabeled data from loss and gradient perspectives, which considers the\nnoises in both positive and negative instances. Specifically, we first propose\nan Inverse Dual Loss (IDL) to boost the true label learning and prevent the\nfalse label learning. Then we further propose an Inverse Gradient (IG) method\nto explore the correct updating gradient and adjust the updating based on\nmeta-learning. Finally, we conduct extensive experiments on both benchmark and\nindustrial datasets where our proposed method can significantly improve AUC by\n9.25% against state-of-the-art methods. Further analysis verifies the proposed\ninverse learning framework is model-agnostic and can improve a variety of\nrecommendation backbones. The source code, along with the best hyper-parameter\nsettings, is available at this link:\nhttps://github.com/Guanyu-Lin/InverseLearning.",
        "translated": ""
    },
    {
        "title": "Mixed Attention Network for Cross-domain Sequential Recommendation",
        "url": "http://arxiv.org/abs/2311.08272v1",
        "pub_date": "2023-11-14",
        "summary": "In modern recommender systems, sequential recommendation leverages\nchronological user behaviors to make effective next-item suggestions, which\nsuffers from data sparsity issues, especially for new users. One promising line\nof work is the cross-domain recommendation, which trains models with data\nacross multiple domains to improve the performance in data-scarce domains.\nRecent proposed cross-domain sequential recommendation models such as PiNet and\nDASL have a common drawback relying heavily on overlapped users in different\ndomains, which limits their usage in practical recommender systems. In this\npaper, we propose a Mixed Attention Network (MAN) with local and global\nattention modules to extract the domain-specific and cross-domain information.\nFirstly, we propose a local/global encoding layer to capture the\ndomain-specific/cross-domain sequential pattern. Then we propose a mixed\nattention layer with item similarity attention, sequence-fusion attention, and\ngroup-prototype attention to capture the local/global item similarity, fuse the\nlocal/global item sequence, and extract the user groups across different\ndomains, respectively. Finally, we propose a local/global prediction layer to\nfurther evolve and combine the domain-specific and cross-domain interests.\nExperimental results on two real-world datasets (each with two domains)\ndemonstrate the superiority of our proposed model. Further study also\nillustrates that our proposed method and components are model-agnostic and\neffective, respectively. The code and data are available at\nhttps://github.com/Guanyu-Lin/MAN.",
        "translated": ""
    },
    {
        "title": "REST: Retrieval-Based Speculative Decoding",
        "url": "http://arxiv.org/abs/2311.08252v1",
        "pub_date": "2023-11-14",
        "summary": "We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm\ndesigned to speed up language model generation. The key insight driving the\ndevelopment of REST is the observation that the process of text generation\noften includes certain common phases and patterns. Unlike previous methods that\nrely on a draft language model for speculative decoding, REST harnesses the\npower of retrieval to generate draft tokens. This method draws from the\nreservoir of existing knowledge, retrieving and employing relevant tokens based\non the current context. Its plug-and-play nature allows for seamless\nintegration and acceleration of any language models, all without necessitating\nadditional training. When benchmarked on 7B and 13B language models in a\nsingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on\ncode or text generation. The code of REST is available at\nhttps://github.com/FasterDecoding/REST.",
        "translated": ""
    },
    {
        "title": "Exploring Semi-supervised Hierarchical Stacked Encoder for Legal\n  Judgement Prediction",
        "url": "http://arxiv.org/abs/2311.08103v1",
        "pub_date": "2023-11-14",
        "summary": "Predicting the judgment of a legal case from its unannotated case facts is a\nchallenging task. The lengthy and non-uniform document structure poses an even\ngreater challenge in extracting information for decision prediction. In this\nwork, we explore and propose a two-level classification mechanism; both\nsupervised and unsupervised; by using domain-specific pre-trained BERT to\nextract information from long documents in terms of sentence embeddings further\nprocessing with transformer encoder layer and use unsupervised clustering to\nextract hidden labels from these embeddings to better predict a judgment of a\nlegal case. We conduct several experiments with this mechanism and see higher\nperformance gains than the previously proposed methods on the ILDC dataset. Our\nexperimental results also show the importance of domain-specific pre-training\nof Transformer Encoders in legal information processing.",
        "translated": ""
    },
    {
        "title": "TempTabQA: Temporal Question Answering for Semi-Structured Tables",
        "url": "http://arxiv.org/abs/2311.08002v1",
        "pub_date": "2023-11-14",
        "summary": "Semi-structured data, such as Infobox tables, often include temporal\ninformation about entities, either implicitly or explicitly. Can current NLP\nsystems reason about such information in semi-structured tables? To tackle this\nquestion, we introduce the task of temporal question answering on\nsemi-structured tables. We present a dataset, TempTabQA, which comprises 11,454\nquestion-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning\nmore than 90 distinct domains. Using this dataset, we evaluate several\nstate-of-the-art models for temporal reasoning. We observe that even the\ntop-performing LLMs lag behind human performance by more than 13.5 F1 points.\nGiven these results, our dataset has the potential to serve as a challenging\nbenchmark to improve the temporal reasoning capabilities of NLP models.",
        "translated": ""
    },
    {
        "title": "Text Retrieval with Multi-Stage Re-Ranking Models",
        "url": "http://arxiv.org/abs/2311.07994v1",
        "pub_date": "2023-11-14",
        "summary": "The text retrieval is the task of retrieving similar documents to a search\nquery, and it is important to improve retrieval accuracy while maintaining a\ncertain level of retrieval speed. Existing studies have reported accuracy\nimprovements using language models, but many of these do not take into account\nthe reduction in search speed that comes with increased performance. In this\nstudy, we propose three-stage re-ranking model using model ensembles or larger\nlanguage models to improve search accuracy while minimizing the search delay.\nWe ranked the documents by BM25 and language models, and then re-ranks by a\nmodel ensemble or a larger language model for documents with high similarity to\nthe query. In our experiments, we train the MiniLM language model on the\nMS-MARCO dataset and evaluate it in a zero-shot setting. Our proposed method\nachieves higher retrieval accuracy while reducing the retrieval speed decay.",
        "translated": ""
    },
    {
        "title": "Towards a Technical Debt for Recommender System",
        "url": "http://arxiv.org/abs/2311.07947v1",
        "pub_date": "2023-11-14",
        "summary": "Balancing the management of technical debt within recommender systems\nrequires effectively juggling the introduction of new features with the ongoing\nmaintenance and enhancement of the current system. Within the realm of\nrecommender systems, technical debt encompasses the trade-offs and expedient\nchoices made during the development and upkeep of the recommendation system,\nwhich could potentially have adverse effects on its long-term performance,\nscalability, and maintainability. In this vision paper, our objective is to\nkickstart a research direction regarding Technical Debt in Recommender Systems.\nWe identified 15 potential factors, along with detailed explanations outlining\nwhy it is advisable to consider them.",
        "translated": ""
    },
    {
        "title": "Evaluating LLMs on Document-Based QA: Exact Answer Selection and\n  Numerical Extraction using Cogtale datase",
        "url": "http://arxiv.org/abs/2311.07878v1",
        "pub_date": "2023-11-14",
        "summary": "Document-based Question-Answering (QA) tasks are crucial for precise\ninformation retrieval. While some existing work focus on evaluating large\nlanguage model's performance on retrieving and answering questions from\ndocuments, assessing the LLMs' performance on QA types that require exact\nanswer selection from predefined options and numerical extraction is yet to be\nfully assessed. In this paper, we specifically focus on this underexplored\ncontext and conduct empirical analysis of LLMs (GPT-4 and GPT 3.5) on question\ntypes, including single-choice, yes-no, multiple-choice, and number extraction\nquestions from documents. We use the Cogtale dataset for evaluation, which\nprovide human expert-tagged responses, offering a robust benchmark for\nprecision and factual grounding. We found that LLMs, particularly GPT-4, can\nprecisely answer many single-choice and yes-no questions given relevant\ncontext, demonstrating their efficacy in information retrieval tasks. However,\ntheir performance diminishes when confronted with multiple-choice and number\nextraction formats, lowering the overall performance of the model on this task,\nindicating that these models may not be reliable for the task. This limits the\napplications of LLMs on applications demanding precise information extraction\nfrom documents, such as meta-analysis tasks. However, these findings hinge on\nthe assumption that the retrievers furnish pertinent context necessary for\naccurate responses, emphasizing the need for further research on the efficacy\nof retriever mechanisms in enhancing question-answering performance. Our work\noffers a framework for ongoing dataset evaluation, ensuring that LLM\napplications for information retrieval and document analysis continue to meet\nevolving standards.",
        "translated": ""
    },
    {
        "title": "PEARL: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers",
        "url": "http://arxiv.org/abs/2311.09180v1",
        "pub_date": "2023-11-15",
        "summary": "Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style\nand specialized knowledge. In this paper, we address this challenge by\nproposing PEARL, a retrieval-augmented LLM writing assistant personalized with\na generation-calibrated retriever. Our retriever is trained to select historic\nuser-authored documents for prompt augmentation, such that they are likely to\nbest personalize LLM generations for a user request. We propose two key\nnovelties for training our retriever: 1) A training data selection method that\nidentifies user requests likely to benefit from personalization and documents\nthat provide that benefit; and 2) A scale-calibrating KL-divergence objective\nthat ensures that our retriever closely tracks the benefit of a document for\npersonalized generation. We demonstrate the effectiveness of PEARL in\ngenerating personalized workplace social media posts and Reddit comments.\nFinally, we showcase the potential of a generation-calibrated retriever to\ndouble as a performance predictor and further improve low-quality generations\nvia LLM chaining.",
        "translated": ""
    },
    {
        "title": "Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword\n  Generation for Zero-Shot Neural Rankers",
        "url": "http://arxiv.org/abs/2311.09175v1",
        "pub_date": "2023-11-15",
        "summary": "Query expansion has been proved to be effective in improving recall and\nprecision of first-stage retrievers, and yet its influence on a complicated,\nstate-of-the-art cross-encoder ranker remains under-explored. We first show\nthat directly applying the expansion techniques in the current literature to\nstate-of-the-art neural rankers can result in deteriorated zero-shot\nperformance. To this end, we propose GFF, a pipeline that includes a large\nlanguage model and a neural ranker, to Generate, Filter, and Fuse query\nexpansions more effectively in order to improve the zero-shot ranking metrics\nsuch as nDCG@10. Specifically, GFF first calls an instruction-following\nlanguage model to generate query-related keywords through a reasoning chain.\nLeveraging self-consistency and reciprocal rank weighting, GFF further filters\nand combines the ranking results of each expanded query dynamically. By\nutilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on\nBEIR and TREC DL 2019/2020. We also analyze different modelling choices in the\nGFF pipeline and shed light on the future directions in query expansion for\nzero-shot neural rankers.",
        "translated": ""
    },
    {
        "title": "Scalable and Effective Generative Information Retrieval",
        "url": "http://arxiv.org/abs/2311.09134v1",
        "pub_date": "2023-11-15",
        "summary": "Recent research has shown that transformer networks can be used as\ndifferentiable search indexes by representing each document as a sequences of\ndocument ID tokens. These generative retrieval models cast the retrieval\nproblem to a document ID generation problem for each given query. Despite their\nelegant design, existing generative retrieval models only perform well on\nartificially-constructed and small-scale collections. This has led to serious\nskepticism in the research community on their real-world impact. This paper\nrepresents an important milestone in generative retrieval research by showing,\nfor the first time, that generative retrieval models can be trained to perform\neffectively on large-scale standard retrieval benchmarks. For doing so, we\npropose RIPOR- an optimization framework for generative retrieval that can be\nadopted by any encoder-decoder architecture. RIPOR is designed based on two\noften-overlooked fundamental design considerations in generative retrieval.\nFirst, given the sequential decoding nature of document ID generation,\nassigning accurate relevance scores to documents based on the whole document ID\nsequence is not sufficient. To address this issue, RIPOR introduces a novel\nprefix-oriented ranking optimization algorithm. Second, initial document IDs\nshould be constructed based on relevance associations between queries and\ndocuments, instead of the syntactic and semantic information in the documents.\nRIPOR addresses this issue using a relevance-based document ID construction\napproach that quantizes relevance-based representations learned for documents.\nEvaluation on MSMARCO and TREC Deep Learning Track reveals that RIPOR surpasses\nstate-of-the-art generative retrieval models by a large margin (e.g., 30.5% MRR\nimprovements on MS MARCO Dev Set), and perform better on par with popular dense\nretrieval models.",
        "translated": ""
    },
    {
        "title": "Explainable Text Classification Techniques in Legal Document Review:\n  Locating Rationales without Using Human Annotated Training Text Snippets",
        "url": "http://arxiv.org/abs/2311.09133v1",
        "pub_date": "2023-11-15",
        "summary": "US corporations regularly spend millions of dollars reviewing\nelectronically-stored documents in legal matters. Recently, attorneys apply\ntext classification to efficiently cull massive volumes of data to identify\nresponsive documents for use in these matters. While text classification is\nregularly used to reduce the discovery costs of legal matters, it also faces a\nperception challenge: amongst lawyers, this technology is sometimes looked upon\nas a \"black box\". Put simply, no extra information is provided for attorneys to\nunderstand why documents are classified as responsive. In recent years,\nexplainable machine learning has emerged as an active research area. In an\nexplainable machine learning system, predictions or decisions made by a machine\nlearning model are human understandable. In legal 'document review' scenarios,\na document is responsive, because one or more of its small text snippets are\ndeemed responsive. In these scenarios, if these responsive snippets can be\nlocated, then attorneys could easily evaluate the model's document\nclassification decisions - this is especially important in the field of\nresponsible AI. Our prior research identified that predictive models created\nusing annotated training text snippets improved the precision of a model when\ncompared to a model created using all of a set of documents' text as training.\nWhile interesting, manually annotating training text snippets is not generally\npractical during a legal document review. However, small increases in precision\ncan drastically decrease the cost of large document reviews. Automating the\nidentification of training text snippets without human review could then make\nthe application of training text snippet-based models a practical approach.",
        "translated": ""
    },
    {
        "title": "Towards A Unified View of Answer Calibration for Multi-Step Reasoning",
        "url": "http://arxiv.org/abs/2311.09101v1",
        "pub_date": "2023-11-15",
        "summary": "Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\nbroadened the scope for improving multi-step reasoning capabilities. Usually,\nanswer calibration strategies such as step-level or path-level calibration play\na vital role in multi-step reasoning. While effective, there remains a\nsignificant gap in our understanding of the key factors that drive their\nsuccess. In this paper, we break down the design of recent answer calibration\nstrategies and present a unified view which establishes connections between\nthem. We then conduct a thorough evaluation on these strategies from a unified\nview, systematically scrutinizing step-level and path-level answer calibration\nacross multiple paths. Our study holds the potential to illuminate key insights\nfor optimizing multi-step reasoning with answer calibration.",
        "translated": ""
    },
    {
        "title": "Adapting Large Language Models by Integrating Collaborative Semantics\n  for Recommendation",
        "url": "http://arxiv.org/abs/2311.09049v1",
        "pub_date": "2023-11-15",
        "summary": "Recently, large language models (LLMs) have shown great potential in\nrecommender systems, either improving existing recommendation models or serving\nas the backbone. However, there exists a large semantic gap between LLMs and\nrecommender systems, since items to be recommended are often indexed by\ndiscrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs\ncapture language semantics while recommender systems imply collaborative\nsemantics, making it difficult to sufficiently leverage the model capacity of\nLLMs for recommendation. To address this challenge, in this paper, we propose a\nnew LLM-based recommendation model called LC-Rec, which can better integrate\nlanguage and collaborative semantics for recommender systems. Our approach can\ndirectly generate items from the entire item set for recommendation, without\nrelying on candidate items. Specifically, we make two major contributions in\nour approach. For item indexing, we design a learning-based vector quantization\nmethod with uniform semantic mapping, which can assign meaningful and\nnon-conflicting IDs (called item indices) for items. For alignment tuning, we\npropose a series of specially designed tuning tasks to enhance the integration\nof collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to\ndeeply integrate language and collaborative semantics (characterized by the\nlearned item indices), so as to achieve an effective adaptation to recommender\nsystems. Extensive experiments demonstrate the effectiveness of our method,\nshowing that our approach can outperform a number of competitive baselines\nincluding traditional recommenders and existing LLM-based recommenders. Our\ncode is available at https://github.com/RUCAIBox/LC-Rec/.",
        "translated": ""
    },
    {
        "title": "Towards Graph-Aware Diffusion Modeling for Collaborative Filtering",
        "url": "http://arxiv.org/abs/2311.08744v1",
        "pub_date": "2023-11-15",
        "summary": "Recovering masked feedback with neural models is a popular paradigm in\nrecommender systems. Seeing the success of diffusion models in solving\nill-posed inverse problems, we introduce a conditional diffusion framework for\ncollaborative filtering that iteratively reconstructs a user's hidden\npreferences guided by its historical interactions. To better align with the\nintrinsic characteristics of implicit feedback data, we implement forward\ndiffusion by applying synthetic smoothing filters to interaction signals on an\nitem-item graph. The resulting reverse diffusion can be interpreted as a\npersonalized process that gradually refines preference scores. Through graph\nFourier transform, we equivalently characterize this model as an anisotropic\nGaussian diffusion in the graph spectral domain, establishing both forward and\nreverse formulations. Our model outperforms state-of-the-art methods by a large\nmargin on one dataset and yields competitive results on the others.",
        "translated": ""
    },
    {
        "title": "Enhancing Recommender System Performance by Histogram Equalization",
        "url": "http://arxiv.org/abs/2311.08682v1",
        "pub_date": "2023-11-15",
        "summary": "Recommender system has been researched for decades with millions of different\nversions of algorithms created in the industry. In spite of the huge amount of\nwork spent on the field, there are many basic questions to be answered in the\nfield. The most fundamental question to be answered is the accuracy problem,\nand in recent years, fairness becomes the new buzz word for researchers. In\nthis paper, we borrow an idea from image processing, namely, histogram\nequalization. As a preprocessing step to recommender system algorithms,\nhistogram equalization could enhance both the accuracy and fairness metrics of\nthe recommender system algorithms. In the experiment section, we prove that our\nnew approach could improve vanilla algorithms by a large margin in accuracy\nmetric and stay competitive on fairness metrics.",
        "translated": ""
    },
    {
        "title": "Multi-Set Inoculation: Assessing Model Robustness Across Multiple\n  Challenge Sets",
        "url": "http://arxiv.org/abs/2311.08662v1",
        "pub_date": "2023-11-15",
        "summary": "Language models, given their black-box nature, often exhibit sensitivity to\ninput perturbations, leading to trust issues due to hallucinations. To bolster\ntrust, it's essential to understand these models' failure modes and devise\nstrategies to enhance their performance. In this study, we propose a framework\nto study the effect of input perturbations on language models of different\nscales, from pre-trained models to large language models (LLMs). We use\nfine-tuning to train a robust model to perturbations, and we investigate\nwhether exposure to one perturbation improves or degrades the model's\nperformance on other perturbations. To address multi-perturbation robustness,\nwe suggest three distinct training strategies. We also extend the framework to\nLLMs via a chain of thought(COT) prompting with exemplars. We instantiate our\nframework for the Tabular-NLI task and show that the proposed strategies train\nthe model robust to different perturbations without losing accuracy on a given\ndataset.",
        "translated": ""
    },
    {
        "title": "Stopping Methods for Technology Assisted Reviews based on Point\n  Processes",
        "url": "http://arxiv.org/abs/2311.08597v1",
        "pub_date": "2023-11-14",
        "summary": "Technology Assisted Review (TAR), which aims to reduce the effort required to\nscreen collections of documents for relevance, is used to develop systematic\nreviews of medical evidence and identify documents that must be disclosed in\nresponse to legal proceedings. Stopping methods are algorithms which determine\nwhen to stop screening documents during the TAR process, helping to ensure that\nworkload is minimised while still achieving a high level of recall. This paper\nproposes a novel stopping method based on point processes, which are\nstatistical models that can be used to represent the occurrence of random\nevents. The approach uses rate functions to model the occurrence of relevant\ndocuments in the ranking and compares four candidates, including one that has\nnot previously been used for this purpose (hyperbolic). Evaluation is carried\nout using standard datasets (CLEF e-Health, TREC Total Recall, TREC Legal), and\nthis work is the first to explore stopping method robustness by reporting\nperformance on a range of rankings of varying effectiveness. Results show that\nthe proposed method achieves the desired level of recall without requiring an\nexcessive number of documents to be examined in the majority of cases and also\ncompares well against multiple alternative approaches.",
        "translated": ""
    },
    {
        "title": "IterCQR: Iterative Conversational Query Reformulation without Human\n  Supervision",
        "url": "http://arxiv.org/abs/2311.09820v1",
        "pub_date": "2023-11-16",
        "summary": "In conversational search, which aims to retrieve passages containing\nessential information, queries suffer from high dependency on the preceding\ndialogue context. Therefore, reformulating conversational queries into\nstandalone forms is essential for the effective utilization of off-the-shelf\nretrievers. Previous methodologies for conversational query search frequently\ndepend on human-annotated gold labels. However, these manually crafted queries\noften result in sub-optimal retrieval performance and require high collection\ncosts. In response to these challenges, we propose Iterative Conversational\nQuery Reformulation (IterCQR), a methodology that conducts query reformulation\nwithout relying on human oracles. IterCQR iteratively trains the QR model by\ndirectly leveraging signal from information retrieval (IR) as a reward. Our\nproposed IterCQR method shows state-of-the-art performance on two datasets,\ndemonstrating its effectiveness on both sparse and dense retrievers. Notably,\nIterCQR exhibits robustness in domain-shift, low-resource, and topic-shift\nscenarios.",
        "translated": ""
    },
    {
        "title": "Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in\n  Dense Encoders",
        "url": "http://arxiv.org/abs/2311.09765v1",
        "pub_date": "2023-11-16",
        "summary": "Prevailing research practice today often relies on training dense retrievers\non existing large datasets such as MSMARCO and then experimenting with ways to\nimprove zero-shot generalization capabilities to unseen domains. While prior\nwork has tackled this challenge through resource-intensive steps such as data\naugmentation, architectural modifications, increasing model size, or even\nfurther base model pretraining, comparatively little investigation has examined\nwhether the training procedures themselves can be improved to yield better\ngeneralization capabilities in the resulting models. In this work, we recommend\na simple recipe for training dense encoders: Train on MSMARCO with\nparameter-efficient methods, such as LoRA, and opt for using in-batch negatives\nunless given well-constructed hard negatives. We validate these recommendations\nusing the BEIR benchmark and find results are persistent across choice of dense\nencoder and base model size and are complementary to other resource-intensive\nstrategies for out-of-domain generalization such as architectural modifications\nor additional pretraining. We hope that this thorough and impartial study\naround various training techniques, which augments other resource-intensive\nmethods, offers practical insights for developing a dense retrieval model that\neffectively generalizes, even when trained on a single dataset.",
        "translated": ""
    },
    {
        "title": "GEO: Generative Engine Optimization",
        "url": "http://arxiv.org/abs/2311.09735v1",
        "pub_date": "2023-11-16",
        "summary": "The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of Generative Engines (GEs), has the potential to generate\naccurate and personalized responses, and is rapidly replacing traditional\nsearch engines like Google and Bing. Generative Engines typically satisfy\nqueries by synthesizing information from multiple sources and summarizing them\nwith the help of LLMs. While this shift significantly improves \\textit{user}\nutility and \\textit{generative search engine} traffic, it results in a huge\nchallenge for the third stakeholder -- website and content creators. Given the\nblack-box and fast-moving nature of Generative Engines, content creators have\nlittle to no control over when and how their content is displayed. With\ngenerative engines here to stay, the right tools should be provided to ensure\nthat creator economy is not severely disadvantaged. To address this, we\nintroduce Generative Engine Optimization (GEO), a novel paradigm to aid content\ncreators in improving the visibility of their content in Generative Engine\nresponses through a black-box optimization framework for optimizing and\ndefining visibility metrics. We facilitate systematic evaluation in this new\nparadigm by introducing GEO-bench, a benchmark of diverse user queries across\nmultiple domains, coupled with sources required to answer these queries.\nThrough rigorous evaluation, we show that GEO can boost visibility by up to\n40\\% in generative engine responses. Moreover, we show the efficacy of these\nstrategies varies across domains, underscoring the need for domain-specific\nmethods. Our work opens a new frontier in the field of information discovery\nsystems, with profound implications for generative engines and content\ncreators.",
        "translated": ""
    },
    {
        "title": "AI Recommendation System for Enhanced Customer Experience: A Novel\n  Image-to-Text Method",
        "url": "http://arxiv.org/abs/2311.09624v1",
        "pub_date": "2023-11-16",
        "summary": "Existing fashion recommendation systems encounter difficulties in using\nvisual data for accurate and personalized recommendations. This research\ndescribes an innovative end-to-end pipeline that uses artificial intelligence\nto provide fine-grained visual interpretation for fashion recommendations. When\ncustomers upload images of desired products or outfits, the system\nautomatically generates meaningful descriptions emphasizing stylistic elements.\nThese captions guide retrieval from a global fashion product catalogue to offer\nsimilar alternatives that fit the visual characteristics of the original image.\nOn a dataset of over 100,000 categorized fashion photos, the pipeline was\ntrained and evaluated. The F1-score for the object detection model was 0.97,\nexhibiting exact fashion object recognition capabilities optimized for\nrecommendation. This visually aware system represents a key advancement in\ncustomer engagement through personalized fashion recommendations",
        "translated": ""
    },
    {
        "title": "Group-Aware Interest Disentangled Dual-Training for Personalized\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.09577v1",
        "pub_date": "2023-11-16",
        "summary": "Personalized recommender systems aim to predict users' preferences for items.\nIt has become an indispensable part of online services. Online social platforms\nenable users to form groups based on their common interests. The users' group\nparticipation on social platforms reveals their interests and can be utilized\nas side information to mitigate the data sparsity and cold-start problem in\nrecommender systems. Users join different groups out of different interests. In\nthis paper, we generate group representation from the user's interests and\npropose IGRec (Interest-based Group enhanced Recommendation) to utilize the\ngroup information accurately. It consists of four modules. (1) Interest\ndisentangler via self-gating that disentangles users' interests from their\ninitial embedding representation. (2) Interest aggregator that generates the\ninterest-based group representation by Gumbel-Softmax aggregation on the group\nmembers' interests. (3) Interest-based group aggregation that fuses user's\nrepresentation with the participated group representation. (4) A dual-trained\nrating prediction module to utilize both user-item and group-item interactions.\nWe conduct extensive experiments on three publicly available datasets. Results\nshow IGRec can effectively alleviate the data sparsity problem and enhance the\nrecommender system with interest-based group representation. Experiments on the\ngroup recommendation task further show the informativeness of interest-based\ngroup representation.",
        "translated": ""
    },
    {
        "title": "Scaling User Modeling: Large-scale Online User Representations for Ads\n  Personalization in Meta",
        "url": "http://arxiv.org/abs/2311.09544v1",
        "pub_date": "2023-11-16",
        "summary": "Effective user representations are pivotal in personalized advertising.\nHowever, stringent constraints on training throughput, serving latency, and\nmemory, often limit the complexity and input feature set of online ads ranking\nmodels. This challenge is magnified in extensive systems like Meta's, which\nencompass hundreds of models with diverse specifications, rendering the\ntailoring of user representation learning for each model impractical. To\naddress these challenges, we present Scaling User Modeling (SUM), a framework\nwidely deployed in Meta's ads ranking system, designed to facilitate efficient\nand scalable sharing of online user representation across hundreds of ads\nmodels. SUM leverages a few designated upstream user models to synthesize user\nembeddings from massive amounts of user features with advanced modeling\ntechniques. These embeddings then serve as inputs to downstream online ads\nranking models, promoting efficient representation sharing. To adapt to the\ndynamic nature of user features and ensure embedding freshness, we designed SUM\nOnline Asynchronous Platform (SOAP), a latency free online serving system\ncomplemented with model freshness and embedding stabilization, which enables\nfrequent user model updates and online inference of user embeddings upon each\nuser request. We share our hands-on deployment experiences for the SUM\nframework and validate its superiority through comprehensive experiments. To\ndate, SUM has been launched to hundreds of ads ranking models in Meta,\nprocessing hundreds of billions of user requests daily, yielding significant\nonline metric gains and infrastructure cost savings.",
        "translated": ""
    },
    {
        "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented\n  Generation Systems",
        "url": "http://arxiv.org/abs/2311.09476v1",
        "pub_date": "2023-11-16",
        "summary": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies\non hand annotations for input queries, passages to retrieve, and responses to\ngenerate. We introduce ARES, an Automated RAG Evaluation System, for evaluating\nRAG systems along the dimensions of context relevance, answer faithfulness, and\nanswer relevance. Using synthetic training data, ARES finetunes lightweight LM\njudges to assess the quality of individual RAG components. To mitigate\npotential prediction errors, ARES utilizes a small set of human-annotated\ndatapoints for prediction-powered inference (PPI). Across six different\nknowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG\nsystems while using a few hundred human annotations during evaluation.\nFurthermore, ARES judges remain effective across domain shifts, proving\naccurate even after changing the type of queries and/or documents used in the\nevaluated RAG systems. We make our datasets and code for replication and\ndeployment available at https://github.com/stanford-futuredata/ARES.",
        "translated": ""
    },
    {
        "title": "Labeled Interactive Topic Models",
        "url": "http://arxiv.org/abs/2311.09438v1",
        "pub_date": "2023-11-15",
        "summary": "Topic models help users understand large document collections; however, topic\nmodels do not always find the ``right'' topics. While classical probabilistic\nand anchor-based topic models have interactive variants to guide models toward\nbetter topics, such interactions are not available for neural topic models such\nas the embedded topic model (\\abr{etm}). We correct this lacuna by adding an\nintuitive interaction to neural topic models: users can label a topic with a\nword, and topics are updated so that the topic words are close to the label.\nThis allows a user to refine topics based on their information need. While,\ninteractivity is intuitive for \\abr{etm}, we extend this framework to work with\nother neural topic models as well. We develop an interactive interface which\nallows users to interact and relabel topic models as they see fit. We evaluate\nour method through a human study, where users can relabel topics to find\nrelevant documents. Using our method, user labeling improves document rank\nscores, helping to find more relevant documents to a given query when compared\nto no user labeling.",
        "translated": ""
    },
    {
        "title": "PEFT-MedAware: Large Language Model for Medical Awareness",
        "url": "http://arxiv.org/abs/2311.10697v1",
        "pub_date": "2023-11-17",
        "summary": "Chat models are capable of answering a wide range of questions, however, the\naccuracy of their responses is highly uncertain. In this research, we propose a\nspecialized PEFT-MedAware model where we utilize parameter-efficient\nfine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized\nMedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of\nits trainable parameters to enhance computational efficiency. The paper adopts\ndata preprocessing and PEFT to optimize model performance, complemented by a\nBitsAndBytesConfig for efficient transformer training. The resulting model was\ncapable of outperforming other LLMs in medical question-answering tasks in\nspecific domains with greater accuracy utilizing limited computational\nresources making it suitable for deployment in resource-constrained\nenvironments. We propose further improvements through expanded datasets, larger\nmodels, and feedback mechanisms for sustained medical relevancy. Our work\nhighlights the efficiency gains and specialized capabilities of PEFT in medical\nAI, outpacing standard models in precision without extensive resource demands.\nThe proposed model and data are released for research purposes only.",
        "translated": ""
    },
    {
        "title": "Ex2Vec: Characterizing Users and Items from the Mere Exposure Effect",
        "url": "http://arxiv.org/abs/2311.10635v1",
        "pub_date": "2023-11-17",
        "summary": "The traditional recommendation framework seeks to connect user and content,\nby finding the best match possible based on users past interaction. However, a\ngood content recommendation is not necessarily similar to what the user has\nchosen in the past. As humans, users naturally evolve, learn, forget, get\nbored, they change their perspective of the world and in consequence, of the\nrecommendable content. One well known mechanism that affects user interest is\nthe Mere Exposure Effect: when repeatedly exposed to stimuli, users' interest\ntends to rise with the initial exposures, reaching a peak, and gradually\ndecreasing thereafter, resulting in an inverted-U shape. Since previous\nresearch has shown that the magnitude of the effect depends on a number of\ninteresting factors such as stimulus complexity and familiarity, leveraging\nthis effect is a way to not only improve repeated recommendation but to gain a\nmore in-depth understanding of both users and stimuli. In this work we present\n(Mere) Exposure2Vec (Ex2Vec) our model that leverages the Mere Exposure Effect\nin repeat consumption to derive user and item characterization and track user\ninterest evolution. We validate our model through predicting future music\nconsumption based on repetition and discuss its implications for recommendation\nscenarios where repetition is common.",
        "translated": ""
    },
    {
        "title": "Cross-Modal Search and Exploration of Greek Painted Pottery",
        "url": "http://arxiv.org/abs/2311.10567v1",
        "pub_date": "2023-11-17",
        "summary": "This paper focuses on digitally-supported research methods for an important\ngroup of cultural heritage objects, the Greek pottery, especially with figured\ndecoration. The design, development and application of new digital methods for\nsearching, comparing, and visually exploring these vases needs an\ninterdisciplinary approach to effectively analyse the various features of the\nvases, like shape, decoration, and manufacturing techniques, and relationships\nbetween the vases. We motivate the need and opportunities by a multimodal\nrepresentation of the objects, including 3D shape, material, and painting. We\nthen illustrate a range of innovative methods for these representations,\nincluding quantified surface and capacity comparison, material analysis, image\nflattening from 3D objects, retrieval and comparison of shapes and paintings,\nand multidimensional data visualization. We also discuss challenges and future\nwork in this area.",
        "translated": ""
    },
    {
        "title": "Collaborative Word-based Pre-trained Item Representation for\n  Transferable Recommendation",
        "url": "http://arxiv.org/abs/2311.10501v1",
        "pub_date": "2023-11-17",
        "summary": "Item representation learning (IRL) plays an essential role in recommender\nsystems, especially for sequential recommendation. Traditional sequential\nrecommendation models usually utilize ID embeddings to represent items, which\nare not shared across different domains and lack the transferable ability.\nRecent studies use pre-trained language models (PLM) for item text embeddings\n(text-based IRL) that are universally applicable across domains. However, the\nexisting text-based IRL is unaware of the important collaborative filtering\n(CF) information. In this paper, we propose CoWPiRec, an approach of\nCollaborative Word-based Pre-trained item representation for Recommendation. To\neffectively incorporate CF information into text-based IRL, we convert the\nitem-level interaction data to a word graph containing word-level\ncollaborations. Subsequently, we design a novel pre-training task to align the\nword-level semantic- and CF-related item representation. Extensive experimental\nresults on multiple public datasets demonstrate that compared to\nstate-of-the-art transferable sequential recommenders, CoWPiRec achieves\nsignificantly better performances in both fine-tuning and zero-shot settings\nfor cross-scenario recommendation and effectively alleviates the cold-start\nissue. The code is available at: https://github.com/ysh-1998/CoWPiRec.",
        "translated": ""
    },
    {
        "title": "A Comparative Analysis of Retrievability and PageRank Measures",
        "url": "http://arxiv.org/abs/2311.10348v1",
        "pub_date": "2023-11-17",
        "summary": "The accessibility of documents within a collection holds a pivotal role in\nInformation Retrieval, signifying the ease of locating specific content in a\ncollection of documents. This accessibility can be achieved via two distinct\navenues. The first is through some retrieval model using a keyword or other\nfeature-based search, and the other is where a document can be navigated using\nlinks associated with them, if available. Metrics such as PageRank, Hub, and\nAuthority illuminate the pathways through which documents can be discovered\nwithin the network of content while the concept of Retrievability is used to\nquantify the ease with which a document can be found by a retrieval model. In\nthis paper, we compare these two perspectives, PageRank and retrievability, as\nthey quantify the importance and discoverability of content in a corpus.\nThrough empirical experimentation on benchmark datasets, we demonstrate a\nsubtle similarity between retrievability and PageRank particularly\ndistinguishable for larger datasets.",
        "translated": ""
    },
    {
        "title": "Graph Variational Embedding Collaborative Filtering",
        "url": "http://arxiv.org/abs/2311.11824v1",
        "pub_date": "2023-11-20",
        "summary": "The customization of recommended content to users holds significant\nimportance in enhancing user experiences across a wide spectrum of applications\nsuch as e-commerce, music, and shopping. Graph-based methods have achieved\nconsiderable performance by capturing user-item interactions. However, these\nmethods tend to utilize randomly constructed embeddings in the dataset used for\ntraining the recommender, which lacks any user preferences. Here, we propose\nthe concept of variational embeddings as a means of pre-training the\nrecommender system to improve the feature propagation through the layers of\ngraph convolutional networks (GCNs). The graph variational embedding\ncollaborative filtering (GVECF) is introduced as a novel framework to\nincorporate representations learned through a variational graph auto-encoder\nwhich are embedded into a GCN-based collaborative filtering. This approach\neffectively transforms latent high-order user-item interactions into more\ntrainable vectors, ultimately resulting in better performance in terms of\nrecall and normalized discounted cumulative gain(NDCG) metrics. The experiments\nconducted on benchmark datasets demonstrate that our proposed method achieves\nup to 13.78% improvement in the recall over the test data.",
        "translated": ""
    },
    {
        "title": "Control in Hybrid Chatbots",
        "url": "http://arxiv.org/abs/2311.11701v1",
        "pub_date": "2023-11-20",
        "summary": "Customer data typically is held in database systems, which can be seen as\nrule-based knowledge base, whereas businesses increasingly want to benefit from\nthe capabilities of large, pre-trained language models.\n  In this technical report, we describe a case study of how a commercial rule\nengine and an integrated neural chatbot may be integrated, and what level of\ncontrol that particular integration mode leads to. We also discuss alternative\nways (including past ways realized in other systems) how researchers strive to\nmaintain control and avoid what has recently been called model \"hallucination\".",
        "translated": ""
    },
    {
        "title": "Towards Robust Text Retrieval with Progressive Learning",
        "url": "http://arxiv.org/abs/2311.11691v1",
        "pub_date": "2023-11-20",
        "summary": "Retrieval augmentation has become an effective solution to empower large\nlanguage models (LLMs) with external and verified knowledge sources from the\ndatabase, which overcomes the limitations and hallucinations of LLMs in\nhandling up-to-date and domain-specific information. However, existing\nembedding models for text retrieval usually have three non-negligible\nlimitations. First, the number and diversity of samples in a batch are too\nrestricted to supervise the modeling of textual nuances at scale. Second, the\nhigh proportional noise are detrimental to the semantic correctness and\nconsistency of embeddings. Third, the equal treatment to easy and difficult\nsamples would cause sub-optimum convergence of embeddings with poorer\ngeneralization. In this paper, we propose the PEG, a progressively learned\nembeddings for robust text retrieval. Specifically, we increase the training\nin-batch negative samples to 80,000, and for each query, we extracted five hard\nnegatives. Concurrently, we incorporated a progressive learning mechanism,\nenabling the model to dynamically modulate its attention to the samples\nthroughout the entire training process. Additionally, PEG is trained on more\nthan 100 million data, encompassing a wide range of domains (e.g., finance,\nmedicine, and tourism) and covering various tasks (e.g., question-answering,\nmachine reading comprehension, and similarity matching). Extensive experiments\nconducted on C-MTEB and DuReader demonstrate that PEG surpasses\nstate-of-the-art embeddings in retrieving true positives, highlighting its\nsignificant potential for applications in LLMs. Our model is publicly available\nat https://huggingface.co/TownsWu/PEG.",
        "translated": ""
    },
    {
        "title": "Scaling Law of Large Sequential Recommendation Models",
        "url": "http://arxiv.org/abs/2311.11351v1",
        "pub_date": "2023-11-19",
        "summary": "Scaling of neural networks has recently shown great potential to improve the\nmodel capacity in various fields. Specifically, model performance has a\npower-law relationship with model size or data size, which provides important\nguidance for the development of large-scale models. However, there is still\nlimited understanding on the scaling effect of user behavior models in\nrecommender systems, where the unique data characteristics (e.g. data scarcity\nand sparsity) pose new challenges to explore the scaling effect in\nrecommendation tasks. In this work, we focus on investigating the scaling laws\nin large sequential recommendation models. Specially, we consider a pure\nID-based task formulation, where the interaction history of a user is formatted\nas a chronological sequence of item IDs. We don't incorporate any side\ninformation (e.g. item text), because we would like to explore how scaling law\nholds from the perspective of user behavior. With specially improved\nstrategies, we scale up the model size to 0.8B parameters, making it feasible\nto explore the scaling effect in a diverse range of model sizes. As the major\nfindings, we empirically show that scaling law still holds for these trained\nmodels, even in data-constrained scenarios. We then fit the curve for scaling\nlaw, and successfully predict the test loss of the two largest tested model\nscales. Furthermore, we examine the performance advantage of scaling effect on\nfive challenging recommendation tasks, considering the unique issues (e.g. cold\nstart, robustness, long-term preference) in recommender systems. We find that\nscaling up the model size can greatly boost the performance on these\nchallenging tasks, which again verifies the benefits of large recommendation\nmodels.",
        "translated": ""
    },
    {
        "title": "Dependency Relationships-Enhanced Attentive Group Recommendation in HINs",
        "url": "http://arxiv.org/abs/2311.11239v1",
        "pub_date": "2023-11-19",
        "summary": "Recommending suitable items to a group of users, commonly referred to as the\ngroup recommendation task, is becoming increasingly urgent with the development\nof group activities. The challenges within the group recommendation task\ninvolve aggregating the individual preferences of group members as the group's\npreferences and facing serious sparsity problems due to the lack of\nuser/group-item interactions. To solve these problems, we propose a novel\napproach called Dependency Relationships-Enhanced Attentive Group\nRecommendation (DREAGR) for the recommendation task of occasional groups.\nSpecifically, we introduce the dependency relationship between items as side\ninformation to enhance the user/group-item interaction and alleviate the\ninteraction sparsity problem. Then, we propose a Path-Aware Attention Embedding\n(PAAE) method to model users' preferences on different types of paths. Next, we\ndesign a gated fusion mechanism to fuse users' preferences into their\ncomprehensive preferences. Finally, we develop an attention aggregator that\naggregates users' preferences as the group's preferences for the group\nrecommendation task. We conducted experiments on two datasets to demonstrate\nthe superiority of DREAGR by comparing it with state-of-the-art group\nrecommender models. The experimental results show that DREAGR outperforms other\nmodels, especially HR@N and NDCG@N (N=5, 10), where DREAGR has improved in the\nrange of 3.64% to 7.01% and 2.57% to 3.39% on both datasets, respectively.",
        "translated": ""
    },
    {
        "title": "An Interactive Query Generation Assistant using LLM-based Prompt\n  Modification and User Feedback",
        "url": "http://arxiv.org/abs/2311.11226v1",
        "pub_date": "2023-11-19",
        "summary": "While search is the predominant method of accessing information, formulating\neffective queries remains a challenging task, especially for situations where\nthe users are not familiar with a domain, or searching for documents in other\nlanguages, or looking for complex information such as events, which are not\neasily expressible as queries. Providing example documents or passages of\ninterest, might be easier for a user, however, such query-by-example scenarios\nare prone to concept drift, and are highly sensitive to the query generation\nmethod. This demo illustrates complementary approaches of using LLMs\ninteractively, assisting and enabling the user to provide edits and feedback at\nall stages of the query formulation process. The proposed Query Generation\nAssistant is a novel search interface which supports automatic and interactive\nquery generation over a mono-linguial or multi-lingual document collection.\nSpecifically, the proposed assistive interface enables the users to refine the\nqueries generated by different LLMs, to provide feedback on the retrieved\ndocuments or passages, and is able to incorporate the users' feedback as\nprompts to generate more effective queries. The proposed interface is a\nvaluable experimental tool for exploring fine-tuning and prompting of LLMs for\nquery generation to qualitatively evaluate the effectiveness of retrieval and\nranking models, and for conducting Human-in-the-Loop (HITL) experiments for\ncomplex search tasks where users struggle to formulate queries without such\nassistance.",
        "translated": ""
    },
    {
        "title": "Contextualizing Internet Memes Across Social Media Platforms",
        "url": "http://arxiv.org/abs/2311.11157v1",
        "pub_date": "2023-11-18",
        "summary": "Internet memes have emerged as a novel format for communication and\nexpressing ideas on the web. Their fluidity and creative nature are reflected\nin their widespread use, often across platforms and occasionally for unethical\nor harmful purposes. While computational work has already analyzed their\nhigh-level virality over time and developed specialized classifiers for hate\nspeech detection, there have been no efforts to date that aim to holistically\ntrack, identify, and map internet memes posted on social media. To bridge this\ngap, we investigate whether internet memes across social media platforms can be\ncontextualized by using a semantic repository of knowledge, namely, a knowledge\ngraph. We collect thousands of potential internet meme posts from two social\nmedia platforms, namely Reddit and Discord, and perform an\nextract-transform-load procedure to create a data lake with candidate meme\nposts. By using vision transformer-based similarity, we match these candidates\nagainst the memes cataloged in a recently released knowledge graph of internet\nmemes, IMKG. We provide evidence that memes published online can be identified\nby mapping them to IMKG. We leverage this grounding to study the prevalence of\nmemes on different platforms, discover popular memes, and select common meme\nchannels and subreddits. Finally, we illustrate how the grounding can enable\nusers to get context about memes on social media thanks to their link to the\nknowledge graph.",
        "translated": ""
    },
    {
        "title": "SBTRec- A Transformer Framework for Personalized Tour Recommendation\n  Problem with Sentiment Analysis",
        "url": "http://arxiv.org/abs/2311.11071v1",
        "pub_date": "2023-11-18",
        "summary": "When traveling to an unfamiliar city for holidays, tourists often rely on\nguidebooks, travel websites, or recommendation systems to plan their daily\nitineraries and explore popular points of interest (POIs). However, these\napproaches may lack optimization in terms of time feasibility, localities, and\nuser preferences. In this paper, we propose the SBTRec algorithm: a BERT-based\nTrajectory Recommendation with sentiment analysis, for recommending\npersonalized sequences of POIs as itineraries. The key contributions of this\nwork include analyzing users' check-ins and uploaded photos to understand the\nrelationship between POI visits and distance. We introduce SBTRec, which\nencompasses sentiment analysis to improve recommendation accuracy by\nunderstanding users' preferences and satisfaction levels from reviews and\ncomments about different POIs. Our proposed algorithms are evaluated against\nother sequence prediction methods using datasets from 8 cities. The results\ndemonstrate that SBTRec achieves an average F1 score of 61.45%, outperforming\nbaseline algorithms.\n  The paper further discusses the flexibility of the SBTRec algorithm, its\nability to adapt to different scenarios and cities without modification, and\nits potential for extension by incorporating additional information for more\nreliable predictions. Overall, SBTRec provides personalized and relevant POI\nrecommendations, enhancing tourists' overall trip experiences. Future work\nincludes fine-tuning personalized embeddings for users, with evaluation of\nusers' comments on POIs,~to further enhance prediction accuracy.",
        "translated": ""
    },
    {
        "title": "RecExplainer: Aligning Large Language Models for Recommendation Model\n  Interpretability",
        "url": "http://arxiv.org/abs/2311.10947v1",
        "pub_date": "2023-11-18",
        "summary": "Recommender systems are widely used in various online services, with\nembedding-based models being particularly popular due to their expressiveness\nin representing complex signals. However, these models often lack\ninterpretability, making them less reliable and transparent for both users and\ndevelopers. With the emergence of large language models (LLMs), we find that\ntheir capabilities in language expression, knowledge-aware reasoning, and\ninstruction following are exceptionally powerful. Based on this, we propose a\nnew model interpretation approach for recommender systems, by using LLMs as\nsurrogate models and learn to mimic and comprehend target recommender models.\nSpecifically, we introduce three alignment methods: behavior alignment,\nintention alignment, and hybrid alignment. Behavior alignment operates in the\nlanguage space, representing user preferences and item information as text to\nlearn the recommendation model's behavior; intention alignment works in the\nlatent space of the recommendation model, using user and item representations\nto understand the model's behavior; hybrid alignment combines both language and\nlatent spaces for alignment training. To demonstrate the effectiveness of our\nmethods, we conduct evaluation from two perspectives: alignment effect, and\nexplanation generation ability on three public datasets. Experimental results\nindicate that our approach effectively enables LLMs to comprehend the patterns\nof recommendation models and generate highly credible recommendation\nexplanations.",
        "translated": ""
    },
    {
        "title": "Explainable Product Classification for Customs",
        "url": "http://arxiv.org/abs/2311.10922v1",
        "pub_date": "2023-11-18",
        "summary": "The task of assigning internationally accepted commodity codes (aka HS codes)\nto traded goods is a critical function of customs offices. Like court decisions\nmade by judges, this task follows the doctrine of precedent and can be\nnontrivial even for experienced officers. Together with the Korea Customs\nService (KCS), we propose a first-ever explainable decision supporting model\nthat suggests the most likely subheadings (i.e., the first six digits) of the\nHS code. The model also provides reasoning for its suggestion in the form of a\ndocument that is interpretable by customs officers. We evaluated the model\nusing 5,000 cases that recently received a classification request. The results\nshowed that the top-3 suggestions made by our model had an accuracy of 93.9\\%\nwhen classifying 925 challenging subheadings. A user study with 32 customs\nexperts further confirmed that our algorithmic suggestions accompanied by\nexplainable reasonings, can substantially reduce the time and effort taken by\ncustoms officers for classification reviews.",
        "translated": ""
    },
    {
        "title": "CSMeD: Bridging the Dataset Gap in Automated Citation Screening for\n  Systematic Literature Reviews",
        "url": "http://arxiv.org/abs/2311.12474v1",
        "pub_date": "2023-11-21",
        "summary": "Systematic literature reviews (SLRs) play an essential role in summarising,\nsynthesising and validating scientific evidence. In recent years, there has\nbeen a growing interest in using machine learning techniques to automate the\nidentification of relevant studies for SLRs. However, the lack of standardised\nevaluation datasets makes comparing the performance of such automated\nliterature screening systems difficult. In this paper, we analyse the citation\nscreening evaluation datasets, revealing that many of the available datasets\nare either too small, suffer from data leakage or have limited applicability to\nsystems treating automated literature screening as a classification task, as\nopposed to, for example, a retrieval or question-answering task. To address\nthese challenges, we introduce CSMeD, a meta-dataset consolidating nine\npublicly released collections, providing unified access to 325 SLRs from the\nfields of medicine and computer science. CSMeD serves as a comprehensive\nresource for training and evaluating the performance of automated citation\nscreening models. Additionally, we introduce CSMeD-FT, a new dataset designed\nexplicitly for evaluating the full text publication screening task. To\ndemonstrate the utility of CSMeD, we conduct experiments and establish\nbaselines on new datasets.",
        "translated": ""
    },
    {
        "title": "InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk\n  Factors in Reddit Posts",
        "url": "http://arxiv.org/abs/2311.12404v1",
        "pub_date": "2023-11-21",
        "summary": "Mental health professionals and clinicians have observed the upsurge of\nmental disorders due to Interpersonal Risk Factors (IRFs). To simulate the\nhuman-in-the-loop triaging scenario for early detection of mental health\ndisorders, we recognized textual indications to ascertain these IRFs : Thwarted\nBelongingness (TBe) and Perceived Burdensomeness (PBu) within personal\nnarratives. In light of this, we use N-shot learning with GPT-3 model on the\nIRF dataset, and underscored the importance of fine-tuning GPT-3 model to\nincorporate the context-specific sensitivity and the interconnectedness of\ntextual cues that represent both IRFs.\n  In this paper, we introduce an Interpretable Prompting (InterPrompt)} method\nto boost the attention mechanism by fine-tuning the GPT-3 model. This allows a\nmore sophisticated level of language modification by adjusting the pre-trained\nweights. Our model learns to detect usual patterns and underlying connections\nacross both the IRFs, which leads to better system-level explainability and\ntrustworthiness. The results of our research demonstrate that all four variants\nof GPT-3 model, when fine-tuned with InterPrompt, perform considerably better\nas compared to the baseline methods, both in terms of classification and\nexplanation generation.",
        "translated": ""
    },
    {
        "title": "Linear-time online visibility graph transformation algorithm: for both\n  natural and horizontal visibility criteria",
        "url": "http://arxiv.org/abs/2311.12389v1",
        "pub_date": "2023-11-21",
        "summary": "Visibility graph (VG) transformation is a technique used to convert a time\nseries into a graph based on specific visibility criteria. It has attracted\nincreasing interest in the fields of time series analysis, forecasting, and\nclassification. Optimizing the VG transformation algorithm to accelerate the\nprocess is a critical aspect of VG-related research, as it enhances the\napplicability of VG transformation in latency-sensitive areas and conserves\ncomputational resources. In the real world, many time series are presented in\nthe form of data streams. Despite the proposal of the concept of VG's online\nfunctionality, previous studies have not thoroughly explored the acceleration\nof VG transformation by leveraging the characteristics of data streams. In this\npaper, we propose that an efficient online VG algorithm should adhere to two\ncriteria and develop a linear-time method, termed the LOT framework, for both\nnatural and horizontal visibility graph transformations in data stream\nscenarios. Experiments are conducted on two datasets, comparing our approach\nwith five existing methods as baselines. The results demonstrate the validity\nand promising computational efficiency of our framework.",
        "translated": ""
    },
    {
        "title": "Utilizing Language Models for Tour Itinerary Recommendation",
        "url": "http://arxiv.org/abs/2311.12355v1",
        "pub_date": "2023-11-21",
        "summary": "Tour itinerary recommendation involves planning a sequence of relevant\nPoint-of-Interest (POIs), which combines challenges from the fields of both\nOperations Research (OR) and Recommendation Systems (RS). As an OR problem,\nthere is the need to maximize a certain utility (e.g., popularity of POIs in\nthe tour) while adhering to some constraints (e.g., maximum time for the tour).\nAs a RS problem, it is heavily related to problem or filtering or ranking a\nsubset of POIs that are relevant to a user and recommending it as part of an\nitinerary. In this paper, we explore the use of language models for the task of\ntour itinerary recommendation and planning. This task has the unique\nrequirement of recommending personalized POIs relevant to users and planning\nthese POIs as an itinerary that satisfies various constraints. We discuss some\napproaches in this area, such as using word embedding techniques like Word2Vec\nand GloVe for learning POI embeddings and transformer-based techniques like\nBERT for generating\n  itineraries.",
        "translated": ""
    },
    {
        "title": "A Survey on Large Language Models for Personalized and Explainable\n  Recommendations",
        "url": "http://arxiv.org/abs/2311.12338v1",
        "pub_date": "2023-11-21",
        "summary": "In recent years, Recommender Systems(RS) have witnessed a transformative\nshift with the advent of Large Language Models(LLMs) in the field of Natural\nLanguage Processing(NLP). These models such as OpenAI's GPT-3.5/4, Llama from\nMeta, have demonstrated unprecedented capabilities in understanding and\ngenerating human-like text. This has led to a paradigm shift in the realm of\npersonalized and explainable recommendations, as LLMs offer a versatile toolset\nfor processing vast amounts of textual data to enhance user experiences. To\nprovide a comprehensive understanding of the existing LLM-based recommendation\nsystems, this survey aims to analyze how RS can benefit from LLM-based\nmethodologies. Furthermore, we describe major challenges in Personalized\nExplanation Generating(PEG) tasks, which are cold-start problems, unfairness\nand bias problems in RS.",
        "translated": ""
    },
    {
        "title": "Graph Neural Ordinary Differential Equations-based method for\n  Collaborative Filtering",
        "url": "http://arxiv.org/abs/2311.12329v1",
        "pub_date": "2023-11-21",
        "summary": "Graph Convolution Networks (GCNs) are widely considered state-of-the-art for\ncollaborative filtering. Although several GCN-based methods have been proposed\nand achieved state-of-the-art performance in various tasks, they can be\ncomputationally expensive and time-consuming to train if too many layers are\ncreated. However, since the linear GCN model can be interpreted as a\ndifferential equation, it is possible to transfer it to an ODE problem. This\ninspired us to address the computational limitations of GCN-based models by\ndesigning a simple and efficient NODE-based model that can skip some GCN layers\nto reach the final state, thus avoiding the need to create many layers. In this\nwork, we propose a Graph Neural Ordinary Differential Equation-based method for\nCollaborative Filtering (GODE-CF). This method estimates the final embedding by\nutilizing the information captured by one or two GCN layers. To validate our\napproach, we conducted experiments on multiple datasets. The results\ndemonstrate that our model outperforms competitive baselines, including\nGCN-based models and other state-of-the-art CF methods. Notably, our proposed\nGODE-CF model has several advantages over traditional GCN-based models. It is\nsimple, efficient, and has a fast training time, making it a practical choice\nfor real-world situations.",
        "translated": ""
    },
    {
        "title": "Adapting LLMs for Efficient, Personalized Information Retrieval: Methods\n  and Implications",
        "url": "http://arxiv.org/abs/2311.12287v1",
        "pub_date": "2023-11-21",
        "summary": "The advent of Large Language Models (LLMs) heralds a pivotal shift in online\nuser interactions with information. Traditional Information Retrieval (IR)\nsystems primarily relied on query-document matching, whereas LLMs excel in\ncomprehending and generating human-like text, thereby enriching the IR\nexperience significantly. While LLMs are often associated with chatbot\nfunctionalities, this paper extends the discussion to their explicit\napplication in information retrieval. We explore methodologies to optimize the\nretrieval process, select optimal models, and effectively scale and orchestrate\nLLMs, aiming for cost-efficiency and enhanced result accuracy. A notable\nchallenge, model hallucination-where the model yields inaccurate or\nmisinterpreted data-is addressed alongside other model-specific hurdles. Our\ndiscourse extends to crucial considerations including user privacy, data\noptimization, and the necessity for system clarity and interpretability.\nThrough a comprehensive examination, we unveil not only innovative strategies\nfor integrating Language Models (LLMs) with Information Retrieval (IR) systems,\nbut also the consequential considerations that underline the need for a\nbalanced approach aligned with user-centric principles.",
        "translated": ""
    },
    {
        "title": "Equipping Pretrained Unconditional Music Transformers with Instrument\n  and Genre Controls",
        "url": "http://arxiv.org/abs/2311.12257v1",
        "pub_date": "2023-11-21",
        "summary": "The ''pretraining-and-finetuning'' paradigm has become a norm for training\ndomain-specific models in natural language processing and computer vision. In\nthis work, we aim to examine this paradigm for symbolic music generation\nthrough leveraging the largest ever symbolic music dataset sourced from the\nMuseScore forum. We first pretrain a large unconditional transformer model\nusing 1.5 million songs. We then propose a simple technique to equip this\npretrained unconditional music transformer model with instrument and genre\ncontrols by finetuning the model with additional control tokens. Our proposed\nrepresentation offers improved high-level controllability and expressiveness\nagainst two existing representations. The experimental results show that the\nproposed model can successfully generate music with user-specified instruments\nand genre. In a subjective listening test, the proposed model outperforms the\npretrained baseline model in terms of coherence, harmony, arrangement and\noverall quality.",
        "translated": ""
    },
    {
        "title": "Conditional Modeling Based Automatic Video Summarization",
        "url": "http://arxiv.org/abs/2311.12159v1",
        "pub_date": "2023-11-20",
        "summary": "The aim of video summarization is to shorten videos automatically while\nretaining the key information necessary to convey the overall story. Video\nsummarization methods mainly rely on visual factors, such as visual\nconsecutiveness and diversity, which may not be sufficient to fully understand\nthe content of the video. There are other non-visual factors, such as\ninterestingness, representativeness, and storyline consistency that should also\nbe considered for generating high-quality video summaries. Current methods do\nnot adequately take into account these non-visual factors, resulting in\nsuboptimal performance. In this work, a new approach to video summarization is\nproposed based on insights gained from how humans create ground truth video\nsummaries. The method utilizes a conditional modeling perspective and\nintroduces multiple meaningful random variables and joint distributions to\ncharacterize the key components of video summarization. Helper distributions\nare employed to improve the training of the model. A conditional attention\nmodule is designed to mitigate potential performance degradation in the\npresence of multi-modal input. The proposed video summarization method\nincorporates the above innovative design choices that aim to narrow the gap\nbetween human-generated and machine-generated video summaries. Extensive\nexperiments show that the proposed approach outperforms existing methods and\nachieves state-of-the-art performance on commonly used video summarization\ndatasets.",
        "translated": ""
    },
    {
        "title": "Multi-view Graph Convolution for Participant Recommendation",
        "url": "http://arxiv.org/abs/2311.12136v1",
        "pub_date": "2023-11-20",
        "summary": "Social networks have become essential for people's lives. The proliferation\nof web services further expands social networks at an unprecedented scale,\nleading to immeasurable commercial value for online platforms. Recently, the\ngroup buying (GB) business mode is prevalent and also becoming more popular in\nE-commerce. GB explicitly forms groups of users with similar interests to\nsecure better discounts from the merchants, often operating within social\nnetworks. It is a novel way to further unlock the commercial value by\nexplicitly utilizing the online social network in E-commerce. Participant\nrecommendation, a fundamental problem emerging together with GB, aims to find\nthe participants for a launched group buying process with an initiator and a\ntarget item to increase the GB success rate. This paper proposes Multi-View\nGraph Convolution for Participant Recommendation (MVPRec) to tackle this\nproblem. To differentiate the roles of users (Initiator/Participant) within the\nGB process, we explicitly reconstruct historical GB data into initiator-view\nand participant-view graphs. Together with the social graph, we obtain a\nmulti-view user representation with graph encoders. Then MVPRec fuses the GB\nand social representation with an attention module to obtain the user\nrepresentation and learns a matching score with the initiator's social friends\nvia a multi-head attention mechanism. Social friends with the Top-k matching\nscore are recommended for the corresponding GB process. Experiments on three\ndatasets justify the effectiveness of MVPRec in the emerging participant\nrecommendation problem.",
        "translated": ""
    },
    {
        "title": "Drilling Down into the Discourse Structure with LLMs for Long Document\n  Question Answering",
        "url": "http://arxiv.org/abs/2311.13565v1",
        "pub_date": "2023-11-22",
        "summary": "We address the task of evidence retrieval for long document question\nanswering, which involves locating relevant paragraphs within a document to\nanswer a question. We aim to assess the applicability of large language models\n(LLMs) in the task of zero-shot long document evidence retrieval, owing to\ntheir unprecedented performance across various NLP tasks. However, currently\nthe LLMs can consume limited context lengths as input, thus providing document\nchunks as inputs might overlook the global context while missing out on\ncapturing the inter-segment dependencies. Moreover, directly feeding the large\ninput sets can incur significant computational costs, particularly when\nprocessing the entire document (and potentially incurring monetary expenses\nwith enterprise APIs like OpenAI's GPT variants). To address these challenges,\nwe propose a suite of techniques that exploit the discourse structure commonly\nfound in documents. By utilizing this structure, we create a condensed\nrepresentation of the document, enabling a more comprehensive understanding and\nanalysis of relationships between different parts. We retain $99.6\\%$ of the\nbest zero-shot approach's performance, while processing only $26\\%$ of the\ntotal tokens used by the best approach in the information seeking evidence\nretrieval setup. We also show how our approach can be combined with\n\\textit{self-ask} reasoning agent to achieve best zero-shot performance in\ncomplex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot\nperformance using gold evidence.",
        "translated": ""
    },
    {
        "title": "LM-Cocktail: Resilient Tuning of Language Models via Model Merging",
        "url": "http://arxiv.org/abs/2311.13534v1",
        "pub_date": "2023-11-22",
        "summary": "The pre-trained language models are continually fine-tuned to better support\ndownstream applications. However, this operation may result in significant\nperformance degeneration on general tasks beyond the targeted domain. To\novercome this problem, we propose a novel method which enables the fine-tuned\nmodel to stay resilient in general perspectives. Our method is conducted in the\nform of model merging (namely LM-Cocktail), where the fine-tuned language model\nis merged with the pre-trained base model or the peer models from other domains\nthrough weighted average. Despite simplicity, LM-Cocktail is surprisingly\neffective: the resulted model is able to achieve a strong empirical performance\nin the whole scope of general tasks while preserving a superior capacity in its\ntargeted domain. We conduct comprehensive experiments with LLama and BGE model\non popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the\nefficacy of our proposed method. The code and checkpoints are available at\nhttps://github.com/FlagOpen/FlagEmbedding.",
        "translated": ""
    },
    {
        "title": "A Comparative Analysis of Supportive Navigation on Movie Recommenders",
        "url": "http://arxiv.org/abs/2311.13494v1",
        "pub_date": "2023-11-22",
        "summary": "This literature review covers the research and thought process that went into\nmaking a solution for the infinite scrolling problem faced in streaming\nservices such as Netflix. Using the data collected, we have come to the\nconclusion that an alternate layout can somewhat alleviate the problems it\ntakes in navigating a list of movies. We also found out by a comparative\nanalysis that some layouts, the circular one in particular, is advantageous in\ncertain settings making it an ideal candidate for a movie recommender system.",
        "translated": ""
    },
    {
        "title": "Fact-based Court Judgment Prediction",
        "url": "http://arxiv.org/abs/2311.13350v1",
        "pub_date": "2023-11-22",
        "summary": "This extended abstract extends the research presented in \"ILDC for CJPE:\nIndian Legal Documents Corpus for Court Judgment Prediction and Explanation\"\n\\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within\nthe context of Indian legal documents. We introduce two distinct problem\nvariations: one based solely on facts, and another combining facts with rulings\nfrom lower courts (RLC). Our research aims to enhance early-phase case outcome\nprediction, offering significant benefits to legal professionals and the\ngeneral public. The results, however, indicated a performance decline compared\nto the original ILDC for CJPE study, even after implementing various weightage\nschemes in our DELSumm algorithm. Additionally, using only facts for legal\njudgment prediction with different transformer models yielded results inferior\nto the state-of-the-art outcomes reported in the \"ILDC for CJPE\" study.",
        "translated": ""
    },
    {
        "title": "Hierarchical Matrix Factorization for Interpretable Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2311.13277v1",
        "pub_date": "2023-11-22",
        "summary": "Matrix factorization (MF) is a simple collaborative filtering technique that\nachieves superior recommendation accuracy by decomposing the user-item rating\nmatrix into user and item latent matrices. This approach relies on learning\nfrom user-item interactions, which may not effectively capture the underlying\nshared dependencies between users or items. Therefore, there is scope to\nexplicitly capture shared dependencies to further improve recommendation\naccuracy and the interpretability of learning results by summarizing user-item\ninteractions. Based on these insights, we propose \"Hierarchical Matrix\nFactorization\" (HMF), which incorporates clustering concepts to capture the\nhierarchy, where leaf nodes and other nodes correspond to users/items and\nclusters, respectively. Central to our approach, called hierarchical\nembeddings, is the additional decomposition of the user and item latent\nmatrices (embeddings) into probabilistic connection matrices, which link the\nhierarchy, and a root cluster latent matrix. Thus, each node is represented by\nthe weighted average of the embeddings of its parent clusters. The embeddings\nare differentiable, allowing simultaneous learning of interactions and\nclustering using a single gradient descent method. Furthermore, the obtained\ncluster-specific interactions naturally summarize user-item interactions and\nprovide interpretability. Experimental results on rating and ranking\npredictions demonstrated the competitiveness of HMF over vanilla and\nhierarchical MF methods, especially its robustness in sparse interactions.\nAdditionally, it was confirmed that the clustering integration of HMF has the\npotential for faster learning convergence and mitigation of overfitting\ncompared to MF, and also provides interpretability through a cluster-centered\ncase study.",
        "translated": ""
    },
    {
        "title": "GENET: Unleashing the Power of Side Information for Recommendation via\n  Hypergraph Pre-training",
        "url": "http://arxiv.org/abs/2311.13121v1",
        "pub_date": "2023-11-22",
        "summary": "Recommendation with side information has drawn significant research interest\ndue to its potential to mitigate user feedback sparsity. However, existing\nmodels struggle with generalization across diverse domains and types of side\ninformation. In particular, three challenges have not been addressed, and they\nare (1) the diverse formats of side information, including text sequences. (2)\nThe diverse semantics of side information that describes items and users from\nmulti-level in a context different from recommendation systems. (3) The diverse\ncorrelations in side information to measure similarity over multiple objects\nbeyond pairwise relations. In this paper, we introduce GENET (Generalized\nhypErgraph pretraiNing on sidE informaTion), which pre-trains user and item\nrepresentations on feedback-irrelevant side information and fine-tunes the\nrepresentations on user feedback data. GENET leverages pre-training as a means\nto prevent side information from overshadowing critical ID features and\nfeedback signals. It employs a hypergraph framework to accommodate various\ntypes of diverse side information. During pre-training, GENET integrates tasks\nfor hyperlink prediction and self-supervised contrast to capture fine-grained\nsemantics at both local and global levels. Additionally, it introduces a unique\nstrategy to enhance pre-training robustness by perturbing positive samples\nwhile maintaining high-order relations. Extensive experiments demonstrate that\nGENET exhibits strong generalization capabilities, outperforming the SOTA\nmethod by up to 38% in TOP-N recommendation and Sequential recommendation tasks\non various datasets with different side information.",
        "translated": ""
    },
    {
        "title": "Don't forget private retrieval: distributed private similarity search\n  for large language models",
        "url": "http://arxiv.org/abs/2311.12955v1",
        "pub_date": "2023-11-21",
        "summary": "While the flexible capabilities of large language models (LLMs) allow them to\nanswer a range of queries based on existing learned knowledge, information\nretrieval to augment generation is an important tool to allow LLMs to answer\nquestions on information not included in pre-training data. Such private\ninformation is increasingly being generated in a wide array of distributed\ncontexts by organizations and individuals. Performing such information\nretrieval using neural embeddings of queries and documents always leaked\ninformation about queries and database content unless both were stored locally.\nWe present Private Retrieval Augmented Generation (PRAG), an approach that uses\nmulti-party computation (MPC) to securely transmit queries to a distributed set\nof servers containing a privately constructed database to return top-k and\napproximate top-k documents. This is a first-of-its-kind approach to dense\ninformation retrieval that ensures no server observes a client's query or can\nsee the database content. The approach introduces a novel MPC friendly protocol\nfor inverted file approximate search (IVF) that allows for fast document search\nover distributed and private data in sublinear communication complexity. This\nwork presents new avenues through which data for use in LLMs can be accessed\nand used without needing to centralize or forgo privacy.",
        "translated": ""
    },
    {
        "title": "Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale\n  Fine-Grained Image Retrieval",
        "url": "http://arxiv.org/abs/2311.12894v1",
        "pub_date": "2023-11-21",
        "summary": "Our work focuses on tackling large-scale fine-grained image retrieval as\nranking the images depicting the concept of interests (i.e., the same\nsub-category labels) highest based on the fine-grained details in the query. It\nis desirable to alleviate the challenges of both fine-grained nature of small\ninter-class variations with large intra-class variations and explosive growth\nof fine-grained data for such a practical task. In this paper, we propose\nattribute-aware hashing networks with self-consistency for generating\nattribute-aware hash codes to not only make the retrieval process efficient,\nbut also establish explicit correspondences between hash codes and visual\nattributes. Specifically, based on the captured visual representations by\nattention, we develop an encoder-decoder structure network of a reconstruction\ntask to unsupervisedly distill high-level attribute-specific vectors from the\nappearance-specific visual representations without attribute annotations. Our\nmodels are also equipped with a feature decorrelation constraint upon these\nattribute vectors to strengthen their representative abilities. Then, driven by\npreserving original entities' similarity, the required hash codes can be\ngenerated from these attribute-specific vectors and thus become\nattribute-aware. Furthermore, to combat simplicity bias in deep hashing, we\nconsider the model design from the perspective of the self-consistency\nprinciple and propose to further enhance models' self-consistency by equipping\nan additional image reconstruction path. Comprehensive quantitative experiments\nunder diverse empirical settings on six fine-grained retrieval datasets and two\ngeneric retrieval datasets show the superiority of our models over competing\nmethods.",
        "translated": ""
    },
    {
        "title": "GPT Struct Me: Probing GPT Models on Narrative Entity Extraction",
        "url": "http://arxiv.org/abs/2311.14583v1",
        "pub_date": "2023-11-24",
        "summary": "The importance of systems that can extract structured information from\ntextual data becomes increasingly pronounced given the ever-increasing volume\nof text produced on a daily basis. Having a system that can effectively extract\nsuch information in an interoperable manner would be an asset for several\ndomains, be it finance, health, or legal. Recent developments in natural\nlanguage processing led to the production of powerful language models that can,\nto some degree, mimic human intelligence. Such effectiveness raises a pertinent\nquestion: Can these models be leveraged for the extraction of structured\ninformation? In this work, we address this question by evaluating the\ncapabilities of two state-of-the-art language models -- GPT-3 and GPT-3.5,\ncommonly known as ChatGPT -- in the extraction of narrative entities, namely\nevents, participants, and temporal expressions. This study is conducted on the\nText2Story Lusa dataset, a collection of 119 Portuguese news articles whose\nannotation framework includes a set of entity structures along with several\ntags and attribute values. We first select the best prompt template through an\nablation study over prompt components that provide varying degrees of\ninformation on a subset of documents of the dataset. Subsequently, we use the\nbest templates to evaluate the effectiveness of the models on the remaining\ndocuments. The results obtained indicate that GPT models are competitive with\nout-of-the-box baseline systems, presenting an all-in-one alternative for\npractitioners with limited resources. By studying the strengths and limitations\nof these models in the context of information extraction, we offer insights\nthat can guide future improvements and avenues to explore in this field.",
        "translated": ""
    },
    {
        "title": "AI-Generated Images Introduce Invisible Relevance Bias to Text-Image\n  Retrieval",
        "url": "http://arxiv.org/abs/2311.14084v1",
        "pub_date": "2023-11-23",
        "summary": "With the advancement of generation models, AI-generated content (AIGC) is\nbecoming more realistic, flooding the Internet. A recent study suggests that\nthis phenomenon has elevated the issue of source bias in text retrieval for web\nsearches. Specifically, neural retrieval models tend to rank generated texts\nhigher than human-written texts. In this paper, we extend the study of this\nbias to cross-modal retrieval. Firstly, we successfully construct a suitable\nbenchmark to explore the existence of the bias. Subsequent extensive\nexperiments on this benchmark reveal that AI-generated images introduce an\ninvisible relevance bias to text-image retrieval models. Specifically, our\nexperiments show that text-image retrieval models tend to rank the AI-generated\nimages higher than the real images, even though the AI-generated images do not\nexhibit more visually relevant features to the query than real images. This\ninvisible relevance bias is prevalent across retrieval models with varying\ntraining data and architectures. Furthermore, our subsequent exploration\nreveals that the inclusion of AI-generated images in the training data of the\nretrieval models exacerbates the invisible relevance bias. The above phenomenon\ntriggers a vicious cycle, which makes the invisible relevance bias become more\nand more serious. To elucidate the potential causes of invisible relevance and\naddress the aforementioned issues, we introduce an effective training method\naimed at alleviating the invisible relevance bias. Subsequently, we apply our\nproposed debiasing method to retroactively identify the causes of invisible\nrelevance, revealing that the AI-generated images induce the image encoder to\nembed additional information into their representation. This information\nexhibits a certain consistency across generated images with different semantics\nand can make the retriever estimate a higher relevance score.",
        "translated": ""
    },
    {
        "title": "Some Like It Small: Czech Semantic Embedding Models for Industry\n  Applications",
        "url": "http://arxiv.org/abs/2311.13921v1",
        "pub_date": "2023-11-23",
        "summary": "This article focuses on the development and evaluation of Small-sized Czech\nsentence embedding models. Small models are important components for real-time\nindustry applications in resource-constrained environments. Given the limited\navailability of labeled Czech data, alternative approaches, including\npre-training, knowledge distillation, and unsupervised contrastive fine-tuning,\nare investigated. Comprehensive intrinsic and extrinsic analyses are conducted,\nshowcasing the competitive performance of our models compared to significantly\nlarger counterparts, with approximately 8 times smaller size and 5 times faster\nspeed than conventional Base-sized models. To promote cooperation and\nreproducibility, both the models and the evaluation pipeline are made publicly\naccessible. Ultimately, this article presents practical applications of the\ndeveloped sentence embedding models in Seznam.cz, the Czech search engine.\nThese models have effectively replaced previous counterparts, enhancing the\noverall search experience for instance, in organic search, featured snippets,\nand image search. This transition has yielded improved performance.",
        "translated": ""
    },
    {
        "title": "Physics-driven generative adversarial networks empower single-pixel\n  infrared hyperspectral imaging",
        "url": "http://arxiv.org/abs/2311.13626v1",
        "pub_date": "2023-11-22",
        "summary": "A physics-driven generative adversarial network (GAN) was established here\nfor single-pixel hyperspectral imaging (HSI) in the infrared spectrum, to\neliminate the extensive data training work required by traditional data-driven\nmodel. Within the GAN framework, the physical process of single-pixel imaging\n(SPI) was integrated into the generator, and the actual and estimated\none-dimensional (1D) bucket signals were employed as constraints in the\nobjective function to update the network's parameters and optimize the\ngenerator with the assistance of the discriminator. In comparison to\nsingle-pixel infrared HSI methods based on compressed sensing and\nphysics-driven convolution neural networks, our physics-driven GAN-based\nsingle-pixel infrared HSI can achieve higher imaging performance but with fewer\nmeasurements. We believe that this physics-driven GAN will promote practical\napplications of computational imaging, especially various SPI-based techniques.",
        "translated": ""
    },
    {
        "title": "BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical\n  Knowledge Graph Insights",
        "url": "http://arxiv.org/abs/2311.16075v1",
        "pub_date": "2023-11-27",
        "summary": "In this study, we investigate the potential of Large Language Models to\ncomplement biomedical knowledge graphs in the training of semantic models for\nthe biomedical and clinical domains. Drawing on the wealth of the UMLS\nknowledge graph and harnessing cutting-edge Large Language Models, we propose a\nnew state-of-the-art approach for obtaining high-fidelity representations of\nbiomedical concepts and sentences, consisting of three steps: an improved\ncontrastive learning phase, a novel self-distillation phase, and a weight\naveraging phase. Through rigorous evaluations via the extensive BioLORD testing\nsuite and diverse downstream tasks, we demonstrate consistent and substantial\nperformance improvements over the previous state of the art (e.g. +2pts on\nMedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new\nstate-of-the-art biomedical model for English, we also distill and release a\nmultilingual model compatible with 50+ languages and finetuned on 7 European\nlanguages. Many clinical pipelines can benefit from our latest models. Our new\nmultilingual model enables a range of languages to benefit from our\nadvancements in biomedical semantic representation learning, opening a new\navenue for bioinformatics researchers around the world. As a result, we hope to\nsee BioLORD-2023 becoming a precious tool for future biomedical applications.",
        "translated": ""
    },
    {
        "title": "SEINE: SEgment-based Indexing for NEural information retrieval",
        "url": "http://arxiv.org/abs/2311.15923v1",
        "pub_date": "2023-11-27",
        "summary": "Many early neural Information Retrieval (NeurIR) methods are re-rankers that\nrely on a traditional first-stage retriever due to expensive query time\ncomputations. Recently, representation-based retrievers have gained much\nattention, which learns query representation and document representation\nseparately, making it possible to pre-compute document representations offline\nand reduce the workload at query time. Both dense and sparse\nrepresentation-based retrievers have been explored. However, these methods\nfocus on finding the representation that best represents a text (aka metric\nlearning) and the actual retrieval function that is responsible for similarity\nmatching between query and document is kept at a minimum by using dot product.\nOne drawback is that unlike traditional term-level inverted index, the index\nformed by these embeddings cannot be easily re-used by another retrieval\nmethod. Another drawback is that keeping the interaction at minimum hurts\nretrieval effectiveness. On the contrary, interaction-based retrievers are\nknown for their better retrieval effectiveness. In this paper, we propose a\nnovel SEgment-based Neural Indexing method, SEINE, which provides a general\nindexing framework that can flexibly support a variety of interaction-based\nneural retrieval methods. We emphasize on a careful decomposition of common\ncomponents in existing neural retrieval methods and propose to use\nsegment-level inverted index to store the atomic query-document interaction\nvalues. Experiments on LETOR MQ2007 and MQ2008 datasets show that our indexing\nmethod can accelerate multiple neural retrieval methods up to 28-times faster\nwithout sacrificing much effectiveness.",
        "translated": ""
    },
    {
        "title": "A Social-aware Gaussian Pre-trained Model for Effective Cold-start\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.15790v1",
        "pub_date": "2023-11-27",
        "summary": "The use of pre-training is an emerging technique to enhance a neural model's\nperformance, which has been shown to be effective for many neural language\nmodels such as BERT. This technique has also been used to enhance the\nperformance of recommender systems. In such recommender systems, pre-training\nmodels are used to learn a better initialisation for both users and items.\nHowever, recent existing pre-trained recommender systems tend to only\nincorporate the user interaction data at the pre-training stage, making it\ndifficult to deliver good recommendations, especially when the interaction data\nis sparse. To alleviate this common data sparsity issue, we propose to\npre-train the recommendation model not only with the interaction data but also\nwith other available information such as the social relations among users,\nthereby providing the recommender system with a better initialisation compared\nwith solely relying on the user interaction data. We propose a novel\nrecommendation model, the Social-aware Gaussian Pre-trained model (SGP), which\nencodes the user social relations and interaction data at the pre-training\nstage in a Graph Neural Network (GNN). Afterwards, in the subsequent\nfine-tuning stage, our SGP model adopts a Gaussian Mixture Model (GMM) to\nfactorise these pre-trained embeddings for further training, thereby benefiting\nthe cold-start users from these pre-built social relations. Our extensive\nexperiments on three public datasets show that, in comparison to 16 competitive\nbaselines, our SGP model significantly outperforms the best baseline by upto\n7.7% in terms of NDCG@10. In addition, we show that SGP permits to effectively\nalleviate the cold-start problem, especially when users newly register to the\nsystem through their friends' suggestions.",
        "translated": ""
    },
    {
        "title": "Justifiable Artificial Intelligence: Engineering Large Language Models\n  for Legal Applications",
        "url": "http://arxiv.org/abs/2311.15716v1",
        "pub_date": "2023-11-27",
        "summary": "In this work, I discuss how Large Language Models can be applied in the legal\ndomain, circumventing their current drawbacks. Despite their large success and\nacceptance, their lack of explainability hinders legal experts to trust in\ntheir output, and this happens rightfully so. However, in this paper, I argue\nin favor of a new view, Justifiable Artificial Intelligence, instead of\nfocusing on Explainable Artificial Intelligence. I discuss in this paper how\ngaining evidence for and against a Large Language Model's output may make their\ngenerated texts more trustworthy - or hold them accountable for misinformation.",
        "translated": ""
    },
    {
        "title": "Two Approaches to the Identity of Processes in BFO",
        "url": "http://arxiv.org/abs/2311.15689v1",
        "pub_date": "2023-11-27",
        "summary": "This paper aims to explore processes and their identity with a focus on the\nupper ontology Basic Formal Ontology (BFO). We begin with a classification\nbased on two basic classes of changes of independent continuants: changes with\nrespect to a single specifically dependent continuant thereof or with respect\nto the spatial region that its parts occupy. We accordingly distinguish two\nkinds of simple processes: specifically dependent continuant changes and\nspatial changes. Next, we investigate a compositional approach to the identity\nof processes: the identity of any process is determined by the identity of the\nsimple processes that compose them. Then, we consider a causal approach to the\nidentity of processes with recourse to a dispositional view of processes\naccording to which any process is a realization of some disposition. We also\nexamine assumptions on which these two approaches to the identity of processes\nare based.",
        "translated": ""
    },
    {
        "title": "Experimental Analysis of Large-scale Learnable Vector Storage\n  Compression",
        "url": "http://arxiv.org/abs/2311.15578v1",
        "pub_date": "2023-11-27",
        "summary": "Learnable embedding vector is one of the most important applications in\nmachine learning, and is widely used in various database-related domains.\nHowever, the high dimensionality of sparse data in recommendation tasks and the\nhuge volume of corpus in retrieval-related tasks lead to a large memory\nconsumption of the embedding table, which poses a great challenge to the\ntraining and deployment of models. Recent research has proposed various methods\nto compress the embeddings at the cost of a slight decrease in model quality or\nthe introduction of other overheads. Nevertheless, the relative performance of\nthese methods remains unclear. Existing experimental comparisons only cover a\nsubset of these methods and focus on limited metrics. In this paper, we perform\na comprehensive comparative analysis and experimental evaluation of embedding\ncompression. We introduce a new taxonomy that categorizes these techniques\nbased on their characteristics and methodologies, and further develop a modular\nbenchmarking framework that integrates 14 representative methods. Under a\nuniform test environment, our benchmark fairly evaluates each approach,\npresents their strengths and weaknesses under different memory budgets, and\nrecommends the best method based on the use case. In addition to providing\nuseful guidelines, our study also uncovers the limitations of current methods\nand suggests potential directions for future research.",
        "translated": ""
    },
    {
        "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
        "url": "http://arxiv.org/abs/2311.15564v1",
        "pub_date": "2023-11-27",
        "summary": "Neural 'dense' retrieval models are state of the art for many datasets,\nhowever these models often exhibit limited domain transfer ability. Existing\napproaches to adaptation are unwieldy, such as requiring explicit supervision,\ncomplex model architectures, or massive external models. We present\n$\\texttt{ABEL}$, a simple but effective unsupervised method to enhance passage\nretrieval in zero-shot settings. Our technique follows a straightforward loop:\na dense retriever learns from supervision signals provided by a reranker, and\nsubsequently, the reranker is updated based on feedback from the improved\nretriever. By iterating this loop, the two components mutually enhance one\nanother's performance. Experimental results demonstrate that our unsupervised\n$\\texttt{ABEL}$ model outperforms both leading supervised and unsupervised\nretrievers on the BEIR benchmark. Meanwhile, it exhibits strong adaptation\nabilities to tasks and domains that were unseen during training. By either\nfine-tuning $\\texttt{ABEL}$ on labelled data or integrating it with existing\nsupervised dense retrievers, we achieve state-of-the-art\nresults.\\footnote{Source code is available at\n\\url{https://github.com/Fantabulous-J/BootSwitch}.}",
        "translated": ""
    },
    {
        "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
        "url": "http://arxiv.org/abs/2311.15563v1",
        "pub_date": "2023-11-27",
        "summary": "Although existing neural retrieval models reveal promising results when\ntraining data is abundant and the performance keeps improving as training data\nincreases, collecting high-quality annotated data is prohibitively costly. To\nthis end, we introduce a novel noisy self-training framework combined with\nsynthetic queries, showing that neural retrievers can be improved in a\nself-evolution manner with no reliance on any external models. Experimental\nresults show that our method improves consistently over existing methods on\nboth general-domain (e.g., MS-MARCO) and out-of-domain (i.e., BEIR) retrieval\nbenchmarks. Extra analysis on low-resource settings reveals that our method is\ndata efficient and outperforms competitive baselines, with as little as 30% of\nlabelled training data. Further extending the framework for reranker training\ndemonstrates that the proposed method is general and yields additional gains on\ntasks of diverse domains.\\footnote{Source code is available at\n\\url{https://github.com/Fantabulous-J/Self-Training-DPR}}",
        "translated": ""
    },
    {
        "title": "UFIN: Universal Feature Interaction Network for Multi-Domain\n  Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2311.15493v1",
        "pub_date": "2023-11-27",
        "summary": "Click-Through Rate (CTR) prediction, which aims to estimate the probability\nof a user clicking on an item, is a key task in online advertising. Numerous\nexisting CTR models concentrate on modeling the feature interactions within a\nsolitary domain, thereby rendering them inadequate for fulfilling the\nrequisites of multi-domain recommendations in real industrial scenarios. Some\nrecent approaches propose intricate architectures to enhance knowledge sharing\nand augment model training across multiple domains. However, these approaches\nencounter difficulties when being transferred to new recommendation domains,\nowing to their reliance on the modeling of ID features (e.g., item id). To\naddress the above issue, we propose the Universal Feature Interaction Network\n(UFIN) approach for CTR prediction. UFIN exploits textual data to learn\nuniversal feature interactions that can be effectively transferred across\ndiverse domains. For learning universal feature representations, we regard the\ntext and feature as two different modalities and propose an encoder-decoder\nnetwork founded on a Large Language Model (LLM) to enforce the transfer of data\nfrom the text modality to the feature modality. Building upon the above\nfoundation, we further develop a mixtureof-experts (MoE) enhanced adaptive\nfeature interaction model to learn transferable collaborative patterns across\nmultiple domains. Furthermore, we propose a multi-domain knowledge distillation\nframework to enhance feature interaction learning. Based on the above methods,\nUFIN can effectively bridge the semantic gap to learn common knowledge across\nvarious domains, surpassing the constraints of ID-based models. Extensive\nexperiments conducted on eight datasets show the effectiveness of UFIN, in both\nmultidomain and cross-platform settings. Our code is available at\nhttps://github.com/RUCAIBox/UFIN.",
        "translated": ""
    },
    {
        "title": "Data Augmentation for Sample Efficient and Robust Document Ranking",
        "url": "http://arxiv.org/abs/2311.15426v1",
        "pub_date": "2023-11-26",
        "summary": "Contextual ranking models have delivered impressive performance improvements\nover classical models in the document ranking task. However, these highly\nover-parameterized models tend to be data-hungry and require large amounts of\ndata even for fine-tuning. In this paper, we propose data-augmentation methods\nfor effective and robust ranking performance. One of the key benefits of using\ndata augmentation is in achieving sample efficiency or learning effectively\nwhen we have only a small amount of training data. We propose supervised and\nunsupervised data augmentation schemes by creating training data using parts of\nthe relevant documents in the query-document pairs. We then adapt a family of\ncontrastive losses for the document ranking task that can exploit the augmented\ndata to learn an effective ranking model. Our extensive experiments on subsets\nof the MS MARCO and TREC-DL test sets show that data augmentation, along with\nthe ranking-adapted contrastive losses, results in performance improvements\nunder most dataset sizes. Apart from sample efficiency, we conclusively show\nthat data augmentation results in robust models when transferred to\nout-of-domain benchmarks. Our performance improvements in in-domain and more\nprominently in out-of-domain benchmarks show that augmentation regularizes the\nranking model and improves its robustness and generalization capability.",
        "translated": ""
    },
    {
        "title": "Enhancing Item-level Bundle Representation for Bundle Recommendation",
        "url": "http://arxiv.org/abs/2311.16892v1",
        "pub_date": "2023-11-28",
        "summary": "Bundle recommendation approaches offer users a set of related items on a\nparticular topic. The current state-of-the-art (SOTA) method utilizes\ncontrastive learning to learn representations at both the bundle and item\nlevels. However, due to the inherent difference between the bundle-level and\nitem-level preferences, the item-level representations may not receive\nsufficient information from the bundle affiliations to make accurate\npredictions. In this paper, we propose a novel approach EBRec, short of\nEnhanced Bundle Recommendation, which incorporates two enhanced modules to\nexplore inherent item-level bundle representations. First, we propose to\nincorporate the bundle-user-item (B-U-I) high-order correlations to explore\nmore collaborative information, thus to enhance the previous bundle\nrepresentation that solely relies on the bundle-item affiliation information.\nSecond, we further enhance the B-U-I correlations by augmenting the observed\nuser-item interactions with interactions generated from pre-trained models,\nthus improving the item-level bundle representations. We conduct extensive\nexperiments on three public datasets, and the results justify the effectiveness\nof our approach as well as the two core modules. Codes and datasets are\navailable at https://github.com/answermycode/EBRec.",
        "translated": ""
    },
    {
        "title": "Temporal Importance Factor for Loss Functions for CTR Prediction",
        "url": "http://arxiv.org/abs/2311.16878v1",
        "pub_date": "2023-11-28",
        "summary": "Click-through rate (CTR) prediction is an important task for the companies to\nrecommend products which better match user preferences. User behavior in\ndigital advertising is dynamic and changes over time. It is crucial for the\ncompanies to capture the most recent trends to provide more accurate\nrecommendations for users. In CTR prediction, most models use binary\ncross-entropy loss function. However, it does not focus on the data\ndistribution shifts occurring over time. To address this problem, we propose a\nfactor for the loss functions by utilizing the sequential nature of user-item\ninteractions. This approach aims to focus on the most recent samples by\npenalizing them more through the loss function without forgetting the long-term\ninformation. Our solution is model-agnostic, and the temporal importance factor\ncan be used with different loss functions. Offline experiments in both public\nand company datasets show that the temporal importance factor for loss\nfunctions outperforms the baseline loss functions considered.",
        "translated": ""
    },
    {
        "title": "MultiCBR: Multi-view Contrastive Learning for Bundle Recommendation",
        "url": "http://arxiv.org/abs/2311.16751v1",
        "pub_date": "2023-11-28",
        "summary": "Bundle recommendation seeks to recommend a bundle of related items to users\nto improve both user experience and the profits of platform. Existing bundle\nrecommendation models have progressed from capturing only user-bundle\ninteractions to the modeling of multiple relations among users, bundles and\nitems. CrossCBR, in particular, incorporates cross-view contrastive learning\ninto a two-view preference learning framework, significantly improving SOTA\nperformance. It does, however, have two limitations: 1) the two-view\nformulation does not fully exploit all the heterogeneous relations among users,\nbundles and items; and 2) the \"early contrast and late fusion\" framework is\nless effective in capturing user preference and difficult to generalize to\nmultiple views. In this paper, we present MultiCBR, a novel Multi-view\nContrastive learning framework for Bundle Recommendation. First, we devise a\nmulti-view representation learning framework capable of capturing all the\nuser-bundle, user-item and bundle-item relations, especially better utilizing\nthe bundle-item affiliations to enhance sparse bundles' representations.\nSecond, we innovatively adopt an \"early fusion and late contrast\" design that\nfirst fuses the multi-view representations before performing self-supervised\ncontrastive learning. In comparison to existing approaches, our framework\nreverses the order of fusion and contrast, introducing the following\nadvantages: 1)our framework is capable of modeling both cross-view and ego-view\npreferences, allowing us to achieve enhanced user preference modeling; and 2)\ninstead of requiring quadratic number of cross-view contrastive losses, we only\nrequire two self-supervised contrastive losses, resulting in minimal extra\ncosts. Experimental results on three public datasets indicate that our method\noutperforms SOTA methods.",
        "translated": ""
    },
    {
        "title": "RankingGPT: Empowering Large Language Models in Text Ranking with\n  Progressive Enhancement",
        "url": "http://arxiv.org/abs/2311.16720v1",
        "pub_date": "2023-11-28",
        "summary": "Text ranking is a critical task in various information retrieval\napplications, and the recent success of Large Language Models (LLMs) in natural\nlanguage processing has sparked interest in their application to text ranking.\nThese methods primarily involve combining query and candidate documents and\nleveraging prompt learning to determine query-document relevance using the\nLLM's output probabilities for specific tokens or by directly generating a\nranked list of candidate documents. Although these approaches have demonstrated\npromise, a noteworthy disparity arises between the training objective of LLMs,\nwhich typically centers around next token prediction, and the objective of\nevaluating query-document relevance. To address this gap and fully leverage LLM\npotential in text ranking tasks, we propose a progressive multi-stage training\nstrategy. Firstly, we introduce a large-scale weakly supervised dataset of\nrelevance texts to enable the LLMs to acquire the ability to predict relevant\ntokens without altering their original training objective. Subsequently, we\nincorporate supervised training to further enhance LLM ranking capability. Our\nexperimental results on multiple benchmarks demonstrate the superior\nperformance of our proposed method compared to previous competitive approaches,\nboth in in-domain and out-of-domain scenarios.",
        "translated": ""
    },
    {
        "title": "Graph Pre-training and Prompt Learning for Recommendation",
        "url": "http://arxiv.org/abs/2311.16716v1",
        "pub_date": "2023-11-28",
        "summary": "GNN-based recommenders have excelled in modeling intricate user-item\ninteractions through multi-hop message passing. However, existing methods often\noverlook the dynamic nature of evolving user-item interactions, which impedes\nthe adaption to changing user preferences and distribution shifts in newly\narriving data. Thus, their scalability and performances in real-world dynamic\nenvironments are limited. In this study, we propose GraphPL, a framework that\nincorporates parameter-efficient and dynamic graph pre-training with prompt\nlearning. This novel combination empowers GNNs to effectively capture both\nlong-term user preferences and short-term behavior dynamics, enabling the\ndelivery of accurate and timely recommendations. Our GraphPL framework\naddresses the challenge of evolving user preferences by seamlessly integrating\na temporal prompt mechanism and a graph-structural prompt learning mechanism\ninto the pre-trained GNN model. The temporal prompt mechanism encodes time\ninformation on user-item interaction, allowing the model to naturally capture\ntemporal context, while the graph-structural prompt learning mechanism enables\nthe transfer of pre-trained knowledge to adapt to behavior dynamics without the\nneed for continuous incremental training. We further bring in a dynamic\nevaluation setting for recommendation to mimic real-world dynamic scenarios and\nbridge the offline-online gap to a better level. Our extensive experiments\nincluding a large-scale industrial deployment showcases the lightweight plug-in\nscalability of our GraphPL when integrated with various state-of-the-art\nrecommenders, emphasizing the advantages of GraphPL in terms of effectiveness,\nrobustness and efficiency.",
        "translated": ""
    },
    {
        "title": "Hyper-Relational Knowledge Graph Neural Network for Next POI",
        "url": "http://arxiv.org/abs/2311.16683v1",
        "pub_date": "2023-11-28",
        "summary": "With the advancement of mobile technology, Point of Interest (POI)\nrecommendation systems in Location-based Social Networks (LBSN) have brought\nnumerous benefits to both users and companies. Many existing works employ\nKnowledge Graph (KG) to alleviate the data sparsity issue in LBSN. These\napproaches primarily focus on modeling the pair-wise relations in LBSN to\nenrich the semantics and thereby relieve the data sparsity issue. However,\nexisting approaches seldom consider the hyper-relations in LBSN, such as the\nmobility relation (a 3-ary relation: user-POI-time). This makes the model hard\nto exploit the semantics accurately. In addition, prior works overlook the rich\nstructural information inherent in KG, which consists of higher-order relations\nand can further alleviate the impact of data sparsity.To this end, we propose a\nHyper-Relational Knowledge Graph Neural Network (HKGNN) model. In HKGNN, a\nHyper-Relational Knowledge Graph (HKG) that models the LBSN data is constructed\nto maintain and exploit the rich semantics of hyper-relations. Then we proposed\na Hypergraph Neural Network to utilize the structural information of HKG in a\ncohesive way. In addition, a self-attention network is used to leverage\nsequential information and make personalized recommendations. Furthermore, side\ninformation, essential in reducing data sparsity by providing background\nknowledge of POIs, is not fully utilized in current methods. In light of this,\nwe extended the current dataset with available side information to further\nlessen the impact of data sparsity. Results of experiments on four real-world\nLBSN datasets demonstrate the effectiveness of our approach compared to\nexisting state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "l2Match: Optimization Techniques on Subgraph Matching Algorithm using\n  Label Pair, Neighboring Label Index, and Jump-Redo method",
        "url": "http://arxiv.org/abs/2311.16603v1",
        "pub_date": "2023-11-28",
        "summary": "Graph database is designed to store bidirectional relationships between\nobjects and facilitate the traversal process to extract a subgraph. However,\nthe subgraph matching process is an NP-Complete problem. Existing solutions to\nthis problem usually employ a filter-and-verification framework and a\ndivide-and-conquer method. The filter-and-verification framework minimizes the\nnumber of inputs to the verification stage by filtering and pruning invalid\ncandidates as much as possible. Meanwhile, subgraph matching is performed on\nthe substructure decomposed from the larger graph to yield partial embedding.\nSubsequently, the recursive traversal or set intersection technique combines\nthe partial embedding into a complete subgraph. In this paper, we first present\na comprehensive literature review of the state-of-the-art solutions. l2Match, a\nsubgraph isomorphism algorithm for small queries utilizing a Label-Pair Index\nand filtering method, is then proposed and presented as a proof of concept.\nEmpirical experimentation shows that l2Match outperforms related\nstate-of-the-art solutions, and the proposed methods optimize the existing\nalgorithms.",
        "translated": ""
    },
    {
        "title": "SARDINE: A Simulator for Automated Recommendation in Dynamic and\n  Interactive Environments",
        "url": "http://arxiv.org/abs/2311.16586v1",
        "pub_date": "2023-11-28",
        "summary": "Simulators can provide valuable insights for researchers and practitioners\nwho wish to improve recommender systems, because they allow one to easily tweak\nthe experimental setup in which recommender systems operate, and as a result\nlower the cost of identifying general trends and uncovering novel findings\nabout the candidate methods. A key requirement to enable this accelerated\nimprovement cycle is that the simulator is able to span the various sources of\ncomplexity that can be found in the real recommendation environment that it\nsimulates.\n  With the emergence of interactive and data-driven methods - e.g.,\nreinforcement learning or online and counterfactual learning-to-rank - that aim\nto achieve user-related goals beyond the traditional accuracy-centric\nobjectives, adequate simulators are needed. In particular, such simulators must\nmodel the various mechanisms that render the recommendation environment dynamic\nand interactive, e.g., the effect of recommendations on the user or the effect\nof biased data on subsequent iterations of the recommender system. We therefore\npropose SARDINE, a flexible and interpretable recommendation simulator that can\nhelp accelerate research in interactive and data-driven recommender systems. We\ndemonstrate its usefulness by studying existing methods within nine diverse\nenvironments derived from SARDINE, and even uncover novel insights about them.",
        "translated": ""
    },
    {
        "title": "ControlRec: Bridging the Semantic Gap between Language Model and\n  Personalized Recommendation",
        "url": "http://arxiv.org/abs/2311.16441v1",
        "pub_date": "2023-11-28",
        "summary": "The successful integration of large language models (LLMs) into\nrecommendation systems has proven to be a major breakthrough in recent studies,\npaving the way for more generic and transferable recommendations. However, LLMs\nstruggle to effectively utilize user and item IDs, which are crucial\nidentifiers for successful recommendations. This is mainly due to their\ndistinct representation in a semantic space that is different from the natural\nlanguage (NL) typically used to train LLMs. To tackle such issue, we introduce\nControlRec, an innovative Contrastive prompt learning framework for\nRecommendation systems. ControlRec treats user IDs and NL as heterogeneous\nfeatures and encodes them individually. To promote greater alignment and\nintegration between them in the semantic space, we have devised two auxiliary\ncontrastive objectives: (1) Heterogeneous Feature Matching (HFM) aligning item\ndescription with the corresponding ID or user's next preferred ID based on\ntheir interaction sequence, and (2) Instruction Contrastive Learning (ICL)\neffectively merging these two crucial data sources by contrasting probability\ndistributions of output sequences generated by diverse tasks. Experimental\nresults on four public real-world datasets demonstrate the effectiveness of the\nproposed method on improving model performance.",
        "translated": ""
    },
    {
        "title": "Robust Basket Recommendation via Noise-tolerated Graph Contrastive\n  Learning",
        "url": "http://arxiv.org/abs/2311.16334v1",
        "pub_date": "2023-11-27",
        "summary": "The growth of e-commerce has seen a surge in popularity of platforms like\nAmazon, eBay, and Taobao. This has given rise to a unique shopping behavior\ninvolving baskets - sets of items purchased together. As a less studied\ninteraction mode in the community, the question of how should shopping basket\ncomplement personalized recommendation systems remains under-explored. While\nprevious attempts focused on jointly modeling user purchases and baskets, the\ndistinct semantic nature of these elements can introduce noise when directly\nintegrated. This noise negatively impacts the model's performance, further\nexacerbated by significant noise within both user and basket behaviors.\n  In order to cope with the above difficulties, we propose a novel Basket\nrecommendation framework via Noise-tolerated Contrastive Learning, named BNCL,\nto handle the noise existing in the cross-behavior integration and\nwithin-behavior modeling. First, we represent the basket-item interactions as\nthe hypergraph to model the complex basket behavior, where all items appearing\nin the same basket are treated as a single hyperedge. Second, cross-behavior\ncontrastive learning is designed to suppress the noise during the fusion of\ndiverse behaviors. Next, to further inhibit the within-behavior noise of the\nuser and basket interactions, we propose to exploit invariant properties of the\nrecommenders w.r.t augmentations through within-behavior contrastive learning.\nA novel consistency-aware augmentation approach is further designed to better\nidentify noisy interactions with the consideration of the above two types of\ninteractions. Our framework BNCL offers a generic training paradigm that is\napplicable to different backbones. Extensive experiments on three shopping\ntransaction datasets verify the effectiveness of our proposed method. Our code\nis available.",
        "translated": ""
    },
    {
        "title": "$Q_{bias}$ -- A Dataset on Media Bias in Search Queries and Query\n  Suggestions",
        "url": "http://arxiv.org/abs/2311.17780v1",
        "pub_date": "2023-11-29",
        "summary": "This publication describes the motivation and generation of $Q_{bias}$, a\nlarge dataset of Google and Bing search queries, a scraping tool and dataset\nfor biased news articles, as well as language models for the investigation of\nbias in online search. Web search engines are a major factor and trusted source\nin information search, especially in the political domain. However, biased\ninformation can influence opinion formation and lead to biased opinions. To\ninteract with search engines, users formulate search queries and interact with\nsearch query suggestions provided by the search engines. A lack of datasets on\nsearch queries inhibits research on the subject. We use $Q_{bias}$ to evaluate\ndifferent approaches to fine-tuning transformer-based language models with the\ngoal of producing models capable of biasing text with left and right political\nstance. Additionally to this work we provided datasets and language models for\nbiasing texts that allow further research on bias in online information search.",
        "translated": ""
    },
    {
        "title": "Creator Context for Tweet Recommendation",
        "url": "http://arxiv.org/abs/2311.17650v1",
        "pub_date": "2023-11-29",
        "summary": "When discussing a tweet, people usually not only refer to the content it\ndelivers, but also to the person behind the tweet. In other words, grounding\nthe interpretation of the tweet in the context of its creator plays an\nimportant role in deciphering the true intent and the importance of the tweet.\n  In this paper, we attempt to answer the question of how creator context\nshould be used to advance tweet understanding. Specifically, we investigate the\nusefulness of different types of creator context, and examine different model\nstructures for incorporating creator context in tweet modeling. We evaluate our\ntweet understanding models on a practical use case -- recommending relevant\ntweets to news articles. This use case already exists in popular news apps, and\ncan also serve as a useful assistive tool for journalists. We discover that\ncreator context is essential for tweet understanding, and can improve\napplication metrics by a large margin. However, we also observe that not all\ncreator contexts are equal. Creator context can be time sensitive and noisy.\nCareful creator context selection and deliberate model structure design play an\nimportant role in creator context effectiveness.",
        "translated": ""
    },
    {
        "title": "Attribute Simulation for Item Embedding Enhancement in Multi-interest\n  Recommendation",
        "url": "http://arxiv.org/abs/2311.17374v1",
        "pub_date": "2023-11-29",
        "summary": "Although multi-interest recommenders have achieved significant progress in\nthe matching stage, our research reveals that existing models tend to exhibit\nan under-clustered item embedding space, which leads to a low discernibility\nbetween items and hampers item retrieval. This highlights the necessity for\nitem embedding enhancement. However, item attributes, which serve as effective\nand straightforward side information for enhancement, are either unavailable or\nincomplete in many public datasets due to the labor-intensive nature of manual\nannotation tasks. This dilemma raises two meaningful questions: 1. Can we\nbypass manual annotation and directly simulate complete attribute information\nfrom the interaction data? And 2. If feasible, how to simulate attributes with\nhigh accuracy and low complexity in the matching stage?\n  In this paper, we first establish an inspiring theoretical feasibility that\nthe item-attribute correlation matrix can be approximated through elementary\ntransformations on the item co-occurrence matrix. Then based on formula\nderivation, we propose a simple yet effective module, SimEmb (Item Embedding\nEnhancement via Simulated Attribute), in the multi-interest recommendation of\nthe matching stage to implement our findings. By simulating attributes with the\nco-occurrence matrix, SimEmb discards the item ID-based embedding and employs\nthe attribute-weighted summation for item embedding enhancement. Comprehensive\nexperiments on four benchmark datasets demonstrate that our approach notably\nenhances the clustering of item embedding and significantly outperforms SOTA\nmodels with an average improvement of 25.59% on Recall@20.",
        "translated": ""
    },
    {
        "title": "UniIR: Training and Benchmarking Universal Multimodal Information\n  Retrievers",
        "url": "http://arxiv.org/abs/2311.17136v1",
        "pub_date": "2023-11-28",
        "summary": "Existing information retrieval (IR) models often assume a homogeneous format,\nlimiting their applicability to diverse user needs, such as searching for\nimages with text descriptions, searching for a news article with a headline\nimage, or finding a similar photo with a query image. To approach such\ndifferent information-seeking demands, we introduce UniIR, a unified\ninstruction-guided multimodal retriever capable of handling eight distinct\nretrieval tasks across modalities. UniIR, a single retrieval system jointly\ntrained on ten diverse multimodal-IR datasets, interprets user instructions to\nexecute various retrieval tasks, demonstrating robust performance across\nexisting datasets and zero-shot generalization to new tasks. Our experiments\nhighlight that multi-task training and instruction tuning are keys to UniIR's\ngeneralization ability. Additionally, we construct the M-BEIR, a multimodal\nretrieval benchmark with comprehensive results, to standardize the evaluation\nof universal multimodal information retrieval.",
        "translated": ""
    },
    {
        "title": "Routing-Guided Learned Product Quantization for Graph-Based Approximate\n  Nearest Neighbor Search",
        "url": "http://arxiv.org/abs/2311.18724v1",
        "pub_date": "2023-11-30",
        "summary": "Given a vector dataset $\\mathcal{X}$, a query vector $\\vec{x}_q$, graph-based\nApproximate Nearest Neighbor Search (ANNS) aims to build a proximity graph (PG)\nas an index of $\\mathcal{X}$ and approximately return vectors with minimum\ndistances to $\\vec{x}_q$ by searching over the PG index. It suffers from the\nlarge-scale $\\mathcal{X}$ because a PG with full vectors is too large to fit\ninto the memory, e.g., a billion-scale $\\mathcal{X}$ in 128 dimensions would\nconsume nearly 600 GB memory. To solve this, Product Quantization (PQ)\nintegrated graph-based ANNS is proposed to reduce the memory usage, using\nsmaller compact codes of quantized vectors in memory instead of the large\noriginal vectors. Existing PQ methods do not consider the important routing\nfeatures of PG, resulting in low-quality quantized vectors that affect the\nANNS's effectiveness. In this paper, we present an end-to-end Routing-guided\nlearned Product Quantization (RPQ) for graph-based ANNS. It consists of (1) a\n\\textit{differentiable quantizer} used to make the standard discrete PQ\ndifferentiable to suit for back-propagation of end-to-end learning, (2) a\n\\textit{sampling-based feature extractor} used to extract neighborhood and\nrouting features of a PG, and (3) a \\textit{multi-feature joint training\nmodule} with two types of feature-aware losses to continuously optimize the\ndifferentiable quantizer. As a result, the inherent features of a PG would be\nembedded into the learned PQ, generating high-quality quantized vectors.\nMoreover, we integrate our RPQ with the state-of-the-art DiskANN and existing\npopular PGs to improve their performance. Comprehensive experiments on\nreal-world large-scale datasets (from 1M to 1B) demonstrate RPQ's superiority,\ne.g., 1.7$\\times$-4.2$\\times$ improvement on QPS at the same recall@10 of 95\\%.",
        "translated": ""
    },
    {
        "title": "Barwise Music Structure Analysis with the Correlation Block-Matching\n  Segmentation Algorithm",
        "url": "http://arxiv.org/abs/2311.18604v1",
        "pub_date": "2023-11-30",
        "summary": "Music Structure Analysis (MSA) is a Music Information Retrieval task\nconsisting of representing a song in a simplified, organized manner by breaking\nit down into sections typically corresponding to ``chorus'', ``verse'',\n``solo'', etc. In this work, we extend an MSA algorithm called the Correlation\nBlock-Matching (CBM) algorithm introduced by (Marmoret et al., 2020, 2022b).\nThe CBM algorithm is a dynamic programming algorithm that segments\nself-similarity matrices, which are a standard description used in MSA and in\nnumerous other applications. In this work, self-similarity matrices are\ncomputed from the feature representation of an audio signal and time is sampled\nat the bar-scale. This study examines three different standard similarity\nfunctions for the computation of self-similarity matrices. Results show that,\nin optimal conditions, the proposed algorithm achieves a level of performance\nwhich is competitive with supervised state-of-the-art methods while only\nrequiring knowledge of bar positions. In addition, the algorithm is made\nopen-source and is highly customizable.",
        "translated": ""
    },
    {
        "title": "Search Still Matters: Information Retrieval in the Era of Generative AI",
        "url": "http://arxiv.org/abs/2311.18550v1",
        "pub_date": "2023-11-30",
        "summary": "Objective: Information retrieval (IR, also known as search) systems are\nubiquitous in modern times. How does the emergence of generative artificial\nintelligence (AI), based on large language models (LLMs), fit into the IR\nprocess? Process: This perspective explores the use of generative AI in the\ncontext of the motivations, considerations, and outcomes of the IR process with\na focus on the academic use of such systems. Conclusions: There are many\ninformation needs, from simple to complex, that motivate use of IR. Users of\nsuch systems, particularly academics, have concerns for authoritativeness,\ntimeliness, and contextualization of search. While LLMs may provide\nfunctionality that aids the IR process, the continued need for search systems,\nand research into their improvement, remains essential.",
        "translated": ""
    },
    {
        "title": "End-to-End Retrieval with Learned Dense and Sparse Representations Using\n  Lucene",
        "url": "http://arxiv.org/abs/2311.18503v1",
        "pub_date": "2023-11-30",
        "summary": "The bi-encoder architecture provides a framework for understanding\nmachine-learned retrieval models based on dense and sparse vector\nrepresentations. Although these representations capture parametric realizations\nof the same underlying conceptual framework, their respective implementations\nof top-$k$ similarity search require the coordination of different software\ncomponents (e.g., inverted indexes, HNSW indexes, and toolkits for neural\ninference), often knitted together in complex architectures. In this work, we\nask the following question: What's the simplest design, in terms of requiring\nthe fewest changes to existing infrastructure, that can support end-to-end\nretrieval with modern dense and sparse representations? The answer appears to\nbe that Lucene is sufficient, as we demonstrate in Anserini, a toolkit for\nreproducible information retrieval research. That is, effective retrieval with\nmodern single-vector neural models can be efficiently performed directly in\nJava on the CPU. We examine the implications of this design for information\nretrieval researchers pushing the state of the art as well as for software\nengineers building production search systems.",
        "translated": ""
    },
    {
        "title": "Poisoning Attacks Against Contrastive Recommender Systems",
        "url": "http://arxiv.org/abs/2311.18244v1",
        "pub_date": "2023-11-30",
        "summary": "Contrastive learning (CL) has recently gained significant popularity in the\nfield of recommendation. Its ability to learn without heavy reliance on labeled\ndata is a natural antidote to the data sparsity issue. Previous research has\nfound that CL can not only enhance recommendation accuracy but also\ninadvertently exhibit remarkable robustness against noise. However, this paper\nidentifies a vulnerability of CL-based recommender systems: Compared with their\nnon-CL counterparts, they are even more susceptible to poisoning attacks that\naim to promote target items. Our analysis points to the uniform dispersion of\nrepresentations led by the CL loss as the very factor that accounts for this\nvulnerability. We further theoretically and empirically demonstrate that the\noptimization of CL loss can lead to smooth spectral values of representations.\nBased on these insights, we attempt to reveal the potential poisoning attacks\nagainst CL-based recommender systems. The proposed attack encompasses a\ndual-objective framework: One that induces a smoother spectral value\ndistribution to amplify the CL loss's inherent dispersion effect, named\ndispersion promotion; and the other that directly elevates the visibility of\ntarget items, named rank promotion. We validate the destructiveness of our\nattack model through extensive experimentation on four datasets. By shedding\nlight on these vulnerabilities, we aim to facilitate the development of more\nrobust CL-based recommender systems.",
        "translated": ""
    },
    {
        "title": "Beyond Two-Tower Matching: Learning Sparse Retrievable\n  Cross-Interactions for Recommendation",
        "url": "http://arxiv.org/abs/2311.18213v1",
        "pub_date": "2023-11-30",
        "summary": "Two-tower models are a prevalent matching framework for recommendation, which\nhave been widely deployed in industrial applications. The success of two-tower\nmatching attributes to its efficiency in retrieval among a large number of\nitems, since the item tower can be precomputed and used for fast Approximate\nNearest Neighbor (ANN) search. However, it suffers two main challenges,\nincluding limited feature interaction capability and reduced accuracy in online\nserving. Existing approaches attempt to design novel late interactions instead\nof dot products, but they still fail to support complex feature interactions or\nlose retrieval efficiency. To address these challenges, we propose a new\nmatching paradigm named SparCode, which supports not only sophisticated feature\ninteractions but also efficient retrieval. Specifically, SparCode introduces an\nall-to-all interaction module to model fine-grained query-item interactions.\nBesides, we design a discrete code-based sparse inverted index jointly trained\nwith the model to achieve effective and efficient model inference. Extensive\nexperiments have been conducted on open benchmark datasets to demonstrate the\nsuperiority of our framework. The results show that SparCode significantly\nimproves the accuracy of candidate item matching while retaining the same level\nof retrieval efficiency with two-tower models. Our source code will be\navailable at MindSpore/models.",
        "translated": ""
    },
    {
        "title": "COVID-19 Vaccine Misinformation in Middle Income Countries",
        "url": "http://arxiv.org/abs/2311.18195v1",
        "pub_date": "2023-11-30",
        "summary": "This paper introduces a multilingual dataset of COVID-19 vaccine\nmisinformation, consisting of annotated tweets from three middle-income\ncountries: Brazil, Indonesia, and Nigeria. The expertly curated dataset\nincludes annotations for 5,952 tweets, assessing their relevance to COVID-19\nvaccines, presence of misinformation, and the themes of the misinformation. To\naddress challenges posed by domain specificity, the low-resource setting, and\ndata imbalance, we adopt two approaches for developing COVID-19 vaccine\nmisinformation detection models: domain-specific pre-training and text\naugmentation using a large language model. Our best misinformation detection\nmodels demonstrate improvements ranging from 2.7 to 15.9 percentage points in\nmacro F1-score compared to the baseline models. Additionally, we apply our\nmisinformation detection models in a large-scale study of 19 million unlabeled\ntweets from the three countries between 2020 and 2022, showcasing the practical\napplication of our dataset and models for detecting and analyzing vaccine\nmisinformation in multiple countries and languages. Our analysis indicates that\npercentage changes in the number of new COVID-19 cases are positively\nassociated with COVID-19 vaccine misinformation rates in a staggered manner for\nBrazil and Indonesia, and there are significant positive associations between\nthe misinformation rates across the three countries.",
        "translated": ""
    },
    {
        "title": "AnonPSI: An Anonymity Assessment Framework for PSI",
        "url": "http://arxiv.org/abs/2311.18118v1",
        "pub_date": "2023-11-29",
        "summary": "Private Set Intersection (PSI) is a widely used protocol that enables two\nparties to securely compute a function over the intersected part of their\nshared datasets and has been a significant research focus over the years.\nHowever, recent studies have highlighted its vulnerability to Set Membership\nInference Attacks (SMIA), where an adversary might deduce an individual's\nmembership by invoking multiple PSI protocols. This presents a considerable\nrisk, even in the most stringent versions of PSI, which only return the\ncardinality of the intersection. This paper explores the evaluation of\nanonymity within the PSI context. Initially, we highlight the reasons why\nexisting works fall short in measuring privacy leakage, and subsequently\npropose two attack strategies that address these deficiencies. Furthermore, we\nprovide theoretical guarantees on the performance of our proposed methods. In\naddition to these, we illustrate how the integration of auxiliary information,\nsuch as the sum of payloads associated with members of the intersection\n(PSI-SUM), can enhance attack efficiency. We conducted a comprehensive\nperformance evaluation of various attack strategies proposed utilizing two real\ndatasets. Our findings indicate that the methods we propose markedly enhance\nattack efficiency when contrasted with previous research endeavors. {The\neffective attacking implies that depending solely on existing PSI protocols may\nnot provide an adequate level of privacy assurance. It is recommended to\ncombine privacy-enhancing technologies synergistically to enhance privacy\nprotection even further.",
        "translated": ""
    },
    {
        "title": "Context Retrieval via Normalized Contextual Latent Interaction for\n  Conversational Agent",
        "url": "http://arxiv.org/abs/2312.00774v1",
        "pub_date": "2023-12-01",
        "summary": "Conversational agents leveraging AI, particularly deep learning, are emerging\nin both academic research and real-world applications. However, these\napplications still face challenges, including disrespecting knowledge and\nfacts, not personalizing to user preferences, and enormous demand for\ncomputational resources during training and inference. Recent research efforts\nhave been focused on addressing these challenges from various aspects,\nincluding supplementing various types of auxiliary information to the\nconversational agents. However, existing methods are still not able to\neffectively and efficiently exploit relevant information from these auxiliary\nsupplements to further unleash the power of the conversational agents and the\nlanguage models they use. In this paper, we present a novel method, PK-NCLI,\nthat is able to accurately and efficiently identify relevant auxiliary\ninformation to improve the quality of conversational responses by learning the\nrelevance among persona, chat history, and knowledge background through\nlow-level normalized contextual latent interaction. Our experimental results\nindicate that PK-NCLI outperforms the state-of-the-art method, PK-FoCus, by\n47.80%/30.61%/24.14% in terms of perplexity, knowledge grounding, and training\nefficiency, respectively, and maintained the same level of persona grounding\nperformance. We also provide a detailed analysis of how different factors,\nincluding language model choices and trade-offs on training weights, would\naffect the performance of PK-NCLI.",
        "translated": ""
    },
    {
        "title": "Rethinking Detection Based Table Structure Recognition for Visually Rich\n  Documents",
        "url": "http://arxiv.org/abs/2312.00699v1",
        "pub_date": "2023-12-01",
        "summary": "Table Structure Recognition (TSR) aims at transforming unstructured table\nimages into structured formats, such as HTML sequences. One type of popular\nsolution is using detection models to detect components of a table, such as\ncolumns and rows, then applying a rule-based post-processing method to convert\ndetection results into HTML sequences. However, existing detection-based\nstudies often have the following limitations. First, these studies usually pay\nmore attention to improving the detection performance, which does not\nnecessarily lead to better performance regarding cell-level metrics, such as\nTEDS. Second, some solutions over-simplify the problem and can miss some\ncritical information. Lastly, even though some studies defined the problem to\ndetect more components to provide as much information as other types of\nsolutions, these studies ignore the fact this problem definition is a\nmulti-label detection because row, projected row header and column header can\nshare identical bounding boxes. Besides, there is often a performance gap\nbetween two-stage and transformer-based detection models regarding the\nstructure-only TEDS, even though they have similar performance regarding the\nCOCO metrics. Therefore, we revisit the limitations of existing detection-based\nsolutions, compare two-stage and transformer-based detection models, and\nidentify the key design aspects for the success of a two-stage detection model\nfor the TSR task, including the multi-class problem definition, the aspect\nratio for anchor box generation, and the feature generation of the backbone\nnetwork. We applied simple methods to improve these aspects of the Cascade\nR-CNN model, achieved state-of-the-art performance, and improved the baseline\nCascade R-CNN model by 19.32%, 11.56% and 14.77% regarding the structure-only\nTEDS on SciTSR, FinTabNet, and PubTables1M datasets.",
        "translated": ""
    },
    {
        "title": "Attack Detection Using Item Vector Shift in Matrix Factorisation\n  Recommenders",
        "url": "http://arxiv.org/abs/2312.00512v1",
        "pub_date": "2023-12-01",
        "summary": "This paper proposes a novel method for detecting shilling attacks in Matrix\nFactorization (MF)-based Recommender Systems (RS), in which attackers use false\nuser-item feedback to promote a specific item. Unlike existing methods that use\neither use supervised learning to distinguish between attack and genuine\nprofiles or analyse target item rating distributions to detect false ratings,\nour method uses an unsupervised technique to detect false ratings by examining\nshifts in item preference vectors that exploit rating deviations and user\ncharacteristics, making it a promising new direction. The experimental results\ndemonstrate the effectiveness of our approach in various attack scenarios,\nincluding those involving obfuscation techniques.",
        "translated": ""
    },
    {
        "title": "Event-driven Real-time Retrieval in Web Search",
        "url": "http://arxiv.org/abs/2312.00372v1",
        "pub_date": "2023-12-01",
        "summary": "Information retrieval in real-time search presents unique challenges distinct\nfrom those encountered in classical web search. These challenges are\nparticularly pronounced due to the rapid change of user search intent, which is\ninfluenced by the occurrence and evolution of breaking news events, such as\nearthquakes, elections, and wars. Previous dense retrieval methods, which\nprimarily focused on static semantic representation, lack the capacity to\ncapture immediate search intent, leading to inferior performance in retrieving\nthe most recent event-related documents in time-sensitive scenarios. To address\nthis issue, this paper expands the query with event information that represents\nreal-time search intent. The Event information is then integrated with the\nquery through a cross-attention mechanism, resulting in a time-context query\nrepresentation. We further enhance the model's capacity for event\nrepresentation through multi-task training. Since publicly available datasets\nsuch as MS-MARCO do not contain any event information on the query side and\nhave few time-sensitive queries, we design an automatic data collection and\nannotation pipeline to address this issue, which includes ModelZoo-based Coarse\nAnnotation and LLM-driven Fine Annotation processes. In addition, we share the\ntraining tricks such as two-stage training and hard negative sampling. Finally,\nwe conduct a set of offline experiments on a million-scale production dataset\nto evaluate our approach and deploy an A/B testing in a real online system to\nverify the performance. Extensive experimental results demonstrate that our\nproposed approach significantly outperforms existing state-of-the-art baseline\nmethods.",
        "translated": ""
    },
    {
        "title": "Hypergraph Node Representation Learning with One-Stage Message Passing",
        "url": "http://arxiv.org/abs/2312.00336v1",
        "pub_date": "2023-12-01",
        "summary": "Hypergraphs as an expressive and general structure have attracted\nconsiderable attention from various research domains. Most existing hypergraph\nnode representation learning techniques are based on graph neural networks, and\nthus adopt the two-stage message passing paradigm (i.e. node -&gt; hyperedge -&gt;\nnode). This paradigm only focuses on local information propagation and does not\neffectively take into account global information, resulting in less optimal\nrepresentations. Our theoretical analysis of representative two-stage message\npassing methods shows that, mathematically, they model different ways of local\nmessage passing through hyperedges, and can be unified into one-stage message\npassing (i.e. node -&gt; node). However, they still only model local information.\nMotivated by this theoretical analysis, we propose a novel one-stage message\npassing paradigm to model both global and local information propagation for\nhypergraphs. We integrate this paradigm into HGraphormer, a Transformer-based\nframework for hypergraph node representation learning. HGraphormer injects the\nhypergraph structure information (local information) into Transformers (global\ninformation) by combining the attention matrix and hypergraph Laplacian.\nExtensive experiments demonstrate that HGraphormer outperforms recent\nhypergraph learning methods on five representative benchmark datasets on the\nsemi-supervised hypernode classification task, setting new state-of-the-art\nperformance, with accuracy improvements between 2.52% and 6.70%. Our code and\ndatasets are available.",
        "translated": ""
    },
    {
        "title": "Agent-OM: Leveraging Large Language Models for Ontology Matching",
        "url": "http://arxiv.org/abs/2312.00326v1",
        "pub_date": "2023-12-01",
        "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM-based agents\nhave become revolutionary in data engineering and have been applied creatively\nin various domains, their potential for OM remains underexplored. This study\nintroduces a novel agent-powered LLM-based design paradigm for OM systems. With\nthoughtful consideration of several specific challenges to leverage LLMs for\nOM, we propose a generic framework, namely Agent-OM, consisting of two Siamese\nagents for retrieval and matching, with a set of simple prompt-based OM tools.\nOur framework is implemented in a proof-of-concept system. Evaluations of three\nOntology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM\nsystems show that our system can achieve very close results to the best\nlong-standing performance on simple OM tasks and significantly improve the\nperformance on complex and few-shot OM tasks.",
        "translated": ""
    },
    {
        "title": "Virtual Quantum Markov Chains",
        "url": "http://arxiv.org/abs/2312.02031v1",
        "pub_date": "2023-12-04",
        "summary": "Quantum Markov chains generalize classical Markov chains for random variables\nto the quantum realm and exhibit unique inherent properties, making them an\nimportant feature in quantum information theory. In this work, we propose the\nconcept of virtual quantum Markov chains (VQMCs), focusing on scenarios where\nsubsystems retain classical information about global systems from measurement\nstatistics. As a generalization of quantum Markov chains, VQMCs characterize\nstates where arbitrary global shadow information can be recovered from\nsubsystems through local quantum operations and measurements. We present an\nalgebraic characterization for virtual quantum Markov chains and show that the\nvirtual quantum recovery is fully determined by the block matrices of a quantum\nstate on its subsystems. Notably, we find a distinction between two classes of\ntripartite entanglement by showing that the W state is a VQMC while the GHZ\nstate is not. Furthermore, we establish semidefinite programs to determine the\noptimal sampling overhead and the robustness of virtual quantum Markov chains.\nWe demonstrate the optimal sampling overhead is additive, indicating no free\nlunch to further reduce the sampling cost of recovery from parallel calls of\nthe VQMC states. Our findings elucidate distinctions between quantum Markov\nchains and virtual quantum Markov chains, extending our understanding of\nquantum recovery to scenarios prioritizing classical information from\nmeasurement statistics.",
        "translated": ""
    },
    {
        "title": "Language-only Efficient Training of Zero-shot Composed Image Retrieval",
        "url": "http://arxiv.org/abs/2312.01998v1",
        "pub_date": "2023-12-04",
        "summary": "Composed image retrieval (CIR) task takes a composed query of image and text,\naiming to search relative images for both conditions. Conventional CIR\napproaches need a training dataset composed of triplets of query image, query\ntext, and target image, which is very expensive to collect. Several recent\nworks have worked on the zero-shot (ZS) CIR paradigm to tackle the issue\nwithout using pre-collected triplets. However, the existing ZS-CIR methods show\nlimited backbone scalability and generalizability due to the lack of diversity\nof the input texts during training. We propose a novel CIR framework, only\nusing language for its training. Our LinCIR (Language-only training for CIR)\ncan be trained only with text datasets by a novel self-supervision named\nself-masking projection (SMP). We project the text latent embedding to the\ntoken embedding space and construct a new text by replacing the keyword tokens\nof the original text. Then, we let the new and original texts have the same\nlatent embedding vector. With this simple strategy, LinCIR is surprisingly\nefficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in\n48 minutes and shows the best ZS-CIR performances on four different CIR\nbenchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervised\nmethod on FashionIQ. Code is available at https://github.com/navervision/lincir",
        "translated": ""
    },
    {
        "title": "PEACE: Prototype lEarning Augmented transferable framework for\n  Cross-domain rEcommendation",
        "url": "http://arxiv.org/abs/2312.01916v1",
        "pub_date": "2023-12-04",
        "summary": "To help merchants/customers to provide/access a variety of services through\nminiapps, online service platforms have occupied a critical position in the\neffective content delivery, in which how to recommend items in the new domain\nlaunched by the service provider for customers has become more urgent. However,\nthe non-negligible gap between the source and diversified target domains poses\na considerable challenge to cross-domain recommendation systems, which often\nleads to performance bottlenecks in industrial settings. While entity graphs\nhave the potential to serve as a bridge between domains, rudimentary\nutilization still fail to distill useful knowledge and even induce the negative\ntransfer issue. To this end, we propose PEACE, a Prototype lEarning Augmented\ntransferable framework for Cross-domain rEcommendation. For domain gap\nbridging, PEACE is built upon a multi-interest and entity-oriented pre-training\narchitecture which could not only benefit the learning of generalized knowledge\nin a multi-granularity manner, but also help leverage more structural\ninformation in the entity graph. Then, we bring the prototype learning into the\npre-training over source domains, so that representations of users and items\nare greatly improved by the contrastive prototype learning module and the\nprototype enhanced attention mechanism for adaptive knowledge utilization. To\nease the pressure of online serving, PEACE is carefully deployed in a\nlightweight manner, and significant performance improvements are observed in\nboth online and offline environments.",
        "translated": ""
    },
    {
        "title": "An AI-based solution for the cold start and data sparsity problems in\n  the recommendation systems",
        "url": "http://arxiv.org/abs/2312.01840v1",
        "pub_date": "2023-12-04",
        "summary": "In recent years, the amount of data available on the internet and the number\nof users who utilize the Internet have increased at an unparalleled pace. The\nexponential development in the quantity of digital information accessible and\nthe number of Internet users has created the possibility for information\noverload, impeding fast access to items of interest on the Internet.\nInformation retrieval systems like as Google, DevilFinder, and Altavista have\npartly overcome this challenge, but prioritizing and customization of\ninformation (where a system maps accessible material to a user's interests and\npreferences) were lacking. This has resulted in a higher-than-ever need for\nrecommender systems. Recommender systems are information filtering systems that\naddress the issue of information overload by filtering important information\nfragments from a huge volume of dynamically produced data based on the user's\ninterests, favorite things, preferences and ratings on the desired item.\nRecommender systems can figure out if a person would like an item or not based\non their profile.",
        "translated": ""
    },
    {
        "title": "On Gradient Boosted Decision Trees and Neural Rankers: A Case-Study on\n  Short-Video Recommendations at ShareChat",
        "url": "http://arxiv.org/abs/2312.01760v1",
        "pub_date": "2023-12-04",
        "summary": "Practitioners who wish to build real-world applications that rely on ranking\nmodels, need to decide which modelling paradigm to follow. This is not an easy\nchoice to make, as the research literature on this topic has been shifting in\nrecent years. In particular, whilst Gradient Boosted Decision Trees (GBDTs)\nhave reigned supreme for more than a decade, the flexibility of neural networks\nhas allowed them to catch up, and recent works report accuracy metrics that are\non par. Nevertheless, practical systems require considerations beyond mere\naccuracy metrics to decide on a modelling approach.\n  This work describes our experiences in balancing some of the trade-offs that\narise, presenting a case study on a short-video recommendation application. We\nhighlight (1) neural networks' ability to handle large training data size,\nuser- and item-embeddings allows for more accurate models than GBDTs in this\nsetting, and (2) because GBDTs are less reliant on specialised hardware, they\ncan provide an equally accurate model at a lower cost. We believe these\nfindings are of relevance to researchers in both academia and industry, and\nhope they can inspire practitioners who need to make similar modelling choices\nin the future.",
        "translated": ""
    },
    {
        "title": "The Contemporary Art of Image Search: Iterative User Intent Expansion\n  via Vision-Language Model",
        "url": "http://arxiv.org/abs/2312.01656v1",
        "pub_date": "2023-12-04",
        "summary": "Image search is an essential and user-friendly method to explore vast\ngalleries of digital images. However, existing image search methods heavily\nrely on proximity measurements like tag matching or image similarity, requiring\nprecise user inputs for satisfactory results.To meet the growing demand for a\ncontemporary image search engine that enables accurate comprehension of users'\nsearch intentions, we introduce an innovative user intent expansion framework.\nOur framework leverages visual-language models to parse and compose multi-modal\nuser inputs to provide more accurate and satisfying results. It comprises\ntwo-stage processes: 1) a parsing stage that incorporates a language parsing\nmodule with large language models to enhance the comprehension of textual\ninputs, along with a visual parsing module that integrates an interactive\nsegmentation module to swiftly identify detailed visual elements within images;\nand 2) a logic composition stage that combines multiple user search intents\ninto a unified logic expression for more sophisticated operations in complex\nsearching scenarios. Moreover, the intent expansion framework enables users to\nperform flexible contextualized interactions with the search results to further\nspecify or adjust their detailed search intents iteratively. We implemented the\nframework into an image search system for NFT (non-fungible token) search and\nconducted a user study to evaluate its usability and novel properties. The\nresults indicate that the proposed framework significantly improves users'\nimage search experience. Particularly the parsing and contextualized\ninteractions prove useful in allowing users to express their search intents\nmore accurately and engage in a more enjoyable iterative search experience.",
        "translated": ""
    },
    {
        "title": "Searching Dense Representations with Inverted Indexes",
        "url": "http://arxiv.org/abs/2312.01556v1",
        "pub_date": "2023-12-04",
        "summary": "Nearly all implementations of top-$k$ retrieval with dense vector\nrepresentations today take advantage of hierarchical navigable small-world\nnetwork (HNSW) indexes. However, the generation of vector representations and\nefficiently searching large collections of vectors are distinct challenges that\ncan be decoupled. In this work, we explore the contrarian approach of\nperforming top-$k$ retrieval on dense vector representations using inverted\nindexes. We present experiments on the MS MARCO passage ranking dataset,\nevaluating three dimensions of interest: output quality, speed, and index size.\nResults show that searching dense representations using inverted indexes is\npossible. Our approach exhibits reasonable effectiveness with compact indexes,\nbut is impractically slow. Thus, while workable, our solution does not provide\na compelling tradeoff and is perhaps best characterized today as a \"technical\ncuriosity\".",
        "translated": ""
    },
    {
        "title": "Structured, Complex and Time-complete Temporal Event Forecasting",
        "url": "http://arxiv.org/abs/2312.01052v1",
        "pub_date": "2023-12-02",
        "summary": "Temporal event forecasting aims to predict what will happen next given the\nobserved events in history. Previous formulations of temporal event are\nunstructured, atomic, or lacking full temporal information, thus largely\nrestricting the representation quality and forecasting ability of temporal\nevents. To address these limitations, we introduce a novel formulation for\nStructured, Complex, and Time-complete Temporal Event (SCTc-TE). Based on this\nnew formulation, we develop a simple and fully automated pipeline for\nconstructing such SCTc-TEs from a large amount of news articles. Furthermore,\nwe propose a novel model that leverages both Local and Global contexts for\nSCTc-TE forecasting, named LoGo. To evaluate our model, we construct two\nlarge-scale datasets named MidEast-TE and GDELT-TE. Extensive evaluations\ndemonstrate the advantages of our datasets in multiple aspects, while\nexperimental results justify the effectiveness of our forecasting model LoGo.\nWe release the code and dataset via\nhttps://github.com/yecchen/GDELT-ComplexEvent.",
        "translated": ""
    },
    {
        "title": "A Hypergraph-Based Approach to Recommend Online Resources in a Library",
        "url": "http://arxiv.org/abs/2312.01007v1",
        "pub_date": "2023-12-02",
        "summary": "When users in a digital library read or browse online resources, it generates\nan immense amount of data. If the underlying system can recommend items, such\nas books and journals, to the users, it will help them to find the related\nitems. This research analyzes a digital library's usage data to recommend items\nto its users, and it uses different clustering algorithms to design the\nrecommender system. We have used content-based clustering, including\nhierarchical, expectation maximization (EM), K-mean, FarthestFirst, and\ndensity-based clustering algorithms, and user access pattern-based clustering,\nwhich uses a hypergraph-based approach to generate the clusters. This research\nshows that the recommender system designed using the hypergraph algorithm\ngenerates the most accurate recommendation model compared to those designed\nusing the content-based clustering approaches.",
        "translated": ""
    },
    {
        "title": "LLM-TAKE: Theme Aware Keyword Extraction Using Large Language Models",
        "url": "http://arxiv.org/abs/2312.00909v1",
        "pub_date": "2023-12-01",
        "summary": "Keyword extraction is one of the core tasks in natural language processing.\nClassic extraction models are notorious for having a short attention span which\nmake it hard for them to conclude relational connections among the words and\nsentences that are far from each other. This, in turn, makes their usage\nprohibitive for generating keywords that are inferred from the context of the\nwhole text. In this paper, we explore using Large Language Models (LLMs) in\ngenerating keywords for items that are inferred from the items textual\nmetadata. Our modeling framework includes several stages to fine grain the\nresults by avoiding outputting keywords that are non informative or sensitive\nand reduce hallucinations common in LLM. We call our LLM-based framework\nTheme-Aware Keyword Extraction (LLM TAKE). We propose two variations of\nframework for generating extractive and abstractive themes for products in an E\ncommerce setting. We perform an extensive set of experiments on three real data\nsets and show that our modeling framework can enhance accuracy based and\ndiversity based metrics when compared with benchmark models.",
        "translated": ""
    },
    {
        "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on\n  Open-Source Large Language Models",
        "url": "http://arxiv.org/abs/2312.02969v1",
        "pub_date": "2023-12-05",
        "summary": "Listwise rerankers based on large language models (LLM) are the zero-shot\nstate-of-the-art. However, current works in this direction all depend on the\nGPT models, making it a single point of failure in scientific reproducibility.\nMoreover, it raises the concern that the current research findings only hold\nfor GPT models but not LLM in general. In this work, we lift this pre-condition\nand build for the first time effective listwise rerankers without any form of\ndependency on GPT. Our passage retrieval experiments show that our best list se\nreranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves\n97% effectiveness of the ones built on GPT-4. Our results also show that the\nexisting training datasets, which were expressly constructed for pointwise\nranking, are insufficient for building such listwise rerankers. Instead,\nhigh-quality listwise ranking data is required and crucial, calling for further\nwork on building human-annotated listwise data resources.",
        "translated": ""
    },
    {
        "title": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via\n  Zero-Shot LLM-to-LLM Interactions",
        "url": "http://arxiv.org/abs/2312.02913v1",
        "pub_date": "2023-12-05",
        "summary": "Conversational question-answering (CQA) systems aim to create interactive\nsearch systems that effectively retrieve information by interacting with users.\nTo replicate human-to-human conversations, existing work uses human annotators\nto play the roles of the questioner (student) and the answerer (teacher).\nDespite its effectiveness, challenges exist as human annotation is\ntime-consuming, inconsistent, and not scalable. To address this issue and\ninvestigate the applicability of large language models (LLMs) in CQA\nsimulation, we propose a simulation framework that employs zero-shot learner\nLLMs for simulating teacher-student interactions. Our framework involves two\nLLMs interacting on a specific topic, with the first LLM acting as a student,\ngenerating questions to explore a given search topic. The second LLM plays the\nrole of a teacher by answering questions and is equipped with additional\ninformation, including a text on the given topic. We implement both the student\nand teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness\nof LLMs in simulating CQA interactions and understand the disparities between\nLLM- and human-generated conversations, we evaluate the simulated data from\nvarious perspectives. We begin by evaluating the teacher's performance through\nboth automatic and human assessment. Next, we evaluate the performance of the\nstudent, analyzing and comparing the disparities between questions generated by\nthe LLM and those generated by humans. Furthermore, we conduct extensive\nanalyses to thoroughly examine the LLM performance by benchmarking\nstate-of-the-art reading comprehension models on both datasets. Our results\nreveal that the teacher LLM generates lengthier answers that tend to be more\naccurate and complete. The student LLM generates more diverse questions,\ncovering more aspects of a given topic.",
        "translated": ""
    },
    {
        "title": "Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive\n  Review",
        "url": "http://arxiv.org/abs/2312.02901v1",
        "pub_date": "2023-12-05",
        "summary": "Due to the advent and increase in the popularity of the Internet, people have\nbeen producing and disseminating textual data in several ways, such as reviews,\nsocial media posts, and news articles. As a result, numerous researchers have\nbeen working on discovering patterns in textual data, especially because social\nmedia posts function as social sensors, indicating peoples' opinions,\ninterests, etc. However, most tasks regarding natural language processing are\naddressed using traditional machine learning methods and static datasets. This\nsetting can lead to several problems, such as an outdated dataset, which may\nnot correspond to reality, and an outdated model, which has its performance\ndegrading over time. Concept drift is another aspect that emphasizes these\nissues, which corresponds to data distribution and pattern changes. In a text\nstream scenario, it is even more challenging due to its characteristics, such\nas the high speed and data arriving sequentially. In addition, models for this\ntype of scenario must adhere to the constraints mentioned above while learning\nfrom the stream by storing texts for a limited time and consuming low memory.\nIn this study, we performed a systematic literature review regarding concept\ndrift adaptation in text stream scenarios. Considering well-defined criteria,\nwe selected 40 papers to unravel aspects such as text drift categories, types\nof text drift detection, model update mechanism, the addressed stream mining\ntasks, types of text representations, and text representation update mechanism.\nIn addition, we discussed drift visualization and simulation and listed\nreal-world datasets used in the selected papers. Therefore, this paper\ncomprehensively reviews the concept drift adaptation in text stream mining\nscenarios.",
        "translated": ""
    },
    {
        "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a\n  Breeze!",
        "url": "http://arxiv.org/abs/2312.02724v1",
        "pub_date": "2023-12-05",
        "summary": "In information retrieval, proprietary large language models (LLMs) such as\nGPT-4 and open-source counterparts such as LLaMA and Vicuna have played a vital\nrole in reranking. However, the gap between open-source and closed models\npersists, with reliance on proprietary, non-transparent models constraining\nreproducibility. Addressing this gap, we introduce RankZephyr, a\nstate-of-the-art, open-source LLM for listwise zero-shot reranking. RankZephyr\nnot only bridges the effectiveness gap with GPT-4 but in some cases surpasses\nthe proprietary model. Our comprehensive evaluations across several datasets\n(TREC Deep Learning Tracks; NEWS and COVID from BEIR) showcase this ability.\nRankZephyr benefits from strategic training choices and is resilient against\nvariations in initial document ordering and the number of documents reranked.\nAdditionally, our model outperforms GPT-4 on the NovelEval test set, comprising\nqueries and passages past its training period, which addresses concerns about\ndata contamination. To foster further research in this rapidly evolving field,\nwe provide all code necessary to reproduce our results at\nhttps://github.com/castorini/rank_llm.",
        "translated": ""
    },
    {
        "title": "An empirical study of next-basket recommendations",
        "url": "http://arxiv.org/abs/2312.02550v1",
        "pub_date": "2023-12-05",
        "summary": "Next Basket Recommender Systems (NBRs) function to recommend the subsequent\nshopping baskets for users through the modeling of their preferences derived\nfrom purchase history, typically manifested as a sequence of historical\nbaskets. Given their widespread applicability in the E-commerce industry,\ninvestigations into NBRs have garnered increased attention in recent years.\nDespite the proliferation of diverse NBR methodologies, a substantial challenge\nlies in the absence of a systematic and unified evaluation framework across\nthese methodologies. Various studies frequently appraise NBR approaches using\ndisparate datasets and diverse experimental settings, impeding a fair and\neffective comparative assessment of methodological performance. To bridge this\ngap, this study undertakes a systematic empirical inquiry into NBRs, reviewing\nseminal works within the domain and scrutinizing their respective merits and\ndrawbacks. Subsequently, we implement designated NBR algorithms on uniform\ndatasets, employing consistent experimental configurations, and assess their\nperformances via identical metrics. This methodological rigor establishes a\ncohesive framework for the impartial evaluation of diverse NBR approaches. It\nis anticipated that this study will furnish a robust foundation and serve as a\npivotal reference for forthcoming research endeavors in this dynamic field.",
        "translated": ""
    },
    {
        "title": "A Multi-Granularity-Aware Aspect Learning Model for Multi-Aspect Dense\n  Retrieval",
        "url": "http://arxiv.org/abs/2312.02538v1",
        "pub_date": "2023-12-05",
        "summary": "Dense retrieval methods have been mostly focused on unstructured text and\nless attention has been drawn to structured data with various aspects, e.g.,\nproducts with aspects such as category and brand. Recent work has proposed two\napproaches to incorporate the aspect information into item representations for\neffective retrieval by predicting the values associated with the item aspects.\nDespite their efficacy, they treat the values as isolated classes (e.g., \"Smart\nHomes\", \"Home, Garden &amp; Tools\", and \"Beauty &amp; Health\") and ignore their\nfine-grained semantic relation. Furthermore, they either enforce the learning\nof aspects into the CLS token, which could confuse it from its designated use\nfor representing the entire content semantics, or learn extra aspect embeddings\nonly with the value prediction objective, which could be insufficient\nespecially when there are no annotated values for an item aspect. Aware of\nthese limitations, we propose a MUlti-granulaRity-aware Aspect Learning model\n(MURAL) for multi-aspect dense retrieval. It leverages aspect information\nacross various granularities to capture both coarse and fine-grained semantic\nrelations between values. Moreover, MURAL incorporates separate aspect\nembeddings as input to transformer encoders so that the masked language model\nobjective can assist implicit aspect learning even without aspect-value\nannotations. Extensive experiments on two real-world datasets of products and\nmini-programs show that MURAL outperforms state-of-the-art baselines\nsignificantly.",
        "translated": ""
    },
    {
        "title": "DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework",
        "url": "http://arxiv.org/abs/2312.02532v1",
        "pub_date": "2023-12-05",
        "summary": "With the growing volume of diverse information, the demand for classifying\narbitrary topics has become increasingly critical. To address this challenge,\nwe introduce DRAFT, a simple framework designed to train a classifier for\nfew-shot topic classification. DRAFT uses a few examples of a specific topic as\nqueries to construct Customized dataset with a dense retriever model.\nMulti-query retrieval (MQR) algorithm, which effectively handles multiple\nqueries related to a specific topic, is applied to construct the Customized\ndataset. Subsequently, we fine-tune a classifier using the Customized dataset\nto identify the topic. To demonstrate the efficacy of our proposed approach, we\nconduct evaluations on both widely used classification benchmark datasets and\nmanually constructed datasets with 291 diverse topics, which simulate diverse\ncontents encountered in real-world applications. DRAFT shows competitive or\nsuperior performance compared to baselines that use in-context learning, such\nas GPT-3 175B and InstructGPT 175B, on few-shot topic classification tasks\ndespite having 177 times fewer parameters, demonstrating its effectiveness.",
        "translated": ""
    },
    {
        "title": "LLaRA: Aligning Large Language Models with Sequential Recommenders",
        "url": "http://arxiv.org/abs/2312.02445v1",
        "pub_date": "2023-12-05",
        "summary": "Sequential recommendation aims to predict the subsequent items matching user\npreference based on her/his historical interactions. With the development of\nLarge Language Models (LLMs), there is growing interest in exploring the\npotential of LLMs for sequential recommendation by framing it as a language\nmodeling task. Prior works represent items in the textual prompts using either\nID indexing or text indexing and feed the prompts into LLMs, but falling short\nof either encapsulating comprehensive world knowledge or exhibiting sufficient\nsequential understanding. To harness the complementary strengths of traditional\nrecommenders (which encode user behavioral knowledge) and LLMs (which possess\nworld knowledge about items), we propose LLaRA -- a Large Language and\nRecommendation Assistant framework. Specifically, LLaRA represents items in\nLLM's input prompts using a novel hybrid approach that integrates ID-based item\nembeddings from traditional recommenders with textual item features. Viewing\nthe ``sequential behavior of the user'' as a new modality in recommendation, we\nemploy an adapter to bridge the modality gap between ID embeddings of the\ntraditional recommenders and the input space of LLMs. Furthermore, instead of\ndirectly exposing the hybrid prompt to LLMs, we apply a curriculum learning\napproach to gradually ramp up training complexity. We first warm up the LLM\nwith text-only prompting, which aligns more naturally with the LLM's language\nmodeling capabilities. Thereafter, we progressively transition to hybrid\nprompting, training the adapter to incorporate behavioral knowledge from the\ntraditional sequential recommender into the LLM. Extensive experiments\ndemonstrate the efficacy of LLaRA framework. Our code and data are available at\nhttps://github.com/ljy0ustc/LLaRA .",
        "translated": ""
    },
    {
        "title": "E4SRec: An Elegant Effective Efficient Extensible Solution of Large\n  Language Models for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2312.02443v1",
        "pub_date": "2023-12-05",
        "summary": "The recent advancements in Large Language Models (LLMs) have sparked interest\nin harnessing their potential within recommender systems. Since LLMs are\ndesigned for natural language tasks, existing recommendation approaches have\npredominantly transformed recommendation tasks into open-domain natural\nlanguage generation tasks. However, this approach necessitates items to possess\nrich semantic information, often generates out-of-range results, and suffers\nfrom notably low efficiency and limited extensibility. Furthermore, practical\nID-based recommendation strategies, reliant on a huge number of unique\nidentities (IDs) to represent users and items, have gained prominence in\nreal-world recommender systems due to their effectiveness and efficiency.\nNevertheless, the incapacity of LLMs to model IDs presents a formidable\nchallenge when seeking to leverage LLMs for personalized recommendations. In\nthis paper, we introduce an Elegant Effective Efficient Extensible solution for\nlarge language models for Sequential Recommendation (E4SRec), which seamlessly\nintegrates LLMs with traditional recommender systems that exclusively utilize\nIDs to represent items. Specifically, E4SRec takes ID sequences as inputs,\nensuring that the generated outputs fall within the candidate lists.\nFurthermore, E4SRec possesses the capability to generate the entire ranking\nlist in a single forward process, and demands only a minimal set of pluggable\nparameters, which are trained for each dataset while keeping the entire LLM\nfrozen. We substantiate the effectiveness, efficiency, and extensibility of our\nproposed E4SRec through comprehensive experiments conducted on four widely-used\nreal-world datasets. The implementation code is accessible at\nhttps://github.com/HestiaSky/E4SRec/.",
        "translated": ""
    },
    {
        "title": "PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval\n  Models",
        "url": "http://arxiv.org/abs/2312.02429v1",
        "pub_date": "2023-12-05",
        "summary": "Embedding-based Retrieval Models (ERMs) have emerged as a promising framework\nfor large-scale text retrieval problems due to powerful large language models.\nNevertheless, fine-tuning ERMs to reach state-of-the-art results can be\nexpensive due to the extreme scale of data as well as the complexity of\nmulti-stages pipelines (e.g., pre-training, fine-tuning, distillation). In this\nwork, we propose the PEFA framework, namely ParamEter-Free Adapters, for fast\ntuning of ERMs without any backward pass in the optimization. At index building\nstage, PEFA equips the ERM with a non-parametric k-nearest neighbor (kNN)\ncomponent. At inference stage, PEFA performs a convex combination of two\nscoring functions, one from the ERM and the other from the kNN. Based on the\nneighborhood definition, PEFA framework induces two realizations, namely\nPEFA-XL (i.e., extra large) using double ANN indices and PEFA-XS (i.e., extra\nsmall) using a single ANN index. Empirically, PEFA achieves significant\nimprovement on two retrieval applications. For document retrieval, regarding\nRecall@100 metric, PEFA improves not only pre-trained ERMs on Trivia-QA by an\naverage of 13.2%, but also fine-tuned ERMs on NQ-320K by an average of 5.5%,\nrespectively. For product search, PEFA improves the Recall@100 of the\nfine-tuned ERMs by an average of 5.3% and 14.5%, for PEFA-XS and PEFA-XL,\nrespectively. Our code is available at https://github.com/\namzn/pecos/tree/mainline/examples/pefa-wsdm24",
        "translated": ""
    },
    {
        "title": "Boosting legal case retrieval by query content selection with large\n  language models",
        "url": "http://arxiv.org/abs/2312.03494v1",
        "pub_date": "2023-12-06",
        "summary": "Legal case retrieval, which aims to retrieve relevant cases to a given query\ncase, benefits judgment justice and attracts increasing attention. Unlike\ngeneric retrieval queries, legal case queries are typically long and the\ndefinition of relevance is closely related to legal-specific elements.\nTherefore, legal case queries may suffer from noise and sparsity of salient\ncontent, which hinders retrieval models from perceiving correct information in\na query. While previous studies have paid attention to improving retrieval\nmodels and understanding relevance judgments, we focus on enhancing legal case\nretrieval by utilizing the salient content in legal case queries. We first\nannotate the salient content in queries manually and investigate how sparse and\ndense retrieval models attend to those content. Then we experiment with various\nquery content selection methods utilizing large language models (LLMs) to\nextract or summarize salient content and incorporate it into the retrieval\nmodels. Experimental results show that reformulating long queries using LLMs\nimproves the performance of both sparse and dense models in legal case\nretrieval.",
        "translated": ""
    },
    {
        "title": "DBCopilot: Scaling Natural Language Querying to Massive Databases",
        "url": "http://arxiv.org/abs/2312.03463v1",
        "pub_date": "2023-12-06",
        "summary": "Text-to-SQL simplifies database interactions by enabling non-experts to\nconvert their natural language (NL) questions into Structured Query Language\n(SQL) queries. While recent advances in large language models (LLMs) have\nimproved the zero-shot text-to-SQL paradigm, existing methods face scalability\nchallenges when dealing with massive, dynamically changing databases. This\npaper introduces DBCopilot, a framework that addresses these challenges by\nemploying a compact and flexible copilot model for routing across massive\ndatabases. Specifically, DBCopilot decouples the text-to-SQL process into\nschema routing and SQL generation, leveraging a lightweight\nsequence-to-sequence neural network-based router to formulate database\nconnections and navigate natural language questions through databases and\ntables. The routed schemas and questions are then fed into LLMs for efficient\nSQL generation. Furthermore, DBCopilot also introduced a reverse\nschema-to-question generation paradigm, which can learn and adapt the router\nover massive databases automatically without requiring manual intervention.\nExperimental results demonstrate that DBCopilot is a scalable and effective\nsolution for real-world text-to-SQL tasks, providing a significant advancement\nin handling large-scale schemas.",
        "translated": ""
    },
    {
        "title": "Rethinking E-Commerce Search",
        "url": "http://arxiv.org/abs/2312.03217v1",
        "pub_date": "2023-12-06",
        "summary": "E-commerce search and recommendation usually operate on structured data such\nas product catalogs and taxonomies. However, creating better search and\nrecommendation systems often requires a large variety of unstructured data\nincluding customer reviews and articles on the web. Traditionally, the solution\nhas always been converting unstructured data into structured data through\ninformation extraction, and conducting search over the structured data.\nHowever, this is a costly approach that often has low quality. In this paper,\nwe envision a solution that does entirely the opposite. Instead of converting\nunstructured data (web pages, customer reviews, etc) to structured data, we\ninstead convert structured data (product inventory, catalogs, taxonomies, etc)\ninto textual data, which can be easily integrated into the text corpus that\ntrains LLMs. Then, search and recommendation can be performed through a Q/A\nmechanism through an LLM instead of using traditional information retrieval\nmethods over structured data.",
        "translated": ""
    },
    {
        "title": "Combining Counting Processes and Classification Improves a Stopping Rule\n  for Technology Assisted Review",
        "url": "http://arxiv.org/abs/2312.03171v1",
        "pub_date": "2023-12-05",
        "summary": "Technology Assisted Review (TAR) stopping rules aim to reduce the cost of\nmanually assessing documents for relevance by minimising the number of\ndocuments that need to be examined to ensure a desired level of recall. This\npaper extends an effective stopping rule using information derived from a text\nclassifier that can be trained without the need for any additional annotation.\nExperiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal\nand RCV1) showed that the proposed approach consistently improves performance\nand outperforms several alternative methods.",
        "translated": ""
    },
    {
        "title": "Adaptive spectral graph wavelets for collaborative filtering",
        "url": "http://arxiv.org/abs/2312.03167v1",
        "pub_date": "2023-12-05",
        "summary": "Collaborative filtering is a popular approach in recommender systems, whose\nobjective is to provide personalized item suggestions to potential users based\non their purchase or browsing history. However, personalized recommendations\nrequire considerable amount of behavioral data on users, which is usually\nunavailable for new users, giving rise to the cold-start problem. To help\nalleviate this challenging problem, we introduce a spectral graph wavelet\ncollaborative filtering framework for implicit feedback data, where users,\nitems and their interactions are represented as a bipartite graph.\nSpecifically, we first propose an adaptive transfer function by leveraging a\npower transform with the goal of stabilizing the variance of graph frequencies\nin the spectral domain. Then, we design a deep recommendation model for\nefficient learning of low-dimensional embeddings of users and items using\nspectral graph wavelets in an end-to-end fashion. In addition to capturing the\ngraph's local and global structures, our approach yields localization of graph\nsignals in both spatial and spectral domains, and hence not only learns\ndiscriminative representations of users and items, but also promotes the\nrecommendation quality. The effectiveness of our proposed model is demonstrated\nthrough extensive experiments on real-world benchmark datasets, achieving\nbetter recommendation performance compared with strong baseline methods.",
        "translated": ""
    },
    {
        "title": "Multi-agricultural Machinery Collaborative Task Assignment Based on\n  Improved Genetic Hybrid Optimization Algorithm",
        "url": "http://arxiv.org/abs/2312.04264v1",
        "pub_date": "2023-12-07",
        "summary": "To address the challenges of delayed scheduling information, heavy reliance\non manual labour, and low operational efficiency in traditional large-scale\nagricultural machinery operations, this study proposes a method for\nmulti-agricultural machinery collaborative task assignment based on an improved\ngenetic hybrid optimisation algorithm. The proposed method establishes a\nmulti-agricultural machinery task allocation model by combining the path\npre-planning of a simulated annealing algorithm and the static task allocation\nof a genetic algorithm. By sequentially fusing these two algorithms, their\nrespective shortcomings can be overcome, and their advantages in global and\nlocal search can be utilised. Consequently, the search capability of the\npopulation is enhanced, leading to the discovery of more optimal solutions.\nThen, an adaptive crossover operator is constructed according to the task\nassignment model, considering the capacity, path cost, and time of agricultural\nmachinery; two-segment coding and multi-population adaptive mutation are used\nto assign tasks to improve the diversity of the population and enhance the\nexploration ability of the population; and to improve the global optimisation\nability of the hybrid algorithm, a 2-Opt local optimisation operator and an\nCircle modification algorithm are introduced. Finally, simulation experiments\nwere conducted in MATLAB to evaluate the performance of the multi-agricultural\nmachinery collaborative task assignment based on the improved genetic hybrid\nalgorithm. The algorithm's capabilities were assessed through comparative\nanalysis in the simulation trials. The results demonstrate that the developed\nhybrid algorithm can effectively reduce path costs, and the efficiency of the\nassignment outcomes surpasses that of the classical genetic algorithm. This\napproach proves particularly suitable for addressing large-scale task\nallocation problems.",
        "translated": ""
    },
    {
        "title": "Synergistic Signals: Exploiting Co-Engagement and Semantic Links via\n  Graph Neural Networks",
        "url": "http://arxiv.org/abs/2312.04071v1",
        "pub_date": "2023-12-07",
        "summary": "Given a set of candidate entities (e.g. movie titles), the ability to\nidentify similar entities is a core capability of many recommender systems.\nMost often this is achieved by collaborative filtering approaches, i.e. if\nusers co-engage with a pair of entities frequently enough, the embeddings\nshould be similar. However, relying on co-engagement data alone can result in\nlower-quality embeddings for new and unpopular entities. We study this problem\nin the context recommender systems at Netflix. We observe that there is\nabundant semantic information such as genre, content maturity level, themes,\netc. that complements co-engagement signals and provides interpretability in\nsimilarity models. To learn entity similarities from both data sources\nholistically, we propose a novel graph-based approach called SemanticGNN.\nSemanticGNN models entities, semantic concepts, collaborative edges, and\nsemantic edges within a large-scale knowledge graph and conducts representation\nlearning over it. Our key technical contributions are twofold: (1) we develop a\nnovel relation-aware attention graph neural network (GNN) to handle the\nimbalanced distribution of relation types in our graph; (2) to handle web-scale\ngraph data that has millions of nodes and billions of edges, we develop a novel\ndistributed graph training paradigm. The proposed model is successfully\ndeployed within Netflix and empirical experiments indicate it yields up to 35%\nimprovement in performance on similarity judgment tasks.",
        "translated": ""
    },
    {
        "title": "Sports Recommender Systems: Overview and Research Issues",
        "url": "http://arxiv.org/abs/2312.03785v1",
        "pub_date": "2023-12-06",
        "summary": "Sports recommender systems receive an increasing attention due to their\npotential of fostering healthy living, improving personal well-being, and\nincreasing performances in sport. These systems support people in sports, for\nexample, by the recommendation of healthy and performance boosting food items,\nthe recommendation of training practices, talent and team recommendation, and\nthe recommendation of specific tactics in competitions. With applications in\nthe virtual world, for example, the recommendation of maps or opponents in\ne-sports, these systems already transcend conventional sports scenarios where\nphysical presence is needed. On the basis of different working examples, we\npresent an overview of sports recommender systems applications and techniques.\nOverall, we analyze the related state-of-the-art and discuss open research\nissues.",
        "translated": ""
    },
    {
        "title": "Soft Frequency Capping for Improved Ad Click Prediction in Yahoo Gemini\n  Native",
        "url": "http://arxiv.org/abs/2312.05052v1",
        "pub_date": "2023-12-08",
        "summary": "Yahoo's native advertising (also known as Gemini native) serves billions of\nad impressions daily, reaching a yearly run-rate of many hundred of millions\nUSD. Driving the Gemini native models that are used to predict both click\nprobability (pCTR) and conversion probability (pCONV) is OFFSET - a feature\nenhanced collaborative-filtering (CF) based event prediction algorithm. \\offset\nis a one-pass algorithm that updates its model for every new batch of logged\ndata using a stochastic gradient descent (SGD) based approach. Since OFFSET\nrepresents its users by their features (i.e., user-less model) due to sparsity\nissues, rule based hard frequency capping (HFC) is used to control the number\nof times a certain user views a certain ad. Moreover, related statistics reveal\nthat user ad fatigue results in a dramatic drop in click through rate (CTR).\nTherefore, to improve click prediction accuracy, we propose a soft frequency\ncapping (SFC) approach, where the frequency feature is incorporated into the\nOFFSET model as a user-ad feature and its weight vector is learned via logistic\nregression as part of OFFSET training. Online evaluation of the soft frequency\ncapping algorithm via bucket testing showed a significant 7.3% revenue lift.\nSince then, the frequency feature enhanced model has been pushed to production\nserving all traffic, and is generating a hefty revenue lift for Yahoo Gemini\nnative. We also report related statistics that reveal, among other things, that\nwhile users' gender does not affect ad fatigue, the latter seems to increase\nwith users' age.",
        "translated": ""
    },
    {
        "title": "Unbiased Filtering Of Accidental Clicks in Verizon Media Native\n  Advertising",
        "url": "http://arxiv.org/abs/2312.05017v1",
        "pub_date": "2023-12-08",
        "summary": "Verizon Media (VZM) native advertising is one of VZM largest and fastest\ngrowing businesses, reaching a run-rate of several hundred million USDs in the\npast year. Driving the VZM native models that are used to predict event\nprobabilities, such as click and conversion probabilities, is OFFSET - a\nfeature enhanced collaborative-filtering based event-prediction algorithm. In\nthis work we focus on the challenge of predicting click-through rates (CTR)\nwhen we are aware that some of the clicks have short dwell-time and are defined\nas accidental clicks. An accidental click implies little affinity between the\nuser and the ad, so predicting that similar users will click on the ad is\ninaccurate. Therefore, it may be beneficial to remove clicks with dwell-time\nlower than a predefined threshold from the training set. However, we cannot\nignore these positive events, as filtering these will cause the model to under\npredict. Previous approaches have tried to apply filtering and then adding\ncorrective biases to the CTR predictions, but did not yield revenue lifts and\ntherefore were not adopted. In this work, we present a new approach where the\npositive weight of the accidental clicks is distributed among all of the\nnegative events (skips), based on their likelihood of causing accidental\nclicks, as predicted by an auxiliary model. These likelihoods are taken as the\ncorrect labels of the negative events, shifting our training from using only\nbinary labels and adopting a binary cross-entropy loss function in our training\nprocess. After showing offline performance improvements, the modified model was\ntested online serving VZM native users, and provided 1.18% revenue lift over\nthe production model which is agnostic to accidental clicks.",
        "translated": ""
    },
    {
        "title": "Illicit Darkweb Classification via Natural-language Processing:\n  Classifying Illicit Content of Webpages based on Textual Information",
        "url": "http://arxiv.org/abs/2312.04944v1",
        "pub_date": "2023-12-08",
        "summary": "This work aims at expanding previous works done in the context of illegal\nactivities classification, performing three different steps. First, we created\na heterogeneous dataset of 113995 onion sites and dark marketplaces. Then, we\ncompared pre-trained transferable models, i.e., ULMFit (Universal Language\nModel Fine-tuning), Bert (Bidirectional Encoder Representations from\nTransformers), and RoBERTa (Robustly optimized BERT approach) with a\ntraditional text classification approach like LSTM (Long short-term memory)\nneural networks. Finally, we developed two illegal activities classification\napproaches, one for illicit content on the Dark Web and one for identifying the\nspecific types of drugs. Results show that Bert obtained the best approach,\nclassifying the dark web's general content and the types of Drugs with 96.08%\nand 91.98% of accuracy.",
        "translated": ""
    },
    {
        "title": "Predictive Chemistry Augmented with Text Retrieval",
        "url": "http://arxiv.org/abs/2312.04881v1",
        "pub_date": "2023-12-08",
        "summary": "This paper focuses on using natural language descriptions to enhance\npredictive models in the chemistry field. Conventionally, chemoinformatics\nmodels are trained with extensive structured data manually extracted from the\nliterature. In this paper, we introduce TextReact, a novel method that directly\naugments predictive chemistry with texts retrieved from the literature.\nTextReact retrieves text descriptions relevant for a given chemical reaction,\nand then aligns them with the molecular representation of the reaction. This\nalignment is enhanced via an auxiliary masked LM objective incorporated in the\npredictor training. We empirically validate the framework on two chemistry\ntasks: reaction condition recommendation and one-step retrosynthesis. By\nleveraging text retrieval, TextReact significantly outperforms state-of-the-art\nchemoinformatics models trained solely on molecular data.",
        "translated": ""
    },
    {
        "title": "CAR: Consolidation, Augmentation and Regulation for Recipe Retrieval",
        "url": "http://arxiv.org/abs/2312.04763v1",
        "pub_date": "2023-12-08",
        "summary": "Learning recipe and food image representation in common embedding space is\nnon-trivial but crucial for cross-modal recipe retrieval. In this paper, we\npropose CAR framework with three novel techniques, i.e., Consolidation,\nAugmentation and Regulation, for cross-modal recipe retrieval. We introduce\nadapter layers to consolidate pre-trained CLIP model with much less computation\ncost than fully cumbersome fine-tuning all the parameters. Furthermore,\nleveraging on the strong capability of foundation models (i.e., SAM and LLM),\nwe propose to augment recipe and food image by extracting information related\nto the counterpart. SAM generates image segments corresponding to ingredients\nin the recipe, while LLM produces a visual imagination description from the\nrecipe, aiming to capture the visual cues of a food image. In addition, we\nintroduce circle loss to regulate cross-modal embedding space, which assigns\ndifferent penalties for positive and negative pairs. With the extra augmented\ndata from recipe and image, multi-level circle loss is proposed, which applies\ncircle loss not only to original image-recipe pairs, but also to image segments\nand recipe, visual imagination description and food image as well as any two\nsections within a recipe. On Recipe1M dataset, our proposed CAR outperforms all\nthe existing methods by a large margin. Extensive ablation studies are\nconducted to validate the effectiveness of each component of CAR. We will make\nour code and models publicly available.",
        "translated": ""
    },
    {
        "title": "STraceBERT: Source Code Retrieval using Semantic Application Traces",
        "url": "http://arxiv.org/abs/2312.04731v1",
        "pub_date": "2023-12-07",
        "summary": "Software reverse engineering is an essential task in software engineering and\nsecurity, but it can be a challenging process, especially for adversarial\nartifacts. To address this challenge, we present STraceBERT, a novel approach\nthat utilizes a Java dynamic analysis tool to record calls to core Java\nlibraries, and pretrain a BERT-style model on the recorded application traces\nfor effective method source code retrieval from a candidate set. Our\nexperiments demonstrate the effectiveness of STraceBERT in retrieving the\nsource code compared to existing approaches. Our proposed approach offers a\npromising solution to the problem of code retrieval in software reverse\nengineering and opens up new avenues for further research in this area.",
        "translated": ""
    },
    {
        "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
        "url": "http://arxiv.org/abs/2312.06648v1",
        "pub_date": "2023-12-11",
        "summary": "Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our results reveal that\nproposition-based retrieval significantly outperforms traditional passage or\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\nalso enhances the performance of downstream QA tasks, since the retrieved texts\nare more condensed with question-relevant information, reducing the need for\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\ninformation.",
        "translated": ""
    },
    {
        "title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot\n  Disease Phenotyping",
        "url": "http://arxiv.org/abs/2312.06457v1",
        "pub_date": "2023-12-11",
        "summary": "Identifying disease phenotypes from electronic health records (EHRs) is\ncritical for numerous secondary uses. Manually encoding physician knowledge\ninto rules is particularly challenging for rare diseases due to inadequate EHR\ncoding, necessitating review of clinical notes. Large language models (LLMs)\noffer promise in text understanding but may not efficiently handle real-world\nclinical documentation. We propose a zero-shot LLM-based method enriched by\nretrieval-augmented generation and MapReduce, which pre-identifies\ndisease-related text snippets to be used in parallel as queries for the LLM to\nestablish diagnosis. We show that this method as applied to pulmonary\nhypertension (PH), a rare disease characterized by elevated arterial pressures\nin the lungs, significantly outperforms physician logic rules ($F_1$ score of\n0.62 vs. 0.75). This method has the potential to enhance rare disease cohort\nidentification, expanding the scope of robust clinical research and care gap\nidentification.",
        "translated": ""
    },
    {
        "title": "VisionTraj: A Noise-Robust Trajectory Recovery Framework based on\n  Large-scale Camera Network",
        "url": "http://arxiv.org/abs/2312.06428v1",
        "pub_date": "2023-12-11",
        "summary": "Trajectory recovery based on the snapshots from the city-wide multi-camera\nnetwork facilitates urban mobility sensing and driveway optimization. The\nstate-of-the-art solutions devoted to such a vision-based scheme typically\nincorporate predefined rules or unsupervised iterative feedback, struggling\nwith multi-fold challenges such as lack of open-source datasets for training\nthe whole pipeline, and the vulnerability to the noises from visual inputs. In\nresponse to the dilemma, this paper proposes VisionTraj, the first\nlearning-based model that reconstructs vehicle trajectories from snapshots\nrecorded by road network cameras. Coupled with it, we elaborate on two rational\nvision-trajectory datasets, which produce extensive trajectory data along with\ncorresponding visual snapshots, enabling supervised vision-trajectory interplay\nextraction. Following the data creation, based on the results from the\noff-the-shelf multi-modal vehicle clustering, we first re-formulate the\ntrajectory recovery problem as a generative task and introduce the canonical\nTransformer as the autoregressive backbone. Then, to identify clustering noises\n(e.g., false positives) with the bound on the snapshots' spatiotemporal\ndependencies, a GCN-based soft-denoising module is conducted based on the fine-\nand coarse-grained Re-ID clusters. Additionally, we harness strong semantic\ninformation extracted from the tracklet to provide detailed insights into the\nvehicle's entry and exit actions during trajectory recovery. The denoising and\ntracklet components can also act as plug-and-play modules to boost baselines.\nExperimental results on the two hand-crafted datasets show that the proposed\nVisionTraj achieves a maximum +11.5% improvement against the sub-best model.",
        "translated": ""
    },
    {
        "title": "Cross Domain LifeLong Sequential Modeling for Online Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2312.06424v1",
        "pub_date": "2023-12-11",
        "summary": "Deep neural networks (DNNs) that incorporated lifelong sequential modeling\n(LSM) have brought great success to recommendation systems in various social\nmedia platforms. While continuous improvements have been made in\ndomain-specific LSM, limited work has been done in cross-domain LSM, which\nconsiders modeling of lifelong sequences of both target domain and source\ndomain. In this paper, we propose Lifelong Cross Network (LCN) to incorporate\ncross-domain LSM to improve the click-through rate (CTR) prediction in the\ntarget domain. The proposed LCN contains a LifeLong Attention Pyramid (LAP)\nmodule that comprises of three levels of cascaded attentions to effectively\nextract interest representations with respect to the candidate item from\nlifelong sequences. We also propose Cross Representation Production (CRP)\nmodule to enforce additional supervision on the learning and alignment of\ncross-domain representations so that they can be better reused on learning of\nthe CTR prediction in the target domain. We conducted extensive experiments on\nWeChat Channels industrial dataset as well as on benchmark dataset. Results\nhave revealed that the proposed LCN outperforms existing work in terms of both\nprediction accuracy and online performance.",
        "translated": ""
    },
    {
        "title": "MUST: An Effective and Scalable Framework for Multimodal Search of\n  Target Modality",
        "url": "http://arxiv.org/abs/2312.06397v1",
        "pub_date": "2023-12-11",
        "summary": "We investigate the problem of multimodal search of target modality, where the\ntask involves enhancing a query in a specific target modality by integrating\ninformation from auxiliary modalities. The goal is to retrieve relevant objects\nwhose contents in the target modality match the specified multimodal query. The\npaper first introduces two baseline approaches that integrate techniques from\nthe Database, Information Retrieval, and Computer Vision communities. These\nbaselines either merge the results of separate vector searches for each\nmodality or perform a single-channel vector search by fusing all modalities.\nHowever, both baselines have limitations in terms of efficiency and accuracy as\nthey fail to adequately consider the varying importance of fusing information\nacross modalities. To overcome these limitations, the paper proposes a novel\nframework, called MUST. Our framework employs a hybrid fusion mechanism,\ncombining different modalities at multiple stages. Notably, we leverage vector\nweight learning to determine the importance of each modality, thereby enhancing\nthe accuracy of joint similarity measurement. Additionally, the proposed\nframework utilizes a fused proximity graph index, enabling efficient joint\nsearch for multimodal queries. MUST offers several other advantageous\nproperties, including pluggable design to integrate any advanced embedding\ntechniques, user flexibility to customize weight preferences, and modularized\nindex construction. Extensive experiments on real-world datasets demonstrate\nthe superiority of MUST over the baselines in terms of both search accuracy and\nefficiency. Our framework achieves over 10x faster search times while attaining\nan average of 93% higher accuracy. Furthermore, MUST exhibits scalability to\ndatasets containing more than 10 million data elements.",
        "translated": ""
    },
    {
        "title": "Empirical Basis of Engineering Design Knowledge",
        "url": "http://arxiv.org/abs/2312.06355v1",
        "pub_date": "2023-12-11",
        "summary": "Engineering design knowledge is embodied in natural language text through\nintricate placement of entities and relationships. Ontological constructs of\ndesign knowledge often limit the performances of NLP techniques to extract\ndesign knowledge. Also, large-language models could be less useful for\ngenerating and explicating design knowledge, as these are trained predominantly\non common-sense text. In this article, we present the constituents of design\nknowledge based on empirical observations from patent documents. We obtain a\nsample of 33,881 patents and populate over 24 million facts from the sentences\nin these. We conduct Zipf distribution analyses using the frequencies of unique\nentities and relationships that are present in the facts thus populated. While\nthe literal entities cannot be generalised from the sample of patents, the\nrelationships largely capture attributes ('of'), structure ('in', 'with'),\npurpose ('to', 'for'), hierarchy ('include'), exemplification ('such as'), and\nbehaviour ('to', 'from'). The analyses reveal that over half of entities and\nrelationships could be generalised to 64 and 24 linguistic syntaxes\nrespectively, while hierarchical relationships include 75 syntaxes. These\nsyntaxes represent the linguistic basis of engineering design knowledge. We\ncombine facts within each patent into a knowledge graph, from which we discover\nmotifs that are statistically over-represented subgraph patterns. Across all\npatents in the sample, we identify eight patterns that could be simplified into\nsequence [-&gt;...-&gt;], aggregation [-&gt;...&lt;-], and hierarchy [&lt;-...-&gt;] that form\nthe structural basis of engineering design knowledge. We propose regulatory\nprecepts for concretising abstract entities and relationships within subgraphs,\nwhile also explicating hierarchical structures. These precepts could be useful\nfor better construction and management of knowledge in a design environment.",
        "translated": ""
    },
    {
        "title": "Improving Startup Success with Text Analysis",
        "url": "http://arxiv.org/abs/2312.06236v1",
        "pub_date": "2023-12-11",
        "summary": "Investors are interested in predicting future success of startup companies,\npreferably using publicly available data which can be gathered using free\nonline sources. Using public-only data has been shown to work, but there is\nstill much room for improvement. Two of the best performing prediction\nexperiments use 17 and 49 features respectively, mostly numeric and categorical\nin nature. In this paper, we significantly expand and diversify both the\nsources and the number of features (to 171) to achieve better prediction. Data\ncollected from Crunchbase, the Google Search API, and Twitter (now X) are used\nto predict whether a company will raise a round of funding within a fixed time\nhorizon. Much of the new features are textual and the Twitter subset include\nlinguistic metrics such as measures of passive voice and parts-of-speech. A\ntotal of ten machine learning models are also evaluated for best performance.\nThe adaptable model can be used to predict funding 1-5 years into the future,\nwith a variable cutoff threshold to favor either precision or recall.\nPrediction with comparable assumptions generally achieves F scores above 0.730\nwhich outperforms previous attempts in the literature (0.531), and does so with\nfewer examples. Furthermore, we find that the vast majority of the performance\nimpact comes from the top 18 of 171 features which are mostly generic company\nobservations, including the best performing individual feature which is the\nfree-form text description of the company.",
        "translated": ""
    },
    {
        "title": "RecJPQ: Training Large-Catalogue Sequential Recommenders",
        "url": "http://arxiv.org/abs/2312.06165v1",
        "pub_date": "2023-12-11",
        "summary": "Sequential recommender systems rank items based on the likelihood of their\nnext appearance in user-item interactions. Current models such as BERT4Rec and\nSASRec generate sequence embeddings and compute scores for catalogue items, but\nthe increasing catalogue size makes training these models costly. The Joint\nProduct Quantisation method, originally proposed for passage retrieval,\nmarkedly reduces the size of the retrieval index with minimal effect on model\neffectiveness by replacing passage embeddings with a limited number of shared\ncentroid embeddings. This paper introduces RecJPQ, a novel adaptation of JPQ\nfor sequential recommendations. We apply RecJPQ to SASRec, BERT4Rec, and\nGRU4rec models on three large-scale sequential datasets. Our results showed\nthat RecJPQ could notably reduce the model size (e.g., 48x reduction for the\nGowalla dataset with no effectiveness degradation). RecJPQ can also improve\nmodel performance through a regularisation effect (e.g. +0.96% NDCG@10\nimprovement on the Booking.com dataset).",
        "translated": ""
    },
    {
        "title": "\"What's important here?\": Opportunities and Challenges of Using LLMs in\n  Retrieving Information from Web Interfaces",
        "url": "http://arxiv.org/abs/2312.06147v1",
        "pub_date": "2023-12-11",
        "summary": "Large language models (LLMs) that have been trained on a corpus that includes\nlarge amount of code exhibit a remarkable ability to understand HTML code. As\nweb interfaces are primarily constructed using HTML, we design an in-depth\nstudy to see how LLMs can be used to retrieve and locate important elements for\na user given query (i.e. task description) in a web interface. In contrast with\nprior works, which primarily focused on autonomous web navigation, we decompose\nthe problem as an even atomic operation - Can LLMs identify the important\ninformation in the web page for a user given query? This decomposition enables\nus to scrutinize the current capabilities of LLMs and uncover the opportunities\nand challenges they present. Our empirical experiments show that while LLMs\nexhibit a reasonable level of performance in retrieving important UI elements,\nthere is still a substantial room for improvement. We hope our investigation\nwill inspire follow-up works in overcoming the current challenges in this\ndomain.",
        "translated": ""
    },
    {
        "title": "Proxy-based Item Representation for Attribute and Context-aware\n  Recommendation",
        "url": "http://arxiv.org/abs/2312.06145v1",
        "pub_date": "2023-12-11",
        "summary": "Neural network approaches in recommender systems have shown remarkable\nsuccess by representing a large set of items as a learnable vector embedding\ntable. However, infrequent items may suffer from inadequate training\nopportunities, making it difficult to learn meaningful representations. We\nexamine that in attribute and context-aware settings, the poorly learned\nembeddings of infrequent items impair the recommendation accuracy. To address\nsuch an issue, we propose a proxy-based item representation that allows each\nitem to be expressed as a weighted sum of learnable proxy embeddings. Here, the\nproxy weight is determined by the attributes and context of each item and may\nincorporate bias terms in case of frequent items to further reflect\ncollaborative signals. The proxy-based method calculates the item\nrepresentations compositionally, ensuring each representation resides inside a\nwell-trained simplex and, thus, acquires guaranteed quality. Additionally, that\nthe proxy embeddings are shared across all items allows the infrequent items to\nborrow training signals of frequent items in a unified model structure and\nend-to-end manner. Our proposed method is a plug-and-play model that can\nreplace the item encoding layer of any neural network-based recommendation\nmodel, while consistently improving the recommendation performance with much\nsmaller parameter usage. Experiments conducted on real-world recommendation\nbenchmark datasets demonstrate that our proposed model outperforms\nstate-of-the-art models in terms of recommendation accuracy by up to 17% while\nusing only 10% of the parameters.",
        "translated": ""
    },
    {
        "title": "Search Optimization with Query Likelihood Boosting and Two-Level\n  Approximate Search for Edge Devices",
        "url": "http://arxiv.org/abs/2312.07517v1",
        "pub_date": "2023-12-12",
        "summary": "We present a novel search optimization solution for approximate nearest\nneighbor (ANN) search on resource-constrained edge devices. Traditional ANN\napproaches fall short in meeting the specific demands of real-world scenarios,\ne.g., skewed query likelihood distribution and search on large-scale indices\nwith a low latency and small footprint. To address these limitations, we\nintroduce two key components: a Query Likelihood Boosted Tree (QLBT) to\noptimize average search latency for frequently used small datasets, and a\ntwo-level approximate search algorithm to enable efficient retrieval with large\ndatasets on edge devices. We perform thorough evaluation on simulated and real\ndata and demonstrate QLBT can significantly reduce latency by 15% on real data\nand our two-level search algorithm successfully achieve deployable accuracy and\nlatency on a 10 million dataset for edge devices. In addition, we provide a\ncomprehensive protocol for configuring and optimizing on-device search\nalgorithm through extensive empirical studies.",
        "translated": ""
    },
    {
        "title": "Audience Prospecting for Dynamic-Product-Ads in Native Advertising",
        "url": "http://arxiv.org/abs/2312.07160v1",
        "pub_date": "2023-12-12",
        "summary": "With yearly revenue exceeding one billion USD, Yahoo Gemini native\nadvertising marketplace serves more than two billion impressions daily to\nhundreds of millions of unique users. One of the fastest growing segments of\nGemini native is dynamic-product-ads (DPA), where major advertisers, such as\nAmazon and Walmart, provide catalogs with millions of products for the system\nto choose from and present to users. The subject of this work is finding and\nexpanding the right audience for each DPA ad, which is one of the many\nchallenges DPA presents. Approaches such as targeting various user groups,\ne.g., users who already visited the advertisers' websites (Retargeting), users\nthat searched for certain products (Search-Prospecting), or users that reside\nin preferred locations (Location-Prospecting), have limited audience expansion\ncapabilities. In this work we present two new approaches for audience expansion\nthat also maintain predefined performance goals. The Conversion-Prospecting\napproach predicts DPA conversion rates based on Gemini native logged data, and\ncalculates the expected cost-per-action (CPA) for determining users'\neligibility to products and optimizing DPA bids in Gemini native auctions. To\nsupport new advertisers and products, the Trending-Prospecting approach matches\ntrending products to users by learning their tendency towards products from\nadvertisers' sites logged events. The tendency scores indicate the popularity\nof the product and the similarity of the user to those who have previously\nengaged with this product. The two new prospecting approaches were tested\nonline, serving real Gemini native traffic, demonstrating impressive DPA\ndelivery and DPA revenue lifts while maintaining most traffic within the\nacceptable CPA range (i.e., performance goal). After a successful testing\nphase, the proposed approaches are currently in production and serve all Gemini\nnative traffic.",
        "translated": ""
    },
    {
        "title": "Debiasing Sequential Recommenders through Distributionally Robust\n  Optimization over System Exposure",
        "url": "http://arxiv.org/abs/2312.07036v1",
        "pub_date": "2023-12-12",
        "summary": "Sequential recommendation (SR) models are typically trained on user-item\ninteractions which are affected by the system exposure bias, leading to the\nuser preference learned from the biased SR model not being fully consistent\nwith the true user preference. Exposure bias refers to the fact that user\ninteractions are dependent upon the partial items exposed to the user. Existing\ndebiasing methods do not make full use of the system exposure data and suffer\nfrom sub-optimal recommendation performance and high variance. In this paper,\nwe propose to debias sequential recommenders through Distributionally Robust\nOptimization (DRO) over system exposure data. The key idea is to utilize DRO to\noptimize the worst-case error over an uncertainty set to safeguard the model\nagainst distributional discrepancy caused by the exposure bias. The main\nchallenge to apply DRO for exposure debiasing in SR lies in how to construct\nthe uncertainty set and avoid the overestimation of user preference on biased\nsamples. Moreover, how to evaluate the debiasing effect on biased test set is\nalso an open question. To this end, we first introduce an exposure simulator\ntrained upon the system exposure data to calculate the exposure distribution,\nwhich is then regarded as the nominal distribution to construct the uncertainty\nset of DRO. Then, we introduce a penalty to items with high exposure\nprobability to avoid the overestimation of user preference for biased samples.\nFinally, we design a debiased self-normalized inverse propensity score (SNIPS)\nevaluator for evaluating the debiasing effect on the biased offline test set.\nWe conduct extensive experiments on two real-world datasets to verify the\neffectiveness of the proposed methods. Experimental results demonstrate the\nsuperior exposure debiasing performance of proposed methods. Codes and data are\navailable at \\url{https://github.com/nancheng58/DebiasedSR_DRO}.",
        "translated": ""
    },
    {
        "title": "memorAIs: an Optical Character Recognition and Rule-Based Medication\n  Intake Reminder-Generating Solution",
        "url": "http://arxiv.org/abs/2312.06841v1",
        "pub_date": "2023-12-11",
        "summary": "Memory-based medication non-adherence is an unsolved problem that is\nresponsible for considerable disease burden in the United States. Digital\nmedication intake reminder solutions with minimal onboarding requirements that\nare usable at the point of medication acquisition may help to alleviate this\nproblem by offering a low barrier way to help people remember to take their\nmedications. In this paper, we propose memorAIs, a digital medication intake\nreminder solution that mitigates onboarding friction by leveraging optical\ncharacter recognition strategies for text extraction from medication bottles\nand rule based expressions for text processing to create configured medication\nreminders as local device calendar invitations. We describe our ideation and\ndevelopment process, as well as limitations of the current implementation.\nmemorAIs was the winner of the Patient Safety award at the 2023 Columbia\nUniversity DivHacks Hackathon, presented by the Patient Safety Technology\nChallenge, sponsored by the Pittsburgh Regional Health Initiative.",
        "translated": ""
    },
    {
        "title": "Leveraging User Simulation to Develop and Evaluate Conversational\n  Information Access Agents",
        "url": "http://arxiv.org/abs/2312.08041v1",
        "pub_date": "2023-12-13",
        "summary": "We observe a change in the way users access information, that is, the rise of\nconversational information access (CIA) agents. However, the automatic\nevaluation of these agents remains an open challenge. Moreover, the training of\nCIA agents is cumbersome as it mostly relies on conversational corpora, expert\nknowledge, and reinforcement learning. User simulation has been identified as a\npromising solution to tackle automatic evaluation and has been previously used\nin reinforcement learning. In this research, we investigate how user simulation\ncan be leveraged in the context of CIA. We organize the work in three parts. We\nbegin with the identification of requirements for user simulators for training\nand evaluating CIA agents and compare existing types of simulator regarding\nthese. Then, we plan to combine these different types of simulators into a new\nhybrid simulator. Finally, we aim to extend simulators to handle more complex\ninformation seeking scenarios.",
        "translated": ""
    },
    {
        "title": "Improving search relevance of Azure Cognitive Search by Bayesian\n  optimization",
        "url": "http://arxiv.org/abs/2312.08021v1",
        "pub_date": "2023-12-13",
        "summary": "Azure Cognitive Search (ACS) has emerged as a major contender in \"Search as a\nService\" cloud products in recent years. However, one of the major challenges\nfor ACS users is to improve the relevance of the search results for their\nspecific usecases. In this paper, we propose a novel method to find the optimal\nACS configuration that maximizes search relevance for a specific usecase\n(product search, document search...) The proposed solution improves key online\nmarketplace metrics such as click through rates (CTR) by formulating the search\nrelevance problem as hyperparameter tuning. We have observed significant\nimprovements in real-world search call to action (CTA) rate in multiple\nmarketplaces by introducing optimized weights generated from the proposed\napproach.",
        "translated": ""
    },
    {
        "title": "RAT: Reinforcement-Learning-Driven and Adaptive Testing for\n  Vulnerability Discovery in Web Application Firewalls",
        "url": "http://arxiv.org/abs/2312.07885v1",
        "pub_date": "2023-12-13",
        "summary": "Due to the increasing sophistication of web attacks, Web Application\nFirewalls (WAFs) have to be tested and updated regularly to resist the\nrelentless flow of web attacks. In practice, using a brute-force attack to\ndiscover vulnerabilities is infeasible due to the wide variety of attack\npatterns. Thus, various black-box testing techniques have been proposed in the\nliterature. However, these techniques suffer from low efficiency. This paper\npresents Reinforcement-Learning-Driven and Adaptive Testing (RAT), an automated\nblack-box testing strategy to discover injection vulnerabilities in WAFs. In\nparticular, we focus on SQL injection and Cross-site Scripting, which have been\namong the top ten vulnerabilities over the past decade. More specifically, RAT\nclusters similar attack samples together. It then utilizes a reinforcement\nlearning technique combined with a novel adaptive search algorithm to discover\nalmost all bypassing attack patterns efficiently. We compare RAT with three\nstate-of-the-art methods considering their objectives. The experiments show\nthat RAT performs 33.53% and 63.16% on average better than its counterparts in\ndiscovering the most possible bypassing payloads and reducing the number of\nattempts before finding the first bypassing payload when testing\nwell-configured WAFs, respectively.",
        "translated": ""
    },
    {
        "title": "Exploring Popularity Bias in Session-based Recommendation",
        "url": "http://arxiv.org/abs/2312.07855v1",
        "pub_date": "2023-12-13",
        "summary": "Existing work has revealed that large-scale offline evaluation of recommender\nsystems for user-item interactions is prone to bias caused by the deployed\nsystem itself, as a form of closed loop feedback. Many adopt the\n\\textit{propensity} concept to analyze or mitigate this empirical issue. In\nthis work, we extend the analysis to session-based setup and adapted propensity\ncalculation to the unique characteristics of session-based recommendation\ntasks. Our experiments incorporate neural models and KNN-based models, and\ncover both the music and the e-commerce domain. We study the distributions of\npropensity and different stratification techniques on different datasets and\nfind that propensity-related traits are actually dataset-specific. We then\nleverage the effect of stratification and achieve promising results compared to\nthe original models.",
        "translated": ""
    },
    {
        "title": "Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge\n  Gaps",
        "url": "http://arxiv.org/abs/2312.07796v1",
        "pub_date": "2023-12-12",
        "summary": "The paper presents a methodology for uncovering knowledge gaps on the\ninternet using the Retrieval Augmented Generation (RAG) model. By simulating\nuser search behaviour, the RAG system identifies and addresses gaps in\ninformation retrieval systems. The study demonstrates the effectiveness of the\nRAG system in generating relevant suggestions with a consistent accuracy of\n93%. The methodology can be applied in various fields such as scientific\ndiscovery, educational enhancement, research development, market analysis,\nsearch engine optimisation, and content development. The results highlight the\nvalue of identifying and understanding knowledge gaps to guide future\nendeavours.",
        "translated": ""
    },
    {
        "title": "WikiMuTe: A web-sourced dataset of semantic descriptions for music audio",
        "url": "http://arxiv.org/abs/2312.09207v1",
        "pub_date": "2023-12-14",
        "summary": "Multi-modal deep learning techniques for matching free-form text with music\nhave shown promising results in the field of Music Information Retrieval (MIR).\nPrior work is often based on large proprietary data while publicly available\ndatasets are few and small in size. In this study, we present WikiMuTe, a new\nand open dataset containing rich semantic descriptions of music. The data is\nsourced from Wikipedia's rich catalogue of articles covering musical works.\nUsing a dedicated text-mining pipeline, we extract both long and short-form\ndescriptions covering a wide range of topics related to music content such as\ngenre, style, mood, instrumentation, and tempo. To show the use of this data,\nwe train a model that jointly learns text and audio representations and\nperforms cross-modal retrieval. The model is evaluated on two tasks: tag-based\nmusic retrieval and music auto-tagging. The results show that while our\napproach has state-of-the-art performance on multiple tasks, but still observe\na difference in performance depending on the data used for training.",
        "translated": ""
    },
    {
        "title": "FrameFinder: Explorative Multi-Perspective Framing Extraction from News\n  Headlines",
        "url": "http://arxiv.org/abs/2312.08995v1",
        "pub_date": "2023-12-14",
        "summary": "Revealing the framing of news articles is an important yet neglected task in\ninformation seeking and retrieval. In the present work, we present FrameFinder,\nan open tool for extracting and analyzing frames in textual data. FrameFinder\nvisually represents the frames of text from three perspectives, i.e., (i) frame\nlabels, (ii) frame dimensions, and (iii) frame structure. By analyzing the\nwell-established gun violence frame corpus, we demonstrate the merits of our\nproposed solution to support social science research and call for subsequent\nintegration into information interactions.",
        "translated": ""
    },
    {
        "title": "Calibration-compatible Listwise Distillation of Privileged Features for\n  CTR Prediction",
        "url": "http://arxiv.org/abs/2312.08727v1",
        "pub_date": "2023-12-14",
        "summary": "In machine learning systems, privileged features refer to the features that\nare available during offline training but inaccessible for online serving.\nPrevious studies have recognized the importance of privileged features and\nexplored ways to tackle online-offline discrepancies. A typical practice is\nprivileged features distillation (PFD): train a teacher model using all\nfeatures (including privileged ones) and then distill the knowledge from the\nteacher model using a student model (excluding the privileged features), which\nis then employed for online serving. In practice, the pointwise cross-entropy\nloss is often adopted for PFD. However, this loss is insufficient to distill\nthe ranking ability for CTR prediction. First, it does not consider the\nnon-i.i.d. characteristic of the data distribution, i.e., other items on the\nsame page significantly impact the click probability of the candidate item.\nSecond, it fails to consider the relative item order ranked by the teacher\nmodel's predictions, which is essential to distill the ranking ability. To\naddress these issues, we first extend the pointwise-based PFD to the\nlistwise-based PFD. We then define the calibration-compatible property of\ndistillation loss and show that commonly used listwise losses do not satisfy\nthis property when employed as distillation loss, thus compromising the model's\ncalibration ability, which is another important measure for CTR prediction. To\ntackle this dilemma, we propose Calibration-compatible LIstwise Distillation\n(CLID), which employs carefully-designed listwise distillation loss to achieve\nbetter ranking ability than the pointwise-based PFD while preserving the\nmodel's calibration ability. We theoretically prove it is\ncalibration-compatible. Extensive experiments on public datasets and a\nproduction dataset collected from the display advertising system of Alibaba\nfurther demonstrate the effectiveness of CLID.",
        "translated": ""
    },
    {
        "title": "Hybrid Content Dynamic Recommendation System Based in Adapted Tags and\n  Applied to Digital Library",
        "url": "http://arxiv.org/abs/2312.08584v1",
        "pub_date": "2023-12-14",
        "summary": "The technological evolution of the library in the academic environment\nbrought a lot of information and documents that are available to access, but\nthese systems do not always have mechanisms to search in an integrated way the\nrelevant information for the user. To alleviate this problem, we propose a\nrecommendation system that generates the user profile through tags that are\nreshaped over time. To trace the user profile the system uses information from\nyour lending history stored in the library database and it collects their\nopinions (feedback) through a list of recommendations. These data are\nintegrated with the document base of institutional repository.Thus, the\nrecommendation system assists users in identifying relevant items and makes\nsuggestions for content in an integrated environment that contains\ninstitutional repository documents and the university library database. The\nproposed recommendation system uses a hybrid approach being applied in an\nacademic environment with the participation of the users.",
        "translated": ""
    },
    {
        "title": "A novel diffusion recommendation algorithm based on multi-scale cnn and\n  residual lstm",
        "url": "http://arxiv.org/abs/2312.10885v1",
        "pub_date": "2023-12-18",
        "summary": "Sequential recommendation aims to infer user preferences from historical\ninteraction sequences and predict the next item that users may be interested in\nthe future. The current mainstream design approach is to represent items as\nfixed vectors, capturing the underlying relationships between items and user\npreferences based on the order of interactions. However, relying on a single\nfixed-item embedding may weaken the modeling capability of the system, and the\nglobal dynamics and local saliency exhibited by user preferences need to be\ndistinguished. To address these issues, this paper proposes a novel diffusion\nrecommendation algorithm based on multi-scale cnn and residual lstm (AREAL). We\nintroduce diffusion models into the recommend system, representing items as\nprobability distributions instead of fixed vectors. This approach enables\nadaptive reflection of multiple aspects of the items and generates item\ndistributions in a denoising manner. We use multi-scale cnn and residual lstm\nmethods to extract the local and global dependency features of user history\ninteractions, and use attention mechanism to distinguish weights as the guide\nfeatures of reverse diffusion recovery. The effectiveness of the proposed\nmethod is validated through experiments conducted on two real-world datasets.\nSpecifically, AREAL obtains improvements over the best baselines by 2.63% and\n4.25% in terms of HR@20 and 5.05% and 3.94% in terms of NDCG@20 on all\ndatasets.",
        "translated": ""
    },
    {
        "title": "On-Device Recommender Systems: A Tutorial on The New-Generation\n  Recommendation Paradigm",
        "url": "http://arxiv.org/abs/2312.10864v1",
        "pub_date": "2023-12-18",
        "summary": "Given the sheer volume of contemporary e-commerce applications, recommender\nsystems (RSs) have gained significant attention in both academia and industry.\nHowever, traditional cloud-based RSs face inevitable challenges, such as\nresource-intensive computation, reliance on network access, and privacy\nbreaches. In response, a new paradigm called on-device recommender systems\n(ODRSs) has emerged recently in various industries like Taobao, Google, and\nKuaishou. ODRSs unleash the computational capacity of user devices with\nlightweight recommendation models tailored for resource-constrained\nenvironments, enabling real-time inference with users' local data. This\ntutorial aims to systematically introduce methodologies of ODRSs, including (1)\nan overview of existing research on ODRSs; (2) a comprehensive taxonomy of\nODRSs, where the core technical content to be covered span across three major\nODRS research directions, including on-device deployment and inference,\non-device training, and privacy/security of ODRSs; (3) limitations and future\ndirections of ODRSs. This tutorial expects to lay the foundation and spark new\ninsights for follow-up research and applications concerning this new\nrecommendation paradigm.",
        "translated": ""
    },
    {
        "title": "A Unified Framework for Multi-Domain CTR Prediction via Large Language\n  Models",
        "url": "http://arxiv.org/abs/2312.10743v1",
        "pub_date": "2023-12-17",
        "summary": "Click-Through Rate (CTR) prediction is a crucial task in online\nrecommendation platforms as it involves estimating the probability of user\nengagement with advertisements or items by clicking on them. Given the\navailability of various services like online shopping, ride-sharing, food\ndelivery, and professional services on commercial platforms, recommendation\nsystems in these platforms are required to make CTR predictions across multiple\ndomains rather than just a single domain. However, multi-domain click-through\nrate (MDCTR) prediction remains a challenging task in online recommendation due\nto the complex mutual influence between domains. Traditional MDCTR models\ntypically encode domains as discrete identifiers, ignoring rich semantic\ninformation underlying. Consequently, they can hardly generalize to new\ndomains. Besides, existing models can be easily dominated by some specific\ndomains, which results in significant performance drops in the other domains\n(\\ie the ``seesaw phenomenon``). In this paper, we propose a novel solution\nUni-CTR to address the above challenges. Uni-CTR leverages a backbone Large\nLanguage Model (LLM) to learn layer-wise semantic representations that capture\ncommonalities between domains. Uni-CTR also uses several domain-specific\nnetworks to capture the characteristics of each domain. Note that we design a\nmasked loss strategy so that these domain-specific networks are decoupled from\nbackbone LLM. This allows domain-specific networks to remain unchanged when\nincorporating new or removing domains, thereby enhancing the flexibility and\nscalability of the system significantly. Experimental results on three public\ndatasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models\nsignificantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in\nzero-shot prediction. We have applied Uni-CTR in industrial scenarios,\nconfirming its efficiency.",
        "translated": ""
    },
    {
        "title": "Wikiformer: Pre-training with Structured Information of Wikipedia for\n  Ad-hoc Retrieval",
        "url": "http://arxiv.org/abs/2312.10661v1",
        "pub_date": "2023-12-17",
        "summary": "With the development of deep learning and natural language processing\ntechniques, pre-trained language models have been widely used to solve\ninformation retrieval (IR) problems. Benefiting from the pre-training and\nfine-tuning paradigm, these models achieve state-of-the-art performance. In\nprevious works, plain texts in Wikipedia have been widely used in the\npre-training stage. However, the rich structured information in Wikipedia, such\nas the titles, abstracts, hierarchical heading (multi-level title) structure,\nrelationship between articles, references, hyperlink structures, and the\nwriting organizations, has not been fully explored. In this paper, we devise\nfour pre-training objectives tailored for IR tasks based on the structured\nknowledge of Wikipedia. Compared to existing pre-training methods, our approach\ncan better capture the semantic knowledge in the training corpus by leveraging\nthe human-edited structured data from Wikipedia. Experimental results on\nmultiple IR benchmark datasets show the superior performance of our model in\nboth zero-shot and fine-tuning settings compared to existing strong retrieval\nbaselines. Besides, experimental results in biomedical and legal domains\ndemonstrate that our approach achieves better performance in vertical domains\ncompared to previous models, especially in scenarios where long text similarity\nmatching is needed.",
        "translated": ""
    },
    {
        "title": "HyperPIE: Hyperparameter Information Extraction from Scientific\n  Publications",
        "url": "http://arxiv.org/abs/2312.10638v1",
        "pub_date": "2023-12-17",
        "summary": "Automatic extraction of information from publications is key to making\nscientific knowledge machine readable at a large scale. The extracted\ninformation can, for example, facilitate academic search, decision making, and\nknowledge graph construction. An important type of information not covered by\nexisting approaches is hyperparameters. In this paper, we formalize and tackle\nhyperparameter information extraction (HyperPIE) as an entity recognition and\nrelation extraction task. We create a labeled data set covering publications\nfrom a variety of computer science disciplines. Using this data set, we train\nand evaluate BERT-based fine-tuned models as well as five large language\nmodels: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned\nmodels, we develop a relation extraction approach that achieves an improvement\nof 29% F1 over a state-of-the-art baseline. For large language models, we\ndevelop an approach leveraging YAML output for structured data extraction,\nwhich achieves an average improvement of 5.5% F1 in entity recognition over\nusing JSON. With our best performing model we extract hyperparameter\ninformation from a large number of unannotated papers, and analyze patterns\nacross disciplines. All our data and source code is publicly available at\nhttps://github.com/IllDepence/hyperpie",
        "translated": ""
    },
    {
        "title": "A Survey on Query-based API Recommendation",
        "url": "http://arxiv.org/abs/2312.10623v1",
        "pub_date": "2023-12-17",
        "summary": "Application Programming Interfaces (APIs) are designed to help developers\nbuild software more effectively. Recommending the right APIs for specific tasks\nhas gained increasing attention among researchers and developers in recent\nyears. To comprehensively understand this research domain, we have surveyed to\nanalyze API recommendation studies published in the last 10 years. Our study\nbegins with an overview of the structure of API recommendation tools.\nSubsequently, we systematically analyze prior research and pose four key\nresearch questions. For RQ1, we examine the volume of published papers and the\nvenues in which these papers appear within the API recommendation field. In\nRQ2, we categorize and summarize the prevalent data sources and collection\nmethods employed in API recommendation research. In RQ3, we explore the types\nof data and common data representations utilized by API recommendation\napproaches. We also investigate the typical data extraction procedures and\ncollection approaches employed by the existing approaches. RQ4 delves into the\nmodeling techniques employed by API recommendation approaches, encompassing\nboth statistical and deep learning models. Additionally, we compile an overview\nof the prevalent ranking strategies and evaluation metrics used for assessing\nAPI recommendation tools. Drawing from our survey findings, we identify current\nchallenges in API recommendation research that warrant further exploration,\nalong with potential avenues for future research.",
        "translated": ""
    },
    {
        "title": "RIGHT: Retrieval-augmented Generation for Mainstream Hashtag\n  Recommendation",
        "url": "http://arxiv.org/abs/2312.10466v1",
        "pub_date": "2023-12-16",
        "summary": "Automatic mainstream hashtag recommendation aims to accurately provide users\nwith concise and popular topical hashtags before publication. Generally,\nmainstream hashtag recommendation faces challenges in the comprehensive\ndifficulty of newly posted tweets in response to new topics, and the accurate\nidentification of mainstream hashtags beyond semantic correctness. However,\nprevious retrieval-based methods based on a fixed predefined mainstream hashtag\nlist excel in producing mainstream hashtags, but fail to understand the\nconstant flow of up-to-date information. Conversely, generation-based methods\ndemonstrate a superior ability to comprehend newly posted tweets, but their\ncapacity is constrained to identifying mainstream hashtags without additional\nfeatures. Inspired by the recent success of the retrieval-augmented technique,\nin this work, we attempt to adopt this framework to combine the advantages of\nboth approaches. Meantime, with the help of the generator component, we could\nrethink how to further improve the quality of the retriever component at a low\ncost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag\nRecommender (RIGHT), which consists of three components: 1) a retriever seeks\nrelevant hashtags from the entire tweet-hashtags set; 2) a selector enhances\nmainstream identification by introducing global signals; and 3) a generator\nincorporates input tweets and selected hashtags to directly generate the\ndesired hashtags. The experimental results show that our method achieves\nsignificant improvements over state-of-the-art baselines. Moreover, RIGHT can\nbe easily integrated into large language models, improving the performance of\nChatGPT by more than 10%.",
        "translated": ""
    },
    {
        "title": "RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large\n  Language Models",
        "url": "http://arxiv.org/abs/2312.10463v1",
        "pub_date": "2023-12-16",
        "summary": "In the evolving field of personalized news recommendation, understanding the\nsemantics of the underlying data is crucial. Large Language Models (LLMs) like\nGPT-4 have shown promising performance in understanding natural language.\nHowever, the extent of their applicability in news recommendation systems\nremains to be validated. This paper introduces RecPrompt, the first framework\nfor news recommendation that leverages the capabilities of LLMs through prompt\nengineering. This system incorporates a prompt optimizer that applies an\niterative bootstrapping process, enhancing the LLM-based recommender's ability\nto align news content with user preferences and interests more effectively.\nMoreover, this study offers insights into the effective use of LLMs in news\nrecommendation, emphasizing both the advantages and the challenges of\nincorporating LLMs into recommendation systems.",
        "translated": ""
    },
    {
        "title": "Do Similar Entities have Similar Embeddings?",
        "url": "http://arxiv.org/abs/2312.10370v1",
        "pub_date": "2023-12-16",
        "summary": "Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for graph entities, known as embeddings. A common tacit\nassumption is the KGE entity similarity assumption, which states that these\nKGEMs retain the graph's structure within their embedding space, i.e., position\nsimilar entities close to one another. This desirable property make KGEMs\nwidely used in downstream tasks such as recommender systems or drug\nrepurposing. Yet, the alignment of graph similarity with embedding space\nsimilarity has rarely been formally evaluated. Typically, KGEMs are assessed\nbased on their sole link prediction capabilities, using ranked-based metrics\nsuch as Hits@K or Mean Rank. This paper challenges the prevailing assumption\nthat entity similarity in the graph is inherently mirrored in the embedding\nspace. Therefore, we conduct extensive experiments to measure the capability of\nKGEMs to cluster similar entities together, and investigate the nature of the\nunderlying factors. Moreover, we study if different KGEMs expose a different\nnotion of similarity. Datasets, pre-trained embeddings and code are available\nat: https://github.com/nicolas-hbt/similar-embeddings.",
        "translated": ""
    },
    {
        "title": "ProTIP: Progressive Tool Retrieval Improves Planning",
        "url": "http://arxiv.org/abs/2312.10332v1",
        "pub_date": "2023-12-16",
        "summary": "Large language models (LLMs) are increasingly employed for complex multi-step\nplanning tasks, where the tool retrieval (TR) step is crucial for achieving\nsuccessful outcomes. Two prevalent approaches for TR are single-step retrieval,\nwhich utilizes the complete query, and sequential retrieval using task\ndecomposition (TD), where a full query is segmented into discrete atomic\nsubtasks. While single-step retrieval lacks the flexibility to handle\n\"inter-tool dependency,\" the TD approach necessitates maintaining \"subtask-tool\natomicity alignment,\" as the toolbox can evolve dynamically. To address these\nlimitations, we introduce the Progressive Tool retrieval to Improve Planning\n(ProTIP) framework. ProTIP is a lightweight, contrastive learning-based\nframework that implicitly performs TD without the explicit requirement of\nsubtask labels, while simultaneously maintaining subtask-tool atomicity. On the\nToolBench dataset, ProTIP outperforms the ChatGPT task decomposition-based\napproach by a remarkable margin, achieving a 24% improvement in Recall@K=10 for\nTR and a 41% enhancement in tool accuracy for plan generation.",
        "translated": ""
    },
    {
        "title": "dIR -- Discrete Information Retrieval: Conversational Search over\n  Unstructured (and Structured) Data with Large Language Models",
        "url": "http://arxiv.org/abs/2312.13264v1",
        "pub_date": "2023-12-20",
        "summary": "Data is stored in both structured and unstructured form. Querying both, to\npower natural language conversations, is a challenge. This paper introduces\ndIR, Discrete Information Retrieval, providing a unified interface to query\nboth free text and structured knowledge. Specifically, a Large Language Model\n(LLM) transforms text into expressive representation. After the text is\nextracted into columnar form, it can then be queried via a text-to-SQL Semantic\nParser, with an LLM converting natural language into SQL. Where desired, such\nconversation may be effected by a multi-step reasoning conversational agent. We\nvalidate our approach via a proprietary question/answer data set, concluding\nthat dIR makes a whole new class of queries on free text possible when compared\nto traditionally fine-tuned dense-embedding-model-based Information Retrieval\n(IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIR\ncan succeed where no other method stands a chance.",
        "translated": ""
    },
    {
        "title": "BSL: Understanding and Improving Softmax Loss for Recommendation",
        "url": "http://arxiv.org/abs/2312.12882v1",
        "pub_date": "2023-12-20",
        "summary": "Loss functions steer the optimization direction of recommendation models and\nare critical to model performance, but have received relatively little\nattention in recent recommendation research. Among various losses, we find\nSoftmax loss (SL) stands out for not only achieving remarkable accuracy but\nalso better robustness and fairness. Nevertheless, the current literature lacks\na comprehensive explanation for the efficacy of SL. Toward addressing this\nresearch gap, we conduct theoretical analyses on SL and uncover three insights:\n1) Optimizing SL is equivalent to performing Distributionally Robust\nOptimization (DRO) on the negative data, thereby learning against perturbations\non the negative distribution and yielding robustness to noisy negatives. 2)\nComparing with other loss functions, SL implicitly penalizes the prediction\nvariance, resulting in a smaller gap between predicted values and and thus\nproducing fairer results. Building on these insights, we further propose a\nnovel loss function Bilateral SoftMax Loss (BSL) that extends the advantage of\nSL to both positive and negative sides. BSL augments SL by applying the same\nLog-Expectation-Exp structure to positive examples as is used for negatives,\nmaking the model robust to the noisy positives as well. Remarkably, BSL is\nsimple and easy-to-implement -- requiring just one additional line of code\ncompared to SL. Experiments on four real-world datasets and three\nrepresentative backbones demonstrate the effectiveness of our proposal. The\ncode is available at https://github.com/junkangwu/BSL",
        "translated": ""
    },
    {
        "title": "Parallel Ranking of Ads and Creatives in Real-Time Advertising Systems",
        "url": "http://arxiv.org/abs/2312.12750v1",
        "pub_date": "2023-12-20",
        "summary": "\"Creativity is the heart and soul of advertising services\". Effective\ncreatives can create a win-win scenario: advertisers can reach target users and\nachieve marketing objectives more effectively, users can more quickly find\nproducts of interest, and platforms can generate more advertising revenue. With\nthe advent of AI-Generated Content, advertisers now can produce vast amounts of\ncreative content at a minimal cost. The current challenge lies in how\nadvertising systems can select the most pertinent creative in real-time for\neach user personally. Existing methods typically perform serial ranking of ads\nor creatives, limiting the creative module in terms of both effectiveness and\nefficiency. In this paper, we propose for the first time a novel architecture\nfor online parallel estimation of ads and creatives ranking, as well as the\ncorresponding offline joint optimization model. The online architecture enables\nsophisticated personalized creative modeling while reducing overall latency.\nThe offline joint model for CTR estimation allows mutual awareness and\ncollaborative optimization between ads and creatives. Additionally, we optimize\nthe offline evaluation metrics for the implicit feedback sorting task involved\nin ad creative ranking. We conduct extensive experiments to compare ours with\ntwo state-of-the-art approaches. The results demonstrate the effectiveness of\nour approach in both offline evaluations and real-world advertising platforms\nonline in terms of response time, CTR, and CPM.",
        "translated": ""
    },
    {
        "title": "Fine-tuning Large Language Models for Adaptive Machine Translation",
        "url": "http://arxiv.org/abs/2312.12740v1",
        "pub_date": "2023-12-20",
        "summary": "This paper presents the outcomes of fine-tuning Mistral 7B, a general-purpose\nlarge language model (LLM), for adaptive machine translation (MT). The\nfine-tuning process involves utilising a combination of zero-shot and one-shot\ntranslation prompts within the medical domain. The primary objective is to\nenhance real-time adaptive MT capabilities of Mistral 7B, enabling it to adapt\ntranslations to the required domain at inference time. The results,\nparticularly for Spanish-to-English MT, showcase the efficacy of the fine-tuned\nmodel, demonstrating quality improvements in both zero-shot and one-shot\ntranslation scenarios, surpassing Mistral 7B's baseline performance. Notably,\nthe fine-tuned Mistral outperforms ChatGPT \"gpt-3.5-turbo\" in zero-shot\ntranslation while achieving comparable one-shot translation quality. Moreover,\nthe zero-shot translation of the fine-tuned Mistral matches NLLB 3.3B's\nperformance, and its one-shot translation quality surpasses that of NLLB 3.3B.\nThese findings emphasise the significance of fine-tuning efficient LLMs like\nMistral 7B to yield high-quality zero-shot translations comparable to\ntask-oriented models like NLLB 3.3B. Additionally, the adaptive gains achieved\nin one-shot translation are comparable to those of commercial LLMs such as\nChatGPT. Our experiments demonstrate that, with a relatively small dataset of\n20,000 segments that incorporate a mix of zero-shot and one-shot prompts,\nfine-tuning significantly enhances Mistral's in-context learning ability,\nespecially for real-time adaptive MT.",
        "translated": ""
    },
    {
        "title": "Lookahead: An Inference Acceleration Framework for Large Language Model\n  with Lossless Generation Accuracy",
        "url": "http://arxiv.org/abs/2312.12728v1",
        "pub_date": "2023-12-20",
        "summary": "As Large Language Models (LLMs) have made significant advancements across\nvarious tasks, such as question answering, translation, text summarization, and\ndialogue systems, the need for accuracy in information becomes crucial,\nespecially for serious financial products serving billions of users like\nAlipay. To address this, Alipay has developed a Retrieval-Augmented Generation\n(RAG) system that grounds LLMs on the most accurate and up-to-date information.\nHowever, for a real-world product serving millions of users, the inference\nspeed of LLMs becomes a critical factor compared to a mere experimental model.\n  Hence, this paper presents a generic framework for accelerating the inference\nprocess, resulting in a substantial increase in speed and cost reduction for\nour RAG system, with lossless generation accuracy. In the traditional inference\nprocess, each token is generated sequentially by the LLM, leading to a time\nconsumption proportional to the number of generated tokens. To enhance this\nprocess, our framework, named \\textit{lookahead}, introduces a\n\\textit{multi-branch} strategy. Instead of generating a single token at a time,\nwe propose a \\textit{Trie-based Retrieval} (TR) process that enables the\ngeneration of multiple branches simultaneously, each of which is a sequence of\ntokens. Subsequently, for each branch, a \\textit{Verification and Accept} (VA)\nprocess is performed to identify the longest correct sub-sequence as the final\noutput. Our strategy offers two distinct advantages: (1) it guarantees absolute\ncorrectness of the output, avoiding any approximation algorithms, and (2) the\nworst-case performance of our approach is equivalent to the conventional\nprocess. We conduct extensive experiments to demonstrate the significant\nimprovements achieved by applying our inference acceleration framework.",
        "translated": ""
    },
    {
        "title": "Categorical, Ratio, and Professorial Data: The Case for Reciprocal Rank",
        "url": "http://arxiv.org/abs/2312.12672v1",
        "pub_date": "2023-12-20",
        "summary": "Search engine results pages are usually abstracted as binary relevance\nvectors and hence are categorical data, meaning that only a limited set of\noperations is permitted, most notably tabulation of occurrence frequencies,\nwith determination of medians and averages not possible. To compare retrieval\nsystems it is thus usual to make use of a categorical-to-numeric effectiveness\nmapping. A previous paper has argued that any desired categorical-to-numeric\nmapping may be used, provided only that there is an argued connection between\neach category of SERP and the score that is assigned to that category by the\nmapping. Further, once that plausible connection has been established, then the\nmapped values can be treated as real-valued observations on a ratio scale,\nallowing the computation of averages. This article is written in support of\nthat point of view, and to respond to ongoing claims that SERP scores may only\nbe averaged if very restrictive conditions are imposed on the effectiveness\nmapping.",
        "translated": ""
    },
    {
        "title": "Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP",
        "url": "http://arxiv.org/abs/2312.12430v2",
        "pub_date": "2023-12-19",
        "summary": "We introduce Efficient Title Reranker via Broadcasting Query Encoder, a novel\ntitle reranking technique to achieve efficient title reranking 20x-40x faster\nthan vanilla passage reranker. However, one of the challenges with the training\nof Efficient Title Reranker is the instability. Analyzing the issue, we found\nsome very difficult ground truths might act as noisy labels causing accuracy to\ndrop as well as some extreme values in model probability output causing nan. To\naddress these issues, we introduce the Sigmoid Trick, a novel technique that\nreduces the gradient update of both cases resulting in better retrieval\nefficacy. Experiments showed the effectiveness of ETR and sigmoid trick as we\nachieved four state-of-the-art positions on the kilt knowledge benchmark.",
        "translated": ""
    },
    {
        "title": "PEPT: Expert Finding Meets Personalized Pre-training",
        "url": "http://arxiv.org/abs/2312.12162v1",
        "pub_date": "2023-12-19",
        "summary": "Finding appropriate experts is essential in Community Question Answering\n(CQA) platforms as it enables the effective routing of questions to potential\nusers who can provide relevant answers. The key is to personalized learning\nexpert representations based on their historical answered questions, and\naccurately matching them with target questions. There have been some\npreliminary works exploring the usability of PLMs in expert finding, such as\npre-training expert or question representations. However, these models usually\nlearn pure text representations of experts from histories, disregarding\npersonalized and fine-grained expert modeling. For alleviating this, we present\na personalized pre-training and fine-tuning paradigm, which could effectively\nlearn expert interest and expertise simultaneously. Specifically, in our\npre-training framework, we integrate historical answered questions of one\nexpert with one target question, and regard it as a candidate aware\nexpert-level input unit. Then, we fuse expert IDs into the pre-training for\nguiding the model to model personalized expert representations, which can help\ncapture the unique characteristics and expertise of each individual expert.\nAdditionally, in our pre-training task, we design: 1) a question-level masked\nlanguage model task to learn the relatedness between histories, enabling the\nmodeling of question-level expert interest; 2) a vote-oriented task to capture\nquestion-level expert expertise by predicting the vote score the expert would\nreceive. Through our pre-training framework and tasks, our approach could\nholistically learn expert representations including interests and expertise.\nOur method has been extensively evaluated on six real-world CQA datasets, and\nthe experimental results consistently demonstrate the superiority of our\napproach over competitive baseline methods.",
        "translated": ""
    },
    {
        "title": "Designing and Evaluating General-Purpose User Representations Based on\n  Behavioral Logs from a Measurement Process Perspective: A Case Study with\n  Snapchat",
        "url": "http://arxiv.org/abs/2312.12111v1",
        "pub_date": "2023-12-19",
        "summary": "In human-computer interaction, understanding user behaviors and tailoring\nsystems accordingly is pivotal. To this end, general-purpose user\nrepresentation learning based on behavior logs is emerging as a powerful tool\nin user modeling, offering adaptability to various downstream tasks such as\nitem recommendations and ad conversion prediction, without the need to\nfine-tune the upstream user model. While this methodology has shown promise in\ncontexts like search engines and e-commerce platforms, its fit for instant\nmessaging apps, a cornerstone of modern digital communication, remains largely\nuncharted. These apps, with their distinct interaction patterns, data\nstructures, and user expectations, necessitate specialized attention. We\nexplore this user modeling approach with Snapchat data as a case study.\nFurthermore, we introduce a novel design and evaluation framework rooted in the\nprinciples of the Measurement Process Framework from social science research\nmethodology. Using this new framework, we design a Transformer-based user model\nthat can produce high-quality general-purpose user representations for instant\nmessaging platforms like Snapchat.",
        "translated": ""
    },
    {
        "title": "VITA: 'Carefully Chosen and Weighted Less' Is Better in Medication\n  Recommendation",
        "url": "http://arxiv.org/abs/2312.12100v1",
        "pub_date": "2023-12-19",
        "summary": "We address the medication recommendation problem, which aims to recommend\neffective medications for a patient's current visit by utilizing information\n(e.g., diagnoses and procedures) given at the patient's current and past\nvisits. While there exist a number of recommender systems designed for this\nproblem, we point out that they are challenged in accurately capturing the\nrelation (spec., the degree of relevance) between the current and each of the\npast visits for the patient when obtaining her current health status, which is\nthe basis for recommending medications. To address this limitation, we propose\na novel medication recommendation framework, named VITA, based on the following\ntwo novel ideas: (1) relevant-Visit selectIon; (2) Target-aware Attention.\nThrough extensive experiments using real-world datasets, we demonstrate the\nsuperiority of VITA (spec., up to 5.56% higher accuracy, in terms of Jaccard,\nthan the best competitor) and the effectiveness of its two core ideas. The code\nis available at https://github.com/jhheo0123/VITA.",
        "translated": ""
    },
    {
        "title": "Neural Contextual Bandits for Personalized Recommendation",
        "url": "http://arxiv.org/abs/2312.14037v1",
        "pub_date": "2023-12-21",
        "summary": "In the dynamic landscape of online businesses, recommender systems are\npivotal in enhancing user experiences. While traditional approaches have relied\non static supervised learning, the quest for adaptive, user-centric\nrecommendations has led to the emergence of the formulation of contextual\nbandits. This tutorial investigates the contextual bandits as a powerful\nframework for personalized recommendations. We delve into the challenges,\nadvanced algorithms and theories, collaborative strategies, and open challenges\nand future prospects within this field. Different from existing related\ntutorials, (1) we focus on the exploration perspective of contextual bandits to\nalleviate the ``Matthew Effect'' in the recommender systems, i.e., the rich get\nricher and the poor get poorer, concerning the popularity of items; (2) in\naddition to the conventional linear contextual bandits, we will also dedicated\nto neural contextual bandits which have emerged as an important branch in\nrecent years, to investigate how neural networks benefit contextual bandits for\npersonalized recommendation both empirically and theoretically; (3) we will\ncover the latest topic, collaborative neural contextual bandits, to incorporate\nboth user heterogeneity and user correlations customized for recommender\nsystem; (4) we will provide and discuss the new emerging challenges and open\nquestions for neural contextual bandits with applications in the personalized\nrecommendation, especially for large neural models.",
        "translated": ""
    },
    {
        "title": "A Learning oriented DLP System based on Classification Model",
        "url": "http://arxiv.org/abs/2312.13711v1",
        "pub_date": "2023-12-21",
        "summary": "Data is the key asset for organizations and data sharing is lifeline for\norganization growth; which may lead to data loss. Data leakage is the most\ncritical issue being faced by organizations. In order to mitigate the data\nleakage issues data leakage prevention systems (DLPSs) are deployed at various\nlevels by the organizations. DLPSs are capable to protect all kind of data i.e.\nDAR, DIM/DIT, DIU. Statistical analysis, regular expression, data\nfingerprinting are common approaches exercised in DLP system. Out of these\ntechniques; statistical analysis approach is most appropriate for proposed DLP\nmodel of data security. This paper defines a statistical DLP model for document\nclassification. Model uses various statistical approaches like TF-IDF (Term\nFrequency- Inverse Document Frequency) a renowned term count/weighing function,\nVectorization, Gradient boosting document classification etc. to classify the\ndocuments before allowing any access to it. Machine learning is used to test\nand train the model. Proposed model also introduces an extremely efficient and\nmore accurate approach; IGBCA (Improvised Gradient Boosting Classification\nAlgorithm); for document classification, to prevent them from possible data\nleakage. Results depicts that proposed model can classify documents with high\naccuracy and on basis of which data can be prevented from being loss.",
        "translated": ""
    },
    {
        "title": "Unexplored Frontiers: A Review of Empirical Studies of Exploratory\n  Search",
        "url": "http://arxiv.org/abs/2312.13695v1",
        "pub_date": "2023-12-21",
        "summary": "This article reviews how empirical research of exploratory search is\nconducted. We investigated aspects of interdisciplinarity, study settings and\nevaluation methodologies from a systematically selected sample of 231\npublications from 2010-2021, including a total of 172 articles with empirical\nstudies. Our results show that exploratory search is highly interdisciplinary,\nwith the most frequently occurring publication venues including high impact\nvenues in information science, information systems and human-computer\ninteraction. However, taken in aggregate, the breadth of study settings\ninvestigated was limited. We found that a majority of studies (77%) focused on\nevaluating novel retrieval systems as opposed to investigating users' search\nprocesses. Furthermore, a disproportionate number of studies were based on\nscientific literature search (20.7%), a majority of which only considered\nsearching for Computer Science articles. Study participants were generally from\nconvenience samples, with 75% of studies composed exclusively of students and\nother academics. The methodologies used for evaluation were mostly\nquantitative, but lacked consistency between studies and validated\nquestionnaires were rarely used. In discussion, we offer a critical analysis of\nour findings and suggest potential improvements for future exploratory search\nstudies.",
        "translated": ""
    },
    {
        "title": "Empowering Few-Shot Recommender Systems with Large Language Models --\n  Enhanced Representations",
        "url": "http://arxiv.org/abs/2312.13557v1",
        "pub_date": "2023-12-21",
        "summary": "Recommender systems utilizing explicit feedback have witnessed significant\nadvancements and widespread applications over the past years. However,\ngenerating recommendations in few-shot scenarios remains a persistent\nchallenge. Recently, large language models (LLMs) have emerged as a promising\nsolution for addressing natural language processing (NLP) tasks, thereby\noffering novel insights into tackling the few-shot scenarios encountered by\nexplicit feedback-based recommender systems. To bridge recommender systems and\nLLMs, we devise a prompting template that generates user and item\nrepresentations based on explicit feedback. Subsequently, we integrate these\nLLM-processed representations into various recommendation models to evaluate\ntheir significance across diverse recommendation tasks. Our ablation\nexperiments and case study analysis collectively demonstrate the effectiveness\nof LLMs in processing explicit feedback, highlighting that LLMs equipped with\ngenerative and logical reasoning capabilities can effectively serve as a\ncomponent of recommender systems to enhance their performance in few-shot\nscenarios. Furthermore, the broad adaptability of LLMs augments the\ngeneralization potential of recommender models, despite certain inherent\nconstraints. We anticipate that our study can inspire researchers to delve\ndeeper into the multifaceted dimensions of LLMs's involvement in recommender\nsystems and contribute to the advancement of the explicit feedback-based\nrecommender systems field.",
        "translated": ""
    },
    {
        "title": "Accuracy vs Memory Advantage in the Quantum Simulation of Stochastic\n  Processes",
        "url": "http://arxiv.org/abs/2312.13473v1",
        "pub_date": "2023-12-20",
        "summary": "Many inference scenarios rely on extracting relevant information from known\ndata in order to make future predictions. When the underlying stochastic\nprocess satisfies certain assumptions, there is a direct mapping between its\nexact classical and quantum simulators, with the latter asymptotically using\nless memory. Here we focus on studying whether such quantum advantage persists\nwhen those assumptions are not satisfied, and the model is doomed to have\nimperfect accuracy. By studying the trade-off between accuracy and memory\nrequirements, we show that quantum models can reach the same accuracy with less\nmemory, or alternatively, better accuracy with the same memory. Finally, we\ndiscuss the implications of this result for learning tasks.",
        "translated": ""
    },
    {
        "title": "Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of\n  Early-bird Students towards Three Diagnostic Objectives",
        "url": "http://arxiv.org/abs/2312.13434v1",
        "pub_date": "2023-12-20",
        "summary": "Cognitive diagnosis seeks to estimate the cognitive states of students by\nexploring their logged practice quiz data. It plays a pivotal role in\npersonalized learning guidance within intelligent education systems. In this\npaper, we focus on an important, practical, yet often underexplored task:\ndomain-level zero-shot cognitive diagnosis (DZCD), which arises due to the\nabsence of student practice logs in newly launched domains. Recent cross-domain\ndiagnostic models have been demonstrated to be a promising strategy for DZCD.\nThese methods primarily focus on how to transfer student states across domains.\nHowever, they might inadvertently incorporate non-transferable information into\nstudent representations, thereby limiting the efficacy of knowledge transfer.\nTo tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive\ndiagnosis framework via one batch of early-bird students towards three\ndiagnostic objectives. Our approach initiates with pre-training a diagnosis\nmodel with dual regularizers, which decouples student states into domain-shared\nand domain-specific parts. The shared cognitive signals can be transferred to\nthe target domain, enriching the cognitive priors for the new domain, which\nensures the cognitive state propagation objective. Subsequently, we devise a\nstrategy to generate simulated practice logs for cold-start students through\nanalyzing the behavioral patterns from early-bird students, fulfilling the\ndomain-adaption goal. Consequently, we refine the cognitive states of\ncold-start students as diagnostic outcomes via virtual data, aligning with the\ndiagnosis-oriented goal. Finally, extensive experiments on six real-world\ndatasets highlight the efficacy of our model for DZCD and its practical\napplication in question recommendation.",
        "translated": ""
    },
    {
        "title": "VADIS -- a VAriable Detection, Interlinking and Summarization system",
        "url": "http://arxiv.org/abs/2312.13423v1",
        "pub_date": "2023-12-20",
        "summary": "The VADIS system addresses the demand of providing enhanced information\naccess in the domain of the social sciences. This is achieved by allowing users\nto search and use survey variables in context of their underlying research data\nand scholarly publications which have been interlinked with each other.",
        "translated": ""
    },
    {
        "title": "Multi-view user representation learning for user matching without\n  personal information",
        "url": "http://arxiv.org/abs/2312.14533v1",
        "pub_date": "2023-12-22",
        "summary": "As the digitization of travel industry accelerates, analyzing and\nunderstanding travelers' behaviors becomes increasingly important. However,\ntraveler data frequently exhibit high data sparsity due to the relatively low\nfrequency of user interactions with travel providers. Compounding this effect\nthe multiplication of devices, accounts and platforms while browsing travel\nproducts online also leads to data dispersion. To deal with these challenges,\nprobabilistic traveler matching can be used. Most existing solutions for user\nmatching are not suitable for traveler matching as a traveler's browsing\nhistory is typically short and URLs in the travel industry are very\nheterogeneous with many tokens. To deal with these challenges, we propose the\nsimilarity based multi-view information fusion to learn a better user\nrepresentation from URLs by treating the URLs as multi-view data. The\nexperimental results show that the proposed multi-view user representation\nlearning can take advantage of the complementary information from different\nviews, highlight the key information in URLs and perform significantly better\nthan other representation learning solutions for the user matching task.",
        "translated": ""
    },
    {
        "title": "On the Effectiveness of Unlearning in Session-Based Recommendation",
        "url": "http://arxiv.org/abs/2312.14447v1",
        "pub_date": "2023-12-22",
        "summary": "Session-based recommendation predicts users' future interests from previous\ninteractions in a session. Despite the memorizing of historical samples, the\nrequest of unlearning, i.e., to remove the effect of certain training samples,\nalso occurs for reasons such as user privacy or model fidelity. However,\nexisting studies on unlearning are not tailored for the session-based\nrecommendation. On the one hand, these approaches cannot achieve satisfying\nunlearning effects due to the collaborative correlations and sequential\nconnections between the unlearning item and the remaining items in the session.\nOn the other hand, seldom work has conducted the research to verify the\nunlearning effectiveness in the session-based recommendation scenario. In this\npaper, we propose SRU, a session-based recommendation unlearning framework,\nwhich enables high unlearning efficiency, accurate recommendation performance,\nand improved unlearning effectiveness in session-based recommendation.\nSpecifically, we first partition the training sessions into separate sub-models\naccording to the similarity across the sessions, then we utilize an\nattention-based aggregation layer to fuse the hidden states according to the\ncorrelations between the session and the centroid of the data in the sub-model.\nTo improve the unlearning effectiveness, we further propose three extra data\ndeletion strategies, including collaborative extra deletion (CED), neighbor\nextra deletion (NED), and random extra deletion (RED). Besides, we propose an\nevaluation metric that measures whether the unlearning sample can be inferred\nafter the data deletion to verify the unlearning effectiveness. We implement\nSRU with three representative session-based recommendation models and conduct\nexperiments on three benchmark datasets. Experimental results demonstrate the\neffectiveness of our methods.",
        "translated": ""
    },
    {
        "title": "Attribute-driven Disentangled Representation Learning for Multimodal\n  Recommendation",
        "url": "http://arxiv.org/abs/2312.14433v1",
        "pub_date": "2023-12-22",
        "summary": "Recommendation algorithms forecast user preferences by correlating user and\nitem representations derived from historical interaction patterns. In pursuit\nof enhanced performance, many methods focus on learning robust and independent\nrepresentations by disentangling the intricate factors within interaction data\nacross various modalities in an unsupervised manner. However, such an approach\nobfuscates the discernment of how specific factors (e.g., category or brand)\ninfluence the outcomes, making it challenging to regulate their effects. In\nresponse to this challenge, we introduce a novel method called Attribute-Driven\nDisentangled Representation Learning (short for AD-DRL), which explicitly\nincorporates attributes from different modalities into the disentangled\nrepresentation learning process. By assigning a specific attribute to each\nfactor in multimodal features, AD-DRL can disentangle the factors at both\nattribute and attribute-value levels. To obtain robust and independent\nrepresentations for each factor associated with a specific attribute, we first\ndisentangle the representations of features both within and across different\nmodalities. Moreover, we further enhance the robustness of the representations\nby fusing the multimodal features of the same factor. Empirical evaluations\nconducted on three public real-world datasets substantiate the effectiveness of\nAD-DRL, as well as its interpretability and controllability.",
        "translated": ""
    },
    {
        "title": "Context-aware Decoding Reduces Hallucination in Query-focused\n  Summarization",
        "url": "http://arxiv.org/abs/2312.14335v1",
        "pub_date": "2023-12-21",
        "summary": "Query-focused summarization (QFS) aims to provide a summary of a single\ndocument/multi documents that can satisfy the information needs of a given\nquery. It is useful for various real-world applications, such as abstractive\nsnippet generation or more recent retrieval augmented generation (RAG). A\nprototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\nand a generator (usually a large language model). However, applying large\nlanguage models (LLM) potentially leads to hallucinations, especially when the\nevidence contradicts the prior belief of LLMs. There has been growing interest\nin developing new decoding methods to improve generation quality and reduce\nhallucination. In this work, we conduct a large-scale reproducibility on one\nrecently proposed decoding method -- Context-aware Decoding (CAD). In addition\nto replicating CAD's experiments on news summarization datasets, we include\nexperiments on QFS datasets, and conduct more rigorous analysis on\ncomputational complexity and hyperparameter sensitivity. Experiments with eight\ndifferent language models show that performance-wise, CAD improves QFS quality\nby (1) reducing factuality errors/hallucinations while (2) mostly retaining the\nmatch of lexical patterns, measured by ROUGE scores, while also at a cost of\nincreased inference-time FLOPs and reduced decoding speed. The code\nimplementation based on Huggingface Library is made available\nhttps://github.com/zhichaoxu-shufe/context-aware-decoding-qfs",
        "translated": ""
    },
    {
        "title": "Zero-Shot Cross-Lingual Reranking with Large Language Models for\n  Low-Resource Languages",
        "url": "http://arxiv.org/abs/2312.16159v1",
        "pub_date": "2023-12-26",
        "summary": "Large language models (LLMs) have shown impressive zero-shot capabilities in\nvarious document reranking tasks. Despite their successful implementations,\nthere is still a gap in existing literature on their effectiveness in\nlow-resource languages. To address this gap, we investigate how LLMs function\nas rerankers in cross-lingual information retrieval (CLIR) systems for African\nlanguages. Our implementation covers English and four African languages (Hausa,\nSomali, Swahili, and Yoruba) and we examine cross-lingual reranking with\nqueries in English and passages in the African languages. Additionally, we\nanalyze and compare the effectiveness of monolingual reranking using both query\nand document translations. We also evaluate the effectiveness of LLMs when\nleveraging their own generated translations. To get a grasp of the\neffectiveness of multiple LLMs, our study focuses on the proprietary models\nRankGPT-4 and RankGPT-3.5, along with the open-source model, RankZephyr. While\nreranking remains most effective in English, our results reveal that\ncross-lingual reranking may be competitive with reranking in African languages\ndepending on the multilingual capability of the LLM.",
        "translated": ""
    },
    {
        "title": "Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with\n  Seq2seq Encoder-Decoder Models",
        "url": "http://arxiv.org/abs/2312.16098v1",
        "pub_date": "2023-12-26",
        "summary": "Recent work in zero-shot listwise reranking using LLMs has achieved\nstate-of-the-art results. However, these methods are not without drawbacks. The\nproposed methods rely on large LLMs with billions of parameters and limited\ncontext sizes. This paper introduces LiT5-Distill and LiT5-Score, two methods\nfor efficient zero-shot listwise reranking, leveraging T5 sequence-to-sequence\nencoder-decoder models. Our approaches demonstrate competitive reranking\neffectiveness compared to recent state-of-the-art LLM rerankers with\nsubstantially smaller models. Through LiT5-Score, we also explore the use of\ncross-attention to calculate relevance scores to perform reranking, eliminating\nthe reliance on external passage relevance labels for training. We present a\nrange of models from 220M parameters to 3B parameters, all with strong\nreranking results, challenging the necessity of large-scale models for\neffective zero-shot reranking and opening avenues for more efficient listwise\nreranking solutions. We provide code and scripts to reproduce our results at\nhttps://github.com/castorini/LiT5.",
        "translated": ""
    },
    {
        "title": "RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k\n  Recommendation",
        "url": "http://arxiv.org/abs/2312.16018v1",
        "pub_date": "2023-12-26",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities and\nhave been extensively deployed across various domains, including recommender\nsystems. Numerous studies have employed specialized \\textit{prompts} to harness\nthe in-context learning capabilities intrinsic to LLMs. For example, LLMs are\nprompted to act as zero-shot rankers for listwise ranking, evaluating candidate\nitems generated by a retrieval model for recommendation. Recent research\nfurther uses instruction tuning techniques to align LLM with human preference\nfor more promising recommendations. Despite its potential, current research\noverlooks the integration of multiple ranking tasks to enhance model\nperformance. Moreover, the signal from the conventional recommendation model is\nnot integrated into the LLM, limiting the current system performance.\n  In this paper, we introduce RecRanker, tailored for instruction tuning LLM to\nserve as the \\textbf{Ranker} for top-\\textit{k} \\textbf{Rec}ommendations.\nSpecifically, we introduce importance-aware sampling, clustering-based\nsampling, and penalty for repetitive sampling for sampling high-quality,\nrepresentative, and diverse training data. To enhance the prompt, we introduce\nposition shifting strategy to mitigate position bias and augment the prompt\nwith auxiliary information from conventional recommendation models, thereby\nenriching the contextual understanding of the LLM. Subsequently, we utilize the\nsampled data to assemble an instruction-tuning dataset with the augmented\nprompt comprising three distinct ranking tasks: pointwise, pairwise, and\nlistwise rankings. We further propose a hybrid ranking method to enhance the\nmodel performance by ensembling these ranking tasks. Our empirical evaluations\ndemonstrate the effectiveness of our proposed RecRanker in both direct and\nsequential recommendation scenarios.",
        "translated": ""
    },
    {
        "title": "A Comprehensive Survey of Evaluation Techniques for Recommendation\n  Systems",
        "url": "http://arxiv.org/abs/2312.16015v1",
        "pub_date": "2023-12-26",
        "summary": "The effectiveness of recommendation systems is pivotal to user engagement and\nsatisfaction in online platforms. As these recommendation systems increasingly\ninfluence user choices, their evaluation transcends mere technical performance\nand becomes central to business success. This paper addresses the multifaceted\nnature of recommendation system evaluation by introducing a comprehensive suite\nof metrics, each tailored to capture a distinct aspect of system performance.\nWe discuss similarity metrics that quantify the precision of content-based and\ncollaborative filtering mechanisms, along with candidate generation metrics\nwhich measure how well the system identifies a broad yet pertinent range of\nitems. Following this, we delve into predictive metrics that assess the\naccuracy of forecasted preferences, ranking metrics that evaluate the order in\nwhich recommendations are presented, and business metrics that align system\nperformance with economic objectives.\n  Our approach emphasizes the contextual application of these metrics and their\ninterdependencies. In this paper, we identify the strengths and limitations of\ncurrent evaluation practices and highlight the nuanced trade-offs that emerge\nwhen optimizing recommendation systems across different metrics. The paper\nconcludes by proposing a framework for selecting and interpreting these metrics\nto not only improve system performance but also to advance business goals. This\nwork is to aid researchers and practitioners in critically assessing\nrecommendation systems and fosters the development of more nuanced, effective,\nand economically viable personalization strategies. Our code is available at\nGitHub -\nhttps://github.com/aryan-jadon/Evaluation-Metrics-for-Recommendation-Systems.",
        "translated": ""
    },
    {
        "title": "An Incremental Update Framework for Online Recommenders with Data-Driven\n  Prior",
        "url": "http://arxiv.org/abs/2312.15903v1",
        "pub_date": "2023-12-26",
        "summary": "Online recommenders have attained growing interest and created great revenue\nfor businesses. Given numerous users and items, incremental update becomes a\nmainstream paradigm for learning large-scale models in industrial scenarios,\nwhere only newly arrived data within a sliding window is fed into the model,\nmeeting the strict requirements of quick response. However, this strategy would\nbe prone to overfitting to newly arrived data. When there exists a significant\ndrift of data distribution, the long-term information would be discarded, which\nharms the recommendation performance. Conventional methods address this issue\nthrough native model-based continual learning methods, without analyzing the\ndata characteristics for online recommenders. To address the aforementioned\nissue, we propose an incremental update framework for online recommenders with\nData-Driven Prior (DDP), which is composed of Feature Prior (FP) and Model\nPrior (MP). The FP performs the click estimation for each specific value to\nenhance the stability of the training process. The MP incorporates previous\nmodel output into the current update while strictly following the Bayes rules,\nresulting in a theoretically provable prior for the robust update. In this way,\nboth the FP and MP are well integrated into the unified framework, which is\nmodel-agnostic and can accommodate various advanced interaction models.\nExtensive experiments on two publicly available datasets as well as an\nindustrial dataset demonstrate the superior performance of the proposed\nframework.",
        "translated": ""
    },
    {
        "title": "Hypergraph Enhanced Knowledge Tree Prompt Learning for Next-Basket\n  Recommendation",
        "url": "http://arxiv.org/abs/2312.15851v1",
        "pub_date": "2023-12-26",
        "summary": "Next-basket recommendation (NBR) aims to infer the items in the next basket\ngiven the corresponding basket sequence. Existing NBR methods are mainly based\non either message passing in a plain graph or transition modelling in a basket\nsequence. However, these methods only consider point-to-point binary item\nrelations while item dependencies in real world scenarios are often in higher\norder. Additionally, the importance of the same item to different users varies\ndue to variation of user preferences, and the relations between items usually\ninvolve various aspects. As pretrained language models (PLMs) excel in multiple\ntasks in natural language processing (NLP) and computer vision (CV), many\nresearchers have made great efforts in utilizing PLMs to boost recommendation.\nHowever, existing PLM-based recommendation methods degrade when encountering\nOut-Of-Vocabulary (OOV) items. OOV items are those whose IDs are out of PLM's\nvocabulary and thus unintelligible to PLM. To settle the above challenges, we\npropose a novel method HEKP4NBR, which transforms the knowledge graph (KG) into\nprompts, namely Knowledge Tree Prompt (KTP), to help PLM encode the OOV item\nIDs in the user's basket sequence. A hypergraph convolutional module is\ndesigned to build a hypergraph based on item similarities measured by an MoE\nmodel from multiple aspects and then employ convolution on the hypergraph to\nmodel correlations among multiple items. Extensive experiments are conducted on\nHEKP4NBR on two datasets based on real company data and validate its\neffectiveness against multiple state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Adversarial Item Promotion on Visually-Aware Recommender Systems by\n  Guided Diffusion",
        "url": "http://arxiv.org/abs/2312.15826v1",
        "pub_date": "2023-12-25",
        "summary": "Visually-aware recommender systems have found widespread application in\ndomains where visual elements significantly contribute to the inference of\nusers' potential preferences. While the incorporation of visual information\nholds the promise of enhancing recommendation accuracy and alleviating the\ncold-start problem, it is essential to point out that the inclusion of item\nimages may introduce substantial security challenges. Some existing works have\nshown that the item provider can manipulate item exposure rates to its\nadvantage by constructing adversarial images. However, these works cannot\nreveal the real vulnerability of visually-aware recommender systems because (1)\nThe generated adversarial images are markedly distorted, rendering them easily\ndetectable by human observers; (2) The effectiveness of the attacks is\ninconsistent and even ineffective in some scenarios. To shed light on the real\nvulnerabilities of visually-aware recommender systems when confronted with\nadversarial images, this paper introduces a novel attack method, IPDGI (Item\nPromotion by Diffusion Generated Image). Specifically, IPDGI employs a guided\ndiffusion model to generate adversarial samples designed to deceive\nvisually-aware recommender systems. Taking advantage of accurately modeling\nbenign images' distribution by diffusion models, the generated adversarial\nimages have high fidelity with original images, ensuring the stealth of our\nIPDGI. To demonstrate the effectiveness of our proposed methods, we conduct\nextensive experiments on two commonly used e-commerce recommendation datasets\n(Amazon Beauty and Amazon Baby) with several typical visually-aware recommender\nsystems. The experimental results show that our attack method has a significant\nimprovement in both the performance of promoting the long-tailed (i.e.,\nunpopular) items and the quality of generated adversarial images.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Not Stable Recommender Systems",
        "url": "http://arxiv.org/abs/2312.15746v1",
        "pub_date": "2023-12-25",
        "summary": "With the significant successes of large language models (LLMs) in many\nnatural language processing tasks, there is growing interest among researchers\nin exploring LLMs for novel recommender systems. However, we have observed that\ndirectly using LLMs as a recommender system is usually unstable due to its\ninherent position bias. To this end, we introduce exploratory research and find\nconsistent patterns of positional bias in LLMs that influence the performance\nof recommendation across a range of scenarios. Then, we propose a Bayesian\nprobabilistic framework, STELLA (Stable LLM for Recommendation), which involves\na two-stage pipeline. During the first probing stage, we identify patterns in a\ntransition matrix using a probing detection dataset. And in the second\nrecommendation stage, a Bayesian strategy is employed to adjust the biased\noutput of LLMs with an entropy indicator. Therefore, our framework can\ncapitalize on existing pattern information to calibrate instability of LLMs,\nand enhance recommendation performance. Finally, extensive experiments clearly\nvalidate the effectiveness of our framework.",
        "translated": ""
    },
    {
        "title": "Unlocking the Potential of Large Language Models for Explainable\n  Recommendations",
        "url": "http://arxiv.org/abs/2312.15661v1",
        "pub_date": "2023-12-25",
        "summary": "Generating user-friendly explanations regarding why an item is recommended\nhas become increasingly common, largely due to advances in language generation\ntechnology, which can enhance user trust and facilitate more informed\ndecision-making when using online services. However, existing explainable\nrecommendation systems focus on using small-size language models. It remains\nuncertain what impact replacing the explanation generator with the recently\nemerging large language models (LLMs) would have. Can we expect unprecedented\nresults?\n  In this study, we propose LLMXRec, a simple yet effective two-stage\nexplainable recommendation framework aimed at further boosting the explanation\nquality by employing LLMs. Unlike most existing LLM-based recommendation works,\na key characteristic of LLMXRec is its emphasis on the close collaboration\nbetween previous recommender models and LLM-based explanation generators.\nSpecifically, by adopting several key fine-tuning techniques, including\nparameter-efficient instructing tuning and personalized prompt techniques,\ncontrollable and fluent explanations can be well generated to achieve the goal\nof explanation recommendation. Most notably, we provide three different\nperspectives to evaluate the effectiveness of the explanations. Finally, we\nconduct extensive experiments over several benchmark recommender models and\npublicly available datasets. The experimental results not only yield positive\nresults in terms of effectiveness and efficiency but also uncover some\npreviously unknown outcomes. To facilitate further explorations in this area,\nthe full code and detailed original results are open-sourced at\nhttps://anonymous.4open.science/r/LLM_rec_explanation-7028/",
        "translated": ""
    },
    {
        "title": "RDF-star2Vec: RDF-star Graph Embeddings for Data Mining",
        "url": "http://arxiv.org/abs/2312.15626v1",
        "pub_date": "2023-12-25",
        "summary": "Knowledge Graphs (KGs) such as Resource Description Framework (RDF) data\nrepresent relationships between various entities through the structure of\ntriples (&lt;subject, predicate, object&gt;). Knowledge graph embedding (KGE) is\ncrucial in machine learning applications, specifically in node classification\nand link prediction tasks. KGE remains a vital research topic within the\nsemantic web community. RDF-star introduces the concept of a quoted triple\n(QT), a specific form of triple employed either as the subject or object within\nanother triple. Moreover, RDF-star permits a QT to act as compositional\nentities within another QT, thereby enabling the representation of recursive,\nhyper-relational KGs with nested structures. However, existing KGE models fail\nto adequately learn the semantics of QTs and entities, primarily because they\ndo not account for RDF-star graphs containing multi-leveled nested QTs and\nQT-QT relationships. This study introduces RDF-star2Vec, a novel KGE model\nspecifically designed for RDF-star graphs. RDF-star2Vec introduces graph walk\ntechniques that enable probabilistic transitions between a QT and its\ncompositional entities. Feature vectors for QTs, entities, and relations are\nderived from generated sequences through the structured skip-gram model.\nAdditionally, we provide a dataset and a benchmarking framework for data mining\ntasks focused on complex RDF-star graphs. Evaluative experiments demonstrated\nthat RDF-star2Vec yielded superior performance compared to recent extensions of\nRDF2Vec in various tasks including classification, clustering, entity\nrelatedness, and QT similarity.",
        "translated": ""
    },
    {
        "title": "An Adaptive Framework of Geographical Group-Specific Network on O2O\n  Recommendation",
        "url": "http://arxiv.org/abs/2312.17072v1",
        "pub_date": "2023-12-28",
        "summary": "Online to offline recommendation strongly correlates with the user and\nservice's spatiotemporal information, therefore calling for a higher degree of\nmodel personalization. The traditional methodology is based on a uniform model\nstructure trained by collected centralized data, which is unlikely to capture\nall user patterns over different geographical areas or time periods. To tackle\nthis challenge, we propose a geographical group-specific modeling method called\nGeoGrouse, which simultaneously studies the common knowledge as well as\ngroup-specific knowledge of user preferences. An automatic grouping paradigm is\nemployed and verified based on users' geographical grouping indicators. Offline\nand online experiments are conducted to verify the effectiveness of our\napproach, and substantial business improvement is achieved.",
        "translated": ""
    },
    {
        "title": "DiffKG: Knowledge Graph Diffusion Model for Recommendation",
        "url": "http://arxiv.org/abs/2312.16890v1",
        "pub_date": "2023-12-28",
        "summary": "Knowledge Graphs (KGs) have emerged as invaluable resources for enriching\nrecommendation systems by providing a wealth of factual information and\ncapturing semantic relationships among items. Leveraging KGs can significantly\nenhance recommendation performance. However, not all relations within a KG are\nequally relevant or beneficial for the target recommendation task. In fact,\ncertain item-entity connections may introduce noise or lack informative value,\nthus potentially misleading our understanding of user preferences. To bridge\nthis research gap, we propose a novel knowledge graph diffusion model for\nrecommendation, referred to as DiffKG. Our framework integrates a generative\ndiffusion model with a data augmentation paradigm, enabling robust knowledge\ngraph representation learning. This integration facilitates a better alignment\nbetween knowledge-aware item semantics and collaborative relation modeling.\nMoreover, we introduce a collaborative knowledge graph convolution mechanism\nthat incorporates collaborative signals reflecting user-item interaction\npatterns, guiding the knowledge graph diffusion process. We conduct extensive\nexperiments on three publicly available datasets, consistently demonstrating\nthe superiority of our DiffKG compared to various competitive baselines. We\nprovide the source code repository of our proposed DiffKG model at the\nfollowing link: https://github.com/HKUDS/DiffKG.",
        "translated": ""
    },
    {
        "title": "Pareto-based Multi-Objective Recommender System with Forgetting Curve",
        "url": "http://arxiv.org/abs/2312.16868v1",
        "pub_date": "2023-12-28",
        "summary": "Recommender systems with cascading architecture play an increasingly\nsignificant role in online recommendation platforms, where the approach to\ndealing with negative feedback is a vital issue. For instance, in short video\nplatforms, users tend to quickly slip away from candidates that they feel\naversive, and recommender systems are expected to receive these explicit\nnegative feedbacks and make adjustments to avoid these recommendations.\nConsidering recency effect in memories, we propose a forgetting model based on\nEbbinghaus Forgetting Curve to cope with negative feedback. In addition, we\nintroduce a Pareto optimization solver to guarantee a better trade-off between\nrecency and model performance. In conclusion, we propose Pareto-based\nMulti-Objective Recommender System with forgetting curve (PMORS), which can be\napplied to any multi-objective recommendation and show sufficiently superiority\nwhen facing explicit negative feedback. We have conducted evaluations of PMORS\nand achieved favorable outcomes in short-video scenarios on both public dataset\nand industrial dataset. After being deployed on an online short video platform\nnamed WeChat Channels in May, 2023, PMORS has not only demonstrated promising\nresults for both consistency and recency but also achieved an improvement of up\nto +1.45% GMV.",
        "translated": ""
    },
    {
        "title": "GUITAR: Gradient Pruning toward Fast Neural Ranking",
        "url": "http://arxiv.org/abs/2312.16828v1",
        "pub_date": "2023-12-28",
        "summary": "With the continuous popularity of deep learning and representation learning,\nfast vector search becomes a vital task in various ranking/retrieval based\napplications, say recommendation, ads ranking and question answering. Neural\nnetwork based ranking is widely adopted due to its powerful capacity in\nmodeling complex relationships, such as between users and items, questions and\nanswers. However, it is usually exploited in offline or re-ranking manners for\nit is time-consuming in computations. Online neural network ranking--so called\nfast neural ranking--is considered challenging because neural network measures\nare usually non-convex and asymmetric. Traditional Approximate Nearest Neighbor\n(ANN) search which usually focuses on metric ranking measures, is not\napplicable to these advanced measures.\n  In this paper, we introduce a novel graph searching framework to accelerate\nthe searching in the fast neural ranking problem. The proposed graph searching\nalgorithm is bi-level: we first construct a probable candidate set; then we\nonly evaluate the neural network measure over the probable candidate set\ninstead of evaluating the neural network over all neighbors. Specifically, we\npropose a gradient-based algorithm that approximates the rank of the neural\nnetwork matching score to construct the probable candidate set; and we present\nan angle-based heuristic procedure to adaptively identify the proper size of\nthe probable candidate set. Empirical results on public data confirm the\neffectiveness of our proposed algorithms.",
        "translated": ""
    },
    {
        "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
        "url": "http://arxiv.org/abs/2312.16821v1",
        "pub_date": "2023-12-28",
        "summary": "Ranker and retriever are two important components in dense passage retrieval.\nThe retriever typically adopts a dual-encoder model, where queries and\ndocuments are separately input into two pre-trained models, and the vectors\ngenerated by the models are used for similarity calculation. The ranker often\nuses a cross-encoder model, where the concatenated query-document pairs are\ninput into a pre-trained model to obtain word similarities. However, the\ndual-encoder model lacks interaction between queries and documents due to its\nindependent encoding, while the cross-encoder model requires substantial\ncomputational cost for attention calculation, making it difficult to obtain\nreal-time retrieval results. In this paper, we propose a dense retrieval model\ncalled MD2PR based on multi-level distillation. In this model, we distill the\nknowledge learned from the cross-encoder to the dual-encoder at both the\nsentence level and word level. Sentence-level distillation enhances the\ndual-encoder on capturing the themes and emotions of sentences. Word-level\ndistillation improves the dual-encoder in analysis of word semantics and\nrelationships. As a result, the dual-encoder can be used independently for\nsubsequent encoding and retrieval, avoiding the significant computational cost\nassociated with the participation of the cross-encoder. Furthermore, we propose\na simple dynamic filtering method, which updates the threshold during multiple\ntraining iterations to ensure the effective identification of false negatives\nand thus obtains a more comprehensive semantic representation space. The\nexperimental results over two standard datasets show our MD2PR outperforms 11\nbaseline models in terms of MRR and Recall metrics.",
        "translated": ""
    },
    {
        "title": "Performance Comparison of Session-based Recommendation Algorithms based\n  on GNNs",
        "url": "http://arxiv.org/abs/2312.16695v1",
        "pub_date": "2023-12-27",
        "summary": "In session-based recommendation settings, a recommender system has to base\nits suggestions on the user interactions that are ob served in an ongoing\nsession. Since such sessions can consist of only a small set of interactions,\nvarious approaches based on Graph Neural Networks (GNN) were recently proposed,\nas they allow us to integrate various types of side information about the items\nin a natural way. Unfortunately, a variety of evaluation settings are used in\nthe literature, e.g., in terms of protocols, metrics and baselines, making it\ndifficult to assess what represents the state of the art. In this work, we\npresent the results of an evaluation of eight recent GNN-based approaches that\nwere published in high-quality outlets. For a fair comparison, all models are\nsystematically tuned and tested under identical conditions using three common\ndatasets. We furthermore include k-nearest-neighbor and sequential rules-based\nmodels as baselines, as such models have previously exhibited competitive\nperformance results for similar settings. To our surprise, the evaluation\nshowed that the simple models outperform all recent GNN models in terms of the\nMean Reciprocal Rank, which we used as an optimization criterion, and were only\noutperformed in three cases in terms of the Hit Rate. Additional analyses\nfurthermore reveal that several other factors that are often not deeply\ndiscussed in papers, e.g., random seeds, can markedly impact the performance of\nGNN-based models. Our results therefore (a) point to continuing issues in the\ncommunity in terms of research methodology and (b) indicate that there is ample\nroom for improvement in session-based recommendation.",
        "translated": ""
    },
    {
        "title": "Continuous-time Autoencoders for Regular and Irregular Time Series\n  Imputation",
        "url": "http://arxiv.org/abs/2312.16581v1",
        "pub_date": "2023-12-27",
        "summary": "Time series imputation is one of the most fundamental tasks for time series.\nReal-world time series datasets are frequently incomplete (or irregular with\nmissing observations), in which case imputation is strongly required. Many\ndifferent time series imputation methods have been proposed. Recent\nself-attention-based methods show the state-of-the-art imputation performance.\nHowever, it has been overlooked for a long time to design an imputation method\nbased on continuous-time recurrent neural networks (RNNs), i.e., neural\ncontrolled differential equations (NCDEs). To this end, we redesign time series\n(variational) autoencoders based on NCDEs. Our method, called continuous-time\nautoencoder (CTA), encodes an input time series sample into a continuous hidden\npath (rather than a hidden vector) and decodes it to reconstruct and impute the\ninput. In our experiments with 4 datasets and 19 baselines, our method shows\nthe best imputation performance in almost all cases.",
        "translated": ""
    },
    {
        "title": "RDGCL: Reaction-Diffusion Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2312.16563v1",
        "pub_date": "2023-12-27",
        "summary": "Contrastive learning (CL) has emerged as a promising technique for improving\nrecommender systems, addressing the challenge of data sparsity by leveraging\nself-supervised signals from raw data. Integration of CL with graph\nconvolutional network (GCN)-based collaborative filterings (CFs) has been\nexplored in recommender systems. However, current CL-based recommendation\nmodels heavily rely on low-pass filters and graph augmentations. In this paper,\nwe propose a novel CL method for recommender systems called the\nreaction-diffusion graph contrastive learning model (RDGCL). We design our own\nGCN for CF based on both the diffusion, i.e., low-pass filter, and the\nreaction, i.e., high-pass filter, equations. Our proposed CL-based training\noccurs between reaction and diffusion-based embeddings, so there is no need for\ngraph augmentations. Experimental evaluation on 6 benchmark datasets\ndemonstrates that our proposed method outperforms state-of-the-art CL-based\nrecommendation models. By enhancing recommendation accuracy and diversity, our\nmethod brings an advancement in CL for recommender systems.",
        "translated": ""
    },
    {
        "title": "LGMRec: Local and Global Graph Learning for Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2312.16400v1",
        "pub_date": "2023-12-27",
        "summary": "The multimodal recommendation has gradually become the infrastructure of\nonline media platforms, enabling them to provide personalized service to users\nthrough a joint modeling of user historical behaviors (e.g., purchases, clicks)\nand item various modalities (e.g., visual and textual). The majority of\nexisting studies typically focus on utilizing modal features or modal-related\ngraph structure to learn user local interests. Nevertheless, these approaches\nencounter two limitations: (1) Shared updates of user ID embeddings result in\nthe consequential coupling between collaboration and multimodal signals; (2)\nLack of exploration into robust global user interests to alleviate the sparse\ninteraction problems faced by local interest modeling. To address these issues,\nwe propose a novel Local and Global Graph Learning-guided Multimodal\nRecommender (LGMRec), which jointly models local and global user interests.\nSpecifically, we present a local graph embedding module to independently learn\ncollaborative-related and modality-related embeddings of users and items with\nlocal topological relations. Moreover, a global hypergraph embedding module is\ndesigned to capture global user and item embeddings by modeling insightful\nglobal dependency relations. The global embeddings acquired within the\nhypergraph embedding space can then be combined with two decoupled local\nembeddings to improve the accuracy and robustness of recommendations. Extensive\nexperiments conducted on three benchmark datasets demonstrate the superiority\nof our LGMRec over various state-of-the-art recommendation baselines,\nshowcasing its effectiveness in modeling both local and global user interests.",
        "translated": ""
    },
    {
        "title": "K-PERM: Personalized Response Generation Using Dynamic Knowledge\n  Retrieval and Persona-Adaptive Queries",
        "url": "http://arxiv.org/abs/2312.17748v1",
        "pub_date": "2023-12-29",
        "summary": "Personalizing conversational agents can enhance the quality of conversations\nand increase user engagement. However, they often lack external knowledge to\nappropriately tend to a user's persona. This is particularly crucial for\npractical applications like mental health support, nutrition planning,\nculturally sensitive conversations, or reducing toxic behavior in\nconversational agents. To enhance the relevance and comprehensiveness of\npersonalized responses, we propose using a two-step approach that involves (1)\nselectively integrating user personas and (2) contextualizing the response with\nsupplementing information from a background knowledge source. We develop K-PERM\n(Knowledge-guided PErsonalization with Reward Modulation), a dynamic\nconversational agent that combines these elements. K-PERM achieves\nstate-of-the-art performance on the popular FoCus dataset, containing\nreal-world personalized conversations concerning global landmarks. We show that\nusing responses from K-PERM can improve performance in state-of-the-art LLMs\n(GPT 3.5) by 10.5%, highlighting the impact of K-PERM for personalizing\nchatbots.",
        "translated": ""
    },
    {
        "title": "Investigating the Effects of Sparse Attention on Cross-Encoders",
        "url": "http://arxiv.org/abs/2312.17649v1",
        "pub_date": "2023-12-29",
        "summary": "Cross-encoders are effective passage and document re-rankers but less\nefficient than other neural or classic retrieval models. A few previous studies\nhave applied windowed self-attention to make cross-encoders more efficient.\nHowever, these studies did not investigate the potential and limits of\ndifferent attention patterns or window sizes. We close this gap and\nsystematically analyze how token interactions can be reduced without harming\nthe re-ranking effectiveness. Experimenting with asymmetric attention and\ndifferent window sizes, we find that the query tokens do not need to attend to\nthe passage or document tokens for effective re-ranking and that very small\nwindow sizes suffice. In our experiments, even windows of 4 tokens still yield\neffectiveness on par with previous cross-encoders while reducing the memory\nrequirements to at most 78% / 41% and being 1% / 43% faster at inference time\nfor passages / documents.",
        "translated": ""
    },
    {
        "title": "Uncertain research country rankings. Should we continue producing\n  uncertain rankings?",
        "url": "http://arxiv.org/abs/2312.17560v1",
        "pub_date": "2023-12-29",
        "summary": "Citation based country rankings consistently categorize Japan as a developing\ncountry, even in those from the most reputed institutions. This categorization\nchallenges the credibility of such rankings, considering Japan elevated\nscientific standing. In most cases, these rankings use percentile indicators\nand are accurate if country citations fit an ideal model of distribution, but\nthey can be misleading in cases of deviations. The ideal model implies a\nlognormal citation distribution and a power law citation based double rank: in\nthe global and country lists. This report conducts a systematic examination of\ndeviations from the ideal model and their consequential impact on evaluations.\nThe study evaluates six selected countries across three scientifically relevant\ntopics and utilizes Leiden Ranking assessments of over 300 universities. The\nfindings reveal three types of deviations from the lognormal citation\ndistribution: i deviations in the extreme upper tail; ii inflated lower tails;\nand iii deflated lower part of the distributions. These deviations stem from\nstructural differences among research systems that are prevalent and have the\npotential to mislead evaluations across all research levels. Consequently,\nreliable evaluations must consider these deviations. Otherwise, while some\ncountries and institutions will be correctly evaluated, failure to identify\ndeviations in each specific country or institution will render uncertain\nevaluations. For reliable assessments, future research evaluations of countries\nand institutions must identify deviations from the ideal model.",
        "translated": ""
    },
    {
        "title": "Towards Mitigating Dimensional Collapse of Representations in\n  Collaborative Filtering",
        "url": "http://arxiv.org/abs/2312.17468v1",
        "pub_date": "2023-12-29",
        "summary": "Contrastive Learning (CL) has shown promising performance in collaborative\nfiltering. The key idea is to generate augmentation-invariant embeddings by\nmaximizing the Mutual Information between different augmented views of the same\ninstance. However, we empirically observe that existing CL models suffer from\nthe \\textsl{dimensional collapse} issue, where user/item embeddings only span a\nlow-dimension subspace of the entire feature space. This suppresses other\ndimensional information and weakens the distinguishability of embeddings. Here\nwe propose a non-contrastive learning objective, named nCL, which explicitly\nmitigates dimensional collapse of representations in collaborative filtering.\nOur nCL aims to achieve geometric properties of \\textsl{Alignment} and\n\\textsl{Compactness} on the embedding space. In particular, the alignment tries\nto push together representations of positive-related user-item pairs, while\ncompactness tends to find the optimal coding length of user/item embeddings,\nsubject to a given distortion. More importantly, our nCL does not require data\naugmentation nor negative sampling during training, making it scalable to large\ndatasets. Experimental results demonstrate the superiority of our nCL.",
        "translated": ""
    },
    {
        "title": "Break Out of a Pigeonhole: A Unified Framework for Examining\n  Miscalibration, Bias, and Stereotype in Recommender Systems",
        "url": "http://arxiv.org/abs/2312.17443v1",
        "pub_date": "2023-12-29",
        "summary": "Despite the benefits of personalizing items and information tailored to\nusers' needs, it has been found that recommender systems tend to introduce\nbiases that favor popular items or certain categories of items, and dominant\nuser groups. In this study, we aim to characterize the systematic errors of a\nrecommendation system and how they manifest in various accountability issues,\nsuch as stereotypes, biases, and miscalibration. We propose a unified framework\nthat distinguishes the sources of prediction errors into a set of key measures\nthat quantify the various types of system-induced effects, both at the\nindividual and collective levels. Based on our measuring framework, we examine\nthe most widely adopted algorithms in the context of movie recommendation. Our\nresearch reveals three important findings: (1) Differences between algorithms:\nrecommendations generated by simpler algorithms tend to be more stereotypical\nbut less biased than those generated by more complex algorithms. (2) Disparate\nimpact on groups and individuals: system-induced biases and stereotypes have a\ndisproportionate effect on atypical users and minority groups (e.g., women and\nolder users). (3) Mitigation opportunity: using structural equation modeling,\nwe identify the interactions between user characteristics (typicality and\ndiversity), system-induced effects, and miscalibration. We further investigate\nthe possibility of mitigating system-induced effects by oversampling\nunderrepresented groups and individuals, which was found to be effective in\nreducing stereotypes and improving recommendation quality. Our research is the\nfirst systematic examination of not only system-induced effects and\nmiscalibration but also the stereotyping issue in recommender systems.",
        "translated": ""
    },
    {
        "title": "GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension",
        "url": "http://arxiv.org/abs/2312.17294v1",
        "pub_date": "2023-12-28",
        "summary": "While Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated\nexceptional proficiency in natural language processing, their efficacy in\naddressing complex, multifaceted tasks remains limited. A growing area of\nresearch focuses on LLM-based agents equipped with external tools capable of\nperforming diverse tasks. However, existing LLM-based agents only support a\nlimited set of tools which is unable to cover a diverse range of user queries,\nespecially for those involving expertise domains. It remains a challenge for\nLLM-based agents to extend their tools autonomously when confronted with\nvarious user queries. As GitHub has hosted a multitude of repositories which\ncan be seen as a good resource for tools, a promising solution is that\nLLM-based agents can autonomously integrate the repositories in GitHub\naccording to the user queries to extend their tool set. In this paper, we\nintroduce GitAgent, an agent capable of achieving the autonomous tool extension\nfrom GitHub. GitAgent follows a four-phase procedure to incorporate\nrepositories and it can learn human experience by resorting to GitHub\nIssues/PRs to solve problems encountered during the procedure. Experimental\nevaluation involving 30 user queries demonstrates GitAgent's effectiveness,\nachieving a 69.4% success rate on average.",
        "translated": ""
    },
    {
        "title": "Dynamic Decision Making in Engineering System Design: A Deep Q-Learning\n  Approach",
        "url": "http://arxiv.org/abs/2312.17284v1",
        "pub_date": "2023-12-28",
        "summary": "Engineering system design, viewed as a decision-making process, faces\nchallenges due to complexity and uncertainty. In this paper, we present a\nframework proposing the use of the Deep Q-learning algorithm to optimize the\ndesign of engineering systems. We outline a step-by-step framework for\noptimizing engineering system designs. The goal is to find policies that\nmaximize the output of a simulation model given multiple sources of\nuncertainties. The proposed algorithm handles linear and non-linear multi-stage\nstochastic problems, where decision variables are discrete, and the objective\nfunction and constraints are assessed via a Monte Carlo simulation. We\ndemonstrate the effectiveness of our proposed framework by solving two\nengineering system design problems in the presence of multiple uncertainties,\nsuch as price and demand.",
        "translated": ""
    },
    {
        "title": "TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview",
        "url": "http://arxiv.org/abs/2401.01330v1",
        "pub_date": "2024-01-02",
        "summary": "Conversational Information Seeking stands as a pivotal research area with\nsignificant contributions from previous works. The TREC Interactive Knowledge\nAssistance Track (iKAT) builds on the foundational work of the TREC\nConversational Assistance Track (CAsT). However, iKAT distinctively emphasizes\nthe creation and research of conversational search agents that adapt responses\nbased on user's prior interactions and present context. The challenge lies in\nenabling Conversational Search Agents (CSA) to incorporate this personalized\ncontext to efficiency and effectively guide users through the relevant\ninformation to them. iKAT also emphasizes decisional search tasks, where users\nsift through data and information to weigh up options in order to reach a\nconclusion or perform an action. These tasks, prevalent in everyday\ninformation-seeking decisions -- be it related to travel, health, or shopping\n-- often revolve around a subset of high-level information operators where\nqueries or questions about the information space include: finding options,\ncomparing options, identifying the pros and cons of options, etc. Given the\ndifferent personas and their information need (expressed through the sequence\nof questions), diverse conversation trajectories will arise -- because the\nanswers to these similar queries will be very different. In this paper, we\nreport on the first year of TREC iKAT, describing the task, topics, data\ncollection, and evaluation framework. We further review the submissions and\nsummarize the findings.",
        "translated": ""
    },
    {
        "title": "Distillation is All You Need for Practically Using Different Pre-trained\n  Recommendation Models",
        "url": "http://arxiv.org/abs/2401.00797v1",
        "pub_date": "2024-01-01",
        "summary": "Pre-trained recommendation models (PRMs) have attracted widespread attention\nrecently. However, their totally different model structure, huge model size and\ncomputation cost hinder their application in practical recommender systems.\nHence, it is highly essential to explore how to practically utilize PRMs in\nreal-world recommendations. In this paper, we propose a novel joint knowledge\ndistillation from different pre-trained recommendation models named PRM-KD for\nrecommendation, which takes full advantages of diverse PRMs as teacher models\nfor enhancing student models efficiently. Specifically, PRM-KD jointly distills\ndiverse informative knowledge from multiple representative PRMs such as\nUniSRec, Recformer, and UniM^2Rec. The knowledge from the above PRMs are then\nsmartly integrated into the student recommendation model considering their\nconfidence and consistency. We further verify the universality of PRM-KD with\nvarious types of student models, including sequential recommendation, feature\ninteraction, and graph-based models. Extensive experiments on five real-world\ndatasets demonstrate the effectiveness and efficacy of PRM-KD, which could be\nviewed as an economical shortcut in practically and conveniently making full\nuse of different PRMs in online systems.",
        "translated": ""
    },
    {
        "title": "Recent Advances in Text Analysis",
        "url": "http://arxiv.org/abs/2401.00775v1",
        "pub_date": "2024-01-01",
        "summary": "Text analysis is an interesting research area in data science and has various\napplications, such as in artificial intelligence, biomedical research, and\nengineering. We review popular methods for text analysis, ranging from topic\nmodeling to the recent neural language models. In particular, we review\nTopic-SCORE, a statistical approach to topic modeling, and discuss how to use\nit to analyze MADStat - a dataset on statistical publications that we collected\nand cleaned.\n  The application of Topic-SCORE and other methods on MADStat leads to\ninteresting findings. For example, $11$ representative topics in statistics are\nidentified. For each journal, the evolution of topic weights over time can be\nvisualized, and these results are used to analyze the trends in statistical\nresearch. In particular, we propose a new statistical model for ranking the\ncitation impacts of $11$ topics, and we also build a cross-topic citation graph\nto illustrate how research results on different topics spread to one another.\n  The results on MADStat provide a data-driven picture of the statistical\nresearch in $1975$--$2015$, from a text analysis perspective.",
        "translated": ""
    },
    {
        "title": "Searching, fast and slow, through product catalogs",
        "url": "http://arxiv.org/abs/2401.00737v1",
        "pub_date": "2024-01-01",
        "summary": "String matching algorithms in the presence of abbreviations, such as in Stock\nKeeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In\nthis paper, we present a unified architecture for SKU search that provides both\na real-time suggestion system (based on a Trie data structure) as well as a\nlower latency search system (making use of character level TF-IDF in\ncombination with language model vector embeddings) where users initiate the\nsearch process explicitly. We carry out ablation studies that justify designing\na complex search system composed of multiple components to address the delicate\ntrade-off between speed and accuracy. Using SKU search in the Dynamics CRM as\nan example, we show how our system vastly outperforms, in all aspects, the\nresults provided by the default search engine. Finally, we show how SKU\ndescriptions may be enhanced via generative text models (using gpt-3.5-turbo)\nso that the consumers of the search results may get more context and a\ngenerally better experience when presented with the results of their SKU\nsearch.",
        "translated": ""
    },
    {
        "title": "V2X communication coverage analysis for connected vehicles in\n  intelligent transportation networks: A case study for the city of Xanthi,\n  Greece",
        "url": "http://arxiv.org/abs/2401.00465v1",
        "pub_date": "2023-12-31",
        "summary": "Intelligent transportation systems (ITS) have been developed to improve\ntraffic flow, efficiency, and safety in transportation. Technological\nadvancements in communication such as the Vehicle-to-Everything (V2X),\nVehicle-to-Vehicle (V2V) and Vehicle-to Infrastructure (V2I) enable the\nreal-time exchange of information between vehicles and other entities on the\nroad network, and thus play a significant role in their safety and efficiency.\nThis paper presents a simulation study that models V2V and V2I communication to\nidentify the most suitable range of data transmission between vehicles and\ninfrastructure. The provincial city of Xanthi, Greece is used as a cases study,\nand the goal is to evaluate whether the proposed placement of Road Side Unit\n(RSU) provided adequate communication coverage on the city's road network. An\nanalysis through different scenarios identified improvements in traffic\nmanagement, driving behavior and environmental conditions under different RSU\ncoverage. The results highlight that the communication range of 400 meters is\nthe most adequate option for optimum traffic management in the city of Xanthi.",
        "translated": ""
    },
    {
        "title": "Improving Text Embeddings with Large Language Models",
        "url": "http://arxiv.org/abs/2401.00368v1",
        "pub_date": "2023-12-31",
        "summary": "In this paper, we introduce a novel and simple method for obtaining\nhigh-quality text embeddings using only synthetic data and less than 1k\ntraining steps. Unlike existing methods that often depend on multi-stage\nintermediate pre-training with billions of weakly-supervised text pairs,\nfollowed by fine-tuning with a few labeled datasets, our method does not\nrequire building complex training pipelines or relying on manually collected\ndatasets that are often constrained by task diversity and language coverage. We\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\nthousands of text embedding tasks across nearly 100 languages. We then\nfine-tune open-source decoder-only LLMs on the synthetic data using standard\ncontrastive loss. Experiments demonstrate that our method achieves strong\nperformance on highly competitive text embedding benchmarks without using any\nlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\nlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\nbenchmarks.",
        "translated": ""
    },
    {
        "title": "EXPLORE -- Explainable Song Recommendation",
        "url": "http://arxiv.org/abs/2401.00353v1",
        "pub_date": "2023-12-30",
        "summary": "This study explores the development of an explainable music recommendation\nsystem with enhanced user control. Leveraging a hybrid of collaborative\nfiltering and content-based filtering, we address the challenges of opaque\nrecommendation logic and lack of user influence on results. We present a novel\napproach combining advanced algorithms and an interactive user interface. Our\nmethodology integrates Spotify data with user preference analytics to tailor\nmusic suggestions. Evaluation through RMSE and user studies underscores the\nefficacy and user satisfaction with our system. The paper concludes with\npotential directions for future enhancements in group recommendations and\ndynamic feedback integration.",
        "translated": ""
    },
    {
        "title": "Dual-space Hierarchical Learning for Goal-guided Conversational\n  Recommendation",
        "url": "http://arxiv.org/abs/2401.00272v1",
        "pub_date": "2023-12-30",
        "summary": "Proactively and naturally guiding the dialog from the non-recommendation\ncontext (e.g., Chit-chat) to the recommendation scenario (e.g., Music) is\ncrucial for the Conversational Recommender System (CRS). Prior studies mainly\nfocus on planning the next dialog goal~(e.g., chat on a movie star) conditioned\non the previous dialog. However, we find the dialog goals can be simultaneously\nobserved at different levels, which can be utilized to improve CRS. In this\npaper, we propose Dual-space Hierarchical Learning (DHL) to leverage\nmulti-level goal sequences and their hierarchical relationships for\nconversational recommendation. Specifically, we exploit multi-level goal\nsequences from both the representation space and the optimization space. In the\nrepresentation space, we propose the hierarchical representation learning where\na cross attention module derives mutually enhanced multi-level goal\nrepresentations. In the optimization space, we devise the hierarchical weight\nlearning to reweight lower-level goal sequences, and introduce bi-level\noptimization for stable update. Additionally, we propose a soft labeling\nstrategy to guide optimization gradually. Experiments on two real-world\ndatasets verify the effectiveness of our approach. Code and data are available\nhere.",
        "translated": ""
    },
    {
        "title": "Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports",
        "url": "http://arxiv.org/abs/2401.01883v1",
        "pub_date": "2024-01-03",
        "summary": "Defending from cyberattacks requires practitioners to operate on high-level\nadversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack\nincidents describe the chain of malicious actions with respect to time. To\navoid repeating cyberattack incidents, practitioners must proactively identify\nand defend against recurring chain of actions - which we refer to as temporal\nattack patterns. Automatically mining the patterns among actions provides\nstructured and actionable information on the adversary behavior of past\ncyberattacks. The goal of this paper is to aid security practitioners in\nprioritizing and proactive defense against cyberattacks by mining temporal\nattack patterns from cyberthreat intelligence reports. To this end, we propose\nChronoCTI, an automated pipeline for mining temporal attack patterns from\ncyberthreat intelligence (CTI) reports of past cyberattacks. To construct\nChronoCTI, we build the ground truth dataset of temporal attack patterns and\napply state-of-the-art large language models, natural language processing, and\nmachine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,\nwhere we identify 124 temporal attack patterns - which we categorize into nine\npattern categories. We identify that the most prevalent pattern category is to\ntrick victim users into executing malicious code to initiate the attack,\nfollowed by bypassing the anti-malware system in the victim network. Based on\nthe observed patterns, we advocate organizations to train users about\ncybersecurity best practices, introduce immutable operating systems with\nlimited functionalities, and enforce multi-user authentications. Moreover, we\nadvocate practitioners to leverage the automated mining capability of ChronoCTI\nand design countermeasures against the recurring attack patterns.",
        "translated": ""
    },
    {
        "title": "Concurrent Brainstorming &amp; Hypothesis Satisfying: An Iterative Framework\n  for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR)",
        "url": "http://arxiv.org/abs/2401.01835v1",
        "pub_date": "2024-01-03",
        "summary": "Addressing the complexity of comprehensive information retrieval, this study\nintroduces an innovative, iterative retrieval-augmented generation system. Our\napproach uniquely integrates a vector-space driven re-ranking mechanism with\nconcurrent brainstorming to expedite the retrieval of highly relevant\ndocuments, thereby streamlining the generation of potential queries. This sets\nthe stage for our novel hybrid process, which synergistically combines\nhypothesis formulation with satisfying decision-making strategy to determine\ncontent adequacy, leveraging a chain of thought-based prompting technique. This\nunified hypothesize-satisfied phase intelligently distills information to\nascertain whether user queries have been satisfactorily addressed. Upon\nreaching this criterion, the system refines its output into a concise\nrepresentation, maximizing conceptual density with minimal verbosity. The\niterative nature of the workflow enhances process efficiency and accuracy.\nCrucially, the concurrency within the brainstorming phase significantly\naccelerates recursive operations, facilitating rapid convergence to solution\nsatisfaction. Compared to conventional methods, our system demonstrates a\nmarked improvement in computational time and cost-effectiveness. This research\nadvances the state-of-the-art in intelligent retrieval systems, setting a new\nbenchmark for resource-efficient information extraction and abstraction in\nknowledge-intensive applications.",
        "translated": ""
    },
    {
        "title": "Physio: An LLM-Based Physiotherapy Advisor",
        "url": "http://arxiv.org/abs/2401.01825v1",
        "pub_date": "2024-01-03",
        "summary": "The capabilities of the most recent language models have increased the\ninterest in integrating them into real-world applications. However, the fact\nthat these models generate plausible, yet incorrect text poses a constraint\nwhen considering their use in several domains. Healthcare is a prime example of\na domain where text-generative trustworthiness is a hard requirement to\nsafeguard patient well-being. In this paper, we present Physio, a chat-based\napplication for physical rehabilitation. Physio is capable of making an initial\ndiagnosis while citing reliable health sources to support the information\nprovided. Furthermore, drawing upon external knowledge databases, Physio can\nrecommend rehabilitation exercises and over-the-counter medication for symptom\nrelief. By combining these features, Physio can leverage the power of\ngenerative models for language processing while also conditioning its response\non dependable and verifiable sources. A live demo of Physio is available at\nhttps://physio.inesctec.pt.",
        "translated": ""
    },
    {
        "title": "Evaluating Trustworthiness of Online News Publishers via Article\n  Classification",
        "url": "http://arxiv.org/abs/2401.01781v1",
        "pub_date": "2024-01-03",
        "summary": "The proliferation of low-quality online information in today's era has\nunderscored the need for robust and automatic mechanisms to evaluate the\ntrustworthiness of online news publishers. In this paper, we analyse the\ntrustworthiness of online news media outlets by leveraging a dataset of 4033\nnews stories from 40 different sources. We aim to infer the trustworthiness\nlevel of the source based on the classification of individual articles'\ncontent. The trust labels are obtained from NewsGuard, a journalistic\norganization that evaluates news sources using well-established editorial and\npublishing criteria. The results indicate that the classification model is\nhighly effective in classifying the trustworthiness levels of the news\narticles. This research has practical applications in alerting readers to\npotentially untrustworthy news sources, assisting journalistic organizations in\nevaluating new or unfamiliar media outlets and supporting the selection of\narticles for their trustworthiness assessment.",
        "translated": ""
    },
    {
        "title": "Navigating Uncertainty: Optimizing API Dependency for Hallucination\n  Reduction in Closed-Book Question Answering",
        "url": "http://arxiv.org/abs/2401.01780v1",
        "pub_date": "2024-01-03",
        "summary": "While Large Language Models (LLM) are able to accumulate and restore\nknowledge, they are still prone to hallucination. Especially when faced with\nfactual questions, LLM cannot only rely on knowledge stored in parameters to\nguarantee truthful and correct answers. Augmenting these models with the\nability to search on external information sources, such as the web, is a\npromising approach to ground knowledge to retrieve information. However,\nsearching in a large collection of documents introduces additional\ncomputational/time costs. An optimal behavior would be to query external\nresources only when the LLM is not confident about answers. In this paper, we\npropose a new LLM able to self-estimate if it is able to answer directly or\nneeds to request an external tool. We investigate a supervised approach by\nintroducing a hallucination masking mechanism in which labels are generated\nusing a close book question-answering task. In addition, we propose to leverage\nparameter-efficient fine-tuning techniques to train our model on a small amount\nof data. Our model directly provides answers for $78.2\\%$ of the known queries\nand opts to search for $77.2\\%$ of the unknown ones. This results in the API\nbeing utilized only $62\\%$ of the time.",
        "translated": ""
    },
    {
        "title": "Text mining arXiv: a look through quantitative finance papers",
        "url": "http://arxiv.org/abs/2401.01751v1",
        "pub_date": "2024-01-03",
        "summary": "This paper explores articles hosted on the arXiv preprint server with the aim\nto uncover valuable insights hidden in this vast collection of research.\nEmploying text mining techniques and through the application of natural\nlanguage processing methods, we examine the contents of quantitative finance\npapers posted in arXiv from 1997 to 2022. We extract and analyze crucial\ninformation from the entire documents, including the references, to understand\nthe topics trends over time and to find out the most cited researchers and\njournals on this domain. Additionally, we compare numerous algorithms to\nperform topic modeling, including state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "Evaluating Large Language Models in Semantic Parsing for Conversational\n  Question Answering over Knowledge Graphs",
        "url": "http://arxiv.org/abs/2401.01711v1",
        "pub_date": "2024-01-03",
        "summary": "Conversational question answering systems often rely on semantic parsing to\nenable interactive information retrieval, which involves the generation of\nstructured database queries from a natural language input. For\ninformation-seeking conversations about facts stored within a knowledge graph,\ndialogue utterances are transformed into graph queries in a process that is\ncalled knowledge-based conversational question answering. This paper evaluates\nthe performance of large language models that have not been explicitly\npre-trained on this task. Through a series of experiments on an extensive\nbenchmark dataset, we compare models of varying sizes with different prompting\ntechniques and identify common issue types in the generated output. Our results\ndemonstrate that large language models are capable of generating graph queries\nfrom dialogues, with significant improvements achievable through few-shot\nprompting and fine-tuning techniques, especially for smaller models that\nexhibit lower zero-shot performance.",
        "translated": ""
    },
    {
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "url": "http://arxiv.org/abs/2401.01614v1",
        "pub_date": "2024-01-03",
        "summary": "The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents - it\ncan successfully complete 50% of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out not effective for web agents, and the best grounding strategy we\ndevelop in this paper leverages both the HTML text and visuals. Yet, there is\nstill a substantial gap with oracle grounding, leaving ample room for further\nimprovement.",
        "translated": ""
    },
    {
        "title": "Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial\n  Retrieval with Neural Rankers and Large Language Models",
        "url": "http://arxiv.org/abs/2401.01566v1",
        "pub_date": "2024-01-03",
        "summary": "We describe team ielab from CSIRO and The University of Queensland's approach\nto the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers\nbut to utilise Large Language Models to overcome the issue of lack of training\ndata for such rankers. Specifically, we employ ChatGPT to generate relevant\npatient descriptions for randomly selected clinical trials from the corpus.\nThis synthetic dataset, combined with human-annotated training data from\nprevious years, is used to train both dense and sparse retrievers based on\nPubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the\nsystem. To further enhance the effectiveness of our approach, we prompting\nGPT-4 as a TREC annotator to provide judgments on our run files. These\njudgments are subsequently employed to re-rank the results. This architecture\ntightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large\nLanguage Models, demonstrating a new approach to clinical trial retrieval.",
        "translated": ""
    },
    {
        "title": "Poisoning Attacks against Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2401.01527v1",
        "pub_date": "2024-01-03",
        "summary": "Modern recommender systems have seen substantial success, yet they remain\nvulnerable to malicious activities, notably poisoning attacks. These attacks\ninvolve injecting malicious data into the training datasets of RS, thereby\ncompromising their integrity and manipulating recommendation outcomes for\ngaining illicit profits. This survey paper provides a systematic and up-to-date\nreview of the research landscape on Poisoning Attacks against Recommendation\n(PAR). A novel and comprehensive taxonomy is proposed, categorizing existing\nPAR methodologies into three distinct categories: Component-Specific,\nGoal-Driven, and Capability Probing. For each category, we discuss its\nmechanism in detail, along with associated methods. Furthermore, this paper\nhighlights potential future research avenues in this domain. Additionally, to\nfacilitate and benchmark the empirical comparison of PAR, we introduce an\nopen-source library, ARLib, which encompasses a comprehensive collection of PAR\nmodels and common datasets. The library is released at\n\\url{https://github.com/CoderWZW/ARLib}.",
        "translated": ""
    },
    {
        "title": "Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy\n  and Directions",
        "url": "http://arxiv.org/abs/2401.02143v1",
        "pub_date": "2024-01-04",
        "summary": "In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural\nNetworks (GNNs), a domain where deep learning-based approaches have\nincreasingly shown superior performance in both classification and regression\ntasks compared to traditional methods. The survey highlights a critical gap in\ndeep neural TDL methods: the underrepresentation of latent correlations among\ndata instances and feature values. GNNs, with their innate capability to model\nintricate relationships and interactions between diverse elements of tabular\ndata, have garnered significant interest and application across various TDL\ndomains. Our survey provides a systematic review of the methods involved in\ndesigning and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed\ninvestigation into the foundational aspects and an overview of GNN-based TDL\nmethods, offering insights into their evolving landscape. We present a\ncomprehensive taxonomy focused on constructing graph structures and\nrepresentation learning within GNN-based TDL methods. In addition, the survey\nexamines various training plans, emphasizing the integration of auxiliary tasks\nto enhance the effectiveness of instance representations. A critical part of\nour discussion is dedicated to the practical application of GNNs across a\nspectrum of GNN4TDL scenarios, demonstrating their versatility and impact.\nLastly, we discuss the limitations and propose future research directions,\naiming to spur advancements in GNN4TDL. This survey serves as a resource for\nresearchers and practitioners, offering a thorough understanding of GNNs' role\nin revolutionizing TDL and pointing towards future innovations in this\npromising area.",
        "translated": ""
    },
    {
        "title": "Spectral-based Graph Neutral Networks for Complementary Item\n  Recommendation",
        "url": "http://arxiv.org/abs/2401.02130v1",
        "pub_date": "2024-01-04",
        "summary": "Modeling complementary relationships greatly helps recommender systems to\naccurately and promptly recommend the subsequent items when one item is\npurchased. Unlike traditional similar relationships, items with complementary\nrelationships may be purchased successively (such as iPhone and Airpods Pro),\nand they not only share relevance but also exhibit dissimilarity. Since the two\nattributes are opposites, modeling complementary relationships is challenging.\nPrevious attempts to exploit these relationships have either ignored or\noversimplified the dissimilarity attribute, resulting in ineffective modeling\nand an inability to balance the two attributes. Since Graph Neural Networks\n(GNNs) can capture the relevance and dissimilarity between nodes in the\nspectral domain, we can leverage spectral-based GNNs to effectively understand\nand model complementary relationships. In this study, we present a novel\napproach called Spectral-based Complementary Graph Neural Networks (SComGNN)\nthat utilizes the spectral properties of complementary item graphs. We make the\nfirst observation that complementary relationships consist of low-frequency and\nmid-frequency components, corresponding to the relevance and dissimilarity\nattributes, respectively. Based on this spectral observation, we design\nspectral graph convolutional networks with low-pass and mid-pass filters to\ncapture the low-frequency and mid-frequency components. Additionally, we\npropose a two-stage attention mechanism to adaptively integrate and balance the\ntwo attributes. Experimental results on four e-commerce datasets demonstrate\nthe effectiveness of our model, with SComGNN significantly outperforming\nexisting baseline models.",
        "translated": ""
    },
    {
        "title": "Starling: An I/O-Efficient Disk-Resident Graph Index Framework for\n  High-Dimensional Vector Similarity Search on Data Segment",
        "url": "http://arxiv.org/abs/2401.02116v1",
        "pub_date": "2024-01-04",
        "summary": "High-dimensional vector similarity search (HVSS) is receiving a spotlight as\na powerful tool for various data science and AI applications. As vector data\ngrows larger, in-memory indexes become extremely expensive because they\nnecessitate substantial expansion of main memory resources. One possible\nsolution is to use disk-based implementation, which stores and searches vector\ndata in high-performance devices like NVMe SSDs. However, HVSS for data\nsegments is still challenging in vector databases, where one machine has\nmultiple segments for system features (like scaling) purposes. In this setting,\neach segment has limited memory and disk space, so HVSS on the data segment\nneeds to balance accuracy, efficiency, and space cost. Existing disk-based\nmethods are sub-optimal because they do not consider all these requirements\ntogether. In this paper, we present Starling, an I/O-efficient disk-resident\ngraph index framework that optimizes data layout and search strategy in the\nsegment. It has two main components: (1) a data layout that includes an\nin-memory navigation graph and a reordered disk-based graph with locality\nenhancement, which reduces the search path length and disk bandwidth wastage;\nand (2) a block search strategy that minimizes expensive disk I/Os when\nexecuting a vector query. We conduct extensive experiments to verify Starling's\neffectiveness, efficiency, and scalability. On a data segment with 2GB memory\nand 10GB disk capacity, Starling can maintain up to 33 million vectors in 128\ndimensions, and serve HVSS with more than 0.9 average precision and top-10\nrecall rate, and latency of under 1 millisecond. The results show that Starling\nexhibits 43.9$\\times$ higher throughput with 98% lower query latency than\nstate-of-the-art methods under the same accuracy.",
        "translated": ""
    },
    {
        "title": "Tailor: Size Recommendations for High-End Fashion Marketplaces",
        "url": "http://arxiv.org/abs/2401.01978v1",
        "pub_date": "2024-01-03",
        "summary": "In the ever-changing and dynamic realm of high-end fashion marketplaces,\nproviding accurate and personalized size recommendations has become a critical\naspect. Meeting customer expectations in this regard is not only crucial for\nensuring their satisfaction but also plays a pivotal role in driving customer\nretention, which is a key metric for the success of any fashion retailer. We\npropose a novel sequence classification approach to address this problem,\nintegrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our\napproach comprises two distinct models: one employs LSTMs to encode the user\nsignals, while the other leverages an Attention mechanism. Our best model\noutperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions\nwe increase the user coverage by 24.5% when compared with only using Orders.\nMoreover, we evaluate the models' usability in real-time recommendation\nscenarios by conducting experiments to measure their latency performance.",
        "translated": ""
    },
    {
        "title": "Plug-in Diffusion Model for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2401.02913v1",
        "pub_date": "2024-01-05",
        "summary": "Pioneering efforts have verified the effectiveness of the diffusion models in\nexploring the informative uncertainty for recommendation. Considering the\ndifference between recommendation and image synthesis tasks, existing methods\nhave undertaken tailored refinements to the diffusion and reverse process.\nHowever, these approaches typically use the highest-score item in corpus for\nuser interest prediction, leading to the ignorance of the user's generalized\npreference contained within other items, thereby remaining constrained by the\ndata sparsity issue. To address this issue, this paper presents a novel Plug-in\nDiffusion Model for Recommendation (PDRec) framework, which employs the\ndiffusion model as a flexible plugin to jointly take full advantage of the\ndiffusion-generating user preferences on all items. Specifically, PDRec first\ninfers the users' dynamic preferences on all items via a time-interval\ndiffusion model and proposes a Historical Behavior Reweighting (HBR) mechanism\nto identify the high-quality behaviors and suppress noisy behaviors. In\naddition to the observed items, PDRec proposes a Diffusion-based Positive\nAugmentation (DPA) strategy to leverage the top-ranked unobserved items as the\npotential positive samples, bringing in informative and diverse soft signals to\nalleviate data sparsity. To alleviate the false negative sampling issue, PDRec\nemploys Noise-free Negative Sampling (NNS) to select stable negative samples\nfor ensuring effective model optimization. Extensive experiments and analyses\non four datasets have verified the superiority of the proposed PDRec over the\nstate-of-the-art baselines and showcased the universality of PDRec as a\nflexible plugin for commonly-used sequential encoders in different\nrecommendation scenarios. The code is available in\nhttps://github.com/hulkima/PDRec.",
        "translated": ""
    },
    {
        "title": "Let's Get It Started: Fostering the Discoverability of New Releases on\n  Deezer",
        "url": "http://arxiv.org/abs/2401.02827v1",
        "pub_date": "2024-01-05",
        "summary": "This paper presents our recent initiatives to foster the discoverability of\nnew releases on the music streaming service Deezer. After introducing our\nsearch and recommendation features dedicated to new releases, we outline our\nshift from editorial to personalized release suggestions using cold start\nembeddings and contextual bandits. Backed by online experiments, we discuss the\nadvantages of this shift in terms of recommendation quality and exposure of new\nreleases on the service.",
        "translated": ""
    },
    {
        "title": "DocGraphLM: Documental Graph Language Model for Information Extraction",
        "url": "http://arxiv.org/abs/2401.02823v1",
        "pub_date": "2024-01-05",
        "summary": "Advances in Visually Rich Document Understanding (VrDU) have enabled\ninformation extraction and question answering over documents with complex\nlayouts. Two tropes of architectures have emerged -- transformer-based models\ninspired by LLMs, and Graph Neural Networks. In this paper, we introduce\nDocGraphLM, a novel framework that combines pre-trained language models with\ngraph semantics. To achieve this, we propose 1) a joint encoder architecture to\nrepresent documents, and 2) a novel link prediction approach to reconstruct\ndocument graphs. DocGraphLM predicts both directions and distances between\nnodes using a convergent joint loss function that prioritizes neighborhood\nrestoration and downweighs distant node detection. Our experiments on three\nSotA datasets show consistent improvement on IE and QA tasks with the adoption\nof graph features. Moreover, we report that adopting the graph features\naccelerates convergence in the learning process during training, despite being\nsolely constructed through link prediction.",
        "translated": ""
    },
    {
        "title": "Variance Reduction in Ratio Metrics for Efficient Online Experiments",
        "url": "http://arxiv.org/abs/2401.04062v1",
        "pub_date": "2024-01-08",
        "summary": "Online controlled experiments, such as A/B-tests, are commonly used by modern\ntech companies to enable continuous system improvements. Despite their\nparamount importance, A/B-tests are expensive: by their very definition, a\npercentage of traffic is assigned an inferior system variant. To ensure\nstatistical significance on top-level metrics, online experiments typically run\nfor several weeks. Even then, a considerable amount of experiments will lead to\ninconclusive results (i.e. false negatives, or type-II error). The main culprit\nfor this inefficiency is the variance of the online metrics. Variance reduction\ntechniques have been proposed in the literature, but their direct applicability\nto commonly used ratio metrics (e.g. click-through rate or user retention) is\nlimited.\n  In this work, we successfully apply variance reduction techniques to ratio\nmetrics on a large-scale short-video platform: ShareChat. Our empirical results\nshow that we can either improve A/B-test confidence in 77% of cases, or can\nretain the same level of confidence with 30% fewer data points. Importantly, we\nshow that the common approach of including as many covariates as possible in\nregression is counter-productive, highlighting that control variates based on\nGradient-Boosted Decision Tree predictors are most effective. We discuss the\npracticalities of implementing these methods at scale and showcase the cost\nreduction they beget.",
        "translated": ""
    },
    {
        "title": "Unveiling Bias in Fairness Evaluations of Large Language Models: A\n  Critical Literature Review of Music and Movie Recommendation Systems",
        "url": "http://arxiv.org/abs/2401.04057v1",
        "pub_date": "2024-01-08",
        "summary": "The rise of generative artificial intelligence, particularly Large Language\nModels (LLMs), has intensified the imperative to scrutinize fairness alongside\naccuracy. Recent studies have begun to investigate fairness evaluations for\nLLMs within domains such as recommendations. Given that personalization is an\nintrinsic aspect of recommendation systems, its incorporation into fairness\nassessments is paramount. Yet, the degree to which current fairness evaluation\nframeworks account for personalization remains unclear. Our comprehensive\nliterature review aims to fill this gap by examining how existing frameworks\nhandle fairness evaluations of LLMs, with a focus on the integration of\npersonalization factors. Despite an exhaustive collection and analysis of\nrelevant works, we discovered that most evaluations overlook personalization, a\ncritical facet of recommendation systems, thereby inadvertently perpetuating\nunfair practices. Our findings shed light on this oversight and underscore the\nurgent need for more nuanced fairness evaluations that acknowledge\npersonalization. Such improvements are vital for fostering equitable\ndevelopment within the AI community.",
        "translated": ""
    },
    {
        "title": "Sparse Meets Dense: A Hybrid Approach to Enhance Scientific Document\n  Retrieval",
        "url": "http://arxiv.org/abs/2401.04055v1",
        "pub_date": "2024-01-08",
        "summary": "Traditional information retrieval is based on sparse bag-of-words vector\nrepresentations of documents and queries. More recent deep-learning approaches\nhave used dense embeddings learned using a transformer-based large language\nmodel. We show that on a classic benchmark on scientific document retrieval in\nthe medical domain of cystic fibrosis, that both of these models perform\nroughly equivalently. Notably, dense vectors from the state-of-the-art SPECTER2\nmodel do not significantly enhance performance. However, a hybrid model that we\npropose combining these methods yields significantly better results,\nunderscoring the merits of integrating classical and contemporary deep learning\ntechniques in information retrieval in the domain of specialized scientific\ndocuments.",
        "translated": ""
    },
    {
        "title": "Learning-to-Rank with Nested Feedback",
        "url": "http://arxiv.org/abs/2401.04053v1",
        "pub_date": "2024-01-08",
        "summary": "Many platforms on the web present ranked lists of content to users, typically\noptimized for engagement-, satisfaction- or retention- driven metrics. Advances\nin the Learning-to-Rank (LTR) research literature have enabled rapid growth in\nthis application area. Several popular interfaces now include nested lists,\nwhere users can enter a 2nd-level feed via any given 1st-level item. Naturally,\nthis has implications for evaluation metrics, objective functions, and the\nranking policies we wish to learn. We propose a theoretically grounded method\nto incorporate 2nd-level feedback into any 1st-level ranking model. Online\nexperiments on a large-scale recommendation system confirm our theoretical\nfindings.",
        "translated": ""
    },
    {
        "title": "The Impact of Differential Privacy on Recommendation Accuracy and\n  Popularity Bias",
        "url": "http://arxiv.org/abs/2401.03883v1",
        "pub_date": "2024-01-08",
        "summary": "Collaborative filtering-based recommender systems leverage vast amounts of\nbehavioral user data, which poses severe privacy risks. Thus, often, random\nnoise is added to the data to ensure Differential Privacy (DP). However, to\ndate, it is not well understood, in which ways this impacts personalized\nrecommendations. In this work, we study how DP impacts recommendation accuracy\nand popularity bias, when applied to the training data of state-of-the-art\nrecommendation models. Our findings are three-fold: First, we find that nearly\nall users' recommendations change when DP is applied. Second, recommendation\naccuracy drops substantially while recommended item popularity experiences a\nsharp increase, suggesting that popularity bias worsens. Third, we find that DP\nexacerbates popularity bias more severely for users who prefer unpopular items\nthan for users that prefer popular items.",
        "translated": ""
    },
    {
        "title": "Recognizing Similar Crises through the Application of Ontology-based\n  Knowledge Mining",
        "url": "http://arxiv.org/abs/2401.03770v1",
        "pub_date": "2024-01-08",
        "summary": "Recognizing and learning from similar crisis situations is crucial for the\ndevelopment of effective response strategies. This study addresses the\nchallenge of identifying similarities within a wide range of crisis-related\ninformation. To overcome this challenge, we employed an ontology-based crisis\nsituation knowledge base enriched with crisis-related information.\nAdditionally, we implemented a semantic similarity measure to assess the degree\nof similarity between crisis situations. Our investigation specifically focuses\non recognizing similar crises through the application of ontology-based\nknowledge mining. Through our experiments, we demonstrate the accuracy and\nefficiency of our approach to recognizing similar crises. These findings\nhighlight the potential of ontology-based knowledge mining for enhancing crisis\nrecognition processes and improving overall crisis management strategies.",
        "translated": ""
    },
    {
        "title": "Towards Efficient Communication Federated Recommendation System via\n  Low-rank Training",
        "url": "http://arxiv.org/abs/2401.03748v1",
        "pub_date": "2024-01-08",
        "summary": "In Federated Recommendation (FedRec) systems, communication costs are a\ncritical bottleneck that arises from the need to transmit neural network models\nbetween user devices and a central server. Prior approaches to these challenges\noften lead to issues such as computational overheads, model specificity\nconstraints, and compatibility issues with secure aggregation protocols. In\nresponse, we propose a novel framework, called Correlated Low-rank Structure\n(CoLR), which leverages the concept of adjusting lightweight trainable\nparameters while keeping most parameters frozen. Our approach substantially\nreduces communication overheads without introducing additional computational\nburdens. Critically, our framework remains fully compatible with secure\naggregation protocols, including the robust use of Homomorphic Encryption. Our\napproach resulted in a reduction of up to 93.75% in payload size, with only an\napproximate 8% decrease in recommendation performance across datasets. Code for\nreproducing our experiments can be found at\nhttps://github.com/NNHieu/CoLR-FedRec.",
        "translated": ""
    },
    {
        "title": "Reproducibility Analysis and Enhancements for Multi-Aspect Dense\n  Retriever with Aspect Learning",
        "url": "http://arxiv.org/abs/2401.03648v1",
        "pub_date": "2024-01-08",
        "summary": "Multi-aspect dense retrieval aims to incorporate aspect information (e.g.,\nbrand and category) into dual encoders to facilitate relevance matching. As an\nearly and representative multi-aspect dense retriever, MADRAL learns several\nextra aspect embeddings and fuses the explicit aspects with an implicit aspect\n\"OTHER\" for final representation. MADRAL was evaluated on proprietary data and\nits code was not released, making it challenging to validate its effectiveness\non other datasets. We failed to reproduce its effectiveness on the public\nMA-Amazon data, motivating us to probe the reasons and re-examine its\ncomponents. We propose several component alternatives for comparisons,\nincluding replacing \"OTHER\" with \"CLS\" and representing aspects with the first\nseveral content tokens. Through extensive experiments, we confirm that learning\n\"OTHER\" from scratch in aspect fusion is harmful. In contrast, our proposed\nvariants can greatly enhance the retrieval performance. Our research not only\nsheds light on the limitations of MADRAL but also provides valuable insights\nfor future studies on more powerful multi-aspect dense retrieval models. Code\nwill be released at:\nhttps://github.com/sunxiaojie99/Reproducibility-for-MADRAL.",
        "translated": ""
    },
    {
        "title": "Bridging the Skills Gap: Evaluating an AI-Assisted Provider Platform to\n  Support Care Providers with Empathetic Delivery of Protocolized Therapy",
        "url": "http://arxiv.org/abs/2401.03631v1",
        "pub_date": "2024-01-08",
        "summary": "Despite the high prevalence and burden of mental health conditions, there is\na global shortage of mental health providers. Artificial Intelligence (AI)\nmethods have been proposed as a way to address this shortage, by supporting\nproviders with less extensive training as they deliver care. To this end, we\ndeveloped the AI-Assisted Provider Platform (A2P2), a text-based virtual\ntherapy interface that includes a response suggestion feature, which supports\nproviders in delivering protocolized therapies empathetically. We studied\nproviders with and without expertise in mental health treatment delivering a\ntherapy session using the platform with (intervention) and without (control)\nAI-assistance features. Upon evaluation, the AI-assisted system significantly\ndecreased response times by 29.34% (p=0.002), tripled empathic response\naccuracy (p=0.0001), and increased goal recommendation accuracy by 66.67%\n(p=0.001) across both user groups compared to the control. Both groups rated\nthe system as having excellent usability.",
        "translated": ""
    },
    {
        "title": "ChatGPT for Conversational Recommendation: Refining Recommendations by\n  Reprompting with Feedback",
        "url": "http://arxiv.org/abs/2401.03605v1",
        "pub_date": "2024-01-07",
        "summary": "Recommendation algorithms have been pivotal in handling the overwhelming\nvolume of online content. However, these algorithms seldom consider direct user\ninput, resulting in superficial interaction between them. Efforts have been\nmade to include the user directly in the recommendation process through\nconversation, but these systems too have had limited interactivity. Recently,\nLarge Language Models (LLMs) like ChatGPT have gained popularity due to their\nease of use and their ability to adapt dynamically to various tasks while\nresponding to feedback. In this paper, we investigate the effectiveness of\nChatGPT as a top-n conversational recommendation system. We build a rigorous\npipeline around ChatGPT to simulate how a user might realistically probe the\nmodel for recommendations: by first instructing and then reprompting with\nfeedback to refine a set of recommendations. We further explore the effect of\npopularity bias in ChatGPT's recommendations, and compare its performance to\nbaseline models. We find that reprompting ChatGPT with feedback is an effective\nstrategy to improve recommendation relevancy, and that popularity bias can be\nmitigated through prompt engineering.",
        "translated": ""
    },
    {
        "title": "Analyzing Coherency in Facet-based Clarification Prompt Generation for\n  Search",
        "url": "http://arxiv.org/abs/2401.04524v1",
        "pub_date": "2024-01-09",
        "summary": "Clarifying user's information needs is an essential component of modern\nsearch systems. While most of the approaches for constructing clarifying\nprompts rely on query facets, the impact of the quality of the facets is\nrelatively unexplored. In this work, we concentrate on facet quality through\nthe notion of facet coherency and assess its importance for overall usefulness\nfor clarification in search. We find that existing evaluation procedures do not\naccount for facet coherency, as evident by the poor correlation of coherency\nwith automated metrics. Moreover, we propose a coherency classifier and assess\nthe prevalence of incoherent facets in a well-established dataset on\nclarification. Our findings can serve as motivation for future work on the\ntopic.",
        "translated": ""
    },
    {
        "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented\n  Code Search",
        "url": "http://arxiv.org/abs/2401.04514v1",
        "pub_date": "2024-01-09",
        "summary": "In code search, the Generation-Augmented Retrieval (GAR) framework, which\ngenerates exemplar code snippets to augment queries, has emerged as a promising\nstrategy to address the principal challenge of modality misalignment between\ncode snippets and natural language queries, particularly with the demonstrated\ncode generation capabilities of Large Language Models (LLMs). Nevertheless, our\npreliminary investigations indicate that the improvements conferred by such an\nLLM-augmented framework are somewhat constrained. This limitation could\npotentially be ascribed to the fact that the generated codes, albeit\nfunctionally accurate, frequently display a pronounced stylistic deviation from\nthe ground truth code in the codebase. In this paper, we extend the\nfoundational GAR framework and propose a simple yet effective method that\nadditionally Rewrites the Code (ReCo) within the codebase for style\nnormalization. Experimental results demonstrate that ReCo significantly boosts\nretrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),\nand fine-tuned dense (up to 23.6%) retrieval settings in diverse search\nscenarios. To further elucidate the advantages of ReCo and stimulate research\nin code style normalization, we introduce Code Style Similarity, the first\nmetric tailored to quantify stylistic similarities in code. Notably, our\nempirical findings reveal the inadequacy of existing metrics in capturing\nstylistic nuances.",
        "translated": ""
    },
    {
        "title": "Combining Embedding-Based and Semantic-Based Models for Post-hoc\n  Explanations in Recommender Systems",
        "url": "http://arxiv.org/abs/2401.04474v1",
        "pub_date": "2024-01-09",
        "summary": "In today's data-rich environment, recommender systems play a crucial role in\ndecision support systems. They provide to users personalized recommendations\nand explanations about these recommendations. Embedding-based models, despite\ntheir widespread use, often suffer from a lack of interpretability, which can\nundermine trust and user engagement. This paper presents an approach that\ncombines embedding-based and semantic-based models to generate post-hoc\nexplanations in recommender systems, leveraging ontology-based knowledge graphs\nto improve interpretability and explainability. By organizing data within a\nstructured framework, ontologies enable the modeling of intricate relationships\nbetween entities, which is essential for generating explanations. By combining\nembedding-based and semantic based models for post-hoc explanations in\nrecommender systems, the framework we defined aims at producing meaningful and\neasy-to-understand explanations, enhancing user trust and satisfaction, and\npotentially promoting the adoption of recommender systems across the e-commerce\nsector.",
        "translated": ""
    },
    {
        "title": "Privacy-Preserving Sequential Recommendation with Collaborative\n  Confusion",
        "url": "http://arxiv.org/abs/2401.04423v1",
        "pub_date": "2024-01-09",
        "summary": "Sequential recommendation has attracted a lot of attention from both academia\nand industry, however the privacy risks associated to gathering and\ntransferring users' personal interaction data are often underestimated or\nignored. Existing privacy-preserving studies are mainly applied to traditional\ncollaborative filtering or matrix factorization rather than sequential\nrecommendation. Moreover, these studies are mostly based on differential\nprivacy or federated learning, which often leads to significant performance\ndegradation, or has high requirements for communication. In this work, we\naddress privacy-preserving from a different perspective. Unlike existing\nresearch, we capture collaborative signals of neighbor interaction sequences\nand directly inject indistinguishable items into the target sequence before the\nrecommendation process begins, thereby increasing the perplexity of the target\nsequence. Even if the target interaction sequence is obtained by attackers, it\nis difficult to discern which ones are the actual user interaction records. To\nachieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer,\nnamely CLOUD, which incorporates a collaborative confusion mechanism to edit\nthe raw interaction sequences before conducting recommendation. Specifically,\nCLOUD first calculates the similarity between the target interaction sequence\nand other neighbor sequences to find similar sequences. Then, CLOUD considers\nthe shared representation of the target sequence and similar sequences to\ndetermine the operation to be performed: keep, delete, or insert. We design a\ncopy mechanism to make items from similar sequences have a higher probability\nto be inserted into the target sequence. Finally, the modified sequence is used\nto train the recommender and predict the next item.",
        "translated": ""
    },
    {
        "title": "Fine-Grained Embedding Dimension Optimization During Training for\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2401.04408v1",
        "pub_date": "2024-01-09",
        "summary": "Huge embedding tables in modern Deep Learning Recommender Models (DLRM)\nrequire prohibitively large memory during training and inference. Aiming to\nreduce the memory footprint of training, this paper proposes FIne-grained\nIn-Training Embedding Dimension optimization (FIITED). Given the observation\nthat embedding vectors are not equally important, FIITED adjusts the dimension\nof each individual embedding vector continuously during training, assigning\nlonger dimensions to more important embeddings while adapting to dynamic\nchanges in data. A novel embedding storage system based on virtually-hashed\nphysically-indexed hash tables is designed to efficiently implement the\nembedding dimension adjustment and effectively enable memory saving.\nExperiments on two industry models show that FIITED is able to reduce the size\nof embeddings by more than 65% while maintaining the trained model's quality,\nsaving significantly more memory than a state-of-the-art in-training embedding\npruning method. On public click-through rate prediction datasets, FIITED is\nable to prune up to 93.75%-99.75% embeddings without significant accuracy loss.",
        "translated": ""
    },
    {
        "title": "G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2401.04338v1",
        "pub_date": "2024-01-09",
        "summary": "Recently, a new paradigm, meta learning, has been widely applied to Deep\nLearning Recommendation Models (DLRM) and significantly improves statistical\nperformance, especially in cold-start scenarios. However, the existing systems\nare not tailored for meta learning based DLRM models and have critical problems\nregarding efficiency in distributed training in the GPU cluster. It is because\nthe conventional deep learning pipeline is not optimized for two task-specific\ndatasets and two update loops in meta learning. This paper provides a\nhigh-performance framework for large-scale training for Optimization-based Meta\nDLRM models over the \\textbf{G}PU cluster, namely \\textbf{G}-Meta. Firstly,\nG-Meta utilizes both data parallelism and model parallelism with careful\norchestration regarding computation and communication efficiency, to enable\nhigh-speed distributed training. Secondly, it proposes a Meta-IO pipeline for\nefficient data ingestion to alleviate the I/O bottleneck. Various experimental\nresults show that G-Meta achieves notable training speed without loss of\nstatistical performance. Since early 2022, G-Meta has been deployed in Alipay's\ncore advertising and recommender system, shrinking the continuous delivery of\nmodels by four times. It also obtains 6.48\\% improvement in Conversion Rate\n(CVR) and 1.06\\% increase in CPM (Cost Per Mille) in Alipay's homepage display\nadvertising, with the benefit of larger training samples and tasks.",
        "translated": ""
    },
    {
        "title": "Divergent Characteristics of Biomedical Research across Publication\n  Types: A Quantitative Analysis on the Aging-related Research",
        "url": "http://arxiv.org/abs/2401.04323v1",
        "pub_date": "2024-01-09",
        "summary": "This paper investigates differences in characteristics across publication\ntypes for aging-related genetic research. We utilized bibliometric data for\nfive model species retrieved from authoritative databases including PubMed.\nPublications are classified into types according to PubMed. Results indicate\nsubstantial divergence across publication types in attention paid to\naging-related research, scopes of studied genes, and topical preferences. For\ninstance, comparative studies and meta-analyses show a greater focus on aging\nthan validation studies. Reviews concentrate more on cell biology while\nclinical studies emphasize translational topics. Publication types also\nmanifest variations in highly studied genes, like APOE for reviews versus GH1\nfor clinical studies. Despite differences, top genes like insulin are\nuniversally emphasized. Publication types demonstrate similar levels of\nimbalance in research efforts to genes. Differences also exist in bibliometrics\nlike authorship numbers, citation counts, etc. Publication types show distinct\npreferences for journals of certain topical specialties and scope of\nreadership. Overall, findings showcase distinct characteristics of publication\ntypes in studying aging-related genetics, owing to their unique nature and\nobjectives. This study is the first endeavor to systematically depict the\ninherent structure of a biomedical research field from the perspective of\npublication types and provides insights into knowledge production and\nevaluation patterns across biomedical communities.",
        "translated": ""
    },
    {
        "title": "Prompt-based Multi-interest Learning Method for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2401.04312v1",
        "pub_date": "2024-01-09",
        "summary": "Multi-interest learning method for sequential recommendation aims to predict\nthe next item according to user multi-faceted interests given the user\nhistorical interactions. Existing methods mainly consist of two modules: the\nmulti-interest extraction module that learns user multi-interest embeddings to\ncapture the user multi-interests, and the multi-interest weight prediction\nmodule that learns the weight of each interest for aggregating the learned\nmulti-interest embeddings to derive the user embedding, used for predicting the\nuser rating to an item. Despite their effectiveness, existing methods have two\nkey limitations: 1) they directly feed the user interactions into the two\nmodules, while ignoring their different learning objectives, and 2) they merely\nconsider the centrality of the user interactions to learn the user\nmulti-interests, while overlooking their dispersion. To tackle these\nlimitations, we propose a prompt-based multi-interest learning method (PoMRec),\nwhere specific prompts are inserted into user interactions to make them\nadaptive to different learning objectives of the two modules. Moreover, we\nutilize both the mean and variance embeddings of user interactions to derive\nthe user multi-interest embeddings for comprehensively model the user\nmulti-interests. We conduct extensive experiments on two public datasets, and\nthe results verify that our proposed PoMRec outperforms the state-of-the-art\nmulti-interest learning methods.",
        "translated": ""
    },
    {
        "title": "Knowledge Sharing in Manufacturing using Large Language Models: User\n  Evaluation and Model Benchmarking",
        "url": "http://arxiv.org/abs/2401.05200v1",
        "pub_date": "2024-01-10",
        "summary": "Managing knowledge efficiently is crucial for organizational success. In\nmanufacturing, operating factories has become increasing knowledge-intensive\nputting strain on the factory's capacity to train and support new operators. In\nthis paper, we introduce a Large Language Model (LLM)-based system designed to\nuse the extensive knowledge contained in factory documentation. The system aims\nto efficiently answer queries from operators and facilitate the sharing of new\nknowledge. To assess its effectiveness, we conducted an evaluation in a factory\nsetting. The results of this evaluation demonstrated the system's benefits;\nnamely, in enabling quicker information retrieval and more efficient resolution\nof issues. However, the study also highlighted a preference for learning from a\nhuman expert when such an option is available. Furthermore, we benchmarked\nseveral closed and open-sourced LLMs for this system. GPT-4 consistently\noutperformed its counterparts, with open-source models like StableBeluga2\ntrailing closely, presenting an attractive option given its data privacy and\ncustomization benefits. Overall, this work offers preliminary insights for\nfactories considering using LLM-tools for knowledge management.",
        "translated": ""
    },
    {
        "title": "Adaptive Hardness Negative Sampling for Collaborative Filtering",
        "url": "http://arxiv.org/abs/2401.05191v1",
        "pub_date": "2024-01-10",
        "summary": "Negative sampling is essential for implicit collaborative filtering to\nprovide proper negative training signals so as to achieve desirable\nperformance. We experimentally unveil a common limitation of all existing\nnegative sampling methods that they can only select negative samples of a fixed\nhardness level, leading to the false positive problem (FPP) and false negative\nproblem (FNP). We then propose a new paradigm called adaptive hardness negative\nsampling (AHNS) and discuss its three key criteria. By adaptively selecting\nnegative samples with appropriate hardnesses during the training process, AHNS\ncan well mitigate the impacts of FPP and FNP. Next, we present a concrete\ninstantiation of AHNS called AHNS_{p&lt;0}, and theoretically demonstrate that\nAHNS_{p&lt;0} can fit the three criteria of AHNS well and achieve a larger lower\nbound of normalized discounted cumulative gain. Besides, we note that existing\nnegative sampling methods can be regarded as more relaxed cases of AHNS.\nFinally, we conduct comprehensive experiments, and the results show that\nAHNS_{p&lt;0} can consistently and substantially outperform several\nstate-of-the-art competitors on multiple datasets.",
        "translated": ""
    },
    {
        "title": "On the Influence of Reading Sequences on Knowledge Gain during Web\n  Search",
        "url": "http://arxiv.org/abs/2401.05148v1",
        "pub_date": "2024-01-10",
        "summary": "Nowadays, learning increasingly involves the usage of search engines and web\nresources. The related interdisciplinary research field search as learning aims\nto understand how people learn on the web. Previous work has investigated\nseveral feature classes to predict, for instance, the expected knowledge gain\nduring web search. Therein, eye-tracking features have not been extensively\nstudied so far. In this paper, we extend a previously used reading model from a\nline-based one to one that can detect reading sequences across multiple lines.\nWe use publicly available study data from a web-based learning task to examine\nthe relationship between our feature set and the participants' test scores. Our\nfindings demonstrate that learners with higher knowledge gain spent\nsignificantly more time reading, and processing more words in total. We also\nfind evidence that faster reading at the expense of more backward regressions\nmay be an indicator of better web-based learning. We make our code publicly\navailable at https://github.com/TIBHannover/reading_web_search.",
        "translated": ""
    },
    {
        "title": "SARA: A Collection of Sensitivity-Aware Relevance Assessments",
        "url": "http://arxiv.org/abs/2401.05144v1",
        "pub_date": "2024-01-10",
        "summary": "Large archival collections, such as email or government documents, must be\nmanually reviewed to identify any sensitive information before the collection\ncan be released publicly. Sensitivity classification has received a lot of\nattention in the literature. However, more recently, there has been increasing\ninterest in developing sensitivity-aware search engines that can provide users\nwith relevant search results, while ensuring that no sensitive documents are\nreturned to the user. Sensitivity-aware search would mitigate the need for a\nmanual sensitivity review prior to collections being made available publicly.\nTo develop such systems, there is a need for test collections that contain\nrelevance assessments for a set of information needs as well as ground-truth\nlabels for a variety of sensitivity categories. The well-known Enron email\ncollection contains a classification ground-truth that can be used to represent\nsensitive information, e.g., the Purely Personal and Personal but in\nProfessional Context categories can be used to represent sensitive personal\ninformation. However, the existing Enron collection does not contain a set of\ninformation needs and relevance assessments. In this work, we present a\ncollection of fifty information needs (topics) with crowdsourced query\nformulations (3 per topic) and relevance assessments (11,471 in total) for the\nEnron collection (mean number of relevant documents per topic = 11, variance =\n34.7). The developed information needs, queries and relevance judgements are\navailable on GitHub and will be available along with the existing Enron\ncollection through the popular ir_datasets library. Our proposed collection\nresults in the first freely available test collection for developing\nsensitivity-aware search systems.",
        "translated": ""
    },
    {
        "title": "Prompting Large Language Models for Recommender Systems: A Comprehensive\n  Framework and Empirical Analysis",
        "url": "http://arxiv.org/abs/2401.04997v1",
        "pub_date": "2024-01-10",
        "summary": "Recently, large language models such as ChatGPT have showcased remarkable\nabilities in solving general tasks, demonstrating the potential for\napplications in recommender systems. To assess how effectively LLMs can be used\nin recommendation tasks, our study primarily focuses on employing LLMs as\nrecommender systems through prompting engineering. We propose a general\nframework for utilizing LLMs in recommendation tasks, focusing on the\ncapabilities of LLMs as recommenders. To conduct our analysis, we formalize the\ninput of LLMs for recommendation into natural language prompts with two key\naspects, and explain how our framework can be generalized to various\nrecommendation scenarios. As for the use of LLMs as recommenders, we analyze\nthe impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the\nclassification of LLMs. As for prompt engineering, we further analyze the\nimpact of four important components of prompts, \\ie task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In\neach section, we first define and categorize concepts in line with the existing\nliterature. Then, we propose inspiring research questions followed by\nexperiments to systematically analyze the impact of different factors on two\npublic datasets. Finally, we summarize promising directions to shed lights on\nfuture research.",
        "translated": ""
    },
    {
        "title": "A Survey on Cross-Domain Sequential Recommendation",
        "url": "http://arxiv.org/abs/2401.04971v1",
        "pub_date": "2024-01-10",
        "summary": "Cross-domain sequential recommendation (CDSR) shifts the modeling of user\npreferences from flat to stereoscopic by integrating and learning interaction\ninformation from multiple domains at different granularities (ranging from\ninter-sequence to intra-sequence and from single-domain to cross-domain).In\nthis survey, we initially define the CDSR problem using a four-dimensional\ntensor and then analyze its multi-type input representations under\nmultidirectional dimensionality reductions. Following that, we provide a\nsystematic overview from both macro and micro views. From a macro view, we\nabstract the multi-level fusion structures of various models across domains and\ndiscuss their bridges for fusion. From a micro view, focusing on the existing\nmodels, we specifically discuss the basic technologies and then explain the\nauxiliary learning technologies. Finally, we exhibit the available public\ndatasets and the representative experimental results as well as provide some\ninsights into future directions for research in CDSR.",
        "translated": ""
    },
    {
        "title": "Improving Tag-Clouds as Visual Information Retrieval Interfaces",
        "url": "http://arxiv.org/abs/2401.04947v1",
        "pub_date": "2024-01-10",
        "summary": "Tagging-based systems enable users to categorize web resources by means of\ntags (freely chosen keywords), in order to refinding these resources later.\nTagging is implicitly also a social indexing process, since users share their\ntags and resources, constructing a social tag index, so-called folksonomy. At\nthe same time of tagging-based system, has been popularised an interface model\nfor visual information retrieval known as Tag-Cloud. In this model, the most\nfrequently used tags are displayed in alphabetical order. This paper presents a\nnovel approach to Tag-Cloud's tags selection, and proposes the use of\nclustering algorithms for visual layout, with the aim of improve browsing\nexperience. The results suggest that presented approach reduces the semantic\ndensity of tag set, and improves the visual consistency of Tag-Cloud layout.",
        "translated": ""
    },
    {
        "title": "DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation",
        "url": "http://arxiv.org/abs/2401.04914v1",
        "pub_date": "2024-01-10",
        "summary": "Learning precise representations of users and items to fit observed\ninteraction data is the fundamental task of collaborative filtering. Existing\nstudies usually infer entangled representations to fit such interaction data,\nneglecting to model the diverse matching relationships between users and items\nbehind their interactions, leading to limited performance and weak\ninterpretability. To address this problem, we propose a Dual Disentangled\nVariational AutoEncoder (DualVAE) for collaborative recommendation, which\ncombines disentangled representation learning with variational inference to\nfacilitate the generation of implicit interaction data. Specifically, we first\nimplement the disentangling concept by unifying an attention-aware dual\ndisentanglement and disentangled variational autoencoder to infer the\ndisentangled latent representations of users and items. Further, to encourage\nthe correspondence and independence of disentangled representations of users\nand items, we design a neighborhood-enhanced representation constraint with a\ncustomized contrastive mechanism to improve the representation quality.\nExtensive experiments on three real-world benchmarks show that our proposed\nmodel significantly outperforms several recent state-of-the-art baselines.\nFurther empirical experimental results also illustrate the interpretability of\nthe disentangled representations learned by DualVAE.",
        "translated": ""
    },
    {
        "title": "User Embedding Model for Personalized Language Prompting",
        "url": "http://arxiv.org/abs/2401.04858v1",
        "pub_date": "2024-01-10",
        "summary": "Modeling long histories plays a pivotal role in enhancing recommendation\nsystems, allowing to capture user's evolving preferences, resulting in more\nprecise and personalized recommendations. In this study we tackle the\nchallenges of modeling long user histories for preference understanding in\nnatural language. Specifically, we introduce a new User Embedding Module (UEM)\nthat efficiently processes user history in free-form text by compressing and\nrepresenting them as embeddings, to use them as soft prompts to a LM. Our\nexperiments demonstrate the superior capability of this approach in handling\nsignificantly longer histories compared to conventional text based prompting\nmethods, yielding substantial improvements in predictive performance. The main\ncontribution of this research is to demonstrate the ability to bias language\nmodels with user signals represented as embeddings.",
        "translated": ""
    },
    {
        "title": "Answer Retrieval in Legal Community Question Answering",
        "url": "http://arxiv.org/abs/2401.04852v1",
        "pub_date": "2024-01-09",
        "summary": "The task of answer retrieval in the legal domain aims to help users to seek\nrelevant legal advice from massive amounts of professional responses. Two main\nchallenges hinder applying existing answer retrieval approaches in other\ndomains to the legal domain: (1) a huge knowledge gap between lawyers and\nnon-professionals; and (2) a mix of informal and formal content on legal QA\nwebsites. To tackle these challenges, we propose CE_FS, a novel cross-encoder\n(CE) re-ranker based on the fine-grained structured inputs. CE_FS uses\nadditional structured information in the CQA data to improve the effectiveness\nof cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world\nbenchmark dataset for evaluating answer retrieval in the legal domain.\nExperiments conducted on LegalQA show that our proposed method significantly\noutperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel\nfinding is that adding the question tags of each question besides the question\ndescription and title into the input of cross-encoder re-rankers structurally\nboosts the rankers' effectiveness. While we study our proposed method in the\nlegal domain, we believe that our method can be applied in similar applications\nin other domains.",
        "translated": ""
    },
    {
        "title": "Improved Capacity Outer Bound for Private Quadratic Monomial Computation",
        "url": "http://arxiv.org/abs/2401.06125v1",
        "pub_date": "2024-01-11",
        "summary": "In private computation, a user wishes to retrieve a function evaluation of\nmessages stored on a set of databases without revealing the function's identity\nto the databases. Obead \\emph{et al.} introduced a capacity outer bound for\nprivate nonlinear computation, dependent on the order of the candidate\nfunctions. Focusing on private \\emph{quadratic monomial} computation, we\npropose three methods for ordering candidate functions: a graph edge-coloring\nmethod, a graph-distance method, and an entropy-based greedy method. We\nconfirm, via an exhaustive search, that all three methods yield an optimal\nordering for $f &lt; 6$ messages. For $6 \\leq f \\leq 12$ messages, we numerically\nevaluate the performance of the proposed methods compared with a directed\nrandom search. For almost all scenarios considered, the entropy-based greedy\nmethod gives the smallest gap to the best-found ordering.",
        "translated": ""
    },
    {
        "title": "End-to-end Learnable Clustering for Intent Learning in Recommendation",
        "url": "http://arxiv.org/abs/2401.05975v1",
        "pub_date": "2024-01-11",
        "summary": "Mining users' intents plays a crucial role in sequential recommendation. The\nrecent approach, ICLRec, was introduced to extract underlying users' intents\nusing contrastive learning and clustering. While it has shown effectiveness,\nthe existing method suffers from complex and cumbersome alternating\noptimization, leading to two main issues. Firstly, the separation of\nrepresentation learning and clustering optimization within a generalized\nexpectation maximization (EM) framework often results in sub-optimal\nperformance. Secondly, performing clustering on the entire dataset hampers\nscalability for large-scale industry data. To address these challenges, we\npropose a novel intent learning method called \\underline{ELCRec}, which\nintegrates representation learning into an \\underline{E}nd-to-end\n\\underline{L}earnable \\underline{C}lustering framework for\n\\underline{Rec}ommendation. Specifically, we encode users' behavior sequences\nand initialize the cluster centers as learnable network parameters.\nAdditionally, we design a clustering loss that guides the networks to\ndifferentiate between different cluster centers and pull similar samples\ntowards their respective cluster centers. This allows simultaneous optimization\nof recommendation and clustering using mini-batch data. Moreover, we leverage\nthe learned cluster centers as self-supervision signals for representation\nlearning, resulting in further enhancement of recommendation performance.\nExtensive experiments conducted on open benchmarks and industry data validate\nthe superiority, effectiveness, and efficiency of our proposed ELCRec method.\nCode is available at: https://github.com/yueliu1999/ELCRec.",
        "translated": ""
    },
    {
        "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph\n  Embedding",
        "url": "http://arxiv.org/abs/2401.05967v1",
        "pub_date": "2024-01-11",
        "summary": "The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.",
        "translated": ""
    },
    {
        "title": "DREQ: Document Re-Ranking Using Entity-based Query Understanding",
        "url": "http://arxiv.org/abs/2401.05939v1",
        "pub_date": "2024-01-11",
        "summary": "While entity-oriented neural IR models have advanced significantly, they\noften overlook a key nuance: the varying degrees of influence individual\nentities within a document have on its overall relevance. Addressing this gap,\nwe present DREQ, an entity-oriented dense document re-ranking model. Uniquely,\nwe emphasize the query-relevant entities within a document's representation\nwhile simultaneously attenuating the less relevant ones, thus obtaining a\nquery-specific entity-centric document representation. We then combine this\nentity-centric document representation with the text-centric representation of\nthe document to obtain a \"hybrid\" representation of the document. We learn a\nrelevance score for the document using this hybrid representation. Using four\nlarge-scale benchmarks, we show that DREQ outperforms state-of-the-art neural\nand non-neural re-ranking methods, highlighting the effectiveness of our\nentity-oriented representation approach.",
        "translated": ""
    },
    {
        "title": "What Else Would I Like? A User Simulator using Alternatives for Improved\n  Evaluation of Fashion Conversational Recommendation Systems",
        "url": "http://arxiv.org/abs/2401.05783v1",
        "pub_date": "2024-01-11",
        "summary": "In Conversational Recommendation Systems (CRS), a user can provide feedback\non recommended items at each interaction turn, leading the CRS towards more\ndesirable recommendations. Currently, different types of CRS offer various\npossibilities for feedback, i.e., natural language feedback, or answering\nclarifying questions. In most cases, a user simulator is employed for training\nas well as evaluating the CRS. Such user simulators typically critique the\ncurrent retrieved items based on knowledge of a single target item. Still,\nevaluating systems in offline settings with simulators suffers from problems,\nsuch as focusing entirely on a single target item (not addressing the\nexploratory nature of a recommender system), and exhibiting extreme patience\n(consistent feedback over a large number of turns). To overcome these\nlimitations, we obtain extra judgements for a selection of alternative items in\ncommon CRS datasets, namely Shoes and Fashion IQ Dresses. Going further, we\npropose improved user simulators that allow simulated users not only to express\ntheir preferences about alternative items to their original target, but also to\nchange their mind and level of patience. In our experiments using the relative\nimage captioning CRS setting and different CRS models, we find that using the\nknowledge of alternatives by the simulator can have a considerable impact on\nthe evaluation of existing CRS models, specifically that the existing\nsingle-target evaluation underestimates their effectiveness, and when simulated\nusers are allowed to instead consider alternatives, the system can rapidly\nrespond to more quickly satisfy the user.",
        "translated": ""
    },
    {
        "title": "Lifelogging As An Extreme Form of Personal Information Management --\n  What Lessons To Learn",
        "url": "http://arxiv.org/abs/2401.05767v1",
        "pub_date": "2024-01-11",
        "summary": "Personal data includes the digital footprints that we leave behind as part of\nour everyday activities, both online and offline in the real world. It includes\ndata we collect ourselves, such as from wearables, as well as the data\ncollected by others about our online behaviour and activities. Sometimes we are\nable to use the personal data we ourselves collect, in order to examine some\nparts of our lives but for the most part, our personal data is leveraged by\nthird parties including internet companies, for services like targeted\nadvertising and recommendations. Lifelogging is a form of extreme personal data\ngathering and in this article we present an overview of the tools used to\nmanage access to lifelogs as demonstrated at the most recent of the annual\nLifelog Search Challenge benchmarking workshops. Here, experimental systems are\nshowcased in live, real time information seeking tasks by real users. This\noverview of these systems' capabilities show the range of possibilities for\naccessing our own personal data which may, in time, become more easily\navailable as consumer-level services.",
        "translated": ""
    },
    {
        "title": "Large Language Models vs. Search Engines: Evaluating User Preferences\n  Across Varied Information Retrieval Scenarios",
        "url": "http://arxiv.org/abs/2401.05761v1",
        "pub_date": "2024-01-11",
        "summary": "This study embarked on a comprehensive exploration of user preferences\nbetween Search Engines and Large Language Models (LLMs) in the context of\nvarious information retrieval scenarios. Conducted with a sample size of 100\ninternet users (N=100) from across the United States, the research delved into\n20 distinct use cases ranging from factual searches, such as looking up\nCOVID-19 guidelines, to more subjective tasks, like seeking interpretations of\ncomplex concepts in layman's terms. Participants were asked to state their\npreference between using a traditional search engine or an LLM for each\nscenario. This approach allowed for a nuanced understanding of how users\nperceive and utilize these two predominant digital tools in differing contexts.\nThe use cases were carefully selected to cover a broad spectrum of typical\nonline queries, thus ensuring a comprehensive analysis of user preferences. The\nfindings reveal intriguing patterns in user choices, highlighting a clear\ntendency for participants to favor search engines for direct, fact-based\nqueries, while LLMs were more often preferred for tasks requiring nuanced\nunderstanding and language processing. These results offer valuable insights\ninto the current state of digital information retrieval and pave the way for\nfuture innovations in this field. This study not only sheds light on the\nspecific contexts in which each tool is favored but also hints at the potential\nfor developing hybrid models that leverage the strengths of both search engines\nand LLMs. The insights gained from this research are pivotal for developers,\nresearchers, and policymakers in understanding the evolving landscape of\ndigital information retrieval and user interaction with these technologies.",
        "translated": ""
    },
    {
        "title": "Attention Is Not the Only Choice: Counterfactual Reasoning for\n  Path-Based Explainable Recommendation",
        "url": "http://arxiv.org/abs/2401.05744v1",
        "pub_date": "2024-01-11",
        "summary": "Compared with only pursuing recommendation accuracy, the explainability of a\nrecommendation model has drawn more attention in recent years. Many graph-based\nrecommendations resort to informative paths with the attention mechanism for\nthe explanation. Unfortunately, these attention weights are intentionally\ndesigned for model accuracy but not explainability. Recently, some researchers\nhave started to question attention-based explainability because the attention\nweights are unstable for different reproductions, and they may not always align\nwith human intuition. Inspired by the counterfactual reasoning from causality\nlearning theory, we propose a novel explainable framework targeting path-based\nrecommendations, wherein the explainable weights of paths are learned to\nreplace attention weights. Specifically, we design two counterfactual reasoning\nalgorithms from both path representation and path topological structure\nperspectives. Moreover, unlike traditional case studies, we also propose a\npackage of explainability evaluation solutions with both qualitative and\nquantitative methods. We conduct extensive experiments on three real-world\ndatasets, the results of which further demonstrate the effectiveness and\nreliability of our method.",
        "translated": ""
    },
    {
        "title": "Cross-modal Retrieval for Knowledge-based Visual Question Answering",
        "url": "http://arxiv.org/abs/2401.05736v1",
        "pub_date": "2024-01-11",
        "summary": "Knowledge-based Visual Question Answering about Named Entities is a\nchallenging task that requires retrieving information from a multimodal\nKnowledge Base. Named entities have diverse visual representations and are\ntherefore difficult to recognize. We argue that cross-modal retrieval may help\nbridge the semantic gap between an entity and its depictions, and is foremost\ncomplementary with mono-modal retrieval. We provide empirical evidence through\nexperiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE,\nInfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different\nstrategies to fine-tune such a model: mono-modal, cross-modal, or joint\ntraining. Our method, which combines mono-and cross-modal retrieval, is\ncompetitive with billion-parameter models on the three datasets, while being\nconceptually simpler and computationally cheaper.",
        "translated": ""
    },
    {
        "title": "Model-Free Approximate Bayesian Learning for Large-Scale Conversion\n  Funnel Optimization",
        "url": "http://arxiv.org/abs/2401.06710v1",
        "pub_date": "2024-01-12",
        "summary": "The flexibility of choosing the ad action as a function of the consumer state\nis critical for modern-day marketing campaigns. We study the problem of\nidentifying the optimal sequential personalized interventions that maximize the\nadoption probability for a new product. We model consumer behavior by a\nconversion funnel that captures the state of each consumer (e.g., interaction\nhistory with the firm) and allows the consumer behavior to vary as a function\nof both her state and firm's sequential interventions. We show our model\ncaptures consumer behavior with very high accuracy (out-of-sample AUC of over\n0.95) in a real-world email marketing dataset. However, it results in a very\nlarge-scale learning problem, where the firm must learn the state-specific\neffects of various interventions from consumer interactions. We propose a novel\nattribution-based decision-making algorithm for this problem that we call\nmodel-free approximate Bayesian learning. Our algorithm inherits the\ninterpretability and scalability of Thompson sampling for bandits and maintains\nan approximate belief over the value of each state-specific intervention. The\nbelief is updated as the algorithm interacts with the consumers. Despite being\nan approximation to the Bayes update, we prove the asymptotic optimality of our\nalgorithm and analyze its convergence rate. We show that our algorithm\nsignificantly outperforms traditional approaches on extensive simulations\ncalibrated to a real-world email marketing dataset.",
        "translated": ""
    },
    {
        "title": "Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies",
        "url": "http://arxiv.org/abs/2401.06703v1",
        "pub_date": "2024-01-12",
        "summary": "We explore leveraging corpus-specific vocabularies that improve both\nefficiency and effectiveness of learned sparse retrieval systems. We find that\npre-training the underlying BERT model on the target corpus, specifically\ntargeting different vocabulary sizes incorporated into the document expansion\nprocess, improves retrieval quality by up to 12% while in some scenarios\ndecreasing latency by up to 50%. Our experiments show that adopting\ncorpus-specific vocabulary and increasing vocabulary size decreases average\npostings list length which in turn reduces latency. Ablation studies show\ninteresting interactions between custom vocabularies, document expansion\ntechniques, and sparsification objectives of sparse models. Both effectiveness\nand efficiency improvements transfer to different retrieval approaches such as\nuniCOIL and SPLADE and offer a simple yet effective approach to providing new\nefficiency-effectiveness trade-offs for learned sparse retrieval systems.",
        "translated": ""
    },
    {
        "title": "DQNC2S: DQN-based Cross-stream Crisis event Summarizer",
        "url": "http://arxiv.org/abs/2401.06683v1",
        "pub_date": "2024-01-12",
        "summary": "Summarizing multiple disaster-relevant data streams simultaneously is\nparticularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from\nthe inherent redundancy of multi-stream data and limited scalability in a\nmulti-query setting. This work proposes an online approach to crisis timeline\ngeneration based on weak annotation with Deep Q-Networks. It selects on-the-fly\nthe relevant pieces of text without requiring neither human annotations nor\ncontent re-ranking. This makes the inference time independent of the number of\ninput queries. The proposed approach also incorporates a redundancy filter into\nthe reward function to effectively handle cross-stream content overlaps. The\nachieved ROUGE and BERTScore results are superior to those of best-performing\nmodels on the CrisisFACTS 2022 benchmark.",
        "translated": ""
    },
    {
        "title": "LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for\n  Software Purchase",
        "url": "http://arxiv.org/abs/2401.06676v1",
        "pub_date": "2024-01-12",
        "summary": "Recommendation systems are ubiquitous, from Spotify playlist suggestions to\nAmazon product suggestions. Nevertheless, depending on the methodology or the\ndataset, these systems typically fail to capture user preferences and generate\ngeneral recommendations. Recent advancements in Large Language Models (LLM)\noffer promising results for analyzing user queries. However, employing these\nmodels to capture user preferences and efficiency remains an open question. In\nthis paper, we propose LLMRS, an LLM-based zero-shot recommender system where\nwe employ pre-trained LLM to encode user reviews into a review score and\ngenerate user-tailored recommendations. We experimented with LLMRS on a\nreal-world dataset, the Amazon product reviews, for software purchase use\ncases. The results show that LLMRS outperforms the ranking-based baseline model\nwhile successfully capturing meaningful information from product reviews,\nthereby providing more reliable recommendations.",
        "translated": ""
    },
    {
        "title": "The SemIoE Ontology: A Semantic Model Solution for an IoE-based Industry",
        "url": "http://arxiv.org/abs/2401.06667v1",
        "pub_date": "2024-01-12",
        "summary": "Recently, the Industry 5.0 is gaining attention as a novel paradigm, defining\nthe next concrete steps toward more and more intelligent, green-aware and\nuser-centric digital systems. In an era in which smart devices typically\nadopted in the industry domain are more and more sophisticated and autonomous,\nthe Internet of Things and its evolution, known as the Internet of Everything\n(IoE, for short), involving also people, robots, processes and data in the\nnetwork, represent the main driver to allow industries to put the experiences\nand needs of human beings at the center of their ecosystems. However, due to\nthe extreme heterogeneity of the involved entities, their intrinsic need and\ncapability to cooperate, and the aim to adapt to a dynamic user-centric\ncontext, special attention is required for the integration and processing of\nthe data produced by such an IoE. This is the objective of the present paper,\nin which we propose a novel semantic model that formalizes the fundamental\nactors, elements and information of an IoE, along with their relationships. In\nour design, we focus on state-of-the-art design principles, in particular\nreuse, and abstraction, to build ``SemIoE'', a lightweight ontology inheriting\nand extending concepts from well-known and consolidated reference ontologies.\nThe defined semantic layer represents a core data model that can be extended to\nembrace any modern industrial scenario. It represents the base of an IoE\nKnowledge Graph, on top of which, as an additional contribution, we analyze and\ndefine some essential services for an IoE-based industry.",
        "translated": ""
    },
    {
        "title": "Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential\n  Recommendations",
        "url": "http://arxiv.org/abs/2401.06633v1",
        "pub_date": "2024-01-12",
        "summary": "Retrieval models aim at selecting a small set of item candidates which match\nthe preference of a given user. They play a vital role in large-scale\nrecommender systems since subsequent models such as rankers highly depend on\nthe quality of item candidates. However, most existing retrieval models employ\na single-round inference paradigm, which may not adequately capture the dynamic\nnature of user preferences and stuck in one area in the item space. In this\npaper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for\nrecommender systems that iteratively refines user representations to better\ncapture potential candidates in the full item space. Ada-Retrieval comprises\ntwo key modules: the item representation adapter and the user representation\nadapter, designed to inject context information into items' and users'\nrepresentations. The framework maintains a model-agnostic design, allowing\nseamless integration with various backbone models such as RNNs or Transformers.\nWe perform experiments on three widely used public datasets, incorporating five\npowerful sequential recommenders as backbone models. Our results demonstrate\nthat Ada-Retrieval significantly enhances the performance of various base\nmodels, with consistent improvements observed across different datasets. Our\ncode and data are publicly available at:\nhttps://github.com/ll0ruc/Ada-Retrieval.",
        "translated": ""
    },
    {
        "title": "Mapping Transformer Leveraged Embeddings for Cross-Lingual Document\n  Representation",
        "url": "http://arxiv.org/abs/2401.06583v1",
        "pub_date": "2024-01-12",
        "summary": "Recommendation systems, for documents, have become tools to find relevant\ncontent on the Web. However, these systems have limitations when it comes to\nrecommending documents in languages different from the query language, which\nmeans they might overlook resources in non-native languages. This research\nfocuses on representing documents across languages by using Transformer\nLeveraged Document Representations (TLDRs) that are mapped to a cross-lingual\ndomain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM\nRoBERTa, ErnieM) were evaluated using three mapping methods across 20 language\npairs representing combinations of five selected languages of the European\nUnion. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to\nmeasure the effectiveness of mapped TLDRs compared to non-mapped ones. The\nresults highlight the power of cross-lingual representations achieved through\npre-trained transformers and mapping approaches suggesting a promising\ndirection for expanding beyond language connections, between two specific\nlanguages.",
        "translated": ""
    },
    {
        "title": "INTERS: Unlocking the Power of Large Language Models in Search with\n  Instruction Tuning",
        "url": "http://arxiv.org/abs/2401.06532v1",
        "pub_date": "2024-01-12",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious natural language processing tasks. Despite this, their application to\ninformation retrieval (IR) tasks is still challenging due to the infrequent\noccurrence of many IR-specific concepts in natural language. While prompt-based\nmethods can provide task descriptions to LLMs, they often fall short in\nfacilitating comprehensive understanding and execution of IR tasks, thereby\nlimiting LLMs' applicability. To address this gap, in this work, we explore the\npotential of instruction tuning to enhance LLMs' proficiency in IR tasks. We\nintroduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks\nacross three fundamental IR categories: query understanding, document\nunderstanding, and query-document relationship understanding. The data are\nderived from 43 distinct datasets with manually written templates. Our\nempirical results reveal that INTERS significantly boosts the performance of\nvarious publicly available LLMs, such as LLaMA, Mistral, and Phi, in\nsearch-related tasks. Furthermore, we conduct a comprehensive analysis to\nascertain the effects of base model selection, instruction design, volume of\ninstructions, and task variety on performance. We make our dataset and the\nmodels fine-tuned on it publicly accessible at https://github.com/DaoD/INTERS.",
        "translated": ""
    },
    {
        "title": "UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender\n  Systems with UNidirectional EXecution",
        "url": "http://arxiv.org/abs/2401.06470v1",
        "pub_date": "2024-01-12",
        "summary": "In recent years, there has been a growing interest in utilizing reinforcement\nlearning (RL) to optimize long-term rewards in recommender systems. Since\nindustrial recommender systems are typically designed as multi-stage systems,\nRL methods with a single agent face challenges when optimizing multiple stages\nsimultaneously. The reason is that different stages have different observation\nspaces, and thus cannot be modeled by a single agent. To address this issue, we\npropose a novel UNidirectional-EXecution-based multi-agent Reinforcement\nLearning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage\nrecommender systems. We show that the unidirectional execution is a key feature\nof multi-stage recommender systems, bringing new challenges to the applications\nof multi-agent reinforcement learning (MARL), namely the observation dependency\nand the cascading effect. To tackle these challenges, we provide a cascading\ninformation chain (CIC) method to separate the independent observations from\naction-dependent observations and use CIC to train UNEX-RL effectively. We also\ndiscuss practical variance reduction techniques for UNEX-RL. Finally, we show\nthe effectiveness of UNEX-RL on both public datasets and an online recommender\nsystem with over 100 million users. Specifically, UNEX-RL reveals a 0.558%\nincrease in users' usage time compared with single-agent RL algorithms in\nonline A/B experiments, highlighting the effectiveness of UNEX-RL in industrial\nrecommender systems.",
        "translated": ""
    },
    {
        "title": "Improving Graph Convolutional Networks with Transformer Layer in\n  social-based items recommendation",
        "url": "http://arxiv.org/abs/2401.06436v1",
        "pub_date": "2024-01-12",
        "summary": "In this work, we have proposed an approach for improving the GCN for\npredicting ratings in social networks. Our model is expanded from the standard\nmodel with several layers of transformer architecture. The main focus of the\npaper is on the encoder architecture for node embedding in the network. Using\nthe embedding layer from the graph-based convolution layer, the attention\nmechanism could rearrange the feature space to get a more efficient embedding\nfor the downstream task. The experiments showed that our proposed architecture\nachieves better performance than GCN on the traditional link prediction task.",
        "translated": ""
    },
    {
        "title": "Siamese Content-based Search Engine for a More Transparent Skin and\n  Breast Cancer Diagnosis through Histological Imaging",
        "url": "http://arxiv.org/abs/2401.08272v1",
        "pub_date": "2024-01-16",
        "summary": "Computer Aid Diagnosis (CAD) has developed digital pathology with Deep\nLearning (DL)-based tools to assist pathologists in decision-making.\nContent-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek\nhighly correlated patches in terms of similarity in histopathological features.\nIn this work, we proposed two CBHIR approaches on breast (Breast-twins) and\nskin cancer (Skin-twins) data sets for robust and accurate patch-level\nretrieval, integrating a custom-built Siamese network as a feature extractor.\nThe proposed Siamese network is able to generalize for unseen images by\nfocusing on the similar histopathological features of the input pairs. The\nproposed CBHIR approaches are evaluated on the Breast (public) and Skin\n(private) data sets with top K accuracy. Finding the optimum amount of K is\nchallenging, but also, as much as K increases, the dissimilarity between the\nquery and the returned images increases which might mislead the pathologists.\nTo the best of the author's belief, this paper is tackling this issue for the\nfirst time on histopathological images by evaluating the top first retrieved\nimages. The Breast-twins model achieves 70% of the F1score at the top first,\nwhich exceeds the other state-of-the-art methods at a higher amount of K such\nas 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto\nEncoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model\ntackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential\n(STUMP) to assist pathologists with retrieving top K images and their\ncorresponding labels. So, this approach can offer a more explainable CAD tool\nto pathologists in terms of transparency, trustworthiness, or reliability among\nother characteristics.",
        "translated": ""
    },
    {
        "title": "Ranking Heterogeneous Search Result Pages using the Interactive\n  Probability Ranking Principle",
        "url": "http://arxiv.org/abs/2401.08267v1",
        "pub_date": "2024-01-16",
        "summary": "The Probability Ranking Principle (PRP) ranks search results based on their\nexpected utility derived solely from document contents, often overlooking the\nnuances of presentation and user interaction. However, with the evolution of\nSearch Engine Result Pages (SERPs), now comprising a variety of result cards,\nthe manner in which these results are presented is pivotal in influencing user\nengagement and satisfaction. This shift prompts the question: How does the PRP\nand its user-centric counterpart, the Interactive Probability Ranking Principle\n(iPRP), compare in the context of these heterogeneous SERPs? Our study draws a\ncomparison between the PRP and the iPRP, revealing significant differences in\ntheir output. The iPRP, accounting for item-specific costs and interaction\nprobabilities to determine the ``Expected Perceived Utility\" (EPU), yields\ndifferent result orderings compared to the PRP. We evaluate the effect of the\nEPU on the ordering of results by observing changes in the ranking within a\nheterogeneous SERP compared to the traditional ``ten blue links''. We find that\nchanging the presentation affects the ranking of items according to the (iPRP)\nby up to 48\\% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the\nTREC WaPo Collection. This work suggests that the iPRP should be employed when\nranking heterogeneous SERPs to provide a user-centric ranking that adapts the\nordering based on the presentation and user engagement.",
        "translated": ""
    },
    {
        "title": "MCRPL: A Pretrain, Prompt &amp; Fine-tune Paradigm for Non-overlapping\n  Many-to-one Cross-domain Recommendation",
        "url": "http://arxiv.org/abs/2401.08228v1",
        "pub_date": "2024-01-16",
        "summary": "Cross-domain Recommendation (CR) is the task that tends to improve the\nrecommendations in the sparse target domain by leveraging the information from\nother rich domains. Existing methods of cross-domain recommendation mainly\nfocus on overlapping scenarios by assuming users are totally or partially\noverlapped, which are taken as bridges to connect different domains. However,\nthis assumption does not always hold since it is illegal to leak users'\nidentity information to other domains. Conducting Non-overlapping MCR (NMCR) is\nchallenging since 1) The absence of overlapping information prevents us from\ndirectly aligning different domains, and this situation may get worse in the\nMCR scenario. 2) The distribution between source and target domains makes it\ndifficult for us to learn common information across domains. To overcome the\nabove challenges, we focus on NMCR, and devise MCRPL as our solution. To\naddress Challenge 1, we first learn shared domain-agnostic and domain-dependent\nprompts, and pre-train them in the pre-training stage. To address Challenge 2,\nwe further update the domain-dependent prompts with other parameters kept fixed\nto transfer the domain knowledge to the target domain. We conduct experiments\non five real-world domains, and the results show the advance of our MCRPL\nmethod compared with several recent SOTA baselines.",
        "translated": ""
    },
    {
        "title": "LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable\n  Recommendation",
        "url": "http://arxiv.org/abs/2401.08217v1",
        "pub_date": "2024-01-16",
        "summary": "As personalized recommendation systems become vital in the age of information\noverload, traditional methods relying solely on historical user interactions\noften fail to fully capture the multifaceted nature of human interests. To\nenable more human-centric modeling of user preferences, this work proposes a\nnovel explainable recommendation framework, i.e., LLMHG, synergizing the\nreasoning capabilities of large language models (LLMs) and the structural\nadvantages of hypergraph neural networks. By effectively profiling and\ninterpreting the nuances of individual user interests, our framework pioneers\nenhancements to recommendation systems with increased explainability. We\nvalidate that explicitly accounting for the intricacies of human preferences\nallows our human-centric and explainable LLMHG approach to consistently\noutperform conventional models across diverse real-world datasets. The proposed\nplug-and-play enhancement framework delivers immediate gains in recommendation\nperformance while offering a pathway to apply advanced LLMs for better\ncapturing the complexity of human interests across machine learning\napplications.",
        "translated": ""
    },
    {
        "title": "Generative Multi-Modal Knowledge Retrieval with Large Language Models",
        "url": "http://arxiv.org/abs/2401.08206v1",
        "pub_date": "2024-01-16",
        "summary": "Knowledge retrieval with multi-modal queries plays a crucial role in\nsupporting knowledge-intensive multi-modal applications. However, existing\nmethods face challenges in terms of their effectiveness and training\nefficiency, especially when it comes to training and integrating multiple\nretrievers to handle multi-modal queries. In this paper, we propose an\ninnovative end-to-end generative framework for multi-modal knowledge retrieval.\nOur framework takes advantage of the fact that large language models (LLMs) can\neffectively serve as virtual knowledge bases, even when trained with limited\ndata. We retrieve knowledge via a two-step process: 1) generating knowledge\nclues related to the queries, and 2) obtaining the relevant document by\nsearching databases using the knowledge clue. In particular, we first introduce\nan object-aware prefix-tuning technique to guide multi-grained visual learning.\nThen, we align multi-grained visual features into the textual feature space of\nthe LLM, employing the LLM to capture cross-modal interactions. Subsequently,\nwe construct instruction data with a unified format for model training.\nFinally, we propose the knowledge-guided generation strategy to impose prior\nconstraints in the decoding steps, thereby promoting the generation of\ndistinctive knowledge clues. Through experiments conducted on three benchmarks,\nwe demonstrate significant improvements ranging from 3.0% to 14.6% across all\nevaluation metrics when compared to strong baselines.",
        "translated": ""
    },
    {
        "title": "A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR",
        "url": "http://arxiv.org/abs/2401.08104v1",
        "pub_date": "2024-01-16",
        "summary": "Screening documents is a tedious and time-consuming aspect of high-recall\nretrieval tasks, such as compiling a systematic literature review, where the\ngoal is to identify all relevant documents for a topic. To help streamline this\nprocess, many Technology-Assisted Review (TAR) methods leverage active learning\ntechniques to reduce the number of documents requiring review. BERT-based\nmodels have shown high effectiveness in text classification, leading to\ninterest in their potential use in TAR workflows. In this paper, we investigate\nrecent work that examined the impact of further pre-training epochs on the\neffectiveness and efficiency of a BERT-based active learning pipeline. We first\nreport that we could replicate the original experiments on two specific TAR\ndatasets, confirming some of the findings: importantly, that further\npre-training is critical to high effectiveness, but requires attention in terms\nof selecting the correct training epoch. We then investigate the\ngeneralisability of the pipeline on a different TAR task, that of medical\nsystematic reviews. In this context, we show that there is no need for further\npre-training if a domain-specific BERT backbone is used within the active\nlearning pipeline. This finding provides practical implications for using the\nstudied active learning pipeline within domain-specific TAR tasks.",
        "translated": ""
    },
    {
        "title": "The Chronicles of RAG: The Retriever, the Chunk and the Generator",
        "url": "http://arxiv.org/abs/2401.07883v1",
        "pub_date": "2024-01-15",
        "summary": "Retrieval Augmented Generation (RAG) has become one of the most popular\nparadigms for enabling LLMs to access external data, and also as a mechanism\nfor grounding to mitigate against hallucinations. When implementing RAG you can\nface several challenges like effective integration of retrieval models,\nefficient representation learning, data diversity, computational efficiency\noptimization, evaluation, and quality of text generation. Given all these\nchallenges, every day a new technique to improve RAG appears, making it\nunfeasible to experiment with all combinations for your problem. In this\ncontext, this paper presents good practices to implement, optimize, and\nevaluate RAG for the Brazilian Portuguese language, focusing on the\nestablishment of a simple pipeline for inference and experiments. We explored a\ndiverse set of methods to answer questions about the first Harry Potter book.\nTo generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview,\ngpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the\nretriever, our approach achieved an improvement of MRR@10 by 35.4% compared to\nthe baseline. When optimizing the input size in the application, we observed\nthat it is possible to further enhance it by 2.4%. Finally, we present the\ncomplete architecture of the RAG with our recommendations. As result, we moved\nfrom a baseline of 57.88% to a maximum relative score of 98.61%.",
        "translated": ""
    },
    {
        "title": "Deep Evolutional Instant Interest Network for CTR Prediction in\n  Trigger-Induced Recommendation",
        "url": "http://arxiv.org/abs/2401.07769v1",
        "pub_date": "2024-01-15",
        "summary": "The recommendation has been playing a key role in many industries, e.g.,\ne-commerce, streaming media, social media, etc. Recently, a new recommendation\nscenario, called Trigger-Induced Recommendation (TIR), where users are able to\nexplicitly express their instant interests via trigger items, is emerging as an\nessential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.\nWithout explicitly modeling the user's instant interest, traditional\nrecommendation methods usually obtain sub-optimal results in TIR. Even though\nthere are a few methods considering the trigger and target items simultaneously\nto solve this problem, they still haven't taken into account temporal\ninformation of user behaviors, the dynamic change of user instant interest when\nthe user scrolls down and the interactions between the trigger and target\nitems. To tackle these problems, we propose a novel method -- Deep Evolutional\nInstant Interest Network (DEI2N), for click-through rate prediction in TIR\nscenarios. Specifically, we design a User Instant Interest Modeling Layer to\npredict the dynamic change of the intensity of instant interest when the user\nscrolls down. Temporal information is utilized in user behavior modeling.\nMoreover, an Interaction Layer is introduced to learn better interactions\nbetween the trigger and target items. We evaluate our method on several offline\nand real-world industrial datasets. Experimental results show that our proposed\nDEI2N outperforms state-of-the-art baselines. In addition, online A/B testing\ndemonstrates the superiority over the existing baseline in real-world\nproduction environments.",
        "translated": ""
    },
    {
        "title": "CREAD: A Classification-Restoration Framework with Error Adaptive\n  Discretization for Watch Time Prediction in Video Recommender Systems",
        "url": "http://arxiv.org/abs/2401.07521v1",
        "pub_date": "2024-01-15",
        "summary": "The watch time is a significant indicator of user satisfaction in video\nrecommender systems. However, the prediction of watch time as a target variable\nis often hindered by its highly imbalanced distribution with a scarcity of\nobservations for larger target values and over-populated samples for small\nvalues. State-of-the-art watch time prediction models discretize the continuous\nwatch time into a set of buckets in order to consider the distribution of watch\ntime. However, it is highly uninvestigated how these discrete buckets should be\ncreated from the continuous watch time distribution, and existing\ndiscretization approaches suffer from either a large learning error or a large\nrestoration error. To address this challenge, we propose a\nClassification-Restoration framework with Error-Adaptive-Discretization (CREAD)\nto accurately predict the watch time. The proposed framework contains a\ndiscretization module, a classification module, and a restoration module. It\npredicts the watch time through multiple classification problems. The\ndiscretization process is a key contribution of the CREAD framework. We\ntheoretically analyze the impacts of the discretization on the learning error\nand the restoration error, and then propose the error-adaptive discretization\n(EAD) technique to better balance the two errors, which achieves better\nperformance over traditional discretization approaches. We conduct detailed\noffline evaluations on a public dataset and an industrial dataset, both showing\nperformance gains through the proposed approach. Moreover, We have fully\nlaunched our framework to Kwai App, an online video platform, which resulted in\na significant increase in users' video watch time by 0.29% through A/B testing.\nThese results highlight the effectiveness of the CREAD framework in watch time\nprediction in video recommender systems.",
        "translated": ""
    },
    {
        "title": "Model Editing at Scale leads to Gradual and Catastrophic Forgetting",
        "url": "http://arxiv.org/abs/2401.07453v1",
        "pub_date": "2024-01-15",
        "summary": "Editing knowledge in large language models is an attractive capability to\nhave which allows us to correct incorrectly learnt facts during pre-training,\nas well as update the model with an ever-growing list of new facts. While\nexisting model editing techniques have shown promise, they are usually\nevaluated using metrics for reliability, specificity and generalization over\none or few edits. We argue that for model editing to have practical utility, we\nmust be able to make multiple edits to the same model. With this in mind, we\nevaluate the current model editing methods at scale, focusing on two state of\nthe art methods: ROME and MEMIT. We find that as the model is edited\nsequentially with multiple facts, it continually forgets previously edited\nfacts and the ability to perform downstream tasks. This forgetting happens in\ntwo phases -- an initial gradual but progressive forgetting phase followed by\nabrupt or catastrophic forgetting phase. Both gradual and catastrophic\nforgetting limit the usefulness of model editing methods at scale -- the former\nmaking model editing less effective as multiple edits are made to the model\nwhile the latter caps the scalability of such model editing methods. Our\nanalysis also highlights other key limitations of ROME and MEMIT at scale. With\nour work, we push for the development and evaluation of model editing methods\nkeeping scalability in mind.",
        "translated": ""
    },
    {
        "title": "Foundations of Vector Retrieval",
        "url": "http://arxiv.org/abs/2401.09350v1",
        "pub_date": "2024-01-17",
        "summary": "Vectors are universal mathematical objects that can represent text, images,\nspeech, or a mix of these data modalities. That happens regardless of whether\ndata is represented by hand-crafted features or learnt embeddings. Collect a\nlarge enough quantity of such vectors and the question of retrieval becomes\nurgently relevant: Finding vectors that are more similar to a query vector.\nThis monograph is concerned with the question above and covers fundamental\nconcepts along with advanced data structures and algorithms for vector\nretrieval. In doing so, it recaps this fascinating topic and lowers barriers of\nentry into this rich area of research.",
        "translated": ""
    },
    {
        "title": "BibSonomy Meets ChatLLMs for Publication Management: From Chat to\n  Publication Management: Organizing your related work using BibSonomy &amp; LLMs",
        "url": "http://arxiv.org/abs/2401.09092v1",
        "pub_date": "2024-01-17",
        "summary": "The ever-growing corpus of scientific literature presents significant\nchallenges for researchers with respect to discovery, management, and\nannotation of relevant publications. Traditional platforms like Semantic\nScholar, BibSonomy, and Zotero offer tools for literature management, but\nlargely require manual laborious and error-prone input of tags and metadata.\nHere, we introduce a novel retrieval augmented generation system that leverages\nchat-based large language models (LLMs) to streamline and enhance the process\nof publication management. It provides a unified chat-based interface, enabling\nintuitive interactions with various backends, including Semantic Scholar,\nBibSonomy, and the Zotero Webscraper. It supports two main use-cases: (1)\nExplorative Search &amp; Retrieval - leveraging LLMs to search for and retrieve\nboth specific and general scientific publications, while addressing the\nchallenges of content hallucination and data obsolescence; and (2) Cataloguing\n&amp; Management - aiding in the organization of personal publication libraries, in\nthis case BibSonomy, by automating the addition of metadata and tags, while\nfacilitating manual edits and updates. We compare our system to different LLM\nmodels in three different settings, including a user study, and we can show its\nadvantages in different metrics.",
        "translated": ""
    },
    {
        "title": "Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for\n  Generalized Knowledge Augmentation and Inference",
        "url": "http://arxiv.org/abs/2401.09070v1",
        "pub_date": "2024-01-17",
        "summary": "Knowledge graph (KG) based reasoning has been regarded as an effective means\nfor the analysis of semantic networks and is of great usefulness in areas of\ninformation retrieval, recommendation, decision-making, and man-machine\ninteraction. It is widely used in recommendation, decision-making,\nquestion-answering, search, and other fields. However, previous studies mainly\nused low-level knowledge in the KG for reasoning, which may result in\ninsufficient generalization and poor robustness of reasoning. To this end, this\npaper proposes a new inference approach using a novel knowledge augmentation\nstrategy to improve the generalization capability of KG. This framework\nextracts high-level pyramidal knowledge from low-level knowledge and applies it\nto reasoning in a multi-level hierarchical KG, called knowledge pyramid in this\npaper. We tested some medical data sets using the proposed approach, and the\nexperimental results show that the proposed knowledge pyramid has improved the\nknowledge inference performance with better generalization. Especially, when\nthere are fewer training samples, the inference accuracy can be significantly\nimproved.",
        "translated": ""
    },
    {
        "title": "Algorithmic amplification of biases on Google Search",
        "url": "http://arxiv.org/abs/2401.09044v1",
        "pub_date": "2024-01-17",
        "summary": "The evolution of information-seeking processes, driven by search engines like\nGoogle, has transformed the access to information people have. This paper\ninvestigates how individuals' preexisting attitudes influence the modern\ninformation-seeking process, specifically the results presented by Google\nSearch. Through a comprehensive study involving surveys and information-seeking\ntasks focusing on the topic of abortion, the paper provides four crucial\ninsights: 1) Individuals with opposing attitudes on abortion receive different\nsearch results. 2) Individuals express their beliefs in their choice of\nvocabulary used in formulating the search queries, shaping the outcome of the\nsearch. 3) Additionally, the user's search history contributes to divergent\nresults among those with opposing attitudes. 4) Google Search engine reinforces\npreexisting beliefs in search results. Overall, this study provides insights\ninto the interplay between human biases and algorithmic processes, highlighting\nthe potential for information polarization in modern information-seeking\nprocesses.",
        "translated": ""
    },
    {
        "title": "UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User\n  Experiences in Recommender Systems",
        "url": "http://arxiv.org/abs/2401.09034v1",
        "pub_date": "2024-01-17",
        "summary": "Reinforcement learning (RL) has gained traction for enhancing user long-term\nexperiences in recommender systems by effectively exploring users' interests.\nHowever, modern recommender systems exhibit distinct user behavioral patterns\namong tens of millions of items, which increases the difficulty of exploration.\nFor example, user behaviors with different activity levels require varying\nintensity of exploration, while previous studies often overlook this aspect and\napply a uniform exploration strategy to all users, which ultimately hurts user\nexperiences in the long run. To address these challenges, we propose\nUser-Oriented Exploration Policy (UOEP), a novel approach facilitating\nfine-grained exploration among user groups. We first construct a distributional\ncritic which allows policy optimization under varying quantile levels of\ncumulative reward feedbacks from users, representing user groups with varying\nactivity levels. Guided by this critic, we devise a population of distinct\nactors aimed at effective and fine-grained exploration within its respective\nuser group. To simultaneously enhance diversity and stability during the\nexploration process, we further introduce a population-level diversity\nregularization term and a supervision module. Experimental results on public\nrecommendation datasets demonstrate that our approach outperforms all other\nbaselines in terms of long-term performance, validating its user-oriented\nexploration effectiveness. Meanwhile, further analyses reveal our approach's\nbenefits of improved performance for low-activity users as well as increased\nfairness among users.",
        "translated": ""
    },
    {
        "title": "Estimating Gender Completeness in Wikipedia",
        "url": "http://arxiv.org/abs/2401.08993v1",
        "pub_date": "2024-01-17",
        "summary": "Gender imbalance in Wikipedia content is a known challenge which the editor\ncommunity is actively addressing. The aim of this paper is to provide the\nWikipedia community with instruments to estimate the magnitude of the problem\nfor different entity types (also known as classes) in Wikipedia. To this end,\nwe apply class completeness estimation methods based on the gender attribute.\nOur results show not only which gender for different sub-classes of Person is\nmore prevalent in Wikipedia, but also an idea of how complete the coverage is\nfor difference genders and sub-classes of Person.",
        "translated": ""
    },
    {
        "title": "Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for\n  Tempo Prediction and Search",
        "url": "http://arxiv.org/abs/2401.08902v1",
        "pub_date": "2024-01-17",
        "summary": "Audio embeddings enable large scale comparisons of the similarity of audio\nfiles for applications such as search and recommendation. Due to the\nsubjectivity of audio similarity, it can be desirable to design systems that\nanswer not only whether audio is similar, but similar in what way (e.g., wrt.\ntempo, mood or genre). Previous works have proposed disentangled embedding\nspaces where subspaces representing specific, yet possibly correlated,\nattributes can be weighted to emphasize those attributes in downstream tasks.\nHowever, no research has been conducted into the independence of these\nsubspaces, nor their manipulation, in order to retrieve tracks that are similar\nbut different in a specific way. Here, we explore the manipulation of tempo in\nembedding spaces as a case-study towards this goal. We propose tempo\ntranslation functions that allow for efficient manipulation of tempo within a\npre-existing embedding space whilst maintaining other properties such as genre.\nAs this translation is specific to tempo it enables retrieval of tracks that\nare similar but have specifically different tempi. We show that such a function\ncan be used as an efficient data augmentation strategy for both training of\ndownstream tempo predictors, and improved nearest neighbor retrieval of\nproperties largely independent of tempo.",
        "translated": ""
    },
    {
        "title": "On the Effect of Data-Augmentation on Local Embedding Properties in the\n  Contrastive Learning of Music Audio Representations",
        "url": "http://arxiv.org/abs/2401.08889v1",
        "pub_date": "2024-01-17",
        "summary": "Audio embeddings are crucial tools in understanding large catalogs of music.\nTypically embeddings are evaluated on the basis of the performance they provide\nin a wide range of downstream tasks, however few studies have investigated the\nlocal properties of the embedding spaces themselves which are important in\nnearest neighbor algorithms, commonly used in music search and recommendation.\nIn this work we show that when learning audio representations on music datasets\nvia contrastive learning, musical properties that are typically homogeneous\nwithin a track (e.g., key and tempo) are reflected in the locality of\nneighborhoods in the resulting embedding space. By applying appropriate data\naugmentation strategies, localisation of such properties can not only be\nreduced but the localisation of other attributes is increased. For example,\nlocality of features such as pitch and tempo that are less relevant to\nnon-expert listeners, may be mitigated while improving the locality of more\nsalient features such as genre and mood, achieving state-of-the-art performance\nin nearest neighbor retrieval accuracy. Similarly, we show that the optimal\nselection of data augmentation strategies for contrastive learning of music\naudio embeddings is dependent on the downstream task, highlighting this as an\nimportant embedding design decision.",
        "translated": ""
    },
    {
        "title": "Exploring Content-Based and Meta-Data Analysis for Detecting Fake News\n  Infodemic: A case study on COVID-19",
        "url": "http://arxiv.org/abs/2401.08841v1",
        "pub_date": "2024-01-16",
        "summary": "The coronavirus pandemic (COVID-19) is probably the most disruptive global\nhealth disaster in recent history. It negatively impacted the whole world and\nvirtually brought the global economy to a standstill. However, as the virus was\nspreading, infecting people and claiming thousands of lives so was the spread\nand propagation of fake news, misinformation and disinformation about the\nevent. These included the spread of unconfirmed health advice and remedies on\nsocial media. In this paper, false information about the pandemic is identified\nusing a content-based approach and metadata curated from messages posted to\nonline social networks. A content-based approach combined with metadata as well\nas an initial feature analysis is used and then several supervised learning\nmodels are tested for identifying and predicting misleading posts. Our approach\nshows up to 93% accuracy in the detection of fake news related posts about the\nCOVID-19 pandemic",
        "translated": ""
    },
    {
        "title": "Link Me Baby One More Time: Social Music Discovery on Spotify",
        "url": "http://arxiv.org/abs/2401.08818v1",
        "pub_date": "2024-01-16",
        "summary": "We explore the social and contextual factors that influence the outcome of\nperson-to-person music recommendations and discovery. Specifically, we use data\nfrom Spotify to investigate how a link sent from one user to another results in\nthe receiver engaging with the music of the shared artist. We consider several\nfactors that may influence this process, such as the strength of the\nsender-receiver relationship, the user's role in the Spotify social network,\ntheir music social cohesion, and how similar the new artist is to the\nreceiver's taste. We find that the receiver of a link is more likely to engage\nwith a new artist when (1) they have similar music taste to the sender and the\nshared track is a good fit for their taste, (2) they have a stronger and more\nintimate tie with the sender, and (3) the shared artist is popular with the\nreceiver's connections. Finally, we use these findings to build a Random Forest\nclassifier to predict whether a shared music track will result in the\nreceiver's engagement with the shared artist. This model elucidates which type\nof social and contextual features are most predictive, although peak\nperformance is achieved when a diverse set of features are included. These\nfindings provide new insights into the multifaceted mechanisms underpinning the\ninterplay between music discovery and social processes.",
        "translated": ""
    },
    {
        "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
        "url": "http://arxiv.org/abs/2401.10225v1",
        "pub_date": "2024-01-18",
        "summary": "In this work, we introduce ChatQA, a family of conversational question\nanswering (QA) models, that obtain GPT-4 level accuracies. Specifically, we\npropose a two-stage instruction tuning method that can significantly improve\nthe zero-shot conversational QA results from large language models (LLMs). To\nhandle retrieval in conversational QA, we fine-tune a dense retriever on a\nmulti-turn QA dataset, which provides comparable results to using the\nstate-of-the-art query rewriting model while largely reducing deployment cost.\nNotably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10\nconversational QA datasets (54.14 vs. 53.90), without relying on any synthetic\ndata from OpenAI GPT models.",
        "translated": ""
    },
    {
        "title": "Comparing Traditional and LLM-based Search for Image Geolocation",
        "url": "http://arxiv.org/abs/2401.10184v1",
        "pub_date": "2024-01-18",
        "summary": "Web search engines have long served as indispensable tools for information\nretrieval; user behavior and query formulation strategies have been well\nstudied. The introduction of search engines powered by large language models\n(LLMs) suggested more conversational search and new types of query strategies.\nIn this paper, we compare traditional and LLM-based search for the task of\nimage geolocation, i.e., determining the location where an image was captured.\nOur work examines user interactions, with a particular focus on query\nformulation strategies. In our study, 60 participants were assigned either\ntraditional or LLM-based search engines as assistants for geolocation.\nParticipants using traditional search more accurately predicted the location of\nthe image compared to those using the LLM-based search. Distinct strategies\nemerged between users depending on the type of assistant. Participants using\nthe LLM-based search issued longer, more natural language queries, but had\nshorter search sessions. When reformulating their search queries, traditional\nsearch participants tended to add more terms to their initial queries, whereas\nparticipants using the LLM-based search consistently rephrased their initial\nqueries.",
        "translated": ""
    },
    {
        "title": "LOCALINTEL: Generating Organizational Threat Intelligence from Global\n  and Local Cyber Knowledge",
        "url": "http://arxiv.org/abs/2401.10036v1",
        "pub_date": "2024-01-18",
        "summary": "Security Operations Center (SoC) analysts gather threat reports from openly\naccessible global threat databases and customize them manually to suit a\nparticular organization's needs. These analysts also depend on internal\nrepositories, which act as private local knowledge database for an\norganization. Credible cyber intelligence, critical operational details, and\nrelevant organizational information are all stored in these local knowledge\ndatabases. Analysts undertake a labor intensive task utilizing these global and\nlocal knowledge databases to manually create organization's unique threat\nresponse and mitigation strategies. Recently, Large Language Models (LLMs) have\nshown the capability to efficiently process large diverse knowledge sources. We\nleverage this ability to process global and local knowledge databases to\nautomate the generation of organization-specific threat intelligence.\n  In this work, we present LOCALINTEL, a novel automated knowledge\ncontextualization system that, upon prompting, retrieves threat reports from\nthe global threat repositories and uses its local knowledge database to\ncontextualize them for a specific organization. LOCALINTEL comprises of three\nkey phases: global threat intelligence retrieval, local knowledge retrieval,\nand contextualized completion generation. The former retrieves intelligence\nfrom global threat repositories, while the second retrieves pertinent knowledge\nfrom the local knowledge database. Finally, the fusion of these knowledge\nsources is orchestrated through a generator to produce a contextualized\ncompletion.",
        "translated": ""
    },
    {
        "title": "HGAttack: Transferable Heterogeneous Graph Adversarial Attack",
        "url": "http://arxiv.org/abs/2401.09945v1",
        "pub_date": "2024-01-18",
        "summary": "Heterogeneous Graph Neural Networks (HGNNs) are increasingly recognized for\ntheir performance in areas like the web and e-commerce, where resilience\nagainst adversarial attacks is crucial. However, existing adversarial attack\nmethods, which are primarily designed for homogeneous graphs, fall short when\napplied to HGNNs due to their limited ability to address the structural and\nsemantic complexity of HGNNs. This paper introduces HGAttack, the first\ndedicated gray box evasion attack method for heterogeneous graphs. We design a\nnovel surrogate model to closely resemble the behaviors of the target HGNN and\nutilize gradient-based methods for perturbation generation. Specifically, the\nproposed surrogate model effectively leverages heterogeneous information by\nextracting meta-path induced subgraphs and applying GNNs to learn node\nembeddings with distinct semantics from each subgraph. This approach improves\nthe transferability of generated attacks on the target HGNN and significantly\nreduces memory costs. For perturbation generation, we introduce a\nsemantics-aware mechanism that leverages subgraph gradient information to\nautonomously identify vulnerable edges across a wide range of relations within\na constrained perturbation budget. We validate HGAttack's efficacy with\ncomprehensive experiments on three datasets, providing empirical analyses of\nits generated perturbations. Outperforming baseline methods, HGAttack\ndemonstrated significant efficacy in diminishing the performance of target HGNN\nmodels, affirming the effectiveness of our approach in evaluating the\nrobustness of HGNNs against adversarial attacks.",
        "translated": ""
    },
    {
        "title": "Source Code Clone Detection Using Unsupervised Similarity Measures",
        "url": "http://arxiv.org/abs/2401.09885v1",
        "pub_date": "2024-01-18",
        "summary": "Assessing similarity in source code has gained significant attention in\nrecent years due to its importance in software engineering tasks such as clone\ndetection and code search and recommendation. This work presents a comparative\nanalysis of unsupervised similarity measures for identifying source code clone\ndetection. The goal is to overview the current state-of-the-art techniques,\ntheir strengths, and weaknesses. To do that, we compile the existing\nunsupervised strategies and evaluate their performance on a benchmark dataset\nto guide software engineers in selecting appropriate methods for their specific\nuse cases. The source code of this study is available at\n\\url{https://github.com/jorge-martinez-gil/codesim}",
        "translated": ""
    },
    {
        "title": "MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation\n  Extraction for Material Science Knowledge-base Construction",
        "url": "http://arxiv.org/abs/2401.09839v1",
        "pub_date": "2024-01-18",
        "summary": "Material science literature is a rich source of factual information about\nvarious categories of entities (like materials and compositions) and various\nrelations between these entities, such as conductivity, voltage, etc.\nAutomatically extracting this information to generate a material science\nknowledge base is a challenging task. In this paper, we propose MatSciRE\n(Material Science Relation Extractor), a Pointer Network-based encoder-decoder\nframework, to jointly extract entities and relations from material science\narticles as a triplet ($entity1, relation, entity2$). Specifically, we target\nthe battery materials and identify five relations to work on - conductivity,\ncoulombic efficiency, capacity, voltage, and energy. Our proposed approach\nachieved a much better F1-score (0.771) than a previous attempt using\nChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown\nin Fig 1. The material information is extracted from material science\nliterature in the form of entity-relation triplets using MatSciRE.",
        "translated": ""
    },
    {
        "title": "Enhancing Image-Text Matching with Adaptive Feature Aggregation",
        "url": "http://arxiv.org/abs/2401.09725v1",
        "pub_date": "2024-01-18",
        "summary": "Image-text matching aims to find matched cross-modal pairs accurately. While\ncurrent methods often rely on projecting cross-modal features into a common\nembedding space, they frequently suffer from imbalanced feature representations\nacross different modalities, leading to unreliable retrieval results. To\naddress these limitations, we introduce a novel Feature Enhancement Module that\nadaptively aggregates single-modal features for more balanced and robust\nimage-text retrieval. Additionally, we propose a new loss function that\novercomes the shortcomings of original triplet ranking loss, thereby\nsignificantly improving retrieval performance. The proposed model has been\nevaluated on two public datasets and achieves competitive retrieval performance\nwhen compared with several state-of-the-art models. Implementation codes can be\nfound here.",
        "translated": ""
    },
    {
        "title": "EfficientRec an unlimited user-item scale recommendation system based on\n  clustering and users interaction embedding profile",
        "url": "http://arxiv.org/abs/2401.09693v1",
        "pub_date": "2024-01-18",
        "summary": "Recommendation systems are highly interested in technology companies\nnowadays. The businesses are constantly growing users and products, causing the\nnumber of users and items to continuously increase over time, to very large\nnumbers. Traditional recommendation algorithms with complexity dependent on the\nnumber of users and items make them difficult to adapt to the industrial\nenvironment. In this paper, we introduce a new method applying graph neural\nnetworks with a contrastive learning framework in extracting user preferences.\nWe incorporate a soft clustering architecture that significantly reduces the\ncomputational cost of the inference process. Experiments show that the model is\nable to learn user preferences with low computational cost in both training and\nprediction phases. At the same time, the model gives a very good accuracy. We\ncall this architecture EfficientRec with the implication of model compactness\nand the ability to scale to unlimited users and products.",
        "translated": ""
    },
    {
        "title": "Handling Large-scale Cardinality in building recommendation systems",
        "url": "http://arxiv.org/abs/2401.09572v1",
        "pub_date": "2024-01-17",
        "summary": "Effective recommendation systems rely on capturing user preferences, often\nrequiring incorporating numerous features such as universally unique\nidentifiers (UUIDs) of entities. However, the exceptionally high cardinality of\nUUIDs poses a significant challenge in terms of model degradation and increased\nmodel size due to sparsity. This paper presents two innovative techniques to\naddress the challenge of high cardinality in recommendation systems.\nSpecifically, we propose a bag-of-words approach, combined with layer sharing,\nto substantially decrease the model size while improving performance. Our\ntechniques were evaluated through offline and online experiments on Uber use\ncases, resulting in promising results demonstrating our approach's\neffectiveness in optimizing recommendation systems and enhancing their overall\nperformance.",
        "translated": ""
    },
    {
        "title": "Using LLMs to discover emerging coded antisemitic hate-speech emergence\n  in extremist social media",
        "url": "http://arxiv.org/abs/2401.10841v1",
        "pub_date": "2024-01-19",
        "summary": "Online hate speech proliferation has created a difficult problem for social\nmedia platforms. A particular challenge relates to the use of coded language by\ngroups interested in both creating a sense of belonging for its users and\nevading detection. Coded language evolves quickly and its use varies over time.\nThis paper proposes a methodology for detecting emerging coded hate-laden\nterminology. The methodology is tested in the context of online antisemitic\ndiscourse. The approach considers posts scraped from social media platforms,\noften used by extremist users. The posts are scraped using seed expressions\nrelated to previously known discourse of hatred towards Jews. The method begins\nby identifying the expressions most representative of each post and calculating\ntheir frequency in the whole corpus. It filters out grammatically incoherent\nexpressions as well as previously encountered ones so as to focus on emergent\nwell-formed terminology. This is followed by an assessment of semantic\nsimilarity to known antisemitic terminology using a fine-tuned large language\nmodel, and subsequent filtering out of the expressions that are too distant\nfrom known expressions of hatred. Emergent antisemitic expressions containing\nterms clearly relating to Jewish topics are then removed to return only coded\nexpressions of hatred.",
        "translated": ""
    },
    {
        "title": "Dynamic Q&amp;A of Clinical Documents with Large Language Models",
        "url": "http://arxiv.org/abs/2401.10733v1",
        "pub_date": "2024-01-19",
        "summary": "Electronic health records (EHRs) house crucial patient data in clinical\nnotes. As these notes grow in volume and complexity, manual extraction becomes\nchallenging. This work introduces a natural language interface using large\nlanguage models (LLMs) for dynamic question-answering on clinical notes. Our\nchatbot, powered by Langchain and transformer-based LLMs, allows users to query\nin natural language, receiving relevant answers from clinical notes.\nExperiments, utilizing various embedding models and advanced LLMs, show Wizard\nVicuna's superior accuracy, albeit with high compute demands. Model\noptimization, including weight quantization, improves latency by approximately\n48 times. Promising results indicate potential, yet challenges such as model\nhallucinations and limited diverse medical case evaluations remain. Addressing\nthese gaps is crucial for unlocking the value in clinical notes and advancing\nAI-driven clinical decision-making.",
        "translated": ""
    },
    {
        "title": "Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and\n  unfairness in dyadic regression models",
        "url": "http://arxiv.org/abs/2401.10690v1",
        "pub_date": "2024-01-19",
        "summary": "Dyadic regression models, which predict real-valued outcomes for pairs of\nentities, are fundamental in many domains (e.g. predicting the rating of a user\nto a product in Recommender Systems) and promising and under exploration in\nmany others (e.g. approximating the adequate dosage of a drug for a patient in\npersonalized pharmacology). In this work, we demonstrate that non-uniformity in\nthe observed value distributions of individual entities leads to severely\nbiased predictions in state-of-the-art models, skewing predictions towards the\naverage of observed past values for the entity and providing worse-than-random\npredictive power in eccentric yet equally important cases. We show that the\nusage of global error metrics like Root Mean Squared Error (RMSE) and Mean\nAbsolute Error (MAE) is insufficient to capture this phenomenon, which we name\neccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as\na new complementary metric that can quantify it in all studied models and\ndatasets. We also prove the adequateness of EAUC by using naive de-biasing\ncorrections to demonstrate that a lower model bias correlates with a lower EAUC\nand vice-versa. This work contributes a bias-aware evaluation of dyadic\nregression models to avoid potential unfairness and risks in critical\nreal-world applications of such systems.",
        "translated": ""
    },
    {
        "title": "Automatic Construction of Multi-faceted User Profiles using Text\n  Clustering and its Application to Expert Recommendation and Filtering\n  Problems",
        "url": "http://arxiv.org/abs/2401.10634v1",
        "pub_date": "2024-01-19",
        "summary": "In the information age we are living in today, not only are we interested in\naccessing multimedia objects such as documents, videos, etc. but also in\nsearching for professional experts, people or celebrities, possibly for\nprofessional needs or just for fun. Information access systems need to be able\nto extract and exploit various sources of information (usually in text format)\nabout such individuals, and to represent them in a suitable way usually in the\nform of a profile. In this article, we tackle the problems of profile-based\nexpert recommendation and document filtering from a machine learning\nperspective by clustering expert textual sources to build profiles and capture\nthe different hidden topics in which the experts are interested. The experts\nwill then be represented by means of multi-faceted profiles. Our experiments\nshow that this is a valid technique to improve the performance of expert\nfinding and document filtering.",
        "translated": ""
    },
    {
        "title": "LDA-based Term Profiles for Expert Finding in a Political Setting",
        "url": "http://arxiv.org/abs/2401.10617v1",
        "pub_date": "2024-01-19",
        "summary": "A common task in many political institutions (i.e. Parliament) is to find\npoliticians who are experts in a particular field. In order to tackle this\nproblem, the first step is to obtain politician profiles which include their\ninterests, and these can be automatically learned from their speeches. As a\npolitician may have various areas of expertise, one alternative is to use a set\nof subprofiles, each of which covers a different subject. In this study, we\npropose a novel approach for this task by using latent Dirichlet allocation\n(LDA) to determine the main underlying topics of each political speech, and to\ndistribute the related terms among the different topic-based subprofiles. With\nthis objective, we propose the use of fifteen distance and similarity measures\nto automatically determine the optimal number of topics discussed in a\ndocument, and to demonstrate that every measure converges into five strategies:\nEuclidean, Dice, Sorensen, Cosine and Overlap. Our experimental results showed\nthat the scores of the different accuracy metrics of the proposed strategies\ntended to be higher than those of the baselines for expert recommendation\ntasks, and that the use of an appropriate number of topics has proved relevant.",
        "translated": ""
    },
    {
        "title": "Publication venue recommendation using profiles based on clustering",
        "url": "http://arxiv.org/abs/2401.10611v1",
        "pub_date": "2024-01-19",
        "summary": "In this paper we study the venue recommendation problem in order to help\nresearchers to identify a journal or conference to submit a given paper. A\ncommon approach to tackle this problem is to build profiles defining the scope\nof each venue. Then, these profiles are compared against the target paper. In\nour approach we will study how clustering techniques can be used to construct\ntopic-based profiles and use an Information Retrieval based approach to obtain\nthe final recommendations. Additionally, we will explore how the use of\nauthorship, representing a complementary piece of information, helps to improve\nthe recommendations.",
        "translated": ""
    },
    {
        "title": "Use of topical and temporal profiles and their hybridisation for\n  content-based recommendation",
        "url": "http://arxiv.org/abs/2401.10607v1",
        "pub_date": "2024-01-19",
        "summary": "In the context of content-based recommender systems, the aim of this paper is\nto determine how better profiles can be built and how these affect the\nrecommendation process based on the incorporation of temporality, i.e. the\ninclusion of time in the recommendation process, and topicality, i.e. the\nrepresentation of texts associated with users and items using topics and their\ncombination. The main contribution of the paper is to present two different\nways of hybridising these two dimensions and to evaluate and compare them with\nother alternatives.",
        "translated": ""
    },
    {
        "title": "Understanding Biases in ChatGPT-based Recommender Systems: Provider\n  Fairness, Temporal Stability, and Recency",
        "url": "http://arxiv.org/abs/2401.10545v1",
        "pub_date": "2024-01-19",
        "summary": "This study explores the nuanced capabilities and inherent biases of\nRecommender Systems using Large Language Models (RecLLMs), with a focus on\nChatGPT-based systems. It studies into the contrasting behaviors of generative\nmodels and traditional collaborative filtering models in movie recommendations.\nThe research primarily investigates prompt design strategies and their impact\non various aspects of recommendation quality, including accuracy, provider\nfairness, diversity, stability, genre dominance, and temporal freshness\n(recency).\n  Our experimental analysis reveals that the introduction of specific 'system\nroles' and 'prompt strategies' in RecLLMs significantly influences their\nperformance. For instance, role-based prompts enhance fairness and diversity in\nrecommendations, mitigating popularity bias. We find that while GPT-based\nmodels do not always match the performance of CF baselines, they exhibit a\nunique tendency to recommend newer and more diverse movie genres. Notably,\nGPT-based models tend to recommend more recent films, particularly those\nreleased post-2000, and show a preference for genres like \\sq{Drama} and\nComedy, and Romance (compared to CF Action, Adventure) presumably due to the\nRecLLMs' training on varied data sets, which allows them to capture recent\ntrends and discussions more effectively than CF models. Interestingly, our\nresults demonstrate that the 'Simple' and 'Chain of Thought (COT)' paradigms\nyield the highest accuracy. These findings imply the potential of combining\nthese strategies with scenarios that favor more recent content, thereby\noffering a more balanced and up-to-date recommendation experience. This study\ncontributes significantly to the understanding of emerging RecLLMs,\nparticularly in the context of harms and biases within these systems.",
        "translated": ""
    },
    {
        "title": "Generative Dense Retrieval: Memory Can Be a Burden",
        "url": "http://arxiv.org/abs/2401.10487v1",
        "pub_date": "2024-01-19",
        "summary": "Generative Retrieval (GR), autoregressively decoding relevant document\nidentifiers given a query, has been shown to perform well under the setting of\nsmall-scale corpora. By memorizing the document corpus with model parameters,\nGR implicitly achieves deep interaction between query and document. However,\nsuch a memorizing mechanism faces three drawbacks: (1) Poor memory accuracy for\nfine-grained features of documents; (2) Memory confusion gets worse as the\ncorpus size increases; (3) Huge memory update costs for new documents. To\nalleviate these problems, we propose the Generative Dense Retrieval (GDR)\nparadigm. Specifically, GDR first uses the limited memory volume to achieve\ninter-cluster matching from query to relevant document clusters.\nMemorizing-free matching mechanism from Dense Retrieval (DR) is then introduced\nto conduct fine-grained intra-cluster matching from clusters to relevant\ndocuments. The coarse-to-fine process maximizes the advantages of GR's deep\ninteraction and DR's scalability. Besides, we design a cluster identifier\nconstructing strategy to facilitate corpus memory and a cluster-adaptive\nnegative sampling strategy to enhance the intra-cluster mapping ability.\nEmpirical results show that GDR obtains an average of 3.0 R@100 improvement on\nNQ dataset under multiple settings and has better scalability.",
        "translated": ""
    },
    {
        "title": "Enhancing Scalability in Recommender Systems through Lottery Ticket\n  Hypothesis and Knowledge Distillation-based Neural Network Pruning",
        "url": "http://arxiv.org/abs/2401.10484v1",
        "pub_date": "2024-01-19",
        "summary": "This study introduces an innovative approach aimed at the efficient pruning\nof neural networks, with a particular focus on their deployment on edge\ndevices. Our method involves the integration of the Lottery Ticket Hypothesis\n(LTH) with the Knowledge Distillation (KD) framework, resulting in the\nformulation of three distinct pruning models. These models have been developed\nto address scalability issue in recommender systems, whereby the complexities\nof deep learning models have hindered their practical deployment. With\njudicious application of the pruning techniques, we effectively curtail the\npower consumption and model dimensions without compromising on accuracy.\nEmpirical evaluation has been performed using two real world datasets from\ndiverse domains against two baselines. Gratifyingly, our approaches yielded a\nGPU computation-power reduction of up to 66.67%. Notably, our study contributes\nto the field of recommendation system by pioneering the application of LTH and\nKD.",
        "translated": ""
    },
    {
        "title": "Revisiting Document-Level Relation Extraction with Context-Guided Link\n  Prediction",
        "url": "http://arxiv.org/abs/2401.11800v1",
        "pub_date": "2024-01-22",
        "summary": "Document-level relation extraction (DocRE) poses the challenge of identifying\nrelationships between entities within a document as opposed to the traditional\nRE setting where a single sentence is input. Existing approaches rely on\nlogical reasoning or contextual cues from entities. This paper reframes\ndocument-level RE as link prediction over a knowledge graph with distinct\nbenefits: 1) Our approach combines entity context with document-derived logical\nreasoning, enhancing link prediction quality. 2) Predicted links between\nentities offer interpretability, elucidating employed reasoning. We evaluate\nour approach on three benchmark datasets: DocRED, ReDocRED, and DWIE. The\nresults indicate that our proposed method outperforms the state-of-the-art\nmodels and suggests that incorporating context-based link prediction techniques\ncan enhance the performance of document-level relation extraction models.",
        "translated": ""
    },
    {
        "title": "Knowledge Navigation: Inferring the Interlocking Map of Knowledge from\n  Research Trajectories",
        "url": "http://arxiv.org/abs/2401.11742v1",
        "pub_date": "2024-01-22",
        "summary": "\"If I have seen further, it is by standing on the shoulders of giants,\" Isaac\nNewton's renowned statement hints that new knowledge builds upon existing\nfoundations, which means there exists an interdependent relationship between\nknowledge, which, yet uncovered, is implied in the historical development of\nscientific systems for hundreds of years. By leveraging natural language\nprocessing techniques, this study introduces an innovative embedding scheme\ndesigned to infer the \"knowledge interlocking map.\" This map, derived from the\nresearch trajectories of millions of scholars, reveals the intricate\nconnections among knowledge. We validate that the inferred map effectively\ndelineates disciplinary boundaries and captures the intricate relationships\nbetween diverse concepts. The utility of the interlocking map is showcased\nthrough multiple applications. Firstly, we demonstrated the multi-step analogy\ninferences within the knowledge space and the functional connectivity between\nconcepts in different disciplines. Secondly, we trace the evolution of\nknowledge across domains, observing trends such as shifts from \"Theoretical\" to\n\"Applied\" or \"Chemistry\" to \"Biomedical\" along predefined functional\ndirections. Lastly, by analyzing the high-dimensional knowledge network\nstructure, we found that knowledge connects each other with shorter global\npathways, and the interdisciplinary knowledge plays a critical role in\naccessibility of the global knowledge network. Our framework offers a novel\napproach to mining knowledge inheritance pathways in extensive scientific\nliterature, which is of great significance for understanding scientific\ndevelopment patterns, tailoring scientific learning trajectories, and\naccelerating scientific progress.",
        "translated": ""
    },
    {
        "title": "Domain-Aware Cross-Attention for Cross-domain Recommendation",
        "url": "http://arxiv.org/abs/2401.11705v1",
        "pub_date": "2024-01-22",
        "summary": "Cross-domain recommendation (CDR) is an important method to improve\nrecommender system performance, especially when observations in target domains\nare sparse. However, most existing cross-domain recommendations fail to fully\nutilize the target domain's special features and are hard to be generalized to\nnew domains. The designed network is complex and is not suitable for rapid\nindustrial deployment. Our method introduces a two-step domain-aware\ncross-attention, extracting transferable features of the source domain from\ndifferent granularity, which allows the efficient expression of both domain and\nuser interests. In addition, we simplify the training process, and our model\ncan be easily deployed on new domains. We conduct experiments on both public\ndatasets and industrial datasets, and the experimental results demonstrate the\neffectiveness of our method. We have also deployed the model in an online\nadvertising system and observed significant improvements in both\nClick-Through-Rate (CTR) and effective cost per mille (ECPM).",
        "translated": ""
    },
    {
        "title": "Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal\n  Contrastive EHR Modelling with Hierarchical Regularisation",
        "url": "http://arxiv.org/abs/2401.11648v1",
        "pub_date": "2024-01-22",
        "summary": "Predicting next visit diagnosis using Electronic Health Records (EHR) is an\nessential task in healthcare, critical for devising proactive future plans for\nboth healthcare providers and patients. Nonetheless, many preceding studies\nhave not sufficiently addressed the heterogeneous and hierarchical\ncharacteristics inherent in EHR data, inevitably leading to sub-optimal\nperformance. To this end, we propose NECHO, a novel medical code-centric\nmultimodal contrastive EHR learning framework with hierarchical regularisation.\nFirst, we integrate multifaceted information encompassing medical codes,\ndemographics, and clinical notes using a tailored network design and a pair of\nbimodal contrastive losses, all of which pivot around a medical code\nrepresentation. We also regularise modality-specific encoders using a parental\nlevel information in medical ontology to learn hierarchical structure of EHR\ndata. A series of experiments on MIMIC-III data demonstrates effectiveness of\nour approach.",
        "translated": ""
    },
    {
        "title": "What Are We Optimizing For? A Human-centric Evaluation Of Deep\n  Learning-based Recommender Systems",
        "url": "http://arxiv.org/abs/2401.11632v1",
        "pub_date": "2024-01-21",
        "summary": "Deep learning-based (DL) models in recommender systems (RecSys) have gained\nsignificant recognition for their remarkable accuracy in predicting user\npreferences. However, their performance often lacks a comprehensive evaluation\nfrom a human-centric perspective, which encompasses various dimensions beyond\nsimple interest matching. In this work, we have developed a robust\nhuman-centric evaluation framework that incorporates seven diverse metrics to\nassess the quality of recommendations generated by five recent open-sourced DL\nmodels. Our evaluation datasets consist of both offline benchmark data and\npersonalized online recommendation feedback collected from 445 real users. We\nfind that (1) different DL models have different pros and cons in the\nmulti-dimensional metrics that we test with; (2) users generally want a\ncombination of accuracy with at least one another human values in the\nrecommendation; (3) the degree of combination of different values needs to be\ncarefully experimented to user preferred level.",
        "translated": ""
    },
    {
        "title": "In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey",
        "url": "http://arxiv.org/abs/2401.11624v1",
        "pub_date": "2024-01-21",
        "summary": "Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.",
        "translated": ""
    },
    {
        "title": "Simple Domain Adaptation for Sparse Retrievers",
        "url": "http://arxiv.org/abs/2401.11509v1",
        "pub_date": "2024-01-21",
        "summary": "In Information Retrieval, and more generally in Natural Language Processing,\nadapting models to specific domains is conducted through fine-tuning. Despite\nthe successes achieved by this method and its versatility, the need for\nhuman-curated and labeled data makes it impractical to transfer to new tasks,\ndomains, and/or languages when training data doesn't exist. Using the model\nwithout training (zero-shot) is another option that however suffers an\neffectiveness cost, especially in the case of first-stage retrievers. Numerous\nresearch directions have emerged to tackle these issues, most of them in the\ncontext of adapting to a task or a language. However, the literature is scarcer\nfor domain (or topic) adaptation. In this paper, we address this issue of\ncross-topic discrepancy for a sparse first-stage retriever by transposing a\nmethod initially designed for language adaptation. By leveraging pre-training\non the target data to learn domain-specific knowledge, this technique\nalleviates the need for annotated data and expands the scope of domain\nadaptation. Despite their relatively good generalization ability, we show that\neven sparse retrievers can benefit from our simple domain adaptation method.",
        "translated": ""
    },
    {
        "title": "Enhancing Recommendation Diversity by Re-ranking with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2401.11506v1",
        "pub_date": "2024-01-21",
        "summary": "It has long been recognized that it is not enough for a Recommender System\n(RS) to provide recommendations based only on their relevance to users. Among\nmany other criteria, the set of recommendations may need to be diverse in order\nto handle uncertainty and offer a meaningful choice. The literature reports\nmany ways of measuring diversity and ways of improving the diversity of a set\nof recommendations, most notably by re-ranking and selecting from a larger set\nof candidate recommendations. Driven by promising insights from the literature\non how to incorporate versatile Large Language Models (LLMs) into the RS\npipeline, in this paper, we show how LLMs can be used for diversity re-ranking.\n  We begin with an informal study that verifies that LLMs can be used for\nre-ranking tasks and do have some understanding of the concept of diversity.\nThen, we design a more rigorous methodology where LLMs are prompted to generate\na diverse ranking from a candidate ranking using various prompt templates with\ndifferent re-ranking instructions in a zero-shot fashion. We conduct\ncomprehensive experiments testing state-of-the-art conversational LLMs from the\nGPT and Llama families. We compare their re-ranking capabilities with random\nre-ranking and various traditional re-ranking methods from the literature (MMR,\nxQuAD and RxQuAD). We find that LLM-based re-ranking outperforms random\nre-ranking across all the metrics that we use but does not perform as well as\nthe traditional re-ranking methods. We gain insight into prompt design for this\ntask (e.g.\\ on the whole, it is better to prompt for diversity rather than a\nbalance of diversity and relevance). Given that no special knowledge\nengineering is needed, we conclude that LLM-based re-ranking is a promising\napproach, and we highlight directions for future research. We open-source the\ncode of our experiments for reproducibility.",
        "translated": ""
    },
    {
        "title": "CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray\n  Report Labeling",
        "url": "http://arxiv.org/abs/2401.11505v1",
        "pub_date": "2024-01-21",
        "summary": "Free-text radiology reports present a rich data source for various medical\ntasks, but effectively labeling these texts remains challenging. Traditional\nrule-based labeling methods fall short of capturing the nuances of diverse\nfree-text patterns. Moreover, models using expert-annotated data are limited by\ndata scarcity and pre-defined classes, impacting their performance, flexibility\nand scalability. To address these issues, our study offers three main\ncontributions: 1) We demonstrate the potential of GPT as an adept labeler using\ncarefully designed prompts. 2) Utilizing only the data labeled by GPT, we\ntrained a BERT-based labeler, CheX-GPT, which operates faster and more\nefficiently than its GPT counterpart. 3) To benchmark labeler performance, we\nintroduced a publicly available expert-annotated test set, MIMIC-500,\ncomprising 500 cases from the MIMIC validation set. Our findings demonstrate\nthat CheX-GPT not only excels in labeling accuracy over existing models, but\nalso showcases superior efficiency, flexibility, and scalability, supported by\nour introduction of the MIMIC-500 dataset for robust benchmarking. Code and\nmodels are available at https://github.com/kakaobrain/CheXGPT.",
        "translated": ""
    },
    {
        "title": "D2K: Turning Historical Data into Retrievable Knowledge for Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2401.11478v1",
        "pub_date": "2024-01-21",
        "summary": "A vast amount of user behavior data is constantly accumulating on today's\nlarge recommendation platforms, recording users' various interests and tastes.\nPreserving knowledge from the old data while new data continually arrives is a\nvital problem for recommender systems. Existing approaches generally seek to\nsave the knowledge implicitly in the model parameters. However, such a\nparameter-centric approach lacks scalability and flexibility -- the capacity is\nhard to scale, and the knowledge is inflexible to utilize. Hence, in this work,\nwe propose a framework that turns massive user behavior data to retrievable\nknowledge (D2K). It is a data-centric approach that is model-agnostic and easy\nto scale up. Different from only storing unary knowledge such as the user-side\nor item-side information, D2K propose to store ternary knowledge for\nrecommendation, which is determined by the complete recommendation factors --\nuser, item, and context. The knowledge retrieved by target samples can be\ndirectly used to enhance the performance of any recommendation algorithms.\nSpecifically, we introduce a Transformer-based knowledge encoder to transform\nthe old data into knowledge with the user-item-context cross features. A\npersonalized knowledge adaptation unit is devised to effectively exploit the\ninformation from the knowledge base by adapting the retrieved knowledge to the\ntarget samples. Extensive experiments on two public datasets show that D2K\nsignificantly outperforms existing baselines and is compatible with a major\ncollection of recommendation algorithms.",
        "translated": ""
    },
    {
        "title": "Gradient Flow of Energy: A General and Efficient Approach for Entity\n  Alignment Decoding",
        "url": "http://arxiv.org/abs/2401.12798v1",
        "pub_date": "2024-01-23",
        "summary": "Entity alignment (EA), a pivotal process in integrating multi-source\nKnowledge Graphs (KGs), seeks to identify equivalent entity pairs across these\ngraphs. Most existing approaches regard EA as a graph representation learning\ntask, concentrating on enhancing graph encoders. However, the decoding process\nin EA - essential for effective operation and alignment accuracy - has received\nlimited attention and remains tailored to specific datasets and model\narchitectures, necessitating both entity and additional explicit relation\nembeddings. This specificity limits its applicability, particularly in\nGNN-based models. To address this gap, we introduce a novel, generalized, and\nefficient decoding approach for EA, relying solely on entity embeddings. Our\nmethod optimizes the decoding process by minimizing Dirichlet energy, leading\nto the gradient flow within the graph, to promote graph homophily. The\ndiscretization of the gradient flow produces a fast and scalable approach,\ntermed Triple Feature Propagation (TFP). TFP innovatively channels gradient\nflow through three views: entity-to-entity, entity-to-relation, and\nrelation-to-entity. This generalized gradient flow enables TFP to harness the\nmulti-view structural information of KGs. Rigorous experimentation on diverse\nreal-world datasets demonstrates that our approach significantly enhances\nvarious EA methods. Notably, the approach achieves these advancements with less\nthan 6 seconds of additional computational time, establishing a new benchmark\nin efficiency and adaptability for future EA methods.",
        "translated": ""
    },
    {
        "title": "CDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural\n  Process",
        "url": "http://arxiv.org/abs/2401.12732v1",
        "pub_date": "2024-01-23",
        "summary": "Cross-domain recommendation (CDR) has been proven as a promising way to\ntackle the user cold-start problem, which aims to make recommendations for\nusers in the target domain by transferring the user preference derived from the\nsource domain. Traditional CDR studies follow the embedding and mapping (EMCDR)\nparadigm, which transfers user representations from the source to target domain\nby learning a user-shared mapping function, neglecting the user-specific\npreference. Recent CDR studies attempt to learn user-specific mapping functions\nin meta-learning paradigm, which regards each user's CDR as an individual task,\nbut neglects the preference correlations among users, limiting the beneficial\ninformation for user representations. Moreover, both of the paradigms neglect\nthe explicit user-item interactions from both domains during the mapping\nprocess. To address the above issues, this paper proposes a novel CDR framework\nwith neural process (NP), termed as CDRNP. Particularly, it develops the\nmeta-learning paradigm to leverage user-specific preference, and further\nintroduces a stochastic process by NP to capture the preference correlations\namong the overlapping and cold-start users, thus generating more powerful\nmapping functions by mapping the user-specific preference and common preference\ncorrelations to a predictive probability distribution. In addition, we also\nintroduce a preference remainer to enhance the common preference from the\noverlapping users, and finally devises an adaptive conditional decoder with\npreference modulation to make prediction for cold-start users with items in the\ntarget domain. Experimental results demonstrate that CDRNP outperforms previous\nSOTA methods in three real-world CDR scenarios.",
        "translated": ""
    },
    {
        "title": "MOReGIn: Multi-Objective Recommendation at the Global and Individual\n  Levels",
        "url": "http://arxiv.org/abs/2401.12593v1",
        "pub_date": "2024-01-23",
        "summary": "Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to\nguarantee multiple (often conflicting) goals. Besides accuracy, a MORS can\noperate at the global level, where additional beyond-accuracy goals are met for\nthe system as a whole, or at the individual level, meaning that the\nrecommendations are tailored to the needs of each user. The state-of-the-art\nMORSs either operate at the global or individual level, without assuming the\nco-existence of the two perspectives. In this study, we show that when global\nand individual objectives co-exist, MORSs are not able to meet both types of\ngoals. To overcome this issue, we present an approach that regulates the\nrecommendation lists so as to guarantee both global and individual\nperspectives, while preserving its effectiveness. Specifically, as individual\nperspective, we tackle genre calibration and, as global perspective, provider\nfairness. We validate our approach on two real-world datasets, publicly\nreleased with this paper.",
        "translated": ""
    },
    {
        "title": "PolyCF: Towards the Optimal Spectral Graph Filters for Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2401.12590v1",
        "pub_date": "2024-01-23",
        "summary": "Collaborative Filtering (CF) is a pivotal research area in recommender\nsystems that capitalizes on collaborative similarities between users and items\nto provide personalized recommendations. With the remarkable achievements of\nnode embedding-based Graph Neural Networks (GNNs), we explore the upper bounds\nof expressiveness inherent to embedding-based methodologies and tackle the\nchallenges by reframing the CF task as a graph signal processing problem. To\nthis end, we propose PolyCF, a flexible graph signal filter that leverages\npolynomial graph filters to process interaction signals. PolyCF exhibits the\ncapability to capture spectral features across multiple eigenspaces through a\nseries of Generalized Gram filters and is able to approximate the optimal\npolynomial response function for recovering missing interactions. A graph\noptimization objective and a pair-wise ranking objective are jointly used to\noptimize the parameters of the convolution kernel. Experiments on three widely\nadopted datasets demonstrate the superiority of PolyCF over current\nstate-of-the-art CF methods. Moreover, comprehensive studies empirically\nvalidate each component's efficacy in the proposed PolyCF.",
        "translated": ""
    },
    {
        "title": "InfoRank: Unbiased Learning-to-Rank via Conditional Mutual Information\n  Minimization",
        "url": "http://arxiv.org/abs/2401.12553v1",
        "pub_date": "2024-01-23",
        "summary": "Ranking items regarding individual user interests is a core technique of\nmultiple downstream tasks such as recommender systems. Learning such a\npersonalized ranker typically relies on the implicit feedback from users' past\nclick-through behaviors. However, collected feedback is biased toward\npreviously highly-ranked items and directly learning from it would result in a\n\"rich-get-richer\" phenomenon. In this paper, we propose a simple yet sufficient\nunbiased learning-to-rank paradigm named InfoRank that aims to simultaneously\naddress both position and popularity biases. We begin by consolidating the\nimpacts of those biases into a single observation factor, thereby providing a\nunified approach to addressing bias-related issues. Subsequently, we minimize\nthe mutual information between the observation estimation and the relevance\nestimation conditioned on the input features. By doing so, our relevance\nestimation can be proved to be free of bias. To implement InfoRank, we first\nincorporate an attention mechanism to capture latent correlations within\nuser-item features, thereby generating estimations of observation and\nrelevance. We then introduce a regularization term, grounded in conditional\nmutual information, to promote conditional independence between relevance\nestimation and observation estimation. Experimental evaluations conducted\nacross three extensive recommendation and search datasets reveal that InfoRank\nlearns more precise and unbiased ranking strategies.",
        "translated": ""
    },
    {
        "title": "DREditor: An Time-efficient Approach for Building a Domain-specific\n  Dense Retrieval Model",
        "url": "http://arxiv.org/abs/2401.12540v1",
        "pub_date": "2024-01-23",
        "summary": "Deploying dense retrieval models efficiently is becoming increasingly\nimportant across various industries. This is especially true for enterprise\nsearch services, where customizing search engines to meet the time demands of\ndifferent enterprises in different domains is crucial. Motivated by this, we\ndevelop a time-efficient approach called DREditor to edit the matching rule of\nan off-the-shelf dense retrieval model to suit a specific domain. This is\nachieved by directly calibrating the output embeddings of the model using an\nefficient and effective linear mapping. This mapping is powered by an edit\noperator that is obtained by solving a specially constructed least squares\nproblem. Compared to implicit rule modification via long-time finetuning, our\nexperimental results show that DREditor provides significant advantages on\ndifferent domain-specific datasets, dataset sources, retrieval models, and\ncomputing devices. It consistently enhances time efficiency by 100-300 times\nwhile maintaining comparable or even superior retrieval performance. In a\nbroader context, we take the first step to introduce a novel embedding\ncalibration approach for the retrieval task, filling the technical blank in the\ncurrent field of embedding calibration. This approach also paves the way for\nbuilding domain-specific dense retrieval models efficiently and inexpensively.",
        "translated": ""
    },
    {
        "title": "Key Information Retrieval to Classify the Unstructured Data Content of\n  Preferential Trade Agreements",
        "url": "http://arxiv.org/abs/2401.12520v1",
        "pub_date": "2024-01-23",
        "summary": "With the rapid proliferation of textual data, predicting long texts has\nemerged as a significant challenge in the domain of natural language\nprocessing. Traditional text prediction methods encounter substantial\ndifficulties when grappling with long texts, primarily due to the presence of\nredundant and irrelevant information, which impedes the model's capacity to\ncapture pivotal insights from the text. To address this issue, we introduce a\nnovel approach to long-text classification and prediction. Initially, we employ\nembedding techniques to condense the long texts, aiming to diminish the\nredundancy therein. Subsequently,the Bidirectional Encoder Representations from\nTransformers (BERT) embedding method is utilized for text classification\ntraining. Experimental outcomes indicate that our method realizes considerable\nperformance enhancements in classifying long texts of Preferential Trade\nAgreements. Furthermore, the condensation of text through embedding methods not\nonly augments prediction accuracy but also substantially reduces computational\ncomplexity. Overall, this paper presents a strategy for long-text prediction,\noffering a valuable reference for researchers and engineers in the natural\nlanguage processing sphere.",
        "translated": ""
    },
    {
        "title": "Persona-centric Metamorphic Relation guided Robustness Evaluation for\n  Multi-turn Dialogue Modelling",
        "url": "http://arxiv.org/abs/2401.12483v1",
        "pub_date": "2024-01-23",
        "summary": "Recently there has been significant progress in the field of dialogue system\nthanks to the introduction of training paradigms such as fine-tune and prompt\nlearning. Persona can function as the prior knowledge for maintaining the\npersonality consistency of dialogue systems, which makes it perform well on\naccuracy. Nonetheless, the conventional reference-based evaluation method falls\nshort in capturing the genuine text comprehension prowess of the model,\nsignificantly relying on the quality of data annotation. In contrast, the\napplication of metamorphic testing offers a more profound insight into the\nmodel's distinct capabilities without necessitating supplementary annotation\nlabels. This approach furnishes a more comprehensive portrayal of the model's\nintricacies and exposes intricacies concealed within reference-based validation\ntechniques. Consequently, we introduce a persona-centric metamorphic relation\nconstruction for metamorphic testing, aimed at evaluating both the persona\nconsistency and robustness of personalized dialogue models. For that reason,\nthis work evaluates several widely used training paradigms including learning\nfrom scratch, pretrain + fine-tune and prompt learning in personalized dialogue\nretrieval to know if they are more robust or if they have the same flaws as\ntheir predecessor. Under three kinds of designed metamorphic relations with\nconsistent outputs, our experimental results reveal that prompt learning shows\nstronger robustness compared to training from scratch and fine-tune. Although\ntested retrieval models gain competitively high retrieval accuracy according to\nthe traditional reference-based validation, they are still fragile and\ndemonstrate various unexpected behaviors, thus there is still room for future\nimprovement in personalized dialogue retrieval.",
        "translated": ""
    },
    {
        "title": "Session-level Normalization and Click-through Data Enhancement for\n  Session-based Evaluation",
        "url": "http://arxiv.org/abs/2401.12445v1",
        "pub_date": "2024-01-23",
        "summary": "Since a user usually has to issue a sequence of queries and examine multiple\ndocuments to resolve a complex information need in a search session,\nresearchers have paid much attention to evaluating search systems at the\nsession level rather than the single-query level. Most existing session-level\nmetrics evaluate each query separately and then aggregate the query-level\nscores using a session-level weighting function. The assumptions behind these\nmetrics are that all queries in the session should be involved, and their\norders are fixed. However, if a search system could make the user satisfied\nwith her first few queries, she may not need any subsequent queries. Besides,\nin most real-world search scenarios, due to a lack of explicit feedback from\nreal users, we can only leverage some implicit feedback, such as users' clicks,\nas relevance labels for offline evaluation. Such implicit feedback might be\ndifferent from the real relevance in a search session as some documents may be\nomitted in the previous query but identified in the later reformulations. To\naddress the above issues, we make two assumptions about session-based\nevaluation, which explicitly describe an ideal session-search system and how to\nenhance click-through data in computing session-level evaluation metrics. Based\non our assumptions, we design a session-level metric called Normalized\nU-Measure (NUM). NUM evaluates a session as a whole and utilizes an ideal\nsession to normalize the result of the actual session. Besides, it infers\nsession-level relevance labels based on implicit feedback. Experiments on two\npublic datasets demonstrate the effectiveness of NUM by comparing it with\nexisting session-based metrics in terms of correlation with user satisfaction\nand intuitiveness. We also conduct ablation studies to explore whether these\nassumptions hold.",
        "translated": ""
    },
    {
        "title": "Building Contextual Knowledge Graphs for Personalized Learning\n  Recommendations using Text Mining and Semantic Graph Completion",
        "url": "http://arxiv.org/abs/2401.13609v1",
        "pub_date": "2024-01-24",
        "summary": "Modelling learning objects (LO) within their context enables the learner to\nadvance from a basic, remembering-level, learning objective to a higher-order\none, i.e., a level with an application- and analysis objective. While\nhierarchical data models are commonly used in digital learning platforms, using\ngraph-based models enables representing the context of LOs in those platforms.\nThis leads to a foundation for personalized recommendations of learning paths.\nIn this paper, the transformation of hierarchical data models into knowledge\ngraph (KG) models of LOs using text mining is introduced and evaluated. We\nutilize custom text mining pipelines to mine semantic relations between\nelements of an expert-curated hierarchical model. We evaluate the KG structure\nand relation extraction using graph quality-control metrics and the comparison\nof algorithmic semantic-similarities to expert-defined ones. The results show\nthat the relations in the KG are semantically comparable to those defined by\ndomain experts, and that the proposed KG improves representing and linking the\ncontexts of LOs through increasing graph communities and betweenness\ncentrality.",
        "translated": ""
    },
    {
        "title": "A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in\n  Recommendation",
        "url": "http://arxiv.org/abs/2401.13566v1",
        "pub_date": "2024-01-24",
        "summary": "When devising recommendation services, it is important to account for the\ninterests of all content providers, encompassing not only newcomers but also\nminority demographic groups. In various instances, certain provider groups find\nthemselves underrepresented in the item catalog, a situation that can influence\nrecommendation results. Hence, platform owners often seek to regulate the\nexposure of these provider groups in the recommended lists. In this paper, we\npropose a novel cost-sensitive approach designed to guarantee these target\nexposure levels in pairwise recommendation models. This approach quantifies,\nand consequently mitigate, the discrepancies between the volume of\nrecommendations allocated to groups and their contribution in the item catalog,\nunder the principle of equity. Our results show that this approach, while\naligning groups exposure with their assigned levels, does not compromise to the\noriginal recommendation utility. Source code and pre-processed data can be\nretrieved at\nhttps://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure.",
        "translated": ""
    },
    {
        "title": "Fine-grained Contract NER using instruction based model",
        "url": "http://arxiv.org/abs/2401.13545v1",
        "pub_date": "2024-01-24",
        "summary": "Lately, instruction-based techniques have made significant strides in\nimproving performance in few-shot learning scenarios. They achieve this by\nbridging the gap between pre-trained language models and fine-tuning for\nspecific downstream tasks. Despite these advancements, the performance of Large\nLanguage Models (LLMs) in information extraction tasks like Named Entity\nRecognition (NER), using prompts or instructions, still falls short of\nsupervised baselines. The reason for this performance gap can be attributed to\nthe fundamental disparity between NER and LLMs. NER is inherently a sequence\nlabeling task, where the model must assign entity-type labels to individual\ntokens within a sentence. In contrast, LLMs are designed as a text generation\ntask. This distinction between semantic labeling and text generation leads to\nsubpar performance. In this paper, we transform the NER task into a\ntext-generation task that can be readily adapted by LLMs. This involves\nenhancing source sentences with task-specific instructions and answer choices,\nallowing for the identification of entities and their types within natural\nlanguage. We harness the strength of LLMs by integrating supervised learning\nwithin them. The goal of this combined strategy is to boost the performance of\nLLMs in extraction tasks like NER while simultaneously addressing hallucination\nissues often observed in LLM-generated content. A novel corpus Contract NER\ncomprising seven frequently observed contract categories, encompassing named\nentities associated with 18 distinct legal entity types is released along with\nour baseline models. Our models and dataset are available to the community for\nfuture research * .",
        "translated": ""
    },
    {
        "title": "TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient\n  and Effective Retrieval",
        "url": "http://arxiv.org/abs/2401.13509v1",
        "pub_date": "2024-01-24",
        "summary": "This paper considers Pseudo-Relevance Feedback (PRF) methods for dense\nretrievers in a resource constrained environment such as that of cheap cloud\ninstances or embedded systems (e.g., smartphones and smartwatches), where\nmemory and CPU are limited and GPUs are not present. For this, we propose a\ntransformer-based PRF method (TPRF), which has a much smaller memory footprint\nand faster inference time compared to other deep language models that employ\nPRF mechanisms, with a marginal effectiveness loss. TPRF learns how to\neffectively combine the relevance feedback signals from dense passage\nrepresentations. Specifically, TPRF provides a mechanism for modelling\nrelationships and weights between the query and the relevance feedback signals.\nThe method is agnostic to the specific dense representation used and thus can\nbe generally applied to any dense retriever.",
        "translated": ""
    },
    {
        "title": "SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval",
        "url": "http://arxiv.org/abs/2401.13478v1",
        "pub_date": "2024-01-24",
        "summary": "Multi-modal information retrieval (MMIR) is a rapidly evolving field, where\nsignificant progress, particularly in image-text pairing, has been made through\nadvanced representation learning and cross-modality alignment research.\nHowever, current benchmarks for evaluating MMIR performance in image-text\npairing within the scientific domain show a notable gap, where chart and table\nimages described in scholarly language usually do not play a significant role.\nTo bridge this gap, we develop a specialised scientific MMIR (SciMMIR)\nbenchmark by leveraging open-access paper collections to extract data relevant\nto the scientific domain. This benchmark comprises 530K meticulously curated\nimage-text pairs, extracted from figures and tables with detailed captions in\nscientific documents. We further annotate the image-text pairs with two-level\nsubset-subcategory hierarchy annotations to facilitate a more comprehensive\nevaluation of the baselines. We conducted zero-shot and fine-tuning evaluations\non prominent multi-modal image-captioning and visual language models, such as\nCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific\ndomain, including the impact of pre-training and fine-tuning settings and the\ninfluence of the visual and textual encoders. All our data and checkpoints are\npublicly available at https://github.com/Wusiwei0410/SciMMIR.",
        "translated": ""
    },
    {
        "title": "SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken\n  Question Answering",
        "url": "http://arxiv.org/abs/2401.13463v1",
        "pub_date": "2024-01-24",
        "summary": "Spoken Question Answering (SQA) is essential for machines to reply to user's\nquestion by finding the answer span within a given spoken passage. SQA has been\npreviously achieved without ASR to avoid recognition errors and\nOut-of-Vocabulary (OOV) problems. However, the real-world problem of\nOpen-domain SQA (openSQA), in which the machine needs to first retrieve\npassages that possibly contain the answer from a spoken archive in addition,\nwas never considered. This paper proposes the first known end-to-end framework,\nSpeech Dense Passage Retriever (SpeechDPR), for the retrieval component of the\nopenSQA problem. SpeechDPR learns a sentence-level semantic representation by\ndistilling knowledge from the cascading model of unsupervised ASR (UASR) and\ntext dense retriever (TDR). No manually transcribed speech data is needed.\nInitial experiments showed performance comparable to the cascading model of\nUASR and TDR, and significantly better when UASR was poor, verifying this\napproach is more robust to speech recognition errors.",
        "translated": ""
    },
    {
        "title": "Decentralized Collaborative Learning with Adaptive Reference Data for\n  On-Device POI Recommendation",
        "url": "http://arxiv.org/abs/2401.13448v1",
        "pub_date": "2024-01-24",
        "summary": "In Location-based Social Networks, Point-of-Interest (POI) recommendation\nhelps users discover interesting places. There is a trend to move from the\ncloud-based model to on-device recommendations for privacy protection and\nreduced server reliance. Due to the scarcity of local user-item interactions on\nindividual devices, solely relying on local instances is not adequate.\nCollaborative Learning (CL) emerges to promote model sharing among users, where\nreference data is an intermediary that allows users to exchange their soft\ndecisions without directly sharing their private data or parameters, ensuring\nprivacy and benefiting from collaboration. However, existing CL-based\nrecommendations typically use a single reference for all users. Reference data\nvaluable for one user might be harmful to another, given diverse user\npreferences. Users may not offer meaningful soft decisions on items outside\ntheir interest scope. Consequently, using the same reference data for all\ncollaborations can impede knowledge exchange and lead to sub-optimal\nperformance. To address this gap, we introduce the Decentralized Collaborative\nLearning with Adaptive Reference Data (DARD) framework, which crafts adaptive\nreference data for effective user collaboration. It first generates a\ndesensitized public reference data pool with transformation and probability\ndata generation methods. For each user, the selection of adaptive reference\ndata is executed in parallel by training loss tracking and influence function.\nLocal models are trained with individual private data and collaboratively with\nthe geographical and semantic neighbors. During the collaboration between two\nusers, they exchange soft decisions based on a combined set of their adaptive\nreference data. Our evaluations across two real-world datasets highlight DARD's\nsuperiority in recommendation performance and addressing the scarcity of\navailable reference data.",
        "translated": ""
    },
    {
        "title": "Query Exposure Prediction for Groups of Documents in Rankings",
        "url": "http://arxiv.org/abs/2401.13434v1",
        "pub_date": "2024-01-24",
        "summary": "The main objective of an Information Retrieval system is to provide a user\nwith the most relevant documents to the user's query. To do this, modern IR\nsystems typically deploy a re-ranking pipeline in which a set of documents is\nretrieved by a lightweight first-stage retrieval process and then re-ranked by\na more effective but expensive model. However, the success of a re-ranking\npipeline is heavily dependent on the performance of the first stage retrieval,\nsince new documents are not usually identified during the re-ranking stage.\nMoreover, this can impact the amount of exposure that a particular group of\ndocuments, such as documents from a particular demographic group, can receive\nin the final ranking. For example, the fair allocation of exposure becomes more\nchallenging or impossible if the first stage retrieval returns too few\ndocuments from certain groups, since the number of group documents in the\nranking affects the exposure more than the documents' positions. With this in\nmind, it is beneficial to predict the amount of exposure that a group of\ndocuments is likely to receive in the results of the first stage retrieval\nprocess, in order to ensure that there are a sufficient number of documents\nincluded from each of the groups. In this paper, we introduce the novel task of\nquery exposure prediction (QEP). Specifically, we propose the first approach\nfor predicting the distribution of exposure that groups of documents will\nreceive for a given query. Our new approach, called GEP, uses lexical\ninformation from individual groups of documents to estimate the exposure the\ngroups will receive in a ranking. Our experiments on the TREC 2021 and 2022\nFair Ranking Track test collections show that our proposed GEP approach results\nin exposure predictions that are up to 40 % more accurate than the predictions\nof adapted existing query performance prediction and resource allocation\napproaches.",
        "translated": ""
    },
    {
        "title": "How to Forget Clients in Federated Online Learning to Rank?",
        "url": "http://arxiv.org/abs/2401.13410v1",
        "pub_date": "2024-01-24",
        "summary": "Data protection legislation like the European Union's General Data Protection\nRegulation (GDPR) establishes the \\textit{right to be forgotten}: a user\n(client) can request contributions made using their data to be removed from\nlearned models. In this paper, we study how to remove the contributions made by\na client participating in a Federated Online Learning to Rank (FOLTR) system.\nIn a FOLTR system, a ranker is learned by aggregating local updates to the\nglobal ranking model. Local updates are learned in an online manner at a\nclient-level using queries and implicit interactions that have occurred within\nthat specific client. By doing so, each client's local data is not shared with\nother clients or with a centralised search service, while at the same time\nclients can benefit from an effective global ranking model learned from\ncontributions of each client in the federation.\n  In this paper, we study an effective and efficient unlearning method that can\nremove a client's contribution without compromising the overall ranker\neffectiveness and without needing to retrain the global ranker from scratch. A\nkey challenge is how to measure whether the model has unlearned the\ncontributions from the client $c^*$ that has requested removal. For this, we\ninstruct $c^*$ to perform a poisoning attack (add noise to this client updates)\nand then we measure whether the impact of the attack is lessened when the\nunlearning process has taken place. Through experiments on four datasets, we\ndemonstrate the effectiveness and efficiency of the unlearning strategy under\ndifferent combinations of parameter settings.",
        "translated": ""
    },
    {
        "title": "Predicting IR Personalization Performance using Pre-retrieval Query\n  Predictors",
        "url": "http://arxiv.org/abs/2401.13351v1",
        "pub_date": "2024-01-24",
        "summary": "Personalization generally improves the performance of queries but in a few\ncases it may also harms it. If we are able to predict and therefore to disable\npersonalization for those situations, the overall performance will be higher\nand users will be more satisfied with personalized systems. We use some\nstate-of-the-art pre-retrieval query performance predictors and propose some\nothers including the user profile information for the previous purpose. We\nstudy the correlations among these predictors and the difference between the\npersonalized and the original queries. We also use classification and\nregression techniques to improve the results and finally reach a bit more than\none third of the maximum ideal performance. We think this is a good starting\npoint within this research line, which certainly needs more effort and\nimprovements.",
        "translated": ""
    },
    {
        "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation",
        "url": "http://arxiv.org/abs/2401.14021v1",
        "pub_date": "2024-01-25",
        "summary": "Retrieval-augmented language models (RaLM) have demonstrated the potential to\nsolve knowledge-intensive natural language processing (NLP) tasks by combining\na non-parametric knowledge base with a parametric language model. Instead of\nfine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to\nthe latest data and better source attribution mechanisms. Among various RaLM\napproaches, iterative RaLM delivers a better generation quality due to a more\nfrequent interaction between the retriever and the language model. Despite the\nbenefits, iterative RaLM usually encounters high overheads due to the frequent\nretrieval step. To this end, we propose RaLMSpec, a speculation-inspired\nframework that provides generic speed-up over iterative RaLM while preserving\nthe same model outputs through speculative retrieval and batched verification.\nBy further incorporating prefetching, optimal speculation stride scheduler, and\nasynchronous verification, RaLMSpec can automatically exploit the acceleration\npotential to the fullest. For naive iterative RaLM serving, extensive\nevaluations over three language models on four downstream QA datasets\ndemonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,\n1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,\napproximate dense retriever, and sparse retriever respectively compared with\nthe baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to\n7.59x and 2.45x when the retriever is an exact dense retriever and approximate\ndense retriever, respectively, compared with the baseline.",
        "translated": ""
    },
    {
        "title": "Towards 3D Molecule-Text Interpretation in Language Models",
        "url": "http://arxiv.org/abs/2401.13923v1",
        "pub_date": "2024-01-25",
        "summary": "Language Models (LMs) have greatly influenced diverse domains. However, their\ninherent limitation in comprehending 3D molecular structures has considerably\nconstrained their potential in the biomolecular domain. To bridge this gap, we\nfocus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular\nLanguage Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze\n3D molecules by equipping the LM with a 3D molecular encoder. This integration\nis achieved by a 3D molecule-text projector, bridging the 3D molecular\nencoder's representation space and the LM's input space. Moreover, to enhance\n3D-MoLM's ability of cross-modal molecular understanding and instruction\nfollowing, we meticulously curated a 3D molecule-centric instruction tuning\ndataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric\ninstruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder\nand LM. It significantly surpasses existing baselines on downstream tasks,\nincluding molecule-text retrieval, molecule captioning, and more challenging\nopen-text molecular QA tasks, especially focusing on 3D-dependent properties.",
        "translated": ""
    },
    {
        "title": "Integrating Large Language Models into Recommendation via Mutual\n  Augmentation and Adaptive Aggregation",
        "url": "http://arxiv.org/abs/2401.13870v1",
        "pub_date": "2024-01-25",
        "summary": "Conventional recommendation methods have achieved notable advancements by\nharnessing collaborative or sequential information from user behavior.\nRecently, large language models (LLMs) have gained prominence for their\ncapabilities in understanding and reasoning over textual semantics, and have\nfound utility in various domains, including recommendation. Conventional\nrecommendation methods and LLMs each have their strengths and weaknesses. While\nconventional methods excel at mining collaborative information and modeling\nsequential behavior, they struggle with data sparsity and the long-tail\nproblem. LLMs, on the other hand, are proficient at utilizing rich textual\ncontexts but face challenges in mining collaborative or sequential information.\nDespite their individual successes, there is a significant gap in leveraging\ntheir combined potential to enhance recommendation performance.\n  In this paper, we introduce a general and model-agnostic framework known as\n\\textbf{L}arge \\textbf{la}nguage model with \\textbf{m}utual augmentation and\n\\textbf{a}daptive aggregation for \\textbf{Rec}ommendation (\\textbf{Llama4Rec}).\nLlama4Rec synergistically combines conventional and LLM-based recommendation\nmodels. Llama4Rec proposes data augmentation and prompt augmentation strategies\ntailored to enhance the conventional model and LLM respectively. An adaptive\naggregation module is adopted to combine the predictions of both kinds of\nmodels to refine the final recommendation results. Empirical studies on three\nreal-world datasets validate the superiority of Llama4Rec, demonstrating its\nconsistent outperformance of baseline methods and significant improvements in\nrecommendation performance.",
        "translated": ""
    },
    {
        "title": "Algorithmically Curated Lies: How Search Engines Handle Misinformation\n  about US Biolabs in Ukraine",
        "url": "http://arxiv.org/abs/2401.13832v1",
        "pub_date": "2024-01-24",
        "summary": "The growing volume of online content prompts the need for adopting\nalgorithmic systems of information curation. These systems range from web\nsearch engines to recommender systems and are integral for helping users stay\ninformed about important societal developments. However, unlike journalistic\nediting the algorithmic information curation systems (AICSs) are known to be\nsubject to different forms of malperformance which make them vulnerable to\npossible manipulation. The risk of manipulation is particularly prominent in\nthe case when AICSs have to deal with information about false claims that\nunderpin propaganda campaigns of authoritarian regimes. Using as a case study\nof the Russian disinformation campaign concerning the US biolabs in Ukraine, we\ninvestigate how one of the most commonly used forms of AICSs - i.e. web search\nengines - curate misinformation-related content. For this aim, we conduct\nvirtual agent-based algorithm audits of Google, Bing, and Yandex search outputs\nin June 2022. Our findings highlight the troubling performance of search\nengines. Even though some search engines, like Google, were less likely to\nreturn misinformation results, across all languages and locations, the three\nsearch engines still mentioned or promoted a considerable share of false\ncontent (33% on Google; 44% on Bing, and 70% on Yandex). We also find\nsignificant disparities in misinformation exposure based on the language of\nsearch, with all search engines presenting a higher number of false stories in\nRussian. Location matters as well with users from Germany being more likely to\nbe exposed to search results promoting false information. These observations\nstress the possibility of AICSs being vulnerable to manipulation, in particular\nin the case of the unfolding propaganda campaigns, and underline the importance\nof monitoring performance of these systems to prevent it.",
        "translated": ""
    },
    {
        "title": "Robustness in Fairness against Edge-level Perturbations in GNN-based\n  Recommendation",
        "url": "http://arxiv.org/abs/2401.13823v1",
        "pub_date": "2024-01-24",
        "summary": "Efforts in the recommendation community are shifting from the sole emphasis\non utility to considering beyond-utility factors, such as fairness and\nrobustness. Robustness of recommendation models is typically linked to their\nability to maintain the original utility when subjected to attacks. Limited\nresearch has explored the robustness of a recommendation model in terms of\nfairness, e.g., the parity in performance across groups, under attack\nscenarios. In this paper, we aim to assess the robustness of graph-based\nrecommender systems concerning fairness, when exposed to attacks based on\nedge-level perturbations. To this end, we considered four different fairness\noperationalizations, including both consumer and provider perspectives.\nExperiments on three datasets shed light on the impact of perturbations on the\ntargeted fairness notion, uncovering key shortcomings in existing evaluation\nprotocols for robustness. As an example, we observed perturbations affect\nconsumer fairness on a higher extent than provider fairness, with alarming\nunfairness for the former. Source code:\nhttps://github.com/jackmedda/CPFairRobust",
        "translated": ""
    },
    {
        "title": "Longitudinal Sentiment Topic Modelling of Reddit Posts",
        "url": "http://arxiv.org/abs/2401.13805v1",
        "pub_date": "2024-01-24",
        "summary": "In this study, we analyze texts of Reddit posts written by students of four\nmajor Canadian universities. We gauge the emotional tone and uncover prevailing\nthemes and discussions through longitudinal topic modeling of posts textual\ndata. Our study focuses on four years, 2020-2023, covering COVID-19 pandemic\nand after pandemic years. Our results highlight a gradual uptick in discussions\nrelated to mental health.",
        "translated": ""
    },
    {
        "title": "Embedding-based search in JetBrains IDEs",
        "url": "http://arxiv.org/abs/2401.14975v1",
        "pub_date": "2024-01-26",
        "summary": "Most modern Integrated Development Environments (IDEs) and code editors have\na feature to search across available functionality and items in an open\nproject. In JetBrains IDEs, this feature is called Search Everywhere: it allows\nusers to search for files, actions, classes, symbols, settings, and anything\nfrom VCS history from a single entry point. However, it works with the\ncandidates obtained by algorithms that don't account for semantics, e.g.,\nsynonyms, complex word permutations, part of the speech modifications, and\ntypos. In this work, we describe the machine learning approach we implemented\nto improve the discoverability of search items. We also share the obstacles\nencountered during this process and how we overcame them.",
        "translated": ""
    },
    {
        "title": "Analysing the Influence of Macroeconomic Factors on Credit Risk in the\n  UK Banking Sector",
        "url": "http://arxiv.org/abs/2401.14943v1",
        "pub_date": "2024-01-26",
        "summary": "Macroeconomic factors have a critical impact on banking credit risk, which\ncannot be directly controlled by banks, and therefore, there is a need for an\nearly credit risk warning system based on the macroeconomy. By comparing\ndifferent predictive models (traditional statistical and machine learning\nalgorithms), this study aims to examine the macroeconomic determinants impact\non the UK banking credit risk and assess the most accurate credit risk estimate\nusing predictive analytics. This study found that the variance-based\nmulti-split decision tree algorithm is the most precise predictive model with\ninterpretable, reliable, and robust results. Our model performance achieved 95%\naccuracy and evidenced that unemployment and inflation rate are significant\ncredit risk predictors in the UK banking context. Our findings provided\nvaluable insights such as a positive association between credit risk and\ninflation, the unemployment rate, and national savings, as well as a negative\nrelationship between credit risk and national debt, total trade deficit, and\nnational income. In addition, we empirically showed the relationship between\nnational savings and non-performing loans, thus proving the paradox of thrift.\nThese findings benefit the credit risk management team in monitoring the\nmacroeconomic factors thresholds and implementing critical reforms to mitigate\ncredit risk.",
        "translated": ""
    },
    {
        "title": "Macro Graph Neural Networks for Online Billion-Scale Recommender Systems",
        "url": "http://arxiv.org/abs/2401.14939v1",
        "pub_date": "2024-01-26",
        "summary": "Predicting Click-Through Rate (CTR) in billion-scale recommender systems\nposes a long-standing challenge for Graph Neural Networks (GNNs) due to the\noverwhelming computational complexity involved in aggregating billions of\nneighbors. To tackle this, GNN-based CTR models usually sample hundreds of\nneighbors out of the billions to facilitate efficient online recommendations.\nHowever, sampling only a small portion of neighbors results in a severe\nsampling bias and the failure to encompass the full spectrum of user or item\nbehavioral patterns. To address this challenge, we name the conventional\nuser-item recommendation graph as \"micro recommendation graph\" and introduce a\nmore suitable MAcro Recommendation Graph (MAG) for billion-scale\nrecommendations. MAG resolves the computational complexity problems in the\ninfrastructure by reducing the node count from billions to hundreds.\nSpecifically, MAG groups micro nodes (users and items) with similar behavior\npatterns to form macro nodes. Subsequently, we introduce tailored Macro Graph\nNeural Networks (MacGNN) to aggregate information on a macro level and revise\nthe embeddings of macro nodes. MacGNN has already served Taobao's homepage feed\nfor two months, providing recommendations for over one billion users. Extensive\noffline experiments on three public benchmark datasets and an industrial\ndataset present that MacGNN significantly outperforms twelve CTR baselines\nwhile remaining computationally efficient. Besides, online A/B tests confirm\nMacGNN's superiority in billion-scale recommender systems.",
        "translated": ""
    },
    {
        "title": "The Power of Noise: Redefining Retrieval for RAG Systems",
        "url": "http://arxiv.org/abs/2401.14887v1",
        "pub_date": "2024-01-26",
        "summary": "Retrieval-Augmented Generation (RAG) systems represent a significant\nadvancement over traditional Large Language Models (LLMs). RAG systems enhance\ntheir generation ability by incorporating external data retrieved through an\nInformation Retrieval (IR) phase, overcoming the limitations of standard LLMs,\nwhich are restricted to their pre-trained knowledge and limited context window.\nMost research in this area has predominantly concentrated on the generative\naspect of LLMs within RAG systems. Our study fills this gap by thoroughly and\ncritically analyzing the influence of IR components on RAG systems. This paper\nanalyzes which characteristics a retriever should possess for an effective\nRAG's prompt formulation, focusing on the type of documents that should be\nretrieved. We evaluate various elements, such as the relevance of the documents\nto the prompt, their position, and the number included in the context. Our\nfindings reveal, among other insights, that including irrelevant documents can\nunexpectedly enhance performance by more than 30% in accuracy, contradicting\nour initial assumption of diminished quality. These findings call for\ndeveloping specialized approaches tailored to the specific demands of\nintegrating retrieval with language generation models and pave the way for\nfuture research. These results underscore the need for developing specialized\nstrategies to integrate retrieval with language generation models, thereby\nlaying the groundwork for future research in this field.",
        "translated": ""
    },
    {
        "title": "Expressivity-aware Music Performance Retrieval using Mid-level\n  Perceptual Features and Emotion Word Embeddings",
        "url": "http://arxiv.org/abs/2401.14826v1",
        "pub_date": "2024-01-26",
        "summary": "This paper explores a specific sub-task of cross-modal music retrieval. We\nconsider the delicate task of retrieving a performance or rendition of a\nmusical piece based on a description of its style, expressive character, or\nemotion from a set of different performances of the same piece. We observe that\na general purpose cross-modal system trained to learn a common text-audio\nembedding space does not yield optimal results for this task. By introducing\ntwo changes -- one each to the text encoder and the audio encoder -- we\ndemonstrate improved performance on a dataset of piano performances and\nassociated free-text descriptions. On the text side, we use emotion-enriched\nword embeddings (EWE) and on the audio side, we extract mid-level perceptual\nfeatures instead of generic audio embeddings. Our results highlight the\neffectiveness of mid-level perceptual features learnt from music and emotion\nenriched word embeddings learnt from emotion-labelled text in capturing musical\nexpression in a cross-modal setting. Additionally, our interpretable mid-level\nfeatures provide a route for introducing explainability in the retrieval and\ndownstream recommendation processes.",
        "translated": ""
    },
    {
        "title": "Prompt-enhanced Federated Content Representation Learning for\n  Cross-domain Recommendation",
        "url": "http://arxiv.org/abs/2401.14678v1",
        "pub_date": "2024-01-26",
        "summary": "Cross-domain Recommendation (CDR) as one of the effective techniques in\nalleviating the data sparsity issues has been widely studied in recent years.\nHowever, previous works may cause domain privacy leakage since they necessitate\nthe aggregation of diverse domain data into a centralized server during the\ntraining process. Though several studies have conducted privacy preserving CDR\nvia Federated Learning (FL), they still have the following limitations: 1) They\nneed to upload users' personal information to the central server, posing the\nrisk of leaking user privacy. 2) Existing federated methods mainly rely on\natomic item IDs to represent items, which prevents them from modeling items in\na unified feature space, increasing the challenge of knowledge transfer among\ndomains. 3) They are all based on the premise of knowing overlapped users\nbetween domains, which proves impractical in real-world applications. To\naddress the above limitations, we focus on Privacy-preserving Cross-domain\nRecommendation (PCDR) and propose PFCR as our solution. For Limitation 1, we\ndevelop a FL schema by exclusively utilizing users' interactions with local\nclients and devising an encryption method for gradient encryption. For\nLimitation 2, we model items in a universal feature space by their description\ntexts. For Limitation 3, we initially learn federated content representations,\nharnessing the generality of natural language to establish bridges between\ndomains. Subsequently, we craft two prompt fine-tuning strategies to tailor the\npre-trained model to the target domain. Extensive experiments on two real-world\ndatasets demonstrate the superiority of our PFCR method compared to the SOTA\napproaches.",
        "translated": ""
    },
    {
        "title": "Challenging Low Homophily in Social Recommendation",
        "url": "http://arxiv.org/abs/2401.14606v1",
        "pub_date": "2024-01-26",
        "summary": "Social relations are leveraged to tackle the sparsity issue of user-item\ninteraction data in recommendation under the assumption of social homophily.\nHowever, social recommendation paradigms predominantly focus on homophily based\non user preferences. While social information can enhance recommendations, its\nalignment with user preferences is not guaranteed, thereby posing the risk of\nintroducing informational redundancy. We empirically discover that social\ngraphs in real recommendation data exhibit low preference-aware homophily,\nwhich limits the effect of social recommendation models. To comprehensively\nextract preference-aware homophily information latent in the social graph, we\npropose Social Heterophily-alleviating Rewiring (SHaRe), a data-centric\nframework for enhancing existing graph-based social recommendation models. We\nadopt Graph Rewiring technique to capture and add highly homophilic social\nrelations, and cut low homophilic (or heterophilic) relations. To better refine\nthe user representations from reliable social relations, we integrate a\ncontrastive learning method into the training of SHaRe, aiming to calibrate the\nuser representations for enhancing the result of Graph Rewiring. Experiments on\nreal-world datasets show that the proposed framework not only exhibits enhanced\nperformances across varying homophily ratios but also improves the performance\nof existing state-of-the-art (SOTA) social recommendation models.",
        "translated": ""
    },
    {
        "title": "Recency Ranking by Diversification of Result Set",
        "url": "http://arxiv.org/abs/2401.14595v1",
        "pub_date": "2024-01-26",
        "summary": "In this paper, we propose a web search retrieval approach which automatically\ndetects recency sensitive queries and increases the freshness of the ordinary\ndocument ranking by a degree proportional to the probability of the need in\nrecent content. We propose to solve the recency ranking problem by using result\ndiversification principles and deal with the query's non-topical ambiguity\nappearing when the need in recent content can be detected only with\nuncertainty. Our offline and online experiments with millions of queries from\nreal search engine users demonstrate the significant increase in satisfaction\nof users presented with a search result generated by our approach.",
        "translated": ""
    },
    {
        "title": "Physical Trajectory Inference Attack and Defense in Decentralized POI\n  Recommendation",
        "url": "http://arxiv.org/abs/2401.14583v1",
        "pub_date": "2024-01-26",
        "summary": "As an indispensable personalized service within Location-Based Social\nNetworks (LBSNs), the Point-of-Interest (POI) recommendation aims to assist\nindividuals in discovering attractive and engaging places. However, the\naccurate recommendation capability relies on the powerful server collecting a\nvast amount of users' historical check-in data, posing significant risks of\nprivacy breaches. Although several collaborative learning (CL) frameworks for\nPOI recommendation enhance recommendation resilience and allow users to keep\npersonal data on-device, they still share personal knowledge to improve\nrecommendation performance, thus leaving vulnerabilities for potential\nattackers. Given this, we design a new Physical Trajectory Inference Attack\n(PTIA) to expose users' historical trajectories. Specifically, for each user,\nwe identify the set of interacted POIs by analyzing the aggregated information\nfrom the target POIs and their correlated POIs. We evaluate the effectiveness\nof PTIA on two real-world datasets across two types of decentralized CL\nframeworks for POI recommendation. Empirical results demonstrate that PTIA\nposes a significant threat to users' historical trajectories. Furthermore,\nLocal Differential Privacy (LDP), the traditional privacy-preserving method for\nCL frameworks, has also been proven ineffective against PTIA. In light of this,\nwe propose a novel defense mechanism (AGD) against PTIA based on an adversarial\ngame to eliminate sensitive POIs and their information in correlated POIs.\nAfter conducting intensive experiments, AGD has been proven precise and\npractical, with minimal impact on recommendation performance.",
        "translated": ""
    },
    {
        "title": "Language Modelling Approaches to Adaptive Machine Translation",
        "url": "http://arxiv.org/abs/2401.14559v1",
        "pub_date": "2024-01-25",
        "summary": "Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nin-domain data scarcity is common in translation settings, due to the lack of\nspecialised datasets and terminology, or inconsistency and inaccuracy of\navailable in-domain translations. In such scenarios where there is insufficient\nin-domain data to fine-tune MT models, producing translations that are\nconsistent with the relevant context is challenging. While real-time adaptation\ncan make use of smaller amounts of in-domain data to improve the translation on\nthe fly, it remains challenging due to supported context limitations and\nefficiency constraints. Large language models (LLMs) have recently shown\ninteresting capabilities of in-context learning, where they learn to replicate\ncertain input-output text generation patterns, without further fine-tuning.\nSuch capabilities have opened new horizons for domain-specific data\naugmentation and real-time adaptive MT. This work attempts to address two main\nrelevant questions: 1) in scenarios involving human interaction and continuous\nfeedback, can we employ language models to improve the quality of adaptive MT\nat inference time? and 2) in the absence of sufficient in-domain data, can we\nuse pre-trained large-scale language models to improve the process of MT domain\nadaptation?",
        "translated": ""
    },
    {
        "title": "Future Impact Decomposition in Request-level Recommendations",
        "url": "http://arxiv.org/abs/2401.16108v1",
        "pub_date": "2024-01-29",
        "summary": "In recommender systems, reinforcement learning solutions have shown promising\nresults in optimizing the interaction sequence between users and the system\nover the long-term performance. For practical reasons, the policy's actions are\ntypically designed as recommending a list of items to handle users' frequent\nand continuous browsing requests more efficiently. In this list-wise\nrecommendation scenario, the user state is updated upon every request in the\ncorresponding MDP formulation. However, this request-level formulation is\nessentially inconsistent with the user's item-level behavior. In this study, we\ndemonstrate that an item-level optimization approach can better utilize item\ncharacteristics and optimize the policy's performance even under the\nrequest-level MDP. We support this claim by comparing the performance of\nstandard request-level methods with the proposed item-level actor-critic\nframework in both simulation and online experiments. Furthermore, we found that\nthe naive equal decomposition of future values may not effectively express the\nitem-wise utility in the long term. To address this issue, we propose a future\ndecomposition strategy based on each item's immediate reward, and further show\nthat we can obtain more advanced settings of weight through adversarial\nlearning.",
        "translated": ""
    },
    {
        "title": "Pushing the Limits: Concurrency Detection in Acyclic, Live, and 1-Safe\n  Free-Choice Nets in $O\\big((P + T)^2\\big)$",
        "url": "http://arxiv.org/abs/2401.16097v1",
        "pub_date": "2024-01-29",
        "summary": "Concurrency is an important aspect of (Petri) nets to describe and simulate\nthe behavior of complex systems. Knowing which places and transitions could be\nexecuted in parallel helps to understand nets and enables analysis techniques\nand the computation of other properties, such as causality, exclusivity, etc..\nAll techniques based on concurrency detection depend on the efficiency of this\ndetection methodology. Kovalyov and Esparza have developed algorithms that\ncompute all concurrent places in $O\\big((P+T)TP^2\\big)$ for live nets (where\n$P$ and $T$ are the numbers of places and transitions) and in\n$O\\big(P(P+T)^2\\big)$ for live free-choice nets. Although these algorithms have\na reasonably good computational complexity, large numbers of concurrent pairs\nof nodes may still lead to long computation times. Furthermore, both algorithms\ncannot be parallelized without additional effort. This paper complements the\npalette of concurrency detection algorithms with the Concurrent Paths (CP)\nalgorithm for safe, live, free-choice nets. The algorithm allows\nparallelization and has a worst-case computational complexity of\n$O\\big((P+T)^2\\big)$ for acyclic nets and of $O\\big(P^3+PT^2\\big)$ for cyclic\nnets. Although the computational complexity of cyclic nets has not improved,\nthe evaluation shows the benefits of CP, especially, if the net contains many\nnodes in concurrency relation.",
        "translated": ""
    },
    {
        "title": "Seller-Side Experiments under Interference Induced by Feedback Loops in\n  Two-Sided Platforms",
        "url": "http://arxiv.org/abs/2401.15811v1",
        "pub_date": "2024-01-29",
        "summary": "Two-sided platforms are central to modern commerce and content sharing and\noften utilize A/B testing for developing new features. While user-side\nexperiments are common, seller-side experiments become crucial for specific\ninterventions and metrics. This paper investigates the effects of interference\ncaused by feedback loops on seller-side experiments in two-sided platforms,\nwith a particular focus on the counterfactual interleaving design, proposed in\n\\citet{ha2020counterfactual,nandy2021b}. These feedback loops, often generated\nby pacing algorithms, cause outcomes from earlier sessions to influence\nsubsequent ones. This paper contributes by creating a mathematical framework to\nanalyze this interference, theoretically estimating its impact, and conducting\nempirical evaluations of the counterfactual interleaving design in real-world\nscenarios. Our research shows that feedback loops can result in misleading\nconclusions about the treatment effects.",
        "translated": ""
    },
    {
        "title": "The Impact of Snippet Reliability on Misinformation in Online Health\n  Search",
        "url": "http://arxiv.org/abs/2401.15720v1",
        "pub_date": "2024-01-28",
        "summary": "Search result snippets are crucial in modern search engines, providing users\nwith a quick overview of a website's content. Snippets help users determine the\nrelevance of a document to their information needs, and in certain scenarios\neven enable them to satisfy those needs without visiting web documents. Hence,\nit is crucial for snippets to reliably represent the content of their\ncorresponding documents. While this may be a straightforward requirement for\nsome queries, it can become challenging in the complex domain of healthcare,\nand can lead to misinformation. This paper aims to examine snippets'\nreliability in representing their corresponding documents, specifically in the\nhealth domain. To achieve this, we conduct a series of user studies using\nGoogle's search results, where participants are asked to infer viewpoints of\nsearch results pertaining to queries about the effectiveness of a medical\nintervention for a medical condition, based solely on their titles and\nsnippets. Our findings reveal that a considerable portion of Google's snippets\n(28%) failed to present any viewpoint on the intervention's effectiveness, and\nthat 35% were interpreted by participants as having a different viewpoint\ncompared to their corresponding documents. To address this issue, we propose a\nsnippet extraction solution tailored directly to users' information needs,\ni.e., extracting snippets that summarize documents' viewpoints regarding the\nintervention and condition that appear in the query. User study demonstrates\nthat our information need-focused solution outperforms the mainstream\nquery-based approach. With only 19.67% of snippets generated by our solution\nreported as not presenting a viewpoint and a mere 20.33% misinterpreted by\nparticipants. These results strongly suggest that an information need-focused\napproach can significantly improve the reliability of extracted snippets in\nonline health search.",
        "translated": ""
    },
    {
        "title": "PRE: A Peer Review Based Large Language Model Evaluator",
        "url": "http://arxiv.org/abs/2401.15641v1",
        "pub_date": "2024-01-28",
        "summary": "The impressive performance of large language models (LLMs) has attracted\nconsiderable attention from the academic and industrial communities. Besides\nhow to construct and train LLMs, how to effectively evaluate and compare the\ncapacity of LLMs has also been well recognized as an important yet difficult\nproblem. Existing paradigms rely on either human annotators or model-based\nevaluators to evaluate the performance of LLMs on different tasks. However,\nthese paradigms often suffer from high cost, low generalizability, and\ninherited biases in practice, which make them incapable of supporting the\nsustainable development of LLMs in long term. In order to address these issues,\ninspired by the peer review systems widely used in academic publication\nprocess, we propose a novel framework that can automatically evaluate LLMs\nthrough a peer-review process. Specifically, for the evaluation of a specific\ntask, we first construct a small qualification exam to select \"reviewers\" from\na couple of powerful LLMs. Then, to actually evaluate the \"submissions\" written\nby different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to\nrate or compare the submissions. The final ranking of evaluatee LLMs is\ngenerated based on the results provided by all reviewers. We conducted\nextensive experiments on text summarization tasks with eleven LLMs including\nGPT-4. The results demonstrate the existence of biasness when evaluating using\na single LLM. Also, our PRE model outperforms all the baselines, illustrating\nthe effectiveness of the peer review mechanism.",
        "translated": ""
    },
    {
        "title": "RecDCL: Dual Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2401.15635v1",
        "pub_date": "2024-01-28",
        "summary": "Self-supervised recommendation (SSR) has achieved great success in mining the\npotential interacted behaviors for collaborative filtering in recent years. As\na major branch, Contrastive Learning (CL) based SSR conquers data sparsity in\nWeb platforms by contrasting the embedding between raw data and augmented data.\nHowever, existing CL-based SSR methods mostly focus on contrasting in a\nbatch-wise way, failing to exploit potential regularity in the feature-wise\ndimension, leading to redundant solutions during the representation learning\nprocess of users (items) from Websites. Furthermore, the joint benefits of\nutilizing both Batch-wise CL (BCL) and Feature-wise CL (FCL) for\nrecommendations remain underexplored. To address these issues, we investigate\nthe relationship of objectives between BCL and FCL. Our study suggests a\ncooperative benefit of employing both methods, as evidenced from theoretical\nand experimental perspectives. Based on these insights, we propose a dual CL\nmethod for recommendation, referred to as RecDCL. RecDCL first eliminates\nredundant solutions on user-item positive pairs in a feature-wise manner. It\nthen optimizes the uniform distributions within users and items using a\npolynomial kernel from an FCL perspective. Finally, it generates contrastive\nembedding on output vectors in a batch-wise objective. We conduct experiments\non four widely-used benchmarks and an industrial dataset. The results\nconsistently demonstrate that the proposed RecDCL outperforms the\nstate-of-the-art GNNs-based and SSL-based models (with up to a 5.65\\%\nimprovement in terms of Recall@20), thereby confirming the effectiveness of the\njoint-wise objective. All source codes used in this paper are publicly\navailable at \\url{https://github.com/THUDM/RecDCL}}.",
        "translated": ""
    },
    {
        "title": "Navigating the Post-API Dilemma Search Engine Results Pages Present a\n  Biased View of Social Media Data",
        "url": "http://arxiv.org/abs/2401.15479v1",
        "pub_date": "2024-01-27",
        "summary": "Recent decisions to discontinue access to social media APIs are having\ndetrimental effects on Internet research and the field of computational social\nscience as a whole. This lack of access to data has been dubbed the Post-API\nera of Internet research. Fortunately, popular search engines have the means to\ncrawl, capture, and surface social media data on their Search Engine Results\nPages (SERP) if provided the proper search query, and may provide a solution to\nthis dilemma. In the present work we ask: does SERP provide a complete and\nunbiased sample of social media data? Is SERP a viable alternative to direct\nAPI-access? To answer these questions, we perform a comparative analysis\nbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. We\nfind that SERP results are highly biased in favor of popular posts; against\npolitical, pornographic, and vulgar posts; are more positive in their\nsentiment; and have large topical gaps. Overall, we conclude that SERP is not a\nviable alternative to social media API access.",
        "translated": ""
    },
    {
        "title": "Indexing Portuguese NLP Resources with PT-Pump-Up",
        "url": "http://arxiv.org/abs/2401.15400v1",
        "pub_date": "2024-01-27",
        "summary": "The recent advances in natural language processing (NLP) are linked to\ntraining processes that require vast amounts of corpora. Access to this data is\ncommonly not a trivial process due to resource dispersion and the need to\nmaintain these infrastructures online and up-to-date. New developments in NLP\nare often compromised due to the scarcity of data or lack of a shared\nrepository that works as an entry point to the community. This is especially\ntrue in low and mid-resource languages, such as Portuguese, which lack data and\nproper resource management infrastructures. In this work, we propose\nPT-Pump-Up, a set of tools that aim to reduce resource dispersion and improve\nthe accessibility to Portuguese NLP resources. Our proposal is divided into\nfour software components: a) a web platform to list the available resources; b)\na client-side Python package to simplify the loading of Portuguese NLP\nresources; c) an administrative Python package to manage the platform and d) a\npublic GitHub repository to foster future collaboration and contributions. All\nfour components are accessible using: https://linktr.ee/pt_pump_up",
        "translated": ""
    },
    {
        "title": "Privacy-Preserving Cross-Domain Sequential Recommendation",
        "url": "http://arxiv.org/abs/2401.15369v1",
        "pub_date": "2024-01-27",
        "summary": "Cross-domain sequential recommendation is an important development direction\nof recommender systems. It combines the characteristics of sequential\nrecommender systems and cross-domain recommender systems, which can capture the\ndynamic preferences of users and alleviate the problem of cold-start users.\nHowever, in recent years, people pay more and more attention to their privacy.\nThey do not want other people to know what they just bought, what videos they\njust watched, and where they just came from. How to protect the users' privacy\nhas become an urgent problem to be solved. In this paper, we propose a novel\nprivacy-preserving cross-domain sequential recommender system (PriCDSR), which\ncan provide users with recommendation services while preserving their privacy\nat the same time. Specifically, we define a new differential privacy on the\ndata, taking into account both the ID information and the order information.\nThen, we design a random mechanism that satisfies this differential privacy and\nprovide its theoretical proof. Our PriCDSR is a non-invasive method that can\nadopt any cross-domain sequential recommender system as a base model without\nany modification to it. To the best of our knowledge, our PriCDSR is the first\nwork to investigate privacy issues in cross-domain sequential recommender\nsystems. We conduct experiments on three domains, and the results demonstrate\nthat our PriCDSR, despite introducing noise, still outperforms recommender\nsystems that only use data from a single domain.",
        "translated": ""
    },
    {
        "title": "A Survey on Neural Topic Models: Methods, Applications, and Challenges",
        "url": "http://arxiv.org/abs/2401.15351v1",
        "pub_date": "2024-01-27",
        "summary": "Topic models have been prevalent for decades to discover latent topics and\ninfer topic proportions of documents in an unsupervised fashion. They have been\nwidely used in various applications like text analysis and context\nrecommendation. Recently, the rise of neural networks has facilitated the\nemergence of a new research field -- Neural Topic Models (NTMs). Different from\nconventional topic models, NTMs directly optimize parameters without requiring\nmodel-specific derivations. This endows NTMs with better scalability and\nflexibility, resulting in significant research attention and plentiful new\nmethods and applications. In this paper, we present a comprehensive survey on\nneural topic models concerning methods, applications, and challenges.\nSpecifically, we systematically organize current NTM methods according to their\nnetwork structures and introduce the NTMs for various scenarios like short\ntexts and cross-lingual documents. We also discuss a wide range of popular\napplications built on NTMs. Finally, we highlight the challenges confronted by\nNTMs to inspire future research.",
        "translated": ""
    },
    {
        "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
        "url": "http://arxiv.org/abs/2401.17197v1",
        "pub_date": "2024-01-30",
        "summary": "Leveraging Large Language Models (LLMs) for recommendation has recently\ngarnered considerable attention, where fine-tuning plays a key role in LLMs'\nadaptation. However, the cost of fine-tuning LLMs on rapidly expanding\nrecommendation data limits their practical application. To address this\nchallenge, few-shot fine-tuning offers a promising approach to quickly adapt\nLLMs to new recommendation data. We propose the task of data pruning for\nefficient LLM-based recommendation, aimed at identifying representative samples\ntailored for LLMs' few-shot fine-tuning. While coreset selection is closely\nrelated to the proposed task, existing coreset selection methods often rely on\nsuboptimal heuristic metrics or entail costly optimization on large-scale\nrecommendation data.\n  To tackle these issues, we introduce two objectives for the data pruning task\nin the context of LLM-based recommendation: 1) high accuracy aims to identify\nthe influential samples that can lead to high overall performance; and 2) high\nefficiency underlines the low costs of the data pruning process. To pursue the\ntwo objectives, we propose a novel data pruning method based on two scores,\ni.e., influence score and effort score, to efficiently identify the influential\nsamples. Particularly, the influence score is introduced to accurately estimate\nthe influence of sample removal on the overall performance. To achieve low\ncosts of the data pruning process, we use a small-sized surrogate model to\nreplace LLMs to obtain the influence score. Considering the potential gap\nbetween the surrogate model and LLMs, we further propose an effort score to\nprioritize some hard samples specifically for LLMs. Empirical results on three\nreal-world datasets validate the effectiveness of our proposed method. In\nparticular, the proposed method uses only 2% samples to surpass the full data\nfine-tuning, reducing time costs by 97%.",
        "translated": ""
    },
    {
        "title": "Embracing Language Inclusivity and Diversity in CLIP through Continual\n  Language Learning",
        "url": "http://arxiv.org/abs/2401.17186v1",
        "pub_date": "2024-01-30",
        "summary": "While vision-language pre-trained models (VL-PTMs) have advanced multimodal\nresearch in recent years, their mastery in a few languages like English\nrestricts their applicability in broader communities. To this end, there is an\nincreasing interest in developing multilingual VL models via a joint-learning\nsetup, which, however, could be unrealistic due to expensive costs and data\navailability. In this work, we propose to extend VL-PTMs' language capacity by\ncontinual language learning (CLL), where a model needs to update its linguistic\nknowledge incrementally without suffering from catastrophic forgetting (CF). We\nbegin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP,\na prevailing VL-PTM that has acquired image-English text alignment.\nSpecifically, CLL-CLIP contains an expandable token embedding layer to handle\nlinguistic differences. It solely trains token embeddings to improve memory\nstability and is optimized under cross-modal and cross-lingual objectives to\nlearn the alignment between images and multilingual texts. To alleviate CF\nraised by covariate shift and lexical overlap, we further propose a novel\napproach that ensures the identical distribution of all token embeddings during\ninitialization and regularizes token embedding learning during training. We\nconstruct a CLL benchmark covering 36 languages based on MSCOCO and XM3600\ndatasets and then evaluate multilingual image-text retrieval performance.\nExtensive experiments verify the effectiveness of CLL-CLIP and show that our\napproach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on\nXM3600, and improve various state-of-the-art methods consistently. Our code and\ndata are available at \\url{https://github.com/yangbang18/CLFM}.",
        "translated": ""
    },
    {
        "title": "The Influence of Presentation and Performance on User Satisfaction",
        "url": "http://arxiv.org/abs/2401.17100v1",
        "pub_date": "2024-01-30",
        "summary": "The effectiveness of an IR system is gauged not just by its ability to\nretrieve relevant results but also by how it presents these results to users;\nan engaging presentation often correlates with increased user satisfaction.\nWhile existing research has delved into the link between user satisfaction, IR\nperformance metrics, and presentation, these aspects have typically been\ninvestigated in isolation. Our research aims to bridge this gap by examining\nthe relationship between query performance, presentation and user satisfaction.\nFor our analysis, we conducted a between-subjects experiment comparing the\neffectiveness of various result card layouts for an ad-hoc news search\ninterface. Drawing data from the TREC WaPo 2018 collection, we centered our\nstudy on four specific topics. Within each of these topics, we assessed six\ndistinct queries with varying nDCG values. Our study involved 164 participants\nwho were exposed to one of five distinct layouts containing result cards, such\nas \"title'', \"title+image'', or \"title+image+summary''. Our findings indicate\nthat while nDCG is a strong predictor of user satisfaction at the query level,\nthere exists no linear relationship between the performance of the query,\npresentation of results and user satisfaction. However, when considering the\ntotal gain on the initial result page, we observed that presentation does play\na significant role in user satisfaction (at the query level) for certain\nlayouts with result cards such as, title+image or title+image+summary. Our\nresults also suggest that the layout differences have complex and multifaceted\nimpacts on satisfaction. We demonstrate the capacity to equalize user\nsatisfaction levels between queries of varying performance by changing how\nresults are presented. This emphasizes the necessity to harmonize both\nperformance and presentation in IR systems, considering users' diverse\npreferences.",
        "translated": ""
    },
    {
        "title": "Re3val: Reinforced and Reranked Generative Retrieval",
        "url": "http://arxiv.org/abs/2401.16979v1",
        "pub_date": "2024-01-30",
        "summary": "Generative retrieval models encode pointers to information in a corpus as an\nindex within the model's parameters. These models serve as part of a larger\npipeline, where retrieved information conditions generation for\nknowledge-intensive NLP tasks. However, we identify two limitations: the\ngenerative retrieval does not account for contextual information. Secondly, the\nretrieval can't be tuned for the downstream readers as decoding the page title\nis a non-differentiable operation. This paper introduces Re3val, trained with\ngenerative reranking and reinforcement learning using limited data. Re3val\nleverages context acquired via Dense Passage Retrieval to rerank the retrieved\npage titles and utilizes REINFORCE to maximize rewards generated by constrained\ndecoding. Additionally, we generate questions from our pre-training dataset to\nmitigate epistemic uncertainty and bridge the domain gap between the\npre-training and fine-tuning datasets. Subsequently, we extract and rerank\ncontexts from the KILT database using the rerank page titles. Upon grounding\nthe top five reranked contexts, Re3val demonstrates the Top 1 KILT scores\ncompared to all other generative retrieval models across five KILT datasets.",
        "translated": ""
    },
    {
        "title": "Taxonomy of Mathematical Plagiarism",
        "url": "http://arxiv.org/abs/2401.16969v1",
        "pub_date": "2024-01-30",
        "summary": "Plagiarism is a pressing concern, even more so with the availability of large\nlanguage models. Existing plagiarism detection systems reliably find copied and\nmoderately reworded text but fail for idea plagiarism, especially in\nmathematical science, which heavily uses formal mathematical notation. We make\ntwo contributions. First, we establish a taxonomy of mathematical content reuse\nby annotating potentially plagiarised 122 scientific document pairs. Second, we\nanalyze the best-performing approaches to detect plagiarism and mathematical\ncontent similarity on the newly established taxonomy. We found that the\nbest-performing methods for plagiarism and math content similarity achieve an\noverall detection score (PlagDet) of 0.06 and 0.16, respectively. The\nbest-performing methods failed to detect most cases from all seven newly\nestablished math similarity types. Outlined contributions will benefit research\nin plagiarism detection systems, recommender systems, question-answering\nsystems, and search engines. We make our experiment's code and annotated\ndataset available to the community:\nhttps://github.com/gipplab/Taxonomy-of-Mathematical-Plagiarism",
        "translated": ""
    },
    {
        "title": "Detecting LLM-Assisted Writing in Scientific Communication: Are We There\n  Yet?",
        "url": "http://arxiv.org/abs/2401.16807v1",
        "pub_date": "2024-01-30",
        "summary": "Large Language Models (LLMs), exemplified by ChatGPT, have significantly\nreshaped text generation, particularly in the realm of writing assistance.\nWhile ethical considerations underscore the importance of transparently\nacknowledging LLM use, especially in scientific communication, genuine\nacknowledgment remains infrequent. A potential avenue to encourage accurate\nacknowledging of LLM-assisted writing involves employing automated detectors.\nOur evaluation of four cutting-edge LLM-generated text detectors reveals their\nsuboptimal performance compared to a simple ad-hoc detector designed to\nidentify abrupt writing style changes around the time of LLM proliferation. We\ncontend that the development of specialized detectors exclusively dedicated to\nLLM-assisted writing detection is necessary. Such detectors could play a\ncrucial role in fostering more authentic recognition of LLM involvement in\nscientific communication, addressing the current challenges in acknowledgment\npractices.",
        "translated": ""
    },
    {
        "title": "AutoIE: An Automated Framework for Information Extraction from\n  Scientific Literature",
        "url": "http://arxiv.org/abs/2401.16672v1",
        "pub_date": "2024-01-30",
        "summary": "In the rapidly evolving field of scientific research, efficiently extracting\nkey information from the burgeoning volume of scientific papers remains a\nformidable challenge. This paper introduces an innovative framework designed to\nautomate the extraction of vital data from scientific PDF documents, enabling\nresearchers to discern future research trajectories more readily. AutoIE\nuniquely integrates four novel components: (1) A multi-semantic feature\nfusion-based approach for PDF document layout analysis; (2) Advanced functional\nblock recognition in scientific texts; (3) A synergistic technique for\nextracting and correlating information on molecular sieve synthesis; (4) An\nonline learning paradigm tailored for molecular sieve literature. Our SBERT\nmodel achieves high Marco F1 scores of 87.19 and 89.65 on CoNLL04 and ADE\ndatasets. In addition, a practical application of AutoIE in the petrochemical\nmolecular sieve synthesis domain demonstrates its efficacy, evidenced by an\nimpressive 78\\% accuracy rate. This research paves the way for enhanced data\nmanagement and interpretation in molecular sieve synthesis. It is a valuable\nasset for seasoned experts and newcomers in this specialized field.",
        "translated": ""
    },
    {
        "title": "History-Aware Conversational Dense Retrieval",
        "url": "http://arxiv.org/abs/2401.16659v1",
        "pub_date": "2024-01-30",
        "summary": "Conversational search facilitates complex information retrieval by enabling\nmulti-turn interactions between users and the system. Supporting such\ninteractions requires a comprehensive understanding of the conversational\ninputs to formulate a good search query based on historical information. In\nparticular, the search query should include the relevant information from the\nprevious conversation turns. However, current approaches for conversational\ndense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever\nusing the whole conversational search session, which can be lengthy and noisy.\nMoreover, existing approaches are limited by the amount of manual supervision\nsignals in the existing datasets. To address the aforementioned issues, we\npropose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which\nincorporates two ideas: context-denoised query reformulation and automatic\nmining of supervision signals based on the actual impact of historical turns.\nExperiments on two public conversational search datasets demonstrate the\nimproved history modeling capability of HAConvDR, in particular for long\nconversations with topic shifts.",
        "translated": ""
    },
    {
        "title": "FakeClaim: A Multiple Platform-driven Dataset for Identification of Fake\n  News on 2023 Israel-Hamas War",
        "url": "http://arxiv.org/abs/2401.16625v1",
        "pub_date": "2024-01-29",
        "summary": "We contribute the first publicly available dataset of factual claims from\ndifferent platforms and fake YouTube videos on the 2023 Israel-Hamas war for\nautomatic fake YouTube video classification. The FakeClaim data is collected\nfrom 60 fact-checking organizations in 30 languages and enriched with metadata\nfrom the fact-checking organizations curated by trained journalists specialized\nin fact-checking. Further, we classify fake videos within the subset of YouTube\nvideos using textual information and user comments. We used a pre-trained model\nto classify each video with different feature combinations. Our best-performing\nfine-tuned language model, Universal Sentence Encoder (USE), achieves a Macro\nF1 of 87\\%, which shows that the trained model can be helpful for debunking\nfake videos using the comments from the user discussion. The dataset is\navailable on Github\\footnote{https://github.com/Gautamshahi/FakeClaim}",
        "translated": ""
    },
    {
        "title": "Dissecting users' needs for search result explanations",
        "url": "http://arxiv.org/abs/2401.16509v1",
        "pub_date": "2024-01-29",
        "summary": "There is a growing demand for transparency in search engines to understand\nhow search results are curated and to enhance users' trust. Prior research has\nintroduced search result explanations with a focus on how to explain, assuming\nexplanations are beneficial. Our study takes a step back to examine if search\nexplanations are needed and when they are likely to provide benefits.\nAdditionally, we summarize key characteristics of helpful explanations and\nshare users' perspectives on explanation features provided by Google and Bing.\nInterviews with non-technical individuals reveal that users do not always seek\nor understand search explanations and mostly desire them for complex and\ncritical tasks. They find Google's search explanations too obvious but\nappreciate the ability to contest search results. Based on our findings, we\noffer design recommendations for search engines and explanations to help users\nbetter evaluate search results and enhance their search experience.",
        "translated": ""
    },
    {
        "title": "Neural Locality Sensitive Hashing for Entity Blocking",
        "url": "http://arxiv.org/abs/2401.18064v1",
        "pub_date": "2024-01-31",
        "summary": "Locality-sensitive hashing (LSH) is a fundamental algorithmic technique\nwidely employed in large-scale data processing applications, such as\nnearest-neighbor search, entity resolution, and clustering. However, its\napplicability in some real-world scenarios is limited due to the need for\ncareful design of hashing functions that align with specific metrics. Existing\nLSH-based Entity Blocking solutions primarily rely on generic similarity\nmetrics such as Jaccard similarity, whereas practical use cases often demand\ncomplex and customized similarity rules surpassing the capabilities of generic\nsimilarity metrics. Consequently, designing LSH functions for these customized\nsimilarity rules presents considerable challenges. In this research, we propose\na neuralization approach to enhance locality-sensitive hashing by training deep\nneural networks to serve as hashing functions for complex metrics. We assess\nthe effectiveness of this approach within the context of the entity resolution\nproblem, which frequently involves the use of task-specific metrics in\nreal-world applications. Specifically, we introduce NLSHBlock (Neural-LSH\nBlock), a novel blocking methodology that leverages pre-trained language\nmodels, fine-tuned with a novel LSH-based loss function. Through extensive\nevaluations conducted on a diverse range of real-world datasets, we demonstrate\nthe superiority of NLSHBlock over existing methods, exhibiting significant\nperformance improvements. Furthermore, we showcase the efficacy of NLSHBlock in\nenhancing the performance of the entity matching phase, particularly within the\nsemi-supervised setting.",
        "translated": ""
    },
    {
        "title": "Error-Tolerant E-Discovery Protocols",
        "url": "http://arxiv.org/abs/2401.17952v1",
        "pub_date": "2024-01-31",
        "summary": "We consider the multi-party classification problem introduced by Dong,\nHartline, and Vijayaraghavan (2022) in the context of electronic discovery\n(e-discovery). Based on a request for production from the requesting party, the\nresponding party is required to provide documents that are responsive to the\nrequest except for those that are legally privileged. Our goal is to find a\nprotocol that verifies that the responding party sends almost all responsive\ndocuments while minimizing the disclosure of non-responsive documents. We\nprovide protocols in the challenging non-realizable setting, where the instance\nmay not be perfectly separated by a linear classifier. We demonstrate\nempirically that our protocol successfully manages to find almost all relevant\ndocuments, while incurring only a small disclosure of non-responsive documents.\nWe complement this with a theoretical analysis of our protocol in the\nsingle-dimensional setting, and other experiments on simulated data which\nsuggest that the non-responsive disclosure incurred by our protocol may be\nunavoidable.",
        "translated": ""
    },
    {
        "title": "A Survey on Data-Centric Recommender Systems",
        "url": "http://arxiv.org/abs/2401.17878v1",
        "pub_date": "2024-01-31",
        "summary": "Recommender systems (RS) have become essential tools for mitigating\ninformation overload in a range of real-world scenarios. Recent trends in RS\nhave seen a paradigm shift, moving the spotlight from model-centric innovations\nto the importance of data quality and quantity. This evolution has given rise\nto the concept of data-centric recommender systems (Data-Centric RS), marking a\nsignificant development in the field. This survey provides the first systematic\noverview of Data-Centric RS, covering 1) the foundational concepts of\nrecommendation data and Data-Centric RS; 2) three primary issues in\nrecommendation data; 3) recent research developed to address these issues; and\n4) several potential future directions in Data-Centric RS.",
        "translated": ""
    },
    {
        "title": "Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal\n  Entity Alignment",
        "url": "http://arxiv.org/abs/2401.17859v1",
        "pub_date": "2024-01-31",
        "summary": "In Multi-Modal Knowledge Graphs (MMKGs), Multi-Modal Entity Alignment (MMEA)\nis crucial for identifying identical entities across diverse modal attributes.\nHowever, semantic inconsistency, mainly due to missing modal attributes, poses\na significant challenge. Traditional approaches rely on attribute\ninterpolation, but this often introduces modality noise, distorting the\noriginal semantics. Moreover, the lack of a universal theoretical framework\nlimits advancements in achieving semantic consistency. This study introduces a\nnovel approach, DESAlign, which addresses these issues by applying a\ntheoretical framework based on Dirichlet energy to ensure semantic consistency.\nWe discover that semantic inconsistency leads to model overfitting to modality\nnoise, causing performance fluctuations, particularly when modalities are\nmissing. DESAlign innovatively combats over-smoothing and interpolates absent\nsemantics using existing modalities. Our approach includes a multi-modal\nknowledge graph learning strategy and a propagation technique that employs\nexisting semantic features to compensate for missing ones, providing explicit\nEuler solutions. Comprehensive evaluations across 18 benchmarks, including\nmonolingual and bilingual scenarios, demonstrate that DESAlign surpasses\nexisting methods, setting a new standard in performance. Further testing on 42\nbenchmarks with high rates of missing modalities confirms its robustness,\noffering an effective solution to semantic inconsistency in real-world MMKGs.",
        "translated": ""
    },
    {
        "title": "Network-based Topic Structure Visualization",
        "url": "http://arxiv.org/abs/2401.17855v1",
        "pub_date": "2024-01-31",
        "summary": "In the real world, many topics are inter-correlated, making it challenging to\ninvestigate their structure and relationships. Understanding the interplay\nbetween topics and their relevance can provide valuable insights for\nresearchers, guiding their studies and informing the direction of research. In\nthis paper, we utilize the topic-words distribution, obtained from topic\nmodels, as item-response data to model the structure of topics using a latent\nspace item response model. By estimating the latent positions of topics based\non their distances toward words, we can capture the underlying topic structure\nand reveal their relationships. Visualizing the latent positions of topics in\nEuclidean space allows for an intuitive understanding of their proximity and\nassociations. We interpret relationships among topics by characterizing each\ntopic based on representative words selected using a newly proposed scoring\nscheme. Additionally, we assess the maturity of topics by tracking their latent\npositions using different word sets, providing insights into the robustness of\ntopics. To demonstrate the effectiveness of our approach, we analyze the topic\ncomposition of COVID-19 studies during the early stage of its emergence using\nbiomedical literature in the PubMed database. The software and data used in\nthis paper are publicly available at https://github.com/jeon9677/gViz .",
        "translated": ""
    },
    {
        "title": "Global-Liar: Factuality of LLMs over Time and Geographic Regions",
        "url": "http://arxiv.org/abs/2401.17839v1",
        "pub_date": "2024-01-31",
        "summary": "The increasing reliance on AI-driven solutions, particularly Large Language\nModels (LLMs) like the GPT series, for information retrieval highlights the\ncritical need for their factuality and fairness, especially amidst the rampant\nspread of misinformation and disinformation online. Our study evaluates the\nfactual accuracy, stability, and biases in widely adopted GPT models, including\nGPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated\ninformation dissemination.\n  We introduce 'Global-Liar,' a dataset uniquely balanced in terms of\ngeographic and temporal representation, facilitating a more nuanced evaluation\nof LLM biases. Our analysis reveals that newer iterations of GPT models do not\nalways equate to improved performance. Notably, the GPT-4 version from March\ndemonstrates higher factual accuracy than its subsequent June release.\nFurthermore, a concerning bias is observed, privileging statements from the\nGlobal North over the Global South, thus potentially exacerbating existing\ninformational inequities. Regions such as Africa and the Middle East are at a\ndisadvantage, with much lower factual accuracy. The performance fluctuations\nover time suggest that model updates may not consistently benefit all regions\nequally.\n  Our study also offers insights into the impact of various LLM configuration\nsettings, such as binary decision forcing, model re-runs and temperature, on\nmodel's factuality. Models constrained to binary (true/false) choices exhibit\nreduced factuality compared to those allowing an 'unclear' option. Single\ninference at a low temperature setting matches the reliability of majority\nvoting across various configurations. The insights gained highlight the need\nfor culturally diverse and geographically inclusive model training and\nevaluation. This approach is key to achieving global equity in technology,\ndistributing AI benefits fairly worldwide.",
        "translated": ""
    },
    {
        "title": "LoRec: Large Language Model for Robust Sequential Recommendation against\n  Poisoning Attacks",
        "url": "http://arxiv.org/abs/2401.17723v1",
        "pub_date": "2024-01-31",
        "summary": "Sequential recommender systems stand out for their ability to capture users'\ndynamic interests and the patterns of item-to-item transitions. However, the\ninherent openness of sequential recommender systems renders them vulnerable to\npoisoning attacks, where fraudulent users are injected into the training data\nto manipulate learned patterns. Traditional defense strategies predominantly\ndepend on predefined assumptions or rules extracted from specific known\nattacks, limiting their generalizability to unknown attack types. To solve the\nabove problems, considering the rich open-world knowledge encapsulated in Large\nLanguage Models (LLMs), our research initially focuses on the capabilities of\nLLMs in the detection of unknown fraudulent activities within recommender\nsystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the\nsubstantial capability of LLMs in identifying unknown fraudsters, leveraging\ntheir expansive, open-world knowledge.\n  Building upon this, we propose the integration of LLMs into defense\nstrategies to extend their effectiveness beyond the confines of known attacks.\nWe propose LoRec, an advanced framework that employs LLM-Enhanced Calibration\nto strengthen the robustness of sequential recommender systems against\npoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that\nrefines the training process of sequential recommender systems with knowledge\nderived from LLMs, applying a user-wise reweighting to diminish the impact of\nfraudsters injected by attacks. By incorporating LLMs' open-world knowledge,\nthe LCT effectively converts the limited, specific priors or rules into a more\ngeneral pattern of fraudsters, offering improved defenses against poisoning\nattacks. Our comprehensive experiments validate that LoRec, as a general\nframework, significantly strengthens the robustness of sequential recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "ReSLLM: Large Language Models are Strong Resource Selectors for\n  Federated Search",
        "url": "http://arxiv.org/abs/2401.17645v1",
        "pub_date": "2024-01-31",
        "summary": "Federated search, which involves integrating results from multiple\nindependent search engines, will become increasingly pivotal in the context of\nRetrieval-Augmented Generation pipelines empowering LLM-based applications such\nas chatbots. These systems often distribute queries among various search\nengines, ranging from specialized (e.g., PubMed) to general (e.g., Google),\nbased on the nature of user utterances. A critical aspect of federated search\nis resource selection - the selection of appropriate resources prior to issuing\nthe query to ensure high-quality and rapid responses, and contain costs\nassociated with calling the external search engines. However, current SOTA\nresource selection methodologies primarily rely on feature-based learning\napproaches. These methods often involve the labour intensive and expensive\ncreation of training labels for each resource. In contrast, LLMs have exhibited\nstrong effectiveness as zero-shot methods across NLP and IR tasks. We\nhypothesise that in the context of federated search LLMs can assess the\nrelevance of resources without the need for extensive predefined labels or\nfeatures. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to\ndrive the selection of resources in federated search in a zero-shot setting. In\naddition, we devise an unsupervised fine tuning protocol, the Synthetic Label\nAugmentation Tuning (SLAT), where the relevance of previously logged queries\nand snippets from resources is predicted using an off-the-shelf LLM and then in\nturn used to fine-tune ReSLLM with respect to resource selection. Our empirical\nevaluation and analysis details the factors influencing the effectiveness of\nLLMs in this context. The results showcase the merits of ReSLLM for resource\nselection: not only competitive effectiveness in the zero-shot setting, but\nalso obtaining large when fine-tuned using SLAT-protocol.",
        "translated": ""
    },
    {
        "title": "Towards Personalized Privacy: User-Governed Data Contribution for\n  Federated Recommendation",
        "url": "http://arxiv.org/abs/2401.17630v1",
        "pub_date": "2024-01-31",
        "summary": "Federated recommender systems (FedRecs) have gained significant attention for\ntheir potential to protect user's privacy by keeping user privacy data locally\nand only communicating model parameters/gradients to the server. Nevertheless,\nthe currently existing architecture of FedRecs assumes that all users have the\nsame 0-privacy budget, i.e., they do not upload any data to the server, thus\noverlooking those users who are less concerned about privacy and are willing to\nupload data to get a better recommendation service. To bridge this gap, this\npaper explores a user-governed data contribution federated recommendation\narchitecture where users are free to take control of whether they share data\nand the proportion of data they share to the server. To this end, this paper\npresents a cloud-device collaborative graph neural network federated\nrecommendation model, named CDCGNNFed. It trains user-centric ego graphs\nlocally, and high-order graphs based on user-shared data in the server in a\ncollaborative manner via contrastive learning. Furthermore, a graph mending\nstrategy is utilized to predict missing links in the graph on the server, thus\nleveraging the capabilities of graph neural networks over high-order graphs.\nExtensive experiments were conducted on two public datasets, and the results\ndemonstrate the effectiveness of the proposed method.",
        "translated": ""
    },
    {
        "title": "Fréchet Distance for Offline Evaluation of Information Retrieval\n  Systems with Sparse Labels",
        "url": "http://arxiv.org/abs/2401.17543v1",
        "pub_date": "2024-01-31",
        "summary": "The rapid advancement of natural language processing, information retrieval\n(IR), computer vision, and other technologies has presented significant\nchallenges in evaluating the performance of these systems. One of the main\nchallenges is the scarcity of human-labeled data, which hinders the fair and\naccurate assessment of these systems. In this work, we specifically focus on\nevaluating IR systems with sparse labels, borrowing from recent research on\nevaluating computer vision tasks. taking inspiration from the success of using\nFr\\'echet Inception Distance (FID) in assessing text-to-image generation\nsystems. We propose leveraging the Fr\\'echet Distance to measure the distance\nbetween the distributions of relevant judged items and retrieved results. Our\nexperimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query\nsets demonstrate the effectiveness of the Fr\\'echet Distance as a metric for\nevaluating IR systems, particularly in settings where a few labels are\navailable. This approach contributes to the advancement of evaluation\nmethodologies in real-world scenarios such as the assessment of generative IR\nsystems.",
        "translated": ""
    },
    {
        "title": "A Personalized Framework for Consumer and Producer Group Fairness\n  Optimization in Recommender Systems",
        "url": "http://arxiv.org/abs/2402.00485v1",
        "pub_date": "2024-02-01",
        "summary": "In recent years, there has been an increasing recognition that when machine\nlearning (ML) algorithms are used to automate decisions, they may mistreat\nindividuals or groups, with legal, ethical, or economic implications.\nRecommender systems are prominent examples of these machine learning (ML)\nsystems that aid users in making decisions. The majority of past literature\nresearch on RS fairness treats user and item fairness concerns independently,\nignoring the fact that recommender systems function in a two-sided marketplace.\nIn this paper, we propose CP-FairRank, an optimization-based re-ranking\nalgorithm that seamlessly integrates fairness constraints from both the\nconsumer and producer side in a joint objective framework. The framework is\ngeneralizable and may take into account varied fairness settings based on group\nsegmentation, recommendation model selection, and domain, which is one of its\nkey characteristics. For instance, we demonstrate that the system may jointly\nincrease consumer and producer fairness when (un)protected consumer groups are\ndefined on the basis of their activity level and main-streamness, while\nproducer groups are defined according to their popularity level. For empirical\nvalidation, through large-scale on eight datasets and four mainstream\ncollaborative filtering (CF) recommendation models, we demonstrate that our\nproposed strategy is able to improve both consumer and producer fairness\nwithout compromising or very little overall recommendation quality,\ndemonstrating the role algorithms may play in avoiding data biases.",
        "translated": ""
    },
    {
        "title": "From PARIS to LE-PARIS: Toward Patent Response Automation with\n  Recommender Systems and Collaborative Large Language Models",
        "url": "http://arxiv.org/abs/2402.00421v1",
        "pub_date": "2024-02-01",
        "summary": "In patent prosecution, timely and effective responses to Office Actions (OAs)\nare crucial for acquiring patents, yet past automation and AI research have\nscarcely addressed this aspect. To address this gap, our study introduces the\nPatent Office Action Response Intelligence System (PARIS) and its advanced\nversion, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are\ndesigned to expedite the efficiency of patent attorneys in collaboratively\nhandling OA responses. The systems' key features include the construction of an\nOA Topics Database, development of Response Templates, and implementation of\nRecommender Systems and LLM-based Response Generation. Our validation involves\na multi-paradigmatic analysis using the USPTO Office Action database and\nlongitudinal data of attorney interactions with our systems over six years.\nThrough five studies, we examine the constructiveness of OA topics (studies 1\nand 2) using topic modeling and the proposed Delphi process, the efficacy of\nour proposed hybrid recommender system tailored for OA (both LLM-based and\nnon-LLM-based) (study 3), the quality of response generation (study 4), and the\npractical value of the systems in real-world scenarios via user studies (study\n5). Results demonstrate that both PARIS and LE-PARIS significantly meet key\nmetrics and positively impact attorney performance.",
        "translated": ""
    },
    {
        "title": "EASRec: Elastic Architecture Search for Efficient Long-term Sequential\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2402.00390v1",
        "pub_date": "2024-02-01",
        "summary": "In this age where data is abundant, the ability to distill meaningful\ninsights from the sea of information is essential. Our research addresses the\ncomputational and resource inefficiencies that current Sequential Recommender\nSystems (SRSs) suffer from. especially those employing attention-based models\nlike SASRec, These systems are designed for next-item recommendations in\nvarious applications, from e-commerce to social networks. However, such systems\nsuffer from substantial computational costs and resource consumption during the\ninference stage. To tackle these issues, our research proposes a novel method\nthat combines automatic pruning techniques with advanced model architectures.\nWe also explore the potential of resource-constrained Neural Architecture\nSearch (NAS), a technique prevalent in the realm of recommendation systems, to\nfine-tune models for reduced FLOPs, latency, and energy usage while retaining\nor even enhancing accuracy. The main contribution of our work is developing the\nElastic Architecture Search for Efficient Long-term Sequential Recommender\nSystems (EASRec). This approach aims to find optimal compact architectures for\nattention-based SRSs, ensuring accuracy retention. EASRec introduces data-aware\ngates that leverage historical information from input data batch to improve the\nperformance of the recommendation network. Additionally, it utilizes a dynamic\nresource constraint approach, which standardizes the search process and results\nin more appropriate architectures. The effectiveness of our methodology is\nvalidated through exhaustive experiments on three benchmark datasets, which\ndemonstrates EASRec's superiority in SRSs. Our research set a new standard for\nfuture exploration into efficient and accurate recommender systems, signifying\na substantial advancement within this swiftly advancing field.",
        "translated": ""
    },
    {
        "title": "An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments",
        "url": "http://arxiv.org/abs/2402.00309v1",
        "pub_date": "2024-02-01",
        "summary": "Current IR evaluation is based on relevance judgments, created either\nmanually or automatically, with decisions outsourced to Large Language Models\n(LLMs). We offer an alternative paradigm, that never relies on relevance\njudgments in any form. Instead, a text is defined as relevant if it contains\ninformation that enables the answering of key questions. We use this idea to\ndesign the EXAM Answerability Metric to evaluate information\nretrieval/generation systems for their ability to provide topically relevant\ninformation.\n  We envision the role of a human judge to edit and define an exam question\nbank that will test for the presence of relevant information in text. We\nsupport this step by generating an initial set of exam questions. In the next\nphase, an LLM-based question answering system will automatically grade system\nresponses by tracking which exam questions are answerable with which system\nresponses. We propose two evaluation measures, the recall-oriented EXAM Cover\nmetric, and the precision-oriented EXAM Qrels metric, the latter which can be\nimplemented with trec_eval. This paradigm not only allows for the expansion of\nthe exam question set post-hoc but also facilitates the ongoing evaluation of\nfuture information systems, whether they focus on retrieval, generation, or\nboth.",
        "translated": ""
    },
    {
        "title": "PAP-REC: Personalized Automatic Prompt for Recommendation Language Model",
        "url": "http://arxiv.org/abs/2402.00284v1",
        "pub_date": "2024-02-01",
        "summary": "Recently emerged prompt-based Recommendation Language Models (RLM) can solve\nmultiple recommendation tasks uniformly. The RLMs make full use of the\ninherited knowledge learned from the abundant pre-training data to solve the\ndownstream recommendation tasks by prompts, without introducing additional\nparameters or network training. However, handcrafted prompts require\nsignificant expertise and human effort since slightly rewriting prompts may\ncause massive performance changes. In this paper, we propose PAP-REC, a\nframework to generate the Personalized Automatic Prompt for RECommendation\nlanguage models to mitigate the inefficiency and ineffectiveness problems\nderived from manually designed prompts. Specifically, personalized automatic\nprompts allow different users to have different prompt tokens for the same\ntask, automatically generated using a gradient-based method. One challenge for\npersonalized automatic prompt generation for recommendation language models is\nthe extremely large search space, leading to a long convergence time. To\neffectively and efficiently address the problem, we develop surrogate metrics\nand leverage an alternative updating schedule for prompting recommendation\nlanguage models. Experimental results show that our PAP-REC framework manages\nto generate personalized prompts, and the automatically generated prompts\noutperform manually constructed prompts and also outperform various baseline\nrecommendation models. The source code of the work is available at\nhttps://github.com/rutgerswiselab/PAP-REC.",
        "translated": ""
    },
    {
        "title": "Improving Sequential Recommendations with LLMs",
        "url": "http://arxiv.org/abs/2402.01339v1",
        "pub_date": "2024-02-02",
        "summary": "The sequential recommendation problem has attracted considerable research\nattention in the past few years, leading to the rise of numerous recommendation\nmodels. In this work, we explore how Large Language Models (LLMs), which are\nnowadays introducing disruptive effects in many AI-based applications, can be\nused to build or improve sequential recommendation approaches. Specifically, we\ndesign three orthogonal approaches and hybrids of those to leverage the power\nof LLMs in different ways. In addition, we investigate the potential of each\napproach by focusing on its comprising technical aspects and determining an\narray of alternative choices for each one. We conduct extensive experiments on\nthree datasets and explore a large variety of configurations, including\ndifferent language models and baseline recommendation models, to obtain a\ncomprehensive picture of the performance of each approach. Among other\nobservations, we highlight that initializing state-of-the-art sequential\nrecommendation models such as BERT4Rec or SASRec with embeddings obtained from\nan LLM can lead to substantial performance gains in terms of accuracy.\nFurthermore, we find that fine-tuning an LLM for recommendation tasks enables\nit to learn not only the tasks, but also concepts of a domain to some extent.\nWe also show that fine-tuning OpenAI GPT leads to considerably better\nperformance than fine-tuning Google PaLM 2. Overall, our extensive experiments\nindicate a huge potential value of leveraging LLMs in future recommendation\napproaches. We publicly share the code and data of our experiments to ensure\nreproducibility.",
        "translated": ""
    },
    {
        "title": "Minimizing Regret in Billboard Advertisement under Zonal Influence\n  Constraint",
        "url": "http://arxiv.org/abs/2402.01294v1",
        "pub_date": "2024-02-02",
        "summary": "In a typical billboard advertisement technique, a number of digital\nbillboards are owned by an influence provider, and many advertisers approach\nthe influence provider for a specific number of views of their advertisement\ncontent on a payment basis. If the influence provider provides the demanded or\nmore influence, then he will receive the full payment or else a partial\npayment. In the context of an influence provider, if he provides more or less\nthan an advertiser's demanded influence, it is a loss for him. This is\nformalized as 'Regret', and naturally, in the context of the influence\nprovider, the goal will be to allocate the billboard slots among the\nadvertisers such that the total regret is minimized. In this paper, we study\nthis problem as a discrete optimization problem and propose four solution\napproaches. The first one selects the billboard slots from the available ones\nin an incremental greedy manner, and we call this method the Budget Effective\nGreedy approach. In the second one, we introduce randomness with the first one,\nwhere we perform the marginal gain computation for a sample of randomly chosen\nbillboard slots. The remaining two approaches are further improvements over the\nsecond one. We analyze all the algorithms to understand their time and space\ncomplexity. We implement them with real-life trajectory and billboard datasets\nand conduct a number of experiments. It has been observed that the randomized\nbudget effective greedy approach takes reasonable computational time while\nminimizing the regret.",
        "translated": ""
    },
    {
        "title": "HimiRec: Modeling Hierarchical Multi-interest for Recommendation",
        "url": "http://arxiv.org/abs/2402.01253v1",
        "pub_date": "2024-02-02",
        "summary": "Industrial recommender systems usually consist of the retrieval stage and the\nranking stage, to handle the billion-scale of users and items. The retrieval\nstage retrieves candidate items relevant to user interests for recommendations\nand has attracted much attention. Frequently, users show hierarchical\nmulti-interests reflected in a heavy user of a certain NBA team Golden State\nWarriors in Sports, who is also a light user of almost the whole Animation.\nBoth Sports and Animation are at the same level. However, most existing methods\nimplicitly learn this hierarchical difference, making more fine-grained\ninterest information to be averaged and limiting detailed understanding of the\nuser's different needs in heavy interests and other light interests. Therefore,\nwe propose a novel two-stage approach to explicitly modeling hierarchical\nmulti-interest for recommendation in this work. In the first hierarchical\nmulti-interest mining stage, the hierarchical clustering and transformer-based\nmodel adaptively generate circles or sub-circles that users are interested in.\nIn the second stage, the partition of retrieval space allows the EBR models to\nonly deal with items within each circle and accurately capture user's refined\ninterests. Experimental results show that the proposed approach achieves\nstate-of-the-art performance. Our framework has also successfully deployed at\nLofter (one of the largest derivative content communities with 10 million\nmonthly active users) for over four months.",
        "translated": ""
    },
    {
        "title": "Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing\n  External Corpus",
        "url": "http://arxiv.org/abs/2402.01176v1",
        "pub_date": "2024-02-02",
        "summary": "The advent of large language models (LLMs) has showcased their efficacy\nacross various domains, yet they often hallucinate, especially in\nknowledge-intensive tasks that require external knowledge sources. To improve\nfactual accuracy of language models, retrieval-augmented generation (RAG) has\nemerged as a popular solution. However, traditional retrieval modules often\nrely on large-scale document indexes, which can be disconnected from generative\ntasks. Through generative retrieval (GR) approach, language models can achieve\nsuperior retrieval performance by directly generating relevant document\nidentifiers (DocIDs). However, the relationship between GR and downstream\ntasks, as well as the potential of LLMs in GR, remains unexplored. In this\npaper, we present a unified language model that utilizes external corpus to\nhandle various knowledge-intensive tasks by seamlessly integrating generative\nretrieval, closed-book generation, and RAG. In order to achieve effective\nretrieval and generation through a unified continuous decoding process, we\nintroduce the following mechanisms: (1) a ranking-oriented DocID decoding\nstrategy, which improves ranking ability by directly learning from a DocID\nranking list; (2) a continuous generation strategy to facilitate effective and\nefficient RAG; (3) well-designed auxiliary DocID understanding tasks to enhance\nthe model's comprehension of DocIDs and their relevance to downstream tasks.\nOur approach is evaluated on the widely used KILT benchmark using two variants\nof backbone models: an encoder-decoder T5 model and a decoder-only LLM, Llama2.\nExperimental results showcase the superior performance of our models in both\nretrieval and downstream knowledge-intensive tasks.",
        "translated": ""
    },
    {
        "title": "A Multi-Agent Conversational Recommender System",
        "url": "http://arxiv.org/abs/2402.01135v1",
        "pub_date": "2024-02-02",
        "summary": "Due to strong capabilities in conducting fluent, multi-turn conversations\nwith users, Large Language Models (LLMs) have the potential to further improve\nthe performance of Conversational Recommender System (CRS). Unlike the aimless\nchit-chat that LLM excels at, CRS has a clear target. So it is imperative to\ncontrol the dialogue flow in the LLM to successfully recommend appropriate\nitems to the users. Furthermore, user feedback in CRS can assist the system in\nbetter modeling user preferences, which has been ignored by existing studies.\nHowever, simply prompting LLM to conduct conversational recommendation cannot\naddress the above two key challenges.\n  In this paper, we propose Multi-Agent Conversational Recommender System\n(MACRS) which contains two essential modules. First, we design a multi-agent\nact planning framework, which can control the dialogue flow based on four\nLLM-based agents. This cooperative multi-agent framework will generate various\ncandidate responses based on different dialogue acts and then choose the most\nappropriate response as the system response, which can help MACRS plan suitable\ndialogue acts. Second, we propose a user feedback-aware reflection mechanism\nwhich leverages user feedback to reason errors made in previous turns to adjust\nthe dialogue act planning, and higher-level user information from implicit\nsemantics. We conduct extensive experiments based on user simulator to\ndemonstrate the effectiveness of MACRS in recommendation and user preferences\ncollection. Experimental results illustrate that MACRS demonstrates an\nimprovement in user interaction experience compared to directly using LLMs.",
        "translated": ""
    },
    {
        "title": "TransFR: Transferable Federated Recommendation with Pre-trained Language\n  Models",
        "url": "http://arxiv.org/abs/2402.01124v1",
        "pub_date": "2024-02-02",
        "summary": "Federated recommendations (FRs), facilitating multiple local clients to\ncollectively learn a global model without disclosing user private data, have\nemerged as a prevalent architecture for privacy-preserving recommendations. In\nconventional FRs, a dominant paradigm is to utilize discrete identities to\nrepresent users/clients and items, which are subsequently mapped to\ndomain-specific embeddings to participate in model training. Despite\nconsiderable performance, we reveal three inherent limitations that can not be\nignored in federated settings, i.e., non-transferability across domains,\nunavailability in cold-start settings, and potential privacy violations during\nfederated training. To this end, we propose a transferable federated\nrecommendation model with universal textual representations, TransFR, which\ndelicately incorporates the general capabilities empowered by pre-trained\nlanguage models and the personalized abilities by fine-tuning local private\ndata. Specifically, it first learns domain-agnostic representations of items by\nexploiting pre-trained models with public textual corpora. To tailor for\nfederated recommendation, we further introduce an efficient federated\nfine-tuning and a local training mechanism. This facilitates personalized local\nheads for each client by utilizing their private behavior data. By\nincorporating pre-training and fine-tuning within FRs, it greatly improves the\nadaptation efficiency transferring to a new domain and the generalization\ncapacity to address cold-start issues. Through extensive experiments on several\ndatasets, we demonstrate that our TransFR model surpasses several\nstate-of-the-art FRs in terms of accuracy, transferability, and privacy.",
        "translated": ""
    },
    {
        "title": "CF4J: Collaborative Filtering for Java",
        "url": "http://arxiv.org/abs/2402.01008v1",
        "pub_date": "2024-02-01",
        "summary": "Recommender Systems (RS) provide a relevant tool to mitigate the information\noverload problem. A large number of researchers have published hundreds of\npapers to improve different RS features. It is advisable to use RS frameworks\nthat simplify RS researchers: a) to design and implement recommendations\nmethods and, b) to speed up the execution time of the experiments. In this\npaper, we present CF4J, a Java library designed to carry out Collaborative\nFiltering based RS research experiments. CF4J has been designed from\nresearchers to researchers. It allows: a) RS datasets reading, b) full and easy\naccess to data and intermediate or final results, c) to extend their main\nfunctionalities, d) to concurrently execute the implemented methods, and e) to\nprovide a thorough evaluation for the implementations by quality measures. In\nsummary, CF4J serves as a library specifically designed for the research trial\nand error process.",
        "translated": ""
    },
    {
        "title": "SPARQL Generation with Entity Pre-trained GPT for KG Question Answering",
        "url": "http://arxiv.org/abs/2402.00969v1",
        "pub_date": "2024-02-01",
        "summary": "Knowledge Graphs popularity has been rapidly growing in last years. All that\nknowledge is available for people to query it through the many online databases\non the internet. Though, it would be a great achievement if non-programmer\nusers could access whatever information they want to know. There has been a lot\nof effort oriented to solve this task using natural language processing tools\nand creativity encouragement by way of many challenges. Our approach focuses on\nassuming a correct entity linking on the natural language questions and\ntraining a GPT model to create SPARQL queries from them. We managed to isolate\nwhich property of the task can be the most difficult to solve at few or\nzero-shot and we proposed pre-training on all entities (under CWA) to improve\nthe performance. We obtained a 62.703% accuracy of exact SPARQL matches on\ntesting at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of\n0.009 on the question answering challenge.",
        "translated": ""
    },
    {
        "title": "Approximate Nearest Neighbor Search with Window Filters",
        "url": "http://arxiv.org/abs/2402.00943v1",
        "pub_date": "2024-02-01",
        "summary": "We define and investigate the problem of $\\textit{c-approximate window\nsearch}$: approximate nearest neighbor search where each point in the dataset\nhas a numeric label, and the goal is to find nearest neighbors to queries\nwithin arbitrary label ranges. Many semantic search problems, such as image and\ndocument search with timestamp filters, or product search with cost filters,\nare natural examples of this problem. We propose and theoretically analyze a\nmodular tree-based framework for transforming an index that solves the\ntraditional c-approximate nearest neighbor problem into a data structure that\nsolves window search. On standard nearest neighbor benchmark datasets equipped\nwith random label values, adversarially constructed embeddings, and image\nsearch embeddings with real timestamps, we obtain up to a $75\\times$ speedup\nover existing solutions at the same level of recall.",
        "translated": ""
    },
    {
        "title": "Event-based Product Carousel Recommendation with Query-Click Graph",
        "url": "http://arxiv.org/abs/2402.03277v1",
        "pub_date": "2024-02-05",
        "summary": "Many current recommender systems mainly focus on the product-to-product\nrecommendations and user-to-product recommendations even during the time of\nevents rather than modeling the typical recommendations for the target event\n(e.g., festivals, seasonal activities, or social activities) without addressing\nthe multiple aspects of the shopping demands for the target event. Product\nrecommendations for the multiple aspects of the target event are usually\ngenerated by human curators who manually identify the aspects and select a list\nof aspect-related products (i.e., product carousel) for each aspect as\nrecommendations. However, building a recommender system with machine learning\nis non-trivial due to the lack of both the ground truth of event-related\naspects and the aspect-related products. To fill this gap, we define the novel\nproblem as the event-based product carousel recommendations in e-commerce and\npropose an effective recommender system based on the query-click bipartite\ngraph. We apply the iterative clustering algorithm over the query-click\nbipartite graph and infer the event-related aspects by the clusters of queries.\nThe aspect-related recommendations are powered by the click-through rate of\nproducts regarding each aspect. We show through experiments that this approach\neffectively mines product carousels for the target event.",
        "translated": ""
    },
    {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2402.03190v1",
        "pub_date": "2024-02-05",
        "summary": "Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.",
        "translated": ""
    },
    {
        "title": "Comparison of Topic Modelling Approaches in the Banking Context",
        "url": "http://arxiv.org/abs/2402.03176v1",
        "pub_date": "2024-02-05",
        "summary": "Topic modelling is a prominent task for automatic topic extraction in many\napplications such as sentiment analysis and recommendation systems. The\napproach is vital for service industries to monitor their customer discussions.\nThe use of traditional approaches such as Latent Dirichlet Allocation (LDA) for\ntopic discovery has shown great performances, however, they are not consistent\nin their results as these approaches suffer from data sparseness and inability\nto model the word order in a document. Thus, this study presents the use of\nKernel Principal Component Analysis (KernelPCA) and K-means Clustering in the\nBERTopic architecture. We have prepared a new dataset using tweets from\ncustomers of Nigerian banks and we use this to compare the topic modelling\napproaches. Our findings showed KernelPCA and K-means in the BERTopic\narchitecture-produced coherent topics with a coherence score of 0.8463.",
        "translated": ""
    },
    {
        "title": "Linguistic features for sentence difficulty prediction in ABSA",
        "url": "http://arxiv.org/abs/2402.03163v1",
        "pub_date": "2024-02-05",
        "summary": "One of the challenges of natural language understanding is to deal with the\nsubjectivity of sentences, which may express opinions and emotions that add\nlayers of complexity and nuance. Sentiment analysis is a field that aims to\nextract and analyze these subjective elements from text, and it can be applied\nat different levels of granularity, such as document, paragraph, sentence, or\naspect. Aspect-based sentiment analysis is a well-studied topic with many\navailable data sets and models. However, there is no clear definition of what\nmakes a sentence difficult for aspect-based sentiment analysis. In this paper,\nwe explore this question by conducting an experiment with three data sets:\n\"Laptops\", \"Restaurants\", and \"MTSC\" (Multi-Target-dependent Sentiment\nClassification), and a merged version of these three datasets. We study the\nimpact of domain diversity and syntactic diversity on difficulty. We use a\ncombination of classifiers to identify the most difficult sentences and analyze\ntheir characteristics. We employ two ways of defining sentence difficulty. The\nfirst one is binary and labels a sentence as difficult if the classifiers fail\nto correctly predict the sentiment polarity. The second one is a six-level\nscale based on how many of the top five best-performing classifiers can\ncorrectly predict the sentiment polarity. We also define 9 linguistic features\nthat, combined, aim at estimating the difficulty at sentence level.",
        "translated": ""
    },
    {
        "title": "EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models",
        "url": "http://arxiv.org/abs/2402.03049v1",
        "pub_date": "2024-02-05",
        "summary": "In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with a running demo App at\nhttps://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for\nbroader research centered on instruction data.",
        "translated": ""
    },
    {
        "title": "Understanding and Guiding Weakly Supervised Entity Alignment with\n  Potential Isomorphism Propagation",
        "url": "http://arxiv.org/abs/2402.03025v1",
        "pub_date": "2024-02-05",
        "summary": "Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent\nentities across diverse knowledge graphs (KGs) using only a limited number of\nseed alignments. Despite substantial advances in aggregation-based weakly\nsupervised EA, the underlying mechanisms in this setting remain unexplored. In\nthis paper, we present a propagation perspective to analyze weakly supervised\nEA and explain the existing aggregation-based EA models. Our theoretical\nanalysis reveals that these models essentially seek propagation operators for\npairwise entity similarities. We further prove that, despite the structural\nheterogeneity of different KGs, the potentially aligned entities within\naggregation-based EA models have isomorphic subgraphs, which is the core\npremise of EA but has not been investigated. Leveraging this insight, we\nintroduce a potential isomorphism propagation operator to enhance the\npropagation of neighborhood information across KGs. We develop a general EA\nframework, PipEA, incorporating this operator to improve the accuracy of every\ntype of aggregation-based model without altering the learning process.\nExtensive experiments substantiate our theoretical findings and demonstrate\nPipEA's significant performance gains over state-of-the-art weakly supervised\nEA methods. Our work not only advances the field but also enhances our\ncomprehension of aggregation-based weakly supervised EA.",
        "translated": ""
    },
    {
        "title": "Domain Adaptation of Multilingual Semantic Search - Literature Review",
        "url": "http://arxiv.org/abs/2402.02932v1",
        "pub_date": "2024-02-05",
        "summary": "This literature review gives an overview of current approaches to perform\ndomain adaptation in a low-resource and approaches to perform multilingual\nsemantic search in a low-resource setting. We developed a new typology to\ncluster domain adaptation approaches based on the part of dense textual\ninformation retrieval systems, which they adapt, focusing on how to combine\nthem efficiently. We also explore the possibilities of combining multilingual\nsemantic search with domain adaptation approaches for dense retrievers in a\nlow-resource setting.",
        "translated": ""
    },
    {
        "title": "Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation",
        "url": "http://arxiv.org/abs/2402.02855v1",
        "pub_date": "2024-02-05",
        "summary": "In the realm of deep learning-based recommendation systems, the increasing\ncomputational demands, driven by the growing number of users and items, pose a\nsignificant challenge to practical deployment. This challenge is primarily\ntwofold: reducing the model size while effectively learning user and item\nrepresentations for efficient recommendations. Despite considerable\nadvancements in model compression and architecture search, prevalent approaches\nface notable constraints. These include substantial additional computational\ncosts from pre-training/re-training in model compression and an extensive\nsearch space in architecture design. Additionally, managing complexity and\nadhering to memory constraints is problematic, especially in scenarios with\nstrict time or space limitations. Addressing these issues, this paper\nintroduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored\nfor recommendation models. DSL innovatively trains a lightweight sparse model\nfrom scratch, periodically evaluating and dynamically adjusting each weight's\nsignificance and the model's sparsity distribution during the training. This\napproach ensures a consistent and minimal parameter budget throughout the full\nlearning lifecycle, paving the way for \"end-to-end\" efficiency from training to\ninference. Our extensive experimental results underline DSL's effectiveness,\nsignificantly reducing training and inference costs while delivering comparable\nrecommendation performance.",
        "translated": ""
    },
    {
        "title": "Comparing Knowledge Sources for Open-Domain Scientific Claim\n  Verification",
        "url": "http://arxiv.org/abs/2402.02844v1",
        "pub_date": "2024-02-05",
        "summary": "The increasing rate at which scientific knowledge is discovered and health\nclaims shared online has highlighted the importance of developing efficient\nfact-checking systems for scientific claims. The usual setting for this task in\nthe literature assumes that the documents containing the evidence for claims\nare already provided and annotated or contained in a limited corpus. This\nrenders the systems unrealistic for real-world settings where knowledge sources\nwith potentially millions of documents need to be queried to find relevant\nevidence. In this paper, we perform an array of experiments to test the\nperformance of open-domain claim verification systems. We test the final\nverdict prediction of systems on four datasets of biomedical and health claims\nin different settings. While keeping the pipeline's evidence selection and\nverdict prediction parts constant, document retrieval is performed over three\ncommon knowledge sources (PubMed, Wikipedia, Google) and using two different\ninformation retrieval techniques. We show that PubMed works better with\nspecialized biomedical claims, while Wikipedia is more suited for everyday\nhealth concerns. Likewise, BM25 excels in retrieval precision, while semantic\nsearch in recall of relevant evidence. We discuss the results, outline frequent\nretrieval patterns and challenges, and provide promising future directions.",
        "translated": ""
    },
    {
        "title": "Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One",
        "url": "http://arxiv.org/abs/2402.02842v1",
        "pub_date": "2024-02-05",
        "summary": "Interest modeling in recommender system has been a constant topic for\nimproving user experience, and typical interest modeling tasks (e.g.\nmulti-interest, long-tail interest and long-term interest) have been\ninvestigated in many existing works. However, most of them only consider one\ninterest in isolation, while neglecting their interrelationships. In this\npaper, we argue that these tasks suffer from a common \"interest amnesia\"\nproblem, and a solution exists to mitigate it simultaneously. We figure that\nlong-term cues can be the cornerstone since they reveal multi-interest and\nclarify long-tail interest. Inspired by the observation, we propose a novel and\nunified framework in the retrieval stage, \"Trinity\", to solve interest amnesia\nproblem and improve multiple interest modeling tasks. We construct a real-time\nclustering system that enables us to project items into enumerable clusters,\nand calculate statistical interest histograms over these clusters. Based on\nthese histograms, Trinity recognizes underdelivered themes and remains stable\nwhen facing emerging hot topics. Trinity is more appropriate for large-scale\nindustry scenarios because of its modest computational overheads. Its derived\nretrievers have been deployed on the recommender system of Douyin,\nsignificantly improving user experience and retention. We believe that such\npractical experience can be well generalized to other scenarios.",
        "translated": ""
    },
    {
        "title": "Can Large Language Models Detect Rumors on Social Media?",
        "url": "http://arxiv.org/abs/2402.03916v1",
        "pub_date": "2024-02-06",
        "summary": "In this work, we investigate to use Large Language Models (LLMs) for rumor\ndetection on social media. However, it is challenging for LLMs to reason over\nthe entire propagation information on social media, which contains news\ncontents and numerous comments, due to LLMs may not concentrate on key clues in\nthe complex propagation information, and have trouble in reasoning when facing\nmassive and redundant information. Accordingly, we propose an LLM-empowered\nRumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to\nreason over important clues in news and comments, and divide the entire\npropagation information into a Chain-of-Propagation for reducing LLMs' burden.\nWe conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD\noutperforms several state-of-the-art rumor detection models by 2.4% to 7.6%.\nMeanwhile, by applying LLMs, LeRuD requires no data for training, and thus\nshows more promising rumor detection ability in few-shot or zero-shot\nscenarios.",
        "translated": ""
    },
    {
        "title": "Learning Metrics that Maximise Power for Accelerated A/B-Tests",
        "url": "http://arxiv.org/abs/2402.03915v1",
        "pub_date": "2024-02-06",
        "summary": "Online controlled experiments are a crucial tool to allow for confident\ndecision-making in technology companies. A North Star metric is defined (such\nas long-term revenue or user retention), and system variants that statistically\nsignificantly improve on this metric in an A/B-test can be considered superior.\nNorth Star metrics are typically delayed and insensitive. As a result, the cost\nof experimentation is high: experiments need to run for a long time, and even\nthen, type-II errors (i.e. false negatives) are prevalent.\n  We propose to tackle this by learning metrics from short-term signals that\ndirectly maximise the statistical power they harness with respect to the North\nStar. We show that existing approaches are prone to overfitting, in that higher\naverage metric sensitivity does not imply improved type-II errors, and propose\nto instead minimise the $p$-values a metric would have produced on a log of\npast experiments. We collect such datasets from two social media applications\nwith over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.\nEmpirical results show that we are able to increase statistical power by up to\n78% when using our learnt metrics stand-alone, and by up to 210% when used in\ntandem with the North Star. Alternatively, we can obtain constant statistical\npower at a sample size that is down to 12% of what the North Star requires,\nsignificantly reducing the cost of experimentation.",
        "translated": ""
    },
    {
        "title": "On Practical Diversified Recommendation with Controllable Category\n  Diversity Framework",
        "url": "http://arxiv.org/abs/2402.03801v1",
        "pub_date": "2024-02-06",
        "summary": "Recommender systems have made significant strides in various industries,\nprimarily driven by extensive efforts to enhance recommendation accuracy.\nHowever, this pursuit of accuracy has inadvertently given rise to echo\nchamber/filter bubble effects. Especially in industry, it could impair user's\nexperiences and prevent user from accessing a wider range of items. One of the\nsolutions is to take diversity into account. However, most of existing works\nfocus on user's explicit preferences, while rarely exploring user's\nnon-interaction preferences. These neglected non-interaction preferences are\nespecially important for broadening user's interests in alleviating echo\nchamber/filter bubble effects.Therefore, in this paper, we first define\ndiversity as two distinct definitions, i.e., user-explicit diversity\n(U-diversity) and user-item non-interaction diversity (N-diversity) based on\nuser historical behaviors. Then, we propose a succinct and effective method,\nnamed as Controllable Category Diversity Framework (CCDF) to achieve both high\nU-diversity and N-diversity simultaneously.Specifically, CCDF consists of two\nstages, User-Category Matching and Constrained Item Matching. The User-Category\nMatching utilizes the DeepU2C model and a combined loss to capture user's\npreferences in categories, and then selects the top-$K$ categories with a\ncontrollable parameter $K$.These top-$K$ categories will be used as trigger\ninformation in Constrained Item Matching. Offline experimental results show\nthat our proposed DeepU2C outperforms state-of-the-art diversity-oriented\nmethods, especially on N-diversity task. The whole framework is validated in a\nreal-world production environment by conducting online A/B testing.",
        "translated": ""
    },
    {
        "title": "Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&amp;A Sites",
        "url": "http://arxiv.org/abs/2402.03635v1",
        "pub_date": "2024-02-06",
        "summary": "Posts in software Q\\&amp;A sites often consist of three main parts: title,\ndescription and code, which are interconnected and jointly describe the\nquestion. Existing tag recommendation methods often treat different modalities\nas a whole or inadequately consider the interaction between different\nmodalities. Additionally, they focus on extracting information directly from\nthe post itself, neglecting the information from external knowledge sources.\nTherefore, we propose a Retrieval Augmented Cross-Modal (RACM) Tag\nRecommendation Model in Software Q\\&amp;A Sites. Specifically, we first use the\ninput post as a query and enhance the representation of different modalities by\nretrieving information from external knowledge sources. For the\nretrieval-augmented representations, we employ a cross-modal context-aware\nattention to leverage the main modality description for targeted feature\nextraction across the submodalities title and code. In the fusion process, a\ngate mechanism is employed to achieve fine-grained feature selection,\ncontrolling the amount of information extracted from the submodalities.\nFinally, the fused information is used for tag recommendation. Experimental\nresults on three real-world datasets demonstrate that our model outperforms the\nstate-of-the-art counterparts.",
        "translated": ""
    },
    {
        "title": "Leveraging Large Language Models for Hybrid Workplace Decision Support",
        "url": "http://arxiv.org/abs/2402.03616v1",
        "pub_date": "2024-02-06",
        "summary": "Large Language Models (LLMs) hold the potential to perform a variety of text\nprocessing tasks and provide textual explanations for proposed actions or\ndecisions. In the era of hybrid work, LLMs can provide intelligent decision\nsupport for workers who are designing their hybrid work plans. In particular,\nthey can offer suggestions and explanations to workers balancing numerous\ndecision factors, thereby enhancing their work experience. In this paper, we\npresent a decision support model for workspaces in hybrid work environments,\nleveraging the reasoning skill of LLMs. We first examine LLM's capability of\nmaking suitable workspace suggestions. We find that its reasoning extends\nbeyond the guidelines in the prompt and the LLM can manage the trade-off among\nthe available resources in the workspaces. We conduct an extensive user study\nto understand workers' decision process for workspace choices and evaluate the\neffectiveness of the system. We observe that a worker's decision could be\ninfluenced by the LLM's suggestions and explanations. The participants in our\nstudy find the system to be convenient, regardless of whether reasons are\nprovided or not. Our results show that employees can benefit from the\nLLM-empowered system for their workspace selection in hybrid workplace.",
        "translated": ""
    },
    {
        "title": "Understanding and Counteracting Feature-Level Bias in Click-Through Rate\n  Prediction",
        "url": "http://arxiv.org/abs/2402.03600v1",
        "pub_date": "2024-02-06",
        "summary": "Common click-through rate (CTR) prediction recommender models tend to exhibit\nfeature-level bias, which leads to unfair recommendations among item groups and\ninaccurate recommendations for users. While existing methods address this issue\nby adjusting the learning of CTR models, such as through additional\noptimization objectives, they fail to consider how the bias is caused within\nthese models. To address this research gap, our study performs a top-down\nanalysis on representative CTR models. Through blocking different components of\na trained CTR model one by one, we identify the key contribution of the linear\ncomponent to feature-level bias. We conduct a theoretical analysis of the\nlearning process for the weights in the linear component, revealing how\ngroup-wise properties of training data influence them. Our experimental and\nstatistical analyses demonstrate a strong correlation between imbalanced\npositive sample ratios across item groups and feature-level bias. Based on this\nunderstanding, we propose a minimally invasive yet effective strategy to\ncounteract feature-level bias in CTR models by removing the biased linear\nweights from trained models. Additionally, we present a linear weight adjusting\nstrategy that requires fewer random exposure records than relevant debiasing\nmethods. The superiority of our proposed strategies are validated through\nextensive experiments on three real-world datasets.",
        "translated": ""
    },
    {
        "title": "Identifying Reasons for Contraceptive Switching from Real-World Data\n  Using Large Language Models",
        "url": "http://arxiv.org/abs/2402.03597v1",
        "pub_date": "2024-02-06",
        "summary": "Prescription contraceptives play a critical role in supporting women's\nreproductive health. With nearly 50 million women in the United States using\ncontraceptives, understanding the factors that drive contraceptives selection\nand switching is of significant interest. However, many factors related to\nmedication switching are often only captured in unstructured clinical notes and\ncan be difficult to extract. Here, we evaluate the zero-shot abilities of a\nrecently developed large language model, GPT-4 (via HIPAA-compliant Microsoft\nAzure API), to identify reasons for switching between classes of contraceptives\nfrom the UCSF Information Commons clinical notes dataset. We demonstrate that\nGPT-4 can accurately extract reasons for contraceptive switching, outperforming\nbaseline BERT-based models with microF1 scores of 0.849 and 0.881 for\ncontraceptive start and stop extraction, respectively. Human evaluation of\nGPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal\nhallucinations. Using extracted reasons, we identified patient preference,\nadverse events, and insurance as key reasons for switching using unsupervised\ntopic modeling approaches. Notably, we also showed using our approach that\n\"weight gain/mood change\" and \"insurance coverage\" are disproportionately found\nas reasons for contraceptive switching in specific demographic populations. Our\ncode and supplemental data are available at\nhttps://github.com/BMiao10/contraceptive-switching.",
        "translated": ""
    },
    {
        "title": "Early prediction of onset of sepsis in Clinical Setting",
        "url": "http://arxiv.org/abs/2402.03486v1",
        "pub_date": "2024-02-05",
        "summary": "This study proposes the use of Machine Learning models to predict the early\nonset of sepsis using deidentified clinical data from Montefiore Medical Center\nin Bronx, NY, USA. A supervised learning approach was adopted, wherein an\nXGBoost model was trained utilizing 80\\% of the train dataset, encompassing 107\nfeatures (including the original and derived features). Subsequently, the model\nwas evaluated on the remaining 20\\% of the test data. The model was validated\non prospective data that was entirely unseen during the training phase. To\nassess the model's performance at the individual patient level and timeliness\nof the prediction, a normalized utility score was employed, a widely recognized\nscoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis\nChallenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag\nRate were also devised. The model achieved a normalized utility score of 0.494\non test data and 0.378 on prospective data at threshold 0.3. The F1 scores were\n80.8\\% and 67.1\\% respectively for the test data and the prospective data for\nthe same threshold, highlighting its potential to be integrated into clinical\ndecision-making processes effectively. These results bear testament to the\nmodel's robust predictive capabilities and its potential to substantially\nimpact clinical decision-making processes.",
        "translated": ""
    },
    {
        "title": "Harnessing PubMed User Query Logs for Post Hoc Explanations of\n  Recommended Similar Articles",
        "url": "http://arxiv.org/abs/2402.03484v1",
        "pub_date": "2024-02-05",
        "summary": "Searching for a related article based on a reference article is an integral\npart of scientific research. PubMed, like many academic search engines, has a\n\"similar articles\" feature that recommends articles relevant to the current\narticle viewed by a user. Explaining recommended items can be of great utility\nto users, particularly in the literature search process. With more than a\nmillion biomedical papers being published each year, explaining the recommended\nsimilar articles would facilitate researchers and clinicians in searching for\nrelated articles. Nonetheless, the majority of current literature\nrecommendation systems lack explanations for their suggestions. We employ a\npost hoc approach to explaining recommendations by identifying relevant tokens\nin the titles of similar articles. Our major contribution is building PubCLogs\nby repurposing 5.6 million pairs of coclicked articles from PubMed's user query\nlogs. Using our PubCLogs dataset, we train the Highlight Similar Article Title\n(HSAT), a transformer-based model designed to select the most relevant parts of\nthe title of a similar article, based on the title and abstract of a seed\narticle. HSAT demonstrates strong performance in our empirical evaluations,\nachieving an F1 score of 91.72 percent on the PubCLogs test set, considerably\noutperforming several baselines including BM25 (70.62), MPNet (67.11), MedCPT\n(62.22), GPT-3.5 (46.00), and GPT-4 (64.89). Additional evaluations on a\nseparate, manually annotated test set further verifies HSAT's performance.\nMoreover, participants of our user study indicate a preference for HSAT, due to\nits superior balance between conciseness and comprehensiveness. Our study\nsuggests that repurposing user query logs of academic search engines can be a\npromising way to train state-of-the-art models for explaining literature\nrecommendation.",
        "translated": ""
    },
    {
        "title": "FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning",
        "url": "http://arxiv.org/abs/2402.03481v1",
        "pub_date": "2024-02-05",
        "summary": "Modern recommender systems may output considerably different recommendations\ndue to small perturbations in the training data. Changes in the data from a\nsingle user will alter the recommendations as well as the recommendations of\nother users. In applications like healthcare, housing, and finance, this\nsensitivity can have adverse effects on user experience. We propose a method to\nstabilize a given recommender system against such perturbations. This is a\nchallenging task due to (1) the lack of a ``reference'' rank list that can be\nused to anchor the outputs; and (2) the computational challenges in ensuring\nthe stability of rank lists with respect to all possible perturbations of\ntraining data. Our method, FINEST, overcomes these challenges by obtaining\nreference rank lists from a given recommendation model and then fine-tuning the\nmodel under simulated perturbation scenarios with rank-preserving\nregularization on sampled items. Our experiments on real-world datasets\ndemonstrate that FINEST can ensure that recommender models output stable\nrecommendations under a wide range of different perturbations without\ncompromising next-item prediction accuracy.",
        "translated": ""
    },
    {
        "title": "A Roadmap to Pluralistic Alignment",
        "url": "http://arxiv.org/abs/2402.05070v1",
        "pub_date": "2024-02-07",
        "summary": "With increased power and prevalence of AI systems, it is ever more critical\nthat AI systems are designed to serve all, i.e., people with diverse values and\nperspectives. However, aligning models to serve pluralistic human values\nremains an open research question. In this piece, we propose a roadmap to\npluralistic alignment, specifically using language models as a test bed. We\nidentify and formalize three possible ways to define and operationalize\npluralism in AI systems: 1) Overton pluralistic models that present a spectrum\nof reasonable responses; 2) Steerably pluralistic models that can steer to\nreflect certain perspectives; and 3) Distributionally pluralistic models that\nare well-calibrated to a given population in distribution. We also propose and\nformalize three possible classes of pluralistic benchmarks: 1) Multi-objective\nbenchmarks, 2) Trade-off steerable benchmarks, which incentivize models to\nsteer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which\nexplicitly model diverse human ratings. We use this framework to argue that\ncurrent alignment techniques may be fundamentally limited for pluralistic AI;\nindeed, we highlight empirical evidence, both from our own experiments and from\nother work, that standard alignment procedures might reduce distributional\npluralism in models, motivating the need for further research on pluralistic\nalignment.",
        "translated": ""
    },
    {
        "title": "Detecting Generated Native Ads in Conversational Search",
        "url": "http://arxiv.org/abs/2402.04889v1",
        "pub_date": "2024-02-07",
        "summary": "Conversational search engines such as YouChat and Microsoft Copilot use large\nlanguage models (LLMs) to generate answers to queries. It is only a small step\nto also use this technology to generate and integrate advertising within these\nanswers - instead of placing ads separately from the organic search results.\nThis type of advertising is reminiscent of native advertising and product\nplacement, both of which are very effective forms of subtle and manipulative\nadvertising. It is likely that information seekers will be confronted with such\nuse of LLM technology in the near future, especially when considering the high\ncomputational costs associated with LLMs, for which providers need to develop\nsustainable business models. This paper investigates whether LLMs can also be\nused as a countermeasure against generated native ads, i.e., to block them. For\nthis purpose we compile a large dataset of ad-prone queries and of generated\nanswers with automatically integrated ads to experiment with fine-tuned\nsentence transformers and state-of-the-art LLMs on the task of recognizing the\nads. In our experiments sentence transformers achieve detection precision and\nrecall values above 0.9, while the investigated LLMs struggle with the task.",
        "translated": ""
    },
    {
        "title": "Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from\n  Human Feedback",
        "url": "http://arxiv.org/abs/2402.04867v1",
        "pub_date": "2024-02-07",
        "summary": "In the rapidly evolving landscape of information retrieval, search engines\nstrive to provide more personalized and relevant results to users. Query\nsuggestion systems play a crucial role in achieving this goal by assisting\nusers in formulating effective queries. However, existing query suggestion\nsystems mainly rely on textual inputs, potentially limiting user search\nexperiences for querying images. In this paper, we introduce a novel Multimodal\nQuery Suggestion (MMQS) task, which aims to generate query suggestions based on\nuser query images to improve the intentionality and diversity of search\nresults. We present the RL4Sugg framework, leveraging the power of Large\nLanguage Models (LLMs) with Multi-Agent Reinforcement Learning from Human\nFeedback to optimize the generation process. Through comprehensive experiments,\nwe validate the effectiveness of RL4Sugg, demonstrating a 18% improvement\ncompared to the best existing approach. Moreover, the MMQS has been transferred\ninto real-world search engine products, which yield enhanced user engagement.\nOur research advances query suggestion systems and provides a new perspective\non multimodal information retrieval.",
        "translated": ""
    },
    {
        "title": "Leveraging LLMs for Unsupervised Dense Retriever Ranking",
        "url": "http://arxiv.org/abs/2402.04853v1",
        "pub_date": "2024-02-07",
        "summary": "This paper introduces a novel unsupervised technique that utilizes large\nlanguage models (LLMs) to determine the most suitable dense retriever for a\nspecific test(target) corpus. Selecting the appropriate dense retriever is\nvital for numerous IR applications that employ these retrievers, trained on\npublic datasets, to encode or conduct searches within a new private target\ncorpus. The effectiveness of a dense retriever can significantly diminish when\napplied to a target corpus that diverges in domain or task from the original\ntraining set. The problem becomes more pronounced in cases where the target\ncorpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation\nof the model's effectiveness on the target corpus unattainable. Therefore, the\nunsupervised selection of an optimally pre-trained dense retriever, especially\nunder conditions of domain shift, emerges as a critical challenge. Existing\nmethodologies for ranking dense retrievers fall short in addressing these\ndomain shift scenarios.\n  To tackle this, our method capitalizes on LLMs to create pseudo-relevant\nqueries, labels, and reference lists by analyzing a subset of documents from\nthe target corpus. This allows for the ranking of dense retrievers based on\ntheir performance with these pseudo-relevant signals. Significantly, this\nstrategy is the first to depend exclusively on the target corpus data, removing\nthe necessity for training data and test labels. We assessed the effectiveness\nof our approach by compiling a comprehensive pool of cutting-edge dense\nretrievers and comparing our method against traditional dense retriever\nselection benchmarks. The findings reveal that our proposed solution surpasses\nthe existing benchmarks in both the selection and ranking of dense retrievers.",
        "translated": ""
    },
    {
        "title": "Theoretical and Empirical Analysis of Adaptive Entry Point Selection for\n  Graph-based Approximate Nearest Neighbor Search",
        "url": "http://arxiv.org/abs/2402.04713v1",
        "pub_date": "2024-02-07",
        "summary": "We present a theoretical and empirical analysis of the adaptive entry point\nselection for graph-based approximate nearest neighbor search (ANNS). We\nintroduce novel concepts: $b\\textit{-monotonic path}$ and $B\\textit{-MSNET}$,\nwhich better capture an actual graph in practical algorithms than existing\nconcepts like MSNET. We prove that adaptive entry point selection offers better\nperformance upper bound than the fixed central entry point under more general\nconditions than previous work. Empirically, we validate the method's\neffectiveness in accuracy, speed, and memory usage across various datasets,\nespecially in challenging scenarios with out-of-distribution data and hard\ninstances. Our comprehensive study provides deeper insights into optimizing\nentry points for graph-based ANNS for real-world high-dimensional data\napplications.",
        "translated": ""
    },
    {
        "title": "SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question\n  Answering over a Life Science Knowledge Graph",
        "url": "http://arxiv.org/abs/2402.04627v1",
        "pub_date": "2024-02-07",
        "summary": "The recent success of Large Language Models (LLM) in a wide range of Natural\nLanguage Processing applications opens the path towards novel Question\nAnswering Systems over Knowledge Graphs leveraging LLMs. However, one of the\nmain obstacles preventing their implementation is the scarcity of training data\nfor the task of translating questions into corresponding SPARQL queries,\nparticularly in the case of domain-specific KGs. To overcome this challenge, in\nthis study, we evaluate several strategies for fine-tuning the OpenLlama LLM\nfor question answering over life science knowledge graphs. In particular, we\npropose an end-to-end data augmentation approach for extending a set of\nexisting queries over a given knowledge graph towards a larger dataset of\nsemantically enriched question-to-SPARQL query pairs, enabling fine-tuning even\nfor datasets where these pairs are scarce. In this context, we also investigate\nthe role of semantic \"clues\" in the queries, such as meaningful variable names\nand inline comments. Finally, we evaluate our approach over the real-world Bgee\ngene expression knowledge graph and we show that semantic clues can improve\nmodel performance by up to 33% compared to a baseline with random variable\nnames and no comments included.",
        "translated": ""
    },
    {
        "title": "NORMY: Non-Uniform History Modeling for Open Retrieval Conversational\n  Question Answering",
        "url": "http://arxiv.org/abs/2402.04548v1",
        "pub_date": "2024-02-07",
        "summary": "Open Retrieval Conversational Question Answering (OrConvQA) answers a\nquestion given a conversation as context and a document collection. A typical\nOrConvQA pipeline consists of three modules: a Retriever to retrieve relevant\ndocuments from the collection, a Reranker to rerank them given the question and\nthe context, and a Reader to extract an answer span. The conversational turns\ncan provide valuable context to answer the final query. State-of-the-art\nOrConvQA systems use the same history modeling for all three modules of the\npipeline. We hypothesize this as suboptimal. Specifically, we argue that a\nbroader context is needed in the first modules of the pipeline to not miss\nrelevant documents, while a narrower context is needed in the last modules to\nidentify the exact answer span. We propose NORMY, the first unsupervised\nnon-uniform history modeling pipeline which generates the best conversational\nhistory for each module. We further propose a novel Retriever for NORMY, which\nemploys keyphrase extraction on the conversation history, and leverages\npassages retrieved in previous turns as additional context. We also created a\nnew dataset for OrConvQA, by expanding the doc2dial dataset. We implemented\nvarious state-of-the-art history modeling techniques and comprehensively\nevaluated them separately for each module of the pipeline on three datasets:\nOR-QUAC, our doc2dial extension, and ConvMix. Our extensive experiments show\nthat NORMY outperforms the state-of-the-art in the individual modules and in\nthe end-to-end system.",
        "translated": ""
    },
    {
        "title": "RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based\n  Recommendation",
        "url": "http://arxiv.org/abs/2402.04527v1",
        "pub_date": "2024-02-07",
        "summary": "Large language models (LLM) have recently emerged as a powerful tool for a\nvariety of natural language processing tasks, bringing a new surge of combining\nLLM with recommendation systems, termed as LLM-based RS. Current approaches\ngenerally fall into two main paradigms, the ID direct usage paradigm and the ID\ntranslation paradigm, noting their core weakness stems from lacking\nrecommendation knowledge and uniqueness. To address this limitation, we propose\na new paradigm, ID representation, which incorporates pre-trained ID embeddings\ninto LLMs in a complementary manner. In this work, we present RA-Rec, an\nefficient ID representation alignment framework for LLM-based recommendation,\nwhich is compatible with multiple ID-based methods and LLM architectures.\nSpecifically, we treat ID embeddings as soft prompts and design an innovative\nalignment module and an efficient tuning method with tailored data construction\nfor alignment. Extensive experiments demonstrate RA-Rec substantially\noutperforms current state-of-the-art methods, achieving up to 3.0% absolute\nHitRate@100 improvements while utilizing less than 10x training data.",
        "translated": ""
    },
    {
        "title": "Reliability quality measures for recommender systems",
        "url": "http://arxiv.org/abs/2402.04457v1",
        "pub_date": "2024-02-06",
        "summary": "Users want to know the reliability of the recommendations; they do not accept\nhigh predictions if there is no reliability evidence. Recommender systems\nshould provide reliability values associated with the predictions. Research\ninto reliability measures requires the existence of simple, plausible and\nuniversal reliability quality measures. Research into recommender system\nquality measures has focused on accuracy. Moreover, novelty, serendipity and\ndiversity have been studied; nevertheless there is an important lack of\nresearch into reliability/confidence quality measures.\n  This paper proposes a reliability quality prediction measure (RPI) and a\nreliability quality recommendation measure (RRI). Both quality measures are\nbased on the hypothesis that the more suitable a reliability measure is, the\nbetter accuracy results it will provide when applied. These reliability quality\nmeasures show accuracy improvements when appropriated reliability values are\nassociated with their predictions (i.e. high reliability values associated with\ncorrect predictions or low reliability values associated with incorrect\npredictions).\n  The proposed reliability quality metrics will lead to the design of brand new\nrecommender system reliability measures. These measures could be applied to\ndifferent matrix factorization techniques and to content-based, context-aware\nand social recommendation approaches. The recommender system reliability\nmeasures designed could be tested, compared and improved using the proposed\nreliability quality metrics.",
        "translated": ""
    },
    {
        "title": "The Potential of AutoML for Recommender Systems",
        "url": "http://arxiv.org/abs/2402.04453v1",
        "pub_date": "2024-02-06",
        "summary": "Automated Machine Learning (AutoML) has greatly advanced applications of\nMachine Learning (ML) including model compression, machine translation, and\ncomputer vision. Recommender Systems (RecSys) can be seen as an application of\nML. Yet, AutoML has found little attention in the RecSys community; nor has\nRecSys found notable attention in the AutoML community. Only few and relatively\nsimple Automated Recommender Systems (AutoRecSys) libraries exist that adopt\nAutoML techniques. However, these libraries are based on student projects and\ndo not offer the features and thorough development of AutoML libraries. We set\nout to determine how AutoML libraries perform in the scenario of an\ninexperienced user who wants to implement a recommender system. We compared the\npredictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from\n15 libraries, including a mean predictor baseline, on 14 explicit feedback\nRecSys datasets. To simulate the perspective of an inexperienced user, the\nalgorithms were evaluated with default hyperparameters. We found that AutoML\nand AutoRecSys libraries performed best. AutoML libraries performed best for\nsix of the 14 datasets (43%), but it was not always the same AutoML library\nperforming best. The single-best library was the AutoRecSys library\nAuto-Surprise, which performed best on five datasets (36%). On three datasets\n(21%), AutoML libraries performed poorly, and RecSys libraries with default\nparameters performed best. Although, while obtaining 50% of all placements in\nthe top five per dataset, RecSys algorithms fall behind AutoML on average. ML\nalgorithms generally performed the worst.",
        "translated": ""
    },
    {
        "title": "PromptCrypt: Prompt Encryption for Secure Communication with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2402.05868v1",
        "pub_date": "2024-02-08",
        "summary": "Cloud-based large language models (LLMs) such as ChatGPT have increasingly\nbecome integral to daily operations, serving as vital tools across various\napplications. While these models offer substantial benefits in terms of\naccessibility and functionality, they also introduce significant privacy\nconcerns: the transmission and storage of user data in cloud infrastructures\npose substantial risks of data breaches and unauthorized access to sensitive\ninformation; even if the transmission and storage of data is encrypted, the LLM\nservice provider itself still knows the real contents of the data, preventing\nindividuals or entities from confidently using such LLM services. To address\nthese concerns, this paper proposes a simple yet effective mechanism\nPromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs\nbefore sending them to LLM, effectively rendering them indecipherable to human\nor LLM's examination while retaining the original intent of the prompt, thus\nensuring the model's performance remains unaffected. We conduct experiments on\nthree tasks, personalized recommendation, sentiment analysis, and tabular data\nanalysis. Experiment results reveal that PromptCrypt can encrypt personal\ninformation within prompts in such a manner that not only prevents the\ndiscernment of sensitive data by humans or LLM itself, but also maintains or\neven improves the precision without further tuning, achieving comparable or\neven better task accuracy than directly prompting the LLM without prompt\nencryption. These results highlight the practicality of adopting encryption\nmeasures that safeguard user privacy without compromising the functional\nintegrity and performance of LLMs. Code and dataset are available at\nhttps://github.com/agiresearch/PromptCrypt.",
        "translated": ""
    },
    {
        "title": "Natural Language User Profiles for Transparent and Scrutable\n  Recommendations",
        "url": "http://arxiv.org/abs/2402.05810v1",
        "pub_date": "2024-02-08",
        "summary": "Current state-of-the-art recommender systems predominantly rely on either\nimplicit or explicit feedback from users to suggest new items. While effective\nin recommending novel options, these conventional systems often use\nuninterpretable embeddings. This lack of transparency not only limits user\nunderstanding of why certain items are suggested but also reduces the user's\nability to easily scrutinize and edit their preferences. For example, if a user\nhas a change in interests, they would need to make significant changes to their\ninteraction history to adjust the model's recommendations. To address these\nlimitations, we introduce a novel method that utilizes user reviews to craft\npersonalized, natural language profiles describing users' preferences. Through\nthese descriptive profiles, our system provides transparent recommendations in\nnatural language. Our evaluations show that this novel approach maintains a\nperformance level on par with established recommender systems, but with the\nadded benefits of transparency and user control. By enabling users to\nscrutinize why certain items are recommended, they can more easily verify,\nadjust, and have greater autonomy over their recommendations.",
        "translated": ""
    },
    {
        "title": "CounterCLR: Counterfactual Contrastive Learning with Non-random Missing\n  Data in Recommendation",
        "url": "http://arxiv.org/abs/2402.05740v1",
        "pub_date": "2024-02-08",
        "summary": "Recommender systems are designed to learn user preferences from observed\nfeedback and comprise many fundamental tasks, such as rating prediction and\npost-click conversion rate (pCVR) prediction. However, the observed feedback\nusually suffer from two issues: selection bias and data sparsity, where biased\nand insufficient feedback seriously degrade the performance of recommender\nsystems in terms of accuracy and ranking. Existing solutions for handling the\nissues, such as data imputation and inverse propensity score, are highly\nsusceptible to additional trained imputation or propensity models. In this\nwork, we propose a novel counterfactual contrastive learning framework for\nrecommendation, named CounterCLR, to tackle the problem of non-random missing\ndata by exploiting the advances in contrast learning. Specifically, the\nproposed CounterCLR employs a deep representation network, called CauNet, to\ninfer non-random missing data in recommendations and perform user preference\nmodeling by further introducing a self-supervised contrastive learning task.\nOur CounterCLR mitigates the selection bias problem without the need for\nadditional models or estimators, while also enhancing the generalization\nability in cases of sparse data. Experiments on real-world datasets demonstrate\nthe effectiveness and superiority of our method.",
        "translated": ""
    },
    {
        "title": "Multilingual E5 Text Embeddings: A Technical Report",
        "url": "http://arxiv.org/abs/2402.05672v1",
        "pub_date": "2024-02-08",
        "summary": "This technical report presents the training methodology and evaluation\nresults of the open-source multilingual E5 text embedding models, released in\nmid-2023. Three embedding models of different sizes (small / base / large) are\nprovided, offering a balance between the inference efficiency and embedding\nquality. The training procedure adheres to the English E5 model recipe,\ninvolving contrastive pre-training on 1 billion multilingual text pairs,\nfollowed by fine-tuning on a combination of labeled datasets. Additionally, we\nintroduce a new instruction-tuned embedding model, whose performance is on par\nwith state-of-the-art, English-only models of similar sizes. Information\nregarding the model release can be found at\nhttps://github.com/microsoft/unilm/tree/master/e5 .",
        "translated": ""
    },
    {
        "title": "Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey",
        "url": "http://arxiv.org/abs/2402.05391v1",
        "pub_date": "2024-02-08",
        "summary": "Knowledge Graphs (KGs) play a pivotal role in advancing various AI\napplications, with the semantic web community's exploration into multi-modal\ndimensions unlocking new avenues for innovation. In this survey, we carefully\nreview over 300 articles, focusing on KG-aware research in two principal\naspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal\ntasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into\nthe MMKG realm. We begin by defining KGs and MMKGs, then explore their\nconstruction progress. Our review includes two primary task categories:\nKG-aware multi-modal learning tasks, such as Image Classification and Visual\nQuestion Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph\nCompletion and Entity Alignment, highlighting specific research trajectories.\nFor most of these tasks, we provide definitions, evaluation benchmarks, and\nadditionally outline essential insights for conducting relevant research.\nFinally, we discuss current challenges and identify emerging trends, such as\nprogress in Large Language Modeling and Multi-modal Pre-training strategies.\nThis survey aims to serve as a comprehensive reference for researchers already\ninvolved in or considering delving into KG and multi-modal learning research,\noffering insights into the evolving landscape of MMKG research and supporting\nfuture work.",
        "translated": ""
    },
    {
        "title": "Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs",
        "url": "http://arxiv.org/abs/2402.05318v1",
        "pub_date": "2024-02-07",
        "summary": "Information retrieval is a rapidly evolving field of information retrieval,\nwhich is characterized by a continuous refinement of techniques and\ntechnologies, from basic hyperlink-based navigation to sophisticated\nalgorithm-driven search engines. This paper aims to provide a comprehensive\noverview of the evolution of Information Retrieval Technology, with a\nparticular focus on the role of Large Language Models (LLMs) in bridging the\ngap between traditional search methods and the emerging paradigm of answer\nretrieval. The integration of LLMs in the realms of response retrieval and\nindexing signifies a paradigm shift in how users interact with information\nsystems. This paradigm shift is driven by the integration of large language\nmodels (LLMs) like GPT-4, which are capable of understanding and generating\nhuman-like text, thus enabling them to provide more direct and contextually\nrelevant answers to user queries. Through this exploration, we seek to\nilluminate the technological milestones that have shaped this journey and the\npotential future directions in this rapidly changing field.",
        "translated": ""
    },
    {
        "title": "The Quantified Boolean Bayesian Network: Theory and Experiments with a\n  Logical Graphical Model",
        "url": "http://arxiv.org/abs/2402.06557v1",
        "pub_date": "2024-02-09",
        "summary": "This paper introduces the Quantified Boolean Bayesian Network (QBBN), which\nprovides a unified view of logical and probabilistic reasoning. The QBBN is\nmeant to address a central problem with the Large Language Model (LLM), which\nhas become extremely popular in Information Retrieval, which is that the LLM\nhallucinates. A Bayesian Network, by construction, cannot hallucinate, because\nit can only return answers that it can explain. We show how a Bayesian Network\nover an unbounded number of boolean variables can be configured to represent\nthe logical reasoning underlying human language. We do this by creating a\nkey-value version of the First-Order Calculus, for which we can prove\nconsistency and completeness. We show that the model is trivially trained over\nfully observed data, but that inference is non-trivial. Exact inference in a\nBayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). For\ninference, we investigate the use of Loopy Belief Propagation (LBP), which is\nnot guaranteed to converge, but which has been shown to often converge in\npractice. Our experiments show that LBP indeed does converge very reliably, and\nour analysis shows that a round of LBP takes time $O(N2^n)$, where $N$ bounds\nthe number of variables considered, and $n$ bounds the number of incoming\nconnections to any factor, and further improvements may be possible. Our\nnetwork is specifically designed to alternate between AND and OR gates in a\nBoolean Algebra, which connects more closely to logical reasoning, allowing a\ncompleteness proof for an expanded version of our network, and also allows\ninference to follow specific but adequate pathways, that turn out to be fast.",
        "translated": ""
    },
    {
        "title": "Toward Building a Semantic Network Inventory for Model-Driven Telemetry",
        "url": "http://arxiv.org/abs/2402.06511v1",
        "pub_date": "2024-02-09",
        "summary": "Network telemetry based on data models is expected to become the standard\nmechanism for collecting operational data from network devices efficiently. But\nthe wide variety of standard and proprietary data models along with the\ndifferent implementations of telemetry protocols offered by network vendors,\nbecome a barrier when monitoring heterogeneous network infrastructures. To\nfacilitate the integration and sharing of context information related to\nmodel-driven telemetry, this work proposes a semantic network inventory that\nintegrates new information models specifically developed to capture context\ninformation in a vendor-agnostic fashion using current standards defined for\ncontext management. To automate the integration of this context information\nwithin the network inventory, a reference architecture is designed. Finally, a\nprototype of the solution is implemented and validated through a case study\nthat illustrates how the network inventory can ease the operation of\nmodel-driven telemetry in multi-vendor networks.",
        "translated": ""
    },
    {
        "title": "What's in People's Digital File Collections?",
        "url": "http://arxiv.org/abs/2402.06421v1",
        "pub_date": "2024-02-09",
        "summary": "Thoughtfully designing services and rigorously testing software to support\npersonal information management (PIM) requires understanding the relevant\ncollections, but relatively little is known about what people keep in their\nfile collections, especially personal collections. Complementing recent work on\nthe structure of 348 file collections, we examine those collections' contents,\nhow much content is duplicated, and how collections used for personal matters\ndiffer from those used for study and work. Though all collections contain many\nimages, some intuitively common file types are surprisingly scarce. Personal\ncollections contain more audio than others, knowledge workers' collections\ncontain more text documents but far fewer folders, and IT collections exhibit\nunusual traits. Collection duplication is correlated to collections' structural\ntraits, but surprisingly, not to collection age. We discuss our findings in\nlight of prior works and provide implications for various kinds of information\nresearch.",
        "translated": ""
    },
    {
        "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2402.06360v1",
        "pub_date": "2024-02-09",
        "summary": "Collaborative search supports multiple users working together to accomplish a\nspecific search task. Research has found that designing lightweight\ncollaborative search plugins within instant messaging platforms aligns better\nwith users' collaborative habits. However, due to the complexity of multi-user\ninteraction scenarios, it is challenging to implement a fully functioning\nlightweight collaborative search system. Therefore, previous studies on\nlightweight collaborative search had to rely on the Wizard of Oz paradigm. In\nrecent years, large language models (LLMs) have been demonstrated to interact\nnaturally with users and achieve complex information-seeking tasks through\nLLM-based agents. Hence, to better support the research in collaborative\nsearch, in this demo, we propose CoSearchAgent, a lightweight collaborative\nsearch agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that\ncan support collaborative search during multi-party conversations on this\nplatform. Equipped with the capacity to understand the queries and context in\nmulti-user conversations and the ability to search the Web for relevant\ninformation via APIs, CoSearchAgent can respond to user queries with answers\ngrounded on the relevant search results. It can also ask clarifying questions\nwhen the information needs are unclear. The proposed CoSearchAgent is highly\nflexible and would be useful for supporting further research on collaborative\nsearch. The code and demo video are accessible.",
        "translated": ""
    },
    {
        "title": "ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs",
        "url": "http://arxiv.org/abs/2402.06334v1",
        "pub_date": "2024-02-09",
        "summary": "ExaRanker recently introduced an approach to training information retrieval\n(IR) models, incorporating natural language explanations as additional labels.\nThe method addresses the challenge of limited labeled examples, leading to\nimprovements in the effectiveness of IR models. However, the initial results\nwere based on proprietary language models such as GPT-3.5, which posed\nconstraints on dataset size due to its cost and data privacy. In this paper, we\nintroduce ExaRanker-Open, where we adapt and explore the use of open-source\nlanguage models to generate explanations. The method has been tested using\ndifferent LLMs and datasets sizes to better comprehend the effective\ncontribution of data augmentation. Our findings reveal that incorporating\nexplanations consistently enhances neural rankers, with benefits escalating as\nthe LLM size increases. Notably, the data augmentation method proves\nadvantageous even with large datasets, as evidenced by ExaRanker surpassing the\ntarget baseline by 0.6 nDCG@10 points in our study. To encourage further\nadvancements by the research community, we have open-sourced both the code and\ndatasets at https://github.com/unicamp-dl/ExaRanker.",
        "translated": ""
    },
    {
        "title": "Collaborative filtering, K-nearest neighbor and cosine similarity in\n  home decor recommender systems",
        "url": "http://arxiv.org/abs/2402.06233v1",
        "pub_date": "2024-02-09",
        "summary": "An architectural framework, based on collaborative filtering using K-nearest\nneighbor and cosine similarity, was developed and implemented to fit the\nrequirements for the company DecorRaid. The aim of the paper is to test\ndifferent evaluation techniques within the environment to research the\nrecommender systems performance. Three perspectives were found relevant for\nevaluating a recommender system in the specific environment, namely dataset,\nsystem and user perspective. With these perspectives it was possible to gain a\nbroader view of the recommender systems performance. Online A/B split testing\nwas conducted to compare the performance of small adjustments to the RS and to\ntest the relevance of the evaluation techniques. Key factors are solving the\nsparsity and cold start problem, where the suggestion is to research a hybrid\nRS combining Content-based and CF based techniques.",
        "translated": ""
    },
    {
        "title": "ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume\n  Generation and Refinement",
        "url": "http://arxiv.org/abs/2402.06221v1",
        "pub_date": "2024-02-09",
        "summary": "Crafting the ideal, job-specific resume is a challenging task for many job\napplicants, especially for early-career applicants. While it is highly\nrecommended that applicants tailor their resume to the specific role they are\napplying for, manually tailoring resumes to job descriptions and role-specific\nrequirements is often (1) extremely time-consuming, and (2) prone to human\nerrors. Furthermore, performing such a tailoring step at scale while applying\nto several roles may result in a lack of quality of the edited resumes. To\ntackle this problem, in this demo paper, we propose ResumeFlow: a Large\nLanguage Model (LLM) aided tool that enables an end user to simply provide\ntheir detailed resume and the desired job posting, and obtain a personalized\nresume specifically tailored to that specific job posting in the matter of a\nfew seconds. Our proposed pipeline leverages the language understanding and\ninformation extraction capabilities of state-of-the-art LLMs such as OpenAI's\nGPT-4 and Google's Gemini, in order to (1) extract details from a job\ndescription, (2) extract role-specific details from the user-provided resume,\nand then (3) use these to refine and generate a role-specific resume for the\nuser. Our easy-to-use tool leverages the user-chosen LLM in a completely\noff-the-shelf manner, thus requiring no fine-tuning. We demonstrate the\neffectiveness of our tool via a video demo and propose novel task-specific\nevaluation metrics to control for alignment and hallucination. Our tool is\navailable at https://job-aligned-resume.streamlit.app.",
        "translated": ""
    },
    {
        "title": "Fairly Evaluating Large Language Model-based Recommendation Needs\n  Revisit the Cross-Entropy Loss",
        "url": "http://arxiv.org/abs/2402.06216v1",
        "pub_date": "2024-02-09",
        "summary": "Large language models (LLMs) have gained much attention in the recommendation\ncommunity; some studies have observed that LLMs, fine-tuned by the\ncross-entropy loss with a full softmax, could achieve state-of-the-art\nperformance already. However, these claims are drawn from unobjective and\nunfair comparisons. In view of the substantial quantity of items in reality,\nconventional recommenders typically adopt a pointwise/pairwise loss function\ninstead for training. This substitute however causes severe performance\ndegradation, leading to under-estimation of conventional methods and\nover-confidence in the ranking capability of LLMs.\n  In this work, we theoretically justify the superiority of cross-entropy, and\nshowcase that it can be adequately replaced by some elementary approximations\nwith certain necessary modifications. The remarkable results across three\npublic datasets corroborate that even in a practical sense, existing LLM-based\nmethods are not as effective as claimed for next-item recommendation. We hope\nthat these theoretical understandings in conjunction with the empirical results\nwill facilitate an objective evaluation of LLM-based recommendation in the\nfuture.",
        "translated": ""
    },
    {
        "title": "Task Supportive and Personalized Human-Large Language Model Interaction:\n  A User Study",
        "url": "http://arxiv.org/abs/2402.06170v1",
        "pub_date": "2024-02-09",
        "summary": "Large language model (LLM) applications, such as ChatGPT, are a powerful tool\nfor online information-seeking (IS) and problem-solving tasks. However, users\nstill face challenges initializing and refining prompts, and their cognitive\nbarriers and biased perceptions further impede task completion. These issues\nreflect broader challenges identified within the fields of IS and interactive\ninformation retrieval (IIR). To address these, our approach integrates task\ncontext and user perceptions into human-ChatGPT interactions through prompt\nengineering. We developed a ChatGPT-like platform integrated with supportive\nfunctions, including perception articulation, prompt suggestion, and\nconversation explanation. Our findings of a user study demonstrate that the\nsupportive functions help users manage expectations, reduce cognitive loads,\nbetter refine prompts, and increase user engagement. This research enhances our\ncomprehension of designing proactive and user-centric systems with LLMs. It\noffers insights into evaluating human-LLM interactions and emphasizes potential\nchallenges for under served users.",
        "translated": ""
    },
    {
        "title": "Assortment Planning with Sponsored Products",
        "url": "http://arxiv.org/abs/2402.06158v1",
        "pub_date": "2024-02-09",
        "summary": "In the rapidly evolving landscape of retail, assortment planning plays a\ncrucial role in determining the success of a business. With the rise of\nsponsored products and their increasing prominence in online marketplaces,\nretailers face new challenges in effectively managing their product assortment\nin the presence of sponsored products. Remarkably, previous research in\nassortment planning largely overlooks the existence of sponsored products and\ntheir potential impact on overall recommendation effectiveness. Instead, they\ncommonly make the simplifying assumption that all products are either organic\nor non-sponsored. This research gap underscores the necessity for a more\nthorough investigation of the assortment planning challenge when sponsored\nproducts are in play. We formulate the assortment planning problem in the\npresence of sponsored products as a combinatorial optimization task. The\nultimate objective is to compute an assortment plan that optimizes expected\nrevenue while considering the specific requirements of placing sponsored\nproducts strategically.",
        "translated": ""
    },
    {
        "title": "Retrieval-Augmented Thought Process as Sequential Decision Making",
        "url": "http://arxiv.org/abs/2402.07812v1",
        "pub_date": "2024-02-12",
        "summary": "Large Language Models (LLMs) have demonstrated their strong ability to assist\npeople and show \"sparks of intelligence\". However, several open challenges\nhinder their wider application: such as concerns over privacy, tendencies to\nproduce hallucinations, and difficulties in handling long contexts. In this\nwork, we address those challenges by introducing the Retrieval-Augmented\nThought Process (RATP). Given access to external knowledge, RATP formulates the\nthought generation of LLMs as a multiple-step decision process. To optimize\nsuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns a\nQ-value estimator that permits cost-efficient inference. In addressing the task\nof question-answering with private data, where ethical and security concerns\nlimit LLM training methods, RATP achieves a 50% improvement over existing\nin-context retrieval-augmented language models.",
        "translated": ""
    },
    {
        "title": "Quantitative knowledge retrieval from large language models",
        "url": "http://arxiv.org/abs/2402.07770v1",
        "pub_date": "2024-02-12",
        "summary": "Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. In this\npaper we explore the feasibility of LLMs as a mechanism for quantitative\nknowledge retrieval to aid data analysis tasks such as elicitation of prior\ndistributions for Bayesian models and imputation of missing data. We present a\nprompt engineering framework, treating an LLM as an interface to a latent space\nof scientific literature, comparing responses in different contexts and domains\nagainst more established approaches. Implications and challenges of using LLMs\nas 'experts' are discussed.",
        "translated": ""
    },
    {
        "title": "Multimodal Learned Sparse Retrieval for Image Suggestion",
        "url": "http://arxiv.org/abs/2402.07736v1",
        "pub_date": "2024-02-12",
        "summary": "Learned Sparse Retrieval (LSR) is a group of neural methods designed to\nencode queries and documents into sparse lexical vectors. These vectors can be\nefficiently indexed and retrieved using an inverted index. While LSR has shown\npromise in text retrieval, its potential in multi-modal retrieval remains\nlargely unexplored. Motivated by this, in this work, we explore the application\nof LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse\nRetrieval (MLSR). We conduct experiments using several MLSR model\nconfigurations and evaluate the performance on the image suggestion task. We\nfind that solving the task solely based on the image content is challenging.\nEnriching the image content with its caption improves the model performance\nsignificantly, implying the importance of image captions to provide\nfine-grained concepts and context information of images. Our approach presents\na practical and effective solution for training LSR retrieval models in\nmulti-modal settings.",
        "translated": ""
    },
    {
        "title": "Multi-Behavior Collaborative Filtering with Partial Order Graph\n  Convolutional Networks",
        "url": "http://arxiv.org/abs/2402.07659v1",
        "pub_date": "2024-02-12",
        "summary": "Representing the information of multiple behaviors in the single graph\ncollaborative filtering (CF) vector has been a long-standing challenge. This is\nbecause different behaviors naturally form separate behavior graphs and learn\nseparate CF embeddings. Existing models merge the separate embeddings by\nappointing the CF embeddings for some behaviors as the primary embedding and\nutilizing other auxiliaries to enhance the primary embedding. However, this\napproach often results in the joint embedding performing well on the main tasks\nbut poorly on the auxiliary ones. To address the problem arising from the\nseparate behavior graphs, we propose the concept of Partial Order Graphs (POG).\nPOG defines the partial order relation of multiple behaviors and models\nbehavior combinations as weighted edges to merge separate behavior graphs into\na joint POG. Theoretical proof verifies that POG can be generalized to any\ngiven set of multiple behaviors. Based on POG, we propose the tailored Partial\nOrder Graph Convolutional Networks (POGCN) that convolute neighbors'\ninformation while considering the behavior relations between users and items.\nPOGCN also introduces a partial-order BPR sampling strategy for efficient and\neffective multiple-behavior CF training. POGCN has been successfully deployed\non the homepage of Alibaba for two months, providing recommendation services\nfor over one billion users. Extensive offline experiments conducted on three\npublic benchmark datasets demonstrate that POGCN outperforms state-of-the-art\nmulti-behavior baselines across all types of behaviors. Furthermore, online A/B\ntests confirm the superiority of POGCN in billion-scale recommender systems.",
        "translated": ""
    },
    {
        "title": "GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language\n  Models for Adaptable Conversational Task Assistants",
        "url": "http://arxiv.org/abs/2402.07647v1",
        "pub_date": "2024-02-12",
        "summary": "We tackle the challenge of building real-world multimodal assistants for\ncomplex real-world tasks. We describe the practicalities and challenges of\ndeveloping and deploying GRILLBot, a leading (first and second prize winning in\n2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building\non our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture\nthat leverages Large Language Models (LLMs) and specialised models tuned for\nspecific subtasks requiring very low latency. OAT allows us to define when, how\nand which LLMs should be used in a structured and deployable manner. For\nknowledge-grounded question answering and live task adaptations, we show that\nLLM reasoning abilities over task context and world knowledge outweigh latency\nconcerns. For dialogue state management, we implement a code generation\napproach and show that specialised smaller models have 84% effectiveness with\n100x lower latency. Overall, we provide insights and discuss tradeoffs for\ndeploying both traditional models and LLMs to users in complex real-world\nmultimodal environments in the Alexa TaskBot challenge. These experiences will\ncontinue to evolve as LLMs become more capable and efficient -- fundamentally\nreshaping OAT and future assistant architectures.",
        "translated": ""
    },
    {
        "title": "VCR: Video representation for Contextual Retrieval",
        "url": "http://arxiv.org/abs/2402.07466v1",
        "pub_date": "2024-02-12",
        "summary": "Streamlining content discovery within media archives requires integrating\nadvanced data representations and effective visualization techniques for clear\ncommunication of video topics to users. The proposed system addresses the\nchallenge of efficiently navigating large video collections by exploiting a\nfusion of visual, audio, and textual features to accurately index and\ncategorize video content through a text-based method. Additionally, semantic\nembeddings are employed to provide contextually relevant information and\nrecommendations to users, resulting in an intuitive and engaging exploratory\nexperience over our topics ontology map using OpenAI GPT-4.",
        "translated": ""
    },
    {
        "title": "AraSpider: Democratizing Arabic-to-SQL",
        "url": "http://arxiv.org/abs/2402.07448v1",
        "pub_date": "2024-02-12",
        "summary": "This study presents AraSpider, the first Arabic version of the Spider\ndataset, aimed at improving natural language processing (NLP) in the\nArabic-speaking community. Four multilingual translation models were tested for\ntheir effectiveness in translating English to Arabic. Additionally, two models\nwere assessed for their ability to generate SQL queries from Arabic text. The\nresults showed that using back translation significantly improved the\nperformance of both ChatGPT 3.5 and SQLCoder models, which are considered top\nperformers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated\nhigh-quality translation, while SQLCoder excelled in text-to-SQL tasks. The\nstudy underscores the importance of incorporating contextual schema and\nemploying back translation strategies to enhance model performance in Arabic\nNLP tasks. Moreover, the provision of detailed methodologies for\nreproducibility and translation of the dataset into other languages highlights\nthe research's commitment to promoting transparency and collaborative knowledge\nsharing in the field. Overall, these contributions advance NLP research,\nempower Arabic-speaking researchers, and enrich the global discourse on\nlanguage comprehension and database interrogation.",
        "translated": ""
    },
    {
        "title": "Benchmarking and Building Long-Context Retrieval Models with LoCo and\n  M2-BERT",
        "url": "http://arxiv.org/abs/2402.07440v1",
        "pub_date": "2024-02-12",
        "summary": "Retrieval pipelines-an integral component of many machine learning\nsystems-perform poorly in domains where documents are long (e.g., 10K tokens or\nmore) and where identifying the relevant document requires synthesizing\ninformation across the entire text. Developing long-context retrieval encoders\nsuitable for these domains raises three challenges: (1) how to evaluate\nlong-context retrieval performance, (2) how to pretrain a base language model\nto represent both short contexts (corresponding to queries) and long contexts\n(corresponding to documents), and (3) how to fine-tune this model for retrieval\nunder the batch size limitations imposed by GPU memory constraints. To address\nthese challenges, we first introduce LoCoV1, a novel 12 task benchmark\nconstructed to measure long-context retrieval where chunking is not possible or\nnot effective. We next present the M2-BERT retrieval encoder, an 80M parameter\nstate-space encoder model built from the Monarch Mixer architecture, capable of\nscaling to documents up to 32K tokens long. We describe a pretraining data\nmixture which allows this encoder to process both short and long context\nsequences, and a finetuning approach that adapts this base model to retrieval\nwith only single-sample batches. Finally, we validate the M2-BERT retrieval\nencoder on LoCoV1, finding that it outperforms competitive baselines by up to\n23.3 points, despite containing 5-90x fewer parameters.",
        "translated": ""
    },
    {
        "title": "Debiasing Recommendation with Personal Popularity",
        "url": "http://arxiv.org/abs/2402.07425v1",
        "pub_date": "2024-02-12",
        "summary": "Global popularity (GP) bias is the phenomenon that popular items are\nrecommended much more frequently than they should be, which goes against the\ngoal of providing personalized recommendations and harms user experience and\nrecommendation accuracy. Many methods have been proposed to reduce GP bias but\nthey fail to notice the fundamental problem of GP, i.e., it considers\npopularity from a \\textit{global} perspective of \\textit{all users} and uses a\nsingle set of popular items, and thus cannot capture the interests of\nindividual users. As such, we propose a user-aware version of item popularity\nnamed \\textit{personal popularity} (PP), which identifies different popular\nitems for each user by considering the users that share similar interests. As\nPP models the preferences of individual users, it naturally helps to produce\npersonalized recommendations and mitigate GP bias. To integrate PP into\nrecommendation, we design a general \\textit{personal popularity aware\ncounterfactual} (PPAC) framework, which adapts easily to existing\nrecommendation models. In particular, PPAC recognizes that PP and GP have both\ndirect and indirect effects on recommendations and controls direct effects with\ncounterfactual inference techniques for unbiased recommendations. All codes and\ndatasets are available at \\url{https://github.com/Stevenn9981/PPAC}.",
        "translated": ""
    },
    {
        "title": "Prompt Perturbation in Retrieval-Augmented Generation based Large\n  Language Models",
        "url": "http://arxiv.org/abs/2402.07179v1",
        "pub_date": "2024-02-11",
        "summary": "The robustness of large language models (LLMs) becomes increasingly important\nas their use rapidly grows in a wide range of domains. Retrieval-Augmented\nGeneration (RAG) is considered as a means to improve the trustworthiness of\ntext generation from LLMs. However, how the outputs from RAG-based LLMs are\naffected by slightly different inputs is not well studied. In this work, we\nfind that the insertion of even a short prefix to the prompt leads to the\ngeneration of outputs far away from factually correct answers. We\nsystematically evaluate the effect of such prefixes on RAG by introducing a\nnovel optimization technique called Gradient Guided Prompt Perturbation (GGPP).\nGGPP achieves a high success rate in steering outputs of RAG-based LLMs to\ntargeted wrong answers. It can also cope with instructions in the prompts\nrequesting to ignore irrelevant context. We also exploit LLMs' neuron\nactivation difference between prompts with and without GGPP perturbations to\ngive a method that improves the robustness of RAG-based LLMs through a highly\neffective detector trained on neuron activation triggered by GGPP generated\nprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of\nour methods.",
        "translated": ""
    },
    {
        "title": "Artificial Intelligence for Literature Reviews: Opportunities and\n  Challenges",
        "url": "http://arxiv.org/abs/2402.08565v1",
        "pub_date": "2024-02-13",
        "summary": "This manuscript presents a comprehensive review of the use of Artificial\nIntelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous\nand organised methodology that assesses and integrates previous research on a\ngiven topic. Numerous tools have been developed to assist and partially\nautomate the SLR process. The increasing role of AI in this field shows great\npotential in providing more effective support for researchers, moving towards\nthe semi-automatic creation of literature reviews. Our study focuses on how AI\ntechniques are applied in the semi-automation of SLRs, specifically in the\nscreening and extraction phases. We examine 21 leading SLR tools using a\nframework that combines 23 traditional features with 11 AI features. We also\nanalyse 11 recent tools that leverage large language models for searching the\nliterature and assisting academic writing. Finally, the paper discusses current\ntrends in the field, outlines key research challenges, and suggests directions\nfor future research.",
        "translated": ""
    },
    {
        "title": "Captions Are Worth a Thousand Words: Enhancing Product Retrieval with\n  Pretrained Image-to-Text Models",
        "url": "http://arxiv.org/abs/2402.08532v1",
        "pub_date": "2024-02-13",
        "summary": "This paper explores the usage of multimodal image-to-text models to enhance\ntext-based item retrieval. We propose utilizing pre-trained image captioning\nand tagging models, such as instructBLIP and CLIP, to generate text-based\nproduct descriptions which are combined with existing text descriptions. Our\nwork is particularly impactful for smaller eCommerce businesses who are unable\nto maintain the high-quality text descriptions necessary to effectively perform\nitem retrieval for search and recommendation use cases. We evaluate the\nsearchability of ground-truth text, image-generated text, and combinations of\nboth texts on several subsets of Amazon's publicly available ESCI dataset. The\nresults demonstrate the dual capability of our proposed models to enhance the\nretrieval of existing text and generate highly-searchable standalone\ndescriptions.",
        "translated": ""
    },
    {
        "title": "Frequency-aware Graph Signal Processing for Collaborative Filtering",
        "url": "http://arxiv.org/abs/2402.08426v1",
        "pub_date": "2024-02-13",
        "summary": "Graph Signal Processing (GSP) based recommendation algorithms have recently\nattracted lots of attention due to its high efficiency. However, these methods\nfailed to consider the importance of various interactions that reflect unique\nuser/item characteristics and failed to utilize user and item high-order\nneighborhood information to model user preference, thus leading to sub-optimal\nperformance. To address the above issues, we propose a frequency-aware graph\nsignal processing method (FaGSP) for collaborative filtering. Firstly, we\ndesign a Cascaded Filter Module, consisting of an ideal high-pass filter and an\nideal low-pass filter that work in a successive manner, to capture both unique\nand common user/item characteristics to more accurately model user preference.\nThen, we devise a Parallel Filter Module, consisting of two low-pass filters\nthat can easily capture the hierarchy of neighborhood, to fully utilize\nhigh-order neighborhood information of users/items for more accurate user\npreference modeling. Finally, we combine these two modules via a linear model\nto further improve recommendation accuracy. Extensive experiments on six public\ndatasets demonstrate the superiority of our method from the perspectives of\nprediction accuracy and training efficiency compared with state-of-the-art\nGCN-based recommendation methods and GSP-based recommendation methods.",
        "translated": ""
    },
    {
        "title": "Implementation of Recommendation Algorithm based on Recommendation\n  Sessions in E-commerce IT System",
        "url": "http://arxiv.org/abs/2402.08275v1",
        "pub_date": "2024-02-13",
        "summary": "This paper presents a study on the implementation of the author's Algorithm\nof Recommendation Sessions (ARS) in an operational e-commerce information\nsystem and analyses the basic parameters of the resulting recommendation\nsystem. It begins with a synthetic overview of recommendation systems, followed\nby a presentation of the proprietary ARS algorithm, which is based on\nrecommendation sessions. A mathematical model of the recommendation session,\nconstructed using graph and network theory, serves as the input for the ARS\nalgorithm. This paper also explores graph structure representation methods and\nthe implementation of a G graph (representing a set of recommendation sessions)\nin a relational database using the SQL standard. The ARS algorithm was\nimplemented in a working e-commerce information system, leading to the\ndevelopment of a fully functional recommendation system adaptable to various\ne-commerce IT systems. The effectiveness of the algorithm is demonstrated by\nresearch on the recommendation system's parameters presented in the final\nsection of the paper.",
        "translated": ""
    },
    {
        "title": "Modeling Balanced Explicit and Implicit Relations with Contrastive\n  Learning for Knowledge Concept Recommendation in MOOCs",
        "url": "http://arxiv.org/abs/2402.08256v1",
        "pub_date": "2024-02-13",
        "summary": "The knowledge concept recommendation in Massive Open Online Courses (MOOCs)\nis a significant issue that has garnered widespread attention. Existing methods\nprimarily rely on the explicit relations between users and knowledge concepts\non the MOOC platforms for recommendation. However, there are numerous implicit\nrelations (e.g., shared interests or same knowledge levels between users)\ngenerated within the users' learning activities on the MOOC platforms. Existing\nmethods fail to consider these implicit relations, and these relations\nthemselves are difficult to learn and represent, causing poor performance in\nknowledge concept recommendation and an inability to meet users' personalized\nneeds. To address this issue, we propose a novel framework based on contrastive\nlearning, which can represent and balance the explicit and implicit relations\nfor knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we\nfirst construct a MOOCs heterogeneous information network (HIN) by modeling the\ndata from the MOOC platforms. Then, we utilize a relation-updated graph\nconvolutional network and stacked multi-channel graph neural network to\nrepresent the explicit and implicit relations in the HIN, respectively.\nConsidering that the quantity of explicit relations is relatively fewer\ncompared to implicit relations in MOOCs, we propose a contrastive learning with\nprototypical graph to enhance the representations of both relations to capture\ntheir fruitful inherent relational knowledge, which can guide the propagation\nof students' preferences within the HIN. Based on these enhanced\nrepresentations, to ensure the balanced contribution of both towards the final\nrecommendation, we propose a dual-head attention mechanism for balanced fusion.\nExperimental results demonstrate that CL-KCRec outperforms several\nstate-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.",
        "translated": ""
    },
    {
        "title": "Causal Learning for Trustworthy Recommender Systems: A Survey",
        "url": "http://arxiv.org/abs/2402.08241v1",
        "pub_date": "2024-02-13",
        "summary": "Recommender Systems (RS) have significantly advanced online content discovery\nand personalized decision-making. However, emerging vulnerabilities in RS have\ncatalyzed a paradigm shift towards Trustworthy RS (TRS). Despite numerous\nprogress on TRS, most of them focus on data correlations while overlooking the\nfundamental causal nature in recommendation. This drawback hinders TRS from\nidentifying the cause in addressing trustworthiness issues, leading to limited\nfairness, robustness, and explainability. To bridge this gap, causal learning\nemerges as a class of promising methods to augment TRS. These methods, grounded\nin reliable causality, excel in mitigating various biases and noises while\noffering insightful explanations for TRS. However, there lacks a timely survey\nin this vibrant area. This paper creates an overview of TRS from the\nperspective of causal learning. We begin by presenting the advantages and\ncommon procedures of Causality-oriented TRS (CTRS). Then, we identify potential\ntrustworthiness challenges at each stage and link them to viable causal\nsolutions, followed by a classification of CTRS methods. Finally, we discuss\nseveral future directions for advancing this field.",
        "translated": ""
    },
    {
        "title": "From Data to Decisions: The Transformational Power of Machine Learning\n  in Business Recommendations",
        "url": "http://arxiv.org/abs/2402.08109v1",
        "pub_date": "2024-02-12",
        "summary": "This research aims to explore the impact of Machine Learning (ML) on the\nevolution and efficacy of Recommendation Systems (RS), particularly in the\ncontext of their growing significance in commercial business environments.\nMethodologically, the study delves into the role of ML in crafting and refining\nthese systems, focusing on aspects such as data sourcing, feature engineering,\nand the importance of evaluation metrics, thereby highlighting the iterative\nnature of enhancing recommendation algorithms. The deployment of Recommendation\nEngines (RE), driven by advanced algorithms and data analytics, is explored\nacross various domains, showcasing their significant impact on user experience\nand decision-making processes. These engines not only streamline information\ndiscovery and enhance collaboration but also accelerate knowledge acquisition,\nproving vital in navigating the digital landscape for businesses. They\ncontribute significantly to sales, revenue, and the competitive edge of\nenterprises by offering improved recommendations that align with individual\ncustomer needs. The research identifies the increasing expectation of users for\na seamless, intuitive online experience, where content is personalized and\ndynamically adapted to changing preferences. Future research directions include\nexploring advancements in deep learning models, ethical considerations in the\ndeployment of RS, and addressing scalability challenges. This study emphasizes\nthe indispensability of comprehending and leveraging ML in RS for researchers\nand practitioners, to tap into the full potential of personalized\nrecommendation in commercial business prospects.",
        "translated": ""
    },
    {
        "title": "Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical\n  Similarity Search",
        "url": "http://arxiv.org/abs/2402.07970v1",
        "pub_date": "2024-02-12",
        "summary": "Nearest neighbor-based similarity searching is a common task in chemistry,\nwith notable use cases in drug discovery. Yet, some of the most commonly used\napproaches for this task still leverage a brute-force approach. In practice\nthis can be computationally costly and overly time-consuming, due in part to\nthe sheer size of modern chemical databases. Previous computational\nadvancements for this task have generally relied on improvements to hardware or\ndataset-specific tricks that lack generalizability. Approaches that leverage\nlower-complexity searching algorithms remain relatively underexplored. However,\nmany of these algorithms are approximate solutions and/or struggle with typical\nhigh-dimensional chemical embeddings. Here we evaluate whether a combination of\nlow-dimensional chemical embeddings and a k-d tree data structure can achieve\nfast nearest neighbor queries while maintaining performance on standard\nchemical similarity search benchmarks. We examine different dimensionality\nreductions of standard chemical embeddings as well as a learned,\nstructurally-aware embedding -- SmallSA -- for this task. With this framework,\nsearches on over one billion chemicals execute in less than a second on a\nsingle CPU core, five orders of magnitude faster than the brute-force approach.\nWe also demonstrate that SmallSA achieves competitive performance on chemical\nsimilarity benchmarks.",
        "translated": ""
    },
    {
        "title": "Robust Training of Temporal GNNs using Nearest Neighbours based Hard\n  Negatives",
        "url": "http://arxiv.org/abs/2402.09239v1",
        "pub_date": "2024-02-14",
        "summary": "Temporal graph neural networks Tgnn have exhibited state-of-art performance\nin future-link prediction tasks. Training of these TGNNs is enumerated by\nuniform random sampling based unsupervised loss. During training, in the\ncontext of a positive example, the loss is computed over uninformative\nnegatives, which introduces redundancy and sub-optimal performance. In this\npaper, we propose modified unsupervised learning of Tgnn, by replacing the\nuniform negative sampling with importance-based negative sampling. We\ntheoretically motivate and define the dynamically computed distribution for a\nsampling of negative examples. Finally, using empirical evaluations over three\nreal-world datasets, we show that Tgnn trained using loss based on proposed\nnegative sampling provides consistent superior performance.",
        "translated": ""
    },
    {
        "title": "Large Language Model Interaction Simulator for Cold-Start Item\n  Recommendation",
        "url": "http://arxiv.org/abs/2402.09176v1",
        "pub_date": "2024-02-14",
        "summary": "Recommending cold items is a long-standing challenge for collaborative\nfiltering models because these cold items lack historical user interactions to\nmodel their collaborative features. The gap between the content of cold items\nand their behavior patterns makes it difficult to generate accurate behavioral\nembeddings for cold items. Existing cold-start models use mapping functions to\ngenerate fake behavioral embeddings based on the content feature of cold items.\nHowever, these generated embeddings have significant differences from the real\nbehavioral embeddings, leading to a negative impact on cold recommendation\nperformance. To address this challenge, we propose an LLM Interaction Simulator\n(LLM-InS) to model users' behavior patterns based on the content aspect. This\nsimulator allows recommender systems to simulate vivid interactions for each\ncold item and transform them from cold to warm items directly. Specifically, we\noutline the designing and training process of a tailored LLM-simulator that can\nsimulate the behavioral patterns of users and items. Additionally, we introduce\nan efficient \"filtering-and-refining\" approach to take full advantage of the\nsimulation power of the LLMs. Finally, we propose an updating method to update\nthe embeddings of the items. we unified trains for both cold and warm items\nwithin a recommender model based on the simulated and real interactions.\nExtensive experiments using real behavioral embeddings demonstrate that our\nproposed model, LLM-InS, outperforms nine state-of-the-art cold-start methods\nand three LLM models in cold-start item recommendations.",
        "translated": ""
    },
    {
        "title": "Recommendation Algorithm Based on Recommendation Sessions",
        "url": "http://arxiv.org/abs/2402.09130v1",
        "pub_date": "2024-02-14",
        "summary": "The enormous development of the Internet, both in the geographical scale and\nin the area of using its possibilities in everyday life, determines the\ncreation and collection of huge amounts of data. Due to the scale, it is not\npossible to analyse them using traditional methods, therefore it makes a\nnecessary to use modern methods and techniques. Such methods are provided,\namong others, by the area of recommendations. The aim of this study is to\npresent a new algorithm in the area of recommendation systems, the algorithm\nbased on data from various sets of information, both static (categories of\nobjects, features of objects) and dynamic (user behaviour).",
        "translated": ""
    },
    {
        "title": "Confidence-aware Fine-tuning of Sequential Recommendation Systems via\n  Conformal Prediction",
        "url": "http://arxiv.org/abs/2402.08976v1",
        "pub_date": "2024-02-14",
        "summary": "In Sequential Recommendation Systems, Cross-Entropy (CE) loss is commonly\nused but fails to harness item confidence scores during training. Recognizing\nthe critical role of confidence in aligning training objectives with evaluation\nmetrics, we propose CPFT, a versatile framework that enhances recommendation\nconfidence by integrating Conformal Prediction (CP)-based losses with CE loss\nduring fine-tuning. CPFT dynamically generates a set of items with a high\nprobability of containing the ground truth, enriching the training process by\nincorporating validation data without compromising its role in model selection.\nThis innovative approach, coupled with CP-based losses, sharpens the focus on\nrefining recommendation sets, thereby elevating the confidence in potential\nitem predictions. By fine-tuning item confidence through CP-based losses, CPFT\nsignificantly enhances model performance, leading to more precise and\ntrustworthy recommendations that increase user trust and satisfaction. Our\nextensive evaluation across five diverse datasets and four distinct sequential\nmodels confirms CPFT's substantial impact on improving recommendation quality\nthrough strategic confidence optimization. Access to the framework's code will\nbe provided following the acceptance of the paper.",
        "translated": ""
    },
    {
        "title": "Enhancing ID and Text Fusion via Alternative Training in Session-based\n  Recommendation",
        "url": "http://arxiv.org/abs/2402.08921v1",
        "pub_date": "2024-02-14",
        "summary": "Session-based recommendation has gained increasing attention in recent years,\nwith its aim to offer tailored suggestions based on users' historical behaviors\nwithin sessions.\n  To advance this field, a variety of methods have been developed, with\nID-based approaches typically demonstrating promising performance. However,\nthese methods often face challenges with long-tail items and overlook other\nrich forms of information, notably valuable textual semantic information. To\nintegrate text information, various methods have been introduced, mostly\nfollowing a naive fusion framework. Surprisingly, we observe that fusing these\ntwo modalities does not consistently outperform the best single modality by\nfollowing the naive fusion framework. Further investigation reveals an\npotential imbalance issue in naive fusion, where the ID dominates and text\nmodality is undertrained. This suggests that the unexpected observation may\nstem from naive fusion's failure to effectively balance the two modalities,\noften over-relying on the stronger ID modality. This insight suggests that\nnaive fusion might not be as effective in combining ID and text as previously\nexpected. To address this, we propose a novel alternative training strategy\nAlterRec. It separates the training of ID and text, thereby avoiding the\nimbalance issue seen in naive fusion. Additionally, AlterRec designs a novel\nstrategy to facilitate the interaction between the two modalities, enabling\nthem to mutually learn from each other and integrate the text more effectively.\nComprehensive experiments demonstrate the effectiveness of AlterRec in\nsession-based recommendation. The implementation is available at\nhttps://github.com/Juanhui28/AlterRec.",
        "translated": ""
    },
    {
        "title": "eCeLLM: Generalizing Large Language Models for E-commerce from\n  Large-scale, High-quality Instruction Data",
        "url": "http://arxiv.org/abs/2402.08831v1",
        "pub_date": "2024-02-13",
        "summary": "With tremendous efforts on developing effective e-commerce models,\nconventional e-commerce models show limited success in generalist e-commerce\nmodeling, and suffer from unsatisfactory performance on new users and new\nproducts - a typical out-of-domain generalization challenge. Meanwhile, large\nlanguage models (LLMs) demonstrate outstanding performance in generalist\nmodeling and out-of-domain generalizability in many fields. Toward fully\nunleashing their power for e-commerce, in this paper, we construct ECInstruct,\nthe first open-sourced, large-scale, and high-quality benchmark instruction\ndataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of\ne-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive\nexperiments and evaluation demonstrate that eCeLLM models substantially\noutperform baseline models, including the most advanced GPT-4, and the\nstate-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM\nexhibits excellent generalizability to out-of-domain settings, including unseen\nproducts and unseen instructions, highlighting its superiority as a generalist\ne-commerce model. Both the ECInstruct dataset and the eCeLLM models show great\npotential in empowering versatile and effective LLMs for e-commerce. ECInstruct\nand eCeLLM models are publicly accessible through\nhttps://ninglab.github.io/eCeLLM.",
        "translated": ""
    },
    {
        "title": "Multi-Label Zero-Shot Product Attribute-Value Extraction",
        "url": "http://arxiv.org/abs/2402.08802v1",
        "pub_date": "2024-02-13",
        "summary": "E-commerce platforms should provide detailed product descriptions (attribute\nvalues) for effective product search and recommendation. However, attribute\nvalue information is typically not available for new products. To predict\nunseen attribute values, large quantities of labeled training data are needed\nto train a traditional supervised learning model. Typically, it is difficult,\ntime-consuming, and costly to manually label large quantities of new product\nprofiles. In this paper, we propose a novel method to efficiently and\neffectively extract unseen attribute values from new products in the absence of\nlabeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot\nattribute value extraction model that leverages inductive inference in\nheterogeneous hypergraphs. In particular, our proposed technique constructs\nheterogeneous hypergraphs to capture complex higher-order relations (i.e. user\nbehavior information) to learn more accurate feature representations for graph\nnodes. Furthermore, our proposed HyperPAVE model uses an inductive link\nprediction mechanism to infer future connections between unseen nodes. This\nenables HyperPAVE to identify new attribute values without the need for labeled\ntraining data. We conduct extensive experiments with ablation studies on\ndifferent categories of the MAVE dataset. The results demonstrate that our\nproposed HyperPAVE model significantly outperforms existing\nclassification-based, generation-based large language models for attribute\nvalue extraction in the zero-shot setting.",
        "translated": ""
    },
    {
        "title": "DeepSRGM -- Sequence Classification and Ranking in Indian Classical\n  Music with Deep Learning",
        "url": "http://arxiv.org/abs/2402.10168v1",
        "pub_date": "2024-02-15",
        "summary": "A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a\nmelodic framework for compositions and improvisations alike. Raga Recognition\nis an important music information retrieval task in ICM as it can aid numerous\ndownstream applications ranging from music recommendations to organizing huge\nmusic collections. In this work, we propose a deep learning based approach to\nRaga recognition. Our approach employs efficient pre possessing and learns\ntemporal sequences in music data using Long Short Term Memory based Recurrent\nNeural Networks (LSTM-RNN). We train and test the network on smaller sequences\nsampled from the original audio while the final inference is performed on the\naudio as a whole. Our method achieves an accuracy of 88.1% and 97 % during\ninference on the Comp Music Carnatic dataset and its 10 Raga subset\nrespectively making it the state-of-the-art for the Raga recognition task. Our\napproach also enables sequence ranking which aids us in retrieving melodic\npatterns from a given music data base that are closely related to the presented\nquery sequence.",
        "translated": ""
    },
    {
        "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation",
        "url": "http://arxiv.org/abs/2402.10024v1",
        "pub_date": "2024-02-15",
        "summary": "Recent work has shown that, while large language models (LLMs) demonstrate\nstrong word translation or bilingual lexicon induction (BLI) capabilities in\nfew-shot setups, they still cannot match the performance of 'traditional'\nmapping-based approaches in the unsupervised scenario where no seed translation\npairs are available, especially for lower-resource languages. To address this\nchallenge with LLMs, we propose self-augmented in-context learning (SAIL) for\nunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a\nset of high-confidence word translation pairs for in-context learning (ICL)\nfrom an LLM, which it then reapplies to the same LLM in the ICL fashion. Our\nmethod shows substantial gains over zero-shot prompting of LLMs on two\nestablished BLI benchmarks spanning a wide range of language pairs, also\noutperforming mapping-based baselines across the board. In addition to\nachieving state-of-the-art unsupervised BLI performance, we also conduct\ncomprehensive analyses on SAIL and discuss its limitations.",
        "translated": ""
    },
    {
        "title": "LLM-based Federated Recommendation",
        "url": "http://arxiv.org/abs/2402.09959v1",
        "pub_date": "2024-02-15",
        "summary": "Large Language Models (LLMs), with their advanced contextual understanding\nabilities, have demonstrated considerable potential in enhancing recommendation\nsystems via fine-tuning methods. However, fine-tuning requires users' behavior\ndata, which poses considerable privacy risks due to the incorporation of\nsensitive user information. The unintended disclosure of such data could\ninfringe upon data protection laws and give rise to ethical issues. To mitigate\nthese privacy issues, Federated Learning for Recommendation (Fed4Rec) has\nemerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based\nrecommendation presents two main challenges: first, an increase in the\nimbalance of performance across clients, affecting the system's efficiency over\ntime, and second, a high demand on clients' computational and storage resources\nfor local training and inference of LLMs.\n  To address these challenges, we introduce a Privacy-Preserving LLM-based\nRecommendation (PPLR) framework. The PPLR framework employs two primary\nstrategies. First, it implements a dynamic balance strategy, which involves the\ndesign of dynamic parameter aggregation and adjustment of learning speed for\ndifferent clients during the training phase, to ensure relatively balanced\nperformance across all clients. Second, PPLR adopts a flexible storage\nstrategy, selectively retaining certain sensitive layers of the language model\non the client side while offloading non-sensitive layers to the server. This\napproach aims to preserve user privacy while efficiently saving computational\nand storage resources. Experimental results demonstrate that PPLR not only\nachieves a balanced performance among clients but also enhances overall system\nperformance in a manner that is both computationally and storage-efficient,\nwhile effectively protecting user privacy.",
        "translated": ""
    },
    {
        "title": "Generative AI in the Construction Industry: A State-of-the-art Analysis",
        "url": "http://arxiv.org/abs/2402.09939v1",
        "pub_date": "2024-02-15",
        "summary": "The construction industry is a vital sector of the global economy, but it\nfaces many productivity challenges in various processes, such as design,\nplanning, procurement, inspection, and maintenance. Generative artificial\nintelligence (AI), which can create novel and realistic data or content, such\nas text, image, video, or code, based on some input or prior knowledge, offers\ninnovative and disruptive solutions to address these challenges. However, there\nis a gap in the literature on the current state, opportunities, and challenges\nof generative AI in the construction industry. This study aims to fill this gap\nby providing a state-of-the-art analysis of generative AI in construction, with\nthree objectives: (1) to review and categorize the existing and emerging\ngenerative AI opportunities and challenges in the construction industry; (2) to\npropose a framework for construction firms to build customized generative AI\nsolutions using their own data, comprising steps such as data collection,\ndataset curation, training custom large language model (LLM), model evaluation,\nand deployment; and (3) to demonstrate the framework via a case study of\ndeveloping a generative model for querying contract documents. The results show\nthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\n9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study\nprovides academics and construction professionals with a comprehensive analysis\nand practical framework to guide the adoption of generative AI techniques to\nenhance productivity, quality, safety, and sustainability across the\nconstruction industry.",
        "translated": ""
    },
    {
        "title": "Sequential Recommendation on Temporal Proximities with Contrastive\n  Learning and Self-Attention",
        "url": "http://arxiv.org/abs/2402.09784v1",
        "pub_date": "2024-02-15",
        "summary": "Sequential recommender systems identify user preferences from their past\ninteractions to predict subsequent items optimally. Although traditional\ndeep-learning-based models and modern transformer-based models in previous\nstudies capture unidirectional and bidirectional patterns within user-item\ninteractions, the importance of temporal contexts, such as individual\nbehavioral and societal trend patterns, remains underexplored. Notably, recent\nmodels often neglect similarities in users' actions that occur implicitly among\nusers during analogous timeframes-a concept we term vertical temporal\nproximity. These models primarily adapt the self-attention mechanisms of the\ntransformer to consider the temporal context in individual user actions.\nMeanwhile, this adaptation still remains limited in considering the horizontal\ntemporal proximity within item interactions, like distinguishing between\nsubsequent item purchases within a week versus a month. To address these gaps,\nwe propose a sequential recommendation model called TemProxRec, which includes\ncontrastive learning and self-attention methods to consider temporal\nproximities both across and within user-item interactions. The proposed\ncontrastive learning method learns representations of items selected in close\ntemporal periods across different users to be close. Simultaneously, the\nproposed self-attention mechanism encodes temporal and positional contexts in a\nuser sequence using both absolute and relative embeddings. This way, our\nTemProxRec accurately predicts the relevant items based on the user-item\ninteractions within a specific timeframe. We validate this work through\ncomprehensive experiments on TemProxRec, consistently outperforming existing\nmodels on benchmark datasets as well as showing the significance of considering\nthe vertical and horizontal temporal proximities into sequential\nrecommendation.",
        "translated": ""
    },
    {
        "title": "From Variability to Stability: Advancing RecSys Benchmarking Practices",
        "url": "http://arxiv.org/abs/2402.09766v1",
        "pub_date": "2024-02-15",
        "summary": "In the rapidly evolving domain of Recommender Systems (RecSys), new\nalgorithms frequently claim state-of-the-art performance based on evaluations\nover a limited set of arbitrarily selected datasets. However, this approach may\nfail to holistically reflect their effectiveness due to the significant impact\nof dataset characteristics on algorithm performance. Addressing this\ndeficiency, this paper introduces a novel benchmarking methodology to\nfacilitate a fair and robust comparison of RecSys algorithms, thereby advancing\nevaluation practices. By utilizing a diverse set of $30$ open datasets,\nincluding two introduced in this work, and evaluating $11$ collaborative\nfiltering algorithms across $9$ metrics, we critically examine the influence of\ndataset characteristics on algorithm performance. We further investigate the\nfeasibility of aggregating outcomes from multiple datasets into a unified\nranking. Through rigorous experimental analysis, we validate the reliability of\nour methodology under the variability of datasets, offering a benchmarking\nstrategy that balances quality and computational demands. This methodology\nenables a fair yet effective means of evaluating RecSys algorithms, providing\nvaluable guidance for future research endeavors.",
        "translated": ""
    },
    {
        "title": "Grounding Language Model with Chunking-Free In-Context Retrieval",
        "url": "http://arxiv.org/abs/2402.09760v1",
        "pub_date": "2024-02-15",
        "summary": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval\napproach, specifically tailored for Retrieval-Augmented Generation (RAG)\nsystems. Traditional RAG systems often struggle with grounding responses using\nprecise evidence text due to the challenges of processing lengthy documents and\nfiltering out irrelevant content. Commonly employed solutions, such as document\nchunking and adapting language models to handle longer contexts, have their\nlimitations. These methods either disrupt the semantic coherence of the text or\nfail to effectively address the issues of noise and inaccuracy in evidence\nretrieval.\n  CFIC addresses these challenges by circumventing the conventional chunking\nprocess. It utilizes the encoded hidden states of documents for in-context\nretrieval, employing auto-aggressive decoding to accurately identify the\nspecific evidence text required for user queries, eliminating the need for\nchunking. CFIC is further enhanced by incorporating two decoding strategies,\nnamely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies\nnot only improve the efficiency of the retrieval process but also ensure that\nthe fidelity of the generated grounding text evidence is maintained. Our\nevaluations of CFIC on a range of open QA datasets demonstrate its superiority\nin retrieving relevant and accurate evidence, offering a significant\nimprovement over traditional methods. By doing away with the need for document\nchunking, CFIC presents a more streamlined, effective, and efficient retrieval\nsolution, making it a valuable advancement in the field of RAG systems.",
        "translated": ""
    },
    {
        "title": "POBEVM: Real-time Video Matting via Progressively Optimize the Target\n  Body and Edge",
        "url": "http://arxiv.org/abs/2402.09731v1",
        "pub_date": "2024-02-15",
        "summary": "Deep convolutional neural networks (CNNs) based approaches have achieved\ngreat performance in video matting. Many of these methods can produce accurate\nalpha estimation for the target body but typically yield fuzzy or incorrect\ntarget edges. This is usually caused by the following reasons: 1) The current\nmethods always treat the target body and edge indiscriminately; 2) Target body\ndominates the whole target with only a tiny proportion target edge. For the\nfirst problem, we propose a CNN-based module that separately optimizes the\nmatting target body and edge (SOBE). And on this basis, we introduce a\nreal-time, trimap-free video matting method via progressively optimizing the\nmatting target body and edge (POBEVM) that is much lighter than previous\napproaches and achieves significant improvements in the predicted target edge.\nFor the second problem, we propose an Edge-L1-Loss (ELL) function that enforces\nour network on the matting target edge. Experiments demonstrate our method\noutperforms prior trimap-free matting methods on both Distinctions-646 (D646)\nand VideoMatte240K(VM) dataset, especially in edge optimization.",
        "translated": ""
    },
    {
        "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
        "url": "http://arxiv.org/abs/2402.09727v1",
        "pub_date": "2024-02-15",
        "summary": "Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3-20x.",
        "translated": ""
    },
    {
        "title": "User Modeling and User Profiling: A Comprehensive Survey",
        "url": "http://arxiv.org/abs/2402.09660v1",
        "pub_date": "2024-02-15",
        "summary": "The integration of artificial intelligence (AI) into daily life, particularly\nthrough information retrieval and recommender systems, has necessitated\nadvanced user modeling and profiling techniques to deliver personalized\nexperiences. These techniques aim to construct accurate user representations\nbased on the rich amounts of data generated through interactions with these\nsystems. This paper presents a comprehensive survey of the current state,\nevolution, and future directions of user modeling and profiling research. We\nprovide a historical overview, tracing the development from early stereotype\nmodels to the latest deep learning techniques, and propose a novel taxonomy\nthat encompasses all active topics in this research area, including recent\ntrends. Our survey highlights the paradigm shifts towards more sophisticated\nuser profiling methods, emphasizing implicit data collection, multi-behavior\nmodeling, and the integration of graph data structures. We also address the\ncritical need for privacy-preserving techniques and the push towards\nexplainability and fairness in user modeling approaches. By examining the\ndefinitions of core terminology, we aim to clarify ambiguities and foster a\nclearer understanding of the field by proposing two novel encyclopedic\ndefinitions of the main terms. Furthermore, we explore the application of user\nmodeling in various domains, such as fake news detection, cybersecurity, and\npersonalized education. This survey serves as a comprehensive resource for\nresearchers and practitioners, offering insights into the evolution of user\nmodeling and profiling and guiding the development of more personalized,\nethical, and effective AI systems.",
        "translated": ""
    },
    {
        "title": "Discovering and exploring cases of educational source code plagiarism\n  with Dolos",
        "url": "http://arxiv.org/abs/2402.10853v1",
        "pub_date": "2024-02-16",
        "summary": "Source code plagiarism is a significant issue in educational practice, and\neducators need user-friendly tools to cope with such academic dishonesty. This\narticle introduces the latest version of Dolos, a state-of-the-art ecosystem of\ntools for detecting and preventing plagiarism in educational source code. In\nthis new version, the primary focus has been on enhancing the user experience.\nEducators can now run the entire plagiarism detection pipeline from a new web\napp in their browser, eliminating the need for any installation or\nconfiguration. Completely redesigned analytics dashboards provide an instant\nassessment of whether a collection of source files contains suspected cases of\nplagiarism and how widespread plagiarism is within the collection. The\ndashboards support hierarchically structured navigation to facilitate zooming\nin and out of suspect cases. Clusters are an essential new component of the\ndashboard design, reflecting the observation that plagiarism can occur among\nlarger groups of students. To meet various user needs, the Dolos software stack\nfor source code plagiarism detections now includes a web interface, a JSON\napplication programming interface (API), a command line interface (CLI), a\nJavaScript library and a preconfigured Docker container. Clear documentation\nand a free-to-use instance of the web app can be found at\nhttps://dolos.ugent.be. The source code is also available on GitHub.",
        "translated": ""
    },
    {
        "title": "Generative Cross-Modal Retrieval: Memorizing Images in Multimodal\n  Language Models for Retrieval and Beyond",
        "url": "http://arxiv.org/abs/2402.10805v1",
        "pub_date": "2024-02-16",
        "summary": "The recent advancements in generative language models have demonstrated their\nability to memorize knowledge from documents and recall knowledge to respond to\nuser queries effectively. Building upon this capability, we propose to enable\nmultimodal large language models (MLLMs) to memorize and recall images within\ntheir parameters. Given a user query for visual content, the MLLM is\nanticipated to \"recall\" the relevant image from its parameters as the response.\nAchieving this target presents notable challenges, including inbuilt visual\nmemory and visual recall schemes within MLLMs. To address these challenges, we\nintroduce a generative cross-modal retrieval framework, which assigns unique\nidentifier strings to represent images and involves two training steps:\nlearning to memorize and learning to retrieve. The first step focuses on\ntraining the MLLM to memorize the association between images and their\nrespective identifiers. The latter step teaches the MLLM to generate the\ncorresponding identifier of the target image, given the textual query input. By\nmemorizing images in MLLMs, we introduce a new paradigm to cross-modal\nretrieval, distinct from previous discriminative approaches. The experiments\ndemonstrate that the generative paradigm performs effectively and efficiently\neven with large-scale image candidate sets.",
        "translated": ""
    },
    {
        "title": "Distillation Enhanced Generative Retrieval",
        "url": "http://arxiv.org/abs/2402.10769v1",
        "pub_date": "2024-02-16",
        "summary": "Generative retrieval is a promising new paradigm in text retrieval that\ngenerates identifier strings of relevant passages as the retrieval target. This\nparadigm leverages powerful generative language models, distinct from\ntraditional sparse or dense retrieval methods. In this work, we identify a\nviable direction to further enhance generative retrieval via distillation and\npropose a feasible framework, named DGR. DGR utilizes sophisticated ranking\nmodels, such as the cross-encoder, in a teacher role to supply a passage rank\nlist, which captures the varying relevance degrees of passages instead of\nbinary hard labels; subsequently, DGR employs a specially designed distilled\nRankNet loss to optimize the generative retrieval model, considering the\npassage rank order provided by the teacher model as labels. This framework only\nrequires an additional distillation step to enhance current generative\nretrieval systems and does not add any burden to the inference stage. We\nconduct experiments on four public datasets, and the results indicate that DGR\nachieves state-of-the-art performance among the generative retrieval methods.\nAdditionally, DGR demonstrates exceptional robustness and generalizability with\nvarious teacher models and distillation losses.",
        "translated": ""
    },
    {
        "title": "FairSync: Ensuring Amortized Group Exposure in Distributed\n  Recommendation Retrieval",
        "url": "http://arxiv.org/abs/2402.10628v1",
        "pub_date": "2024-02-16",
        "summary": "In pursuit of fairness and balanced development, recommender systems (RS)\noften prioritize group fairness, ensuring that specific groups maintain a\nminimum level of exposure over a given period. For example, RS platforms aim to\nensure adequate exposure for new providers or specific categories of items\naccording to their needs. Modern industry RS usually adopts a two-stage\npipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from\nmillions of items distributed across various servers, and stage-2 (ranking\nstage) focuses on presenting a small-size but accurate selection from items\nchosen in stage-1. Existing efforts for ensuring amortized group exposures\nfocus on stage-2, however, stage-1 is also critical for the task. Without a\nhigh-quality set of candidates, the stage-2 ranker cannot ensure the required\nexposure of groups. Previous fairness-aware works designed for stage-2\ntypically require accessing and traversing all items. In stage-1, however,\nmillions of items are distributively stored in servers, making it infeasible to\ntraverse all of them. How to ensure group exposures in the distributed\nretrieval process is a challenging question. To address this issue, we\nintroduce a model named FairSync, which transforms the problem into a\nconstrained distributed optimization problem. Specifically, FairSync resolves\nthe issue by moving it to the dual space, where a central node aggregates\nhistorical fairness data into a vector and distributes it to all servers. To\ntrade off the efficiency and accuracy, the gradient descent technique is used\nto periodically update the parameter of the dual vector. The experiment results\non two public recommender retrieval datasets showcased that FairSync\noutperformed all the baselines, achieving the desired minimum level of\nexposures while maintaining a high level of retrieval accuracy.",
        "translated": ""
    },
    {
        "title": "Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for\n  Effective Sequential Recommendation",
        "url": "http://arxiv.org/abs/2402.10602v1",
        "pub_date": "2024-02-16",
        "summary": "Recent sequential recommendation models have combined pre-trained text\nembeddings of items with item ID embeddings to achieve superior recommendation\nperformance. Despite their effectiveness, the expressive power of text features\nin these models remains largely unexplored. While most existing models\nemphasize the importance of ID embeddings in recommendations, our study takes a\nstep further by studying sequential recommendation models that only rely on\ntext features and do not necessitate ID embeddings. Upon examining pretrained\ntext embeddings experimentally, we discover that they reside in an anisotropic\nsemantic space, with an average cosine similarity of over 0.8 between items. We\nalso demonstrate that this anisotropic nature hinders recommendation models\nfrom effectively differentiating between item representations and leads to\ndegenerated performance. To address this issue, we propose to employ a\npre-processing step known as whitening transformation, which transforms the\nanisotropic text feature distribution into an isotropic Gaussian distribution.\nOur experiments show that whitening pre-trained text embeddings in the\nsequential model can significantly improve recommendation performance. However,\nthe full whitening operation might break the potential manifold of items with\nsimilar text semantics. To preserve the original semantics while benefiting\nfrom the isotropy of the whitened text features, we introduce WhitenRec+, an\nensemble approach that leverages both fully whitened and relaxed whitened item\nrepresentations for effective recommendations. We further discuss and analyze\nthe benefits of our design through experiments and proofs. Experimental results\non three public benchmark datasets demonstrate that WhitenRec+ outperforms\nstate-of-the-art methods for sequential recommendation.",
        "translated": ""
    },
    {
        "title": "SPAR: Personalized Content-Based Recommendation via Long Engagement\n  Attention",
        "url": "http://arxiv.org/abs/2402.10555v1",
        "pub_date": "2024-02-16",
        "summary": "Leveraging users' long engagement histories is essential for personalized\ncontent recommendations. The success of pretrained language models (PLMs) in\nNLP has led to their use in encoding user histories and candidate items,\nframing content recommendations as textual semantic matching tasks. However,\nexisting works still struggle with processing very long user historical text\nand insufficient user-item interaction. In this paper, we introduce a\ncontent-based recommendation framework, SPAR, which effectively tackles the\nchallenges of holistic user interest extraction from the long user engagement\nhistory. It achieves so by leveraging PLM, poly-attention layers and attention\nsparsity mechanisms to encode user's history in a session-based manner. The\nuser and item side features are sufficiently fused for engagement prediction\nwhile maintaining standalone representations for both sides, which is efficient\nfor practical model deployment. Moreover, we enhance user profiling by\nexploiting large language model (LLM) to extract global interests from user\nengagement history. Extensive experiments on two benchmark datasets demonstrate\nthat our framework outperforms existing state-of-the-art (SoTA) methods.",
        "translated": ""
    },
    {
        "title": "Cognitive Personalized Search Integrating Large Language Models with an\n  Efficient Memory Mechanism",
        "url": "http://arxiv.org/abs/2402.10548v1",
        "pub_date": "2024-02-16",
        "summary": "Traditional search engines usually provide identical search results for all\nusers, overlooking individual preferences. To counter this limitation,\npersonalized search has been developed to re-rank results based on user\npreferences derived from query logs. Deep learning-based personalized search\nmethods have shown promise, but they rely heavily on abundant training data,\nmaking them susceptible to data sparsity challenges. This paper proposes a\nCognitive Personalized Search (CoPS) model, which integrates Large Language\nModels (LLMs) with a cognitive memory mechanism inspired by human cognition.\nCoPS employs LLMs to enhance user modeling and user search experience. The\ncognitive memory mechanism comprises sensory memory for quick sensory\nresponses, working memory for sophisticated cognitive responses, and long-term\nmemory for storing historical interactions. CoPS handles new queries using a\nthree-step approach: identifying re-finding behaviors, constructing user\nprofiles with relevant historical information, and ranking documents based on\npersonalized query intent. Experiments show that CoPS outperforms baseline\nmodels in zero-shot scenarios.",
        "translated": ""
    },
    {
        "title": "Understanding Survey Paper Taxonomy about Large Language Models via\n  Graph Representation Learning",
        "url": "http://arxiv.org/abs/2402.10409v1",
        "pub_date": "2024-02-16",
        "summary": "As new research on Large Language Models (LLMs) continues, it is difficult to\nkeep up with new research and models. To help researchers synthesize the new\nresearch many have written survey papers, but even those have become numerous.\nIn this paper, we develop a method to automatically assign survey papers to a\ntaxonomy. We collect the metadata of 144 LLM survey papers and explore three\nparadigms to classify papers within the taxonomy. Our work indicates that\nleveraging graph structure information on co-category graphs can significantly\noutperform the language models in two paradigms; pre-trained language models'\nfine-tuning and zero-shot/few-shot classifications using LLMs. We find that our\nmodel surpasses an average human recognition level and that fine-tuning LLMs\nusing weak labels generated by a smaller model, such as the GCN in this study,\ncan be more effective than using ground-truth labels, revealing the potential\nof weak-to-strong generalization in the taxonomy classification task.",
        "translated": ""
    },
    {
        "title": "UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation\n  Fusion with Painting Style",
        "url": "http://arxiv.org/abs/2402.10381v1",
        "pub_date": "2024-02-16",
        "summary": "The rapid advancement of high-quality image generation models based on AI has\ngenerated a deluge of anime illustrations. Recommending illustrations to users\nwithin massive data has become a challenging and popular task. However,\nexisting anime recommendation systems have focused on text features but still\nneed to integrate image features. In addition, most multi-modal recommendation\nresearch is constrained by tightly coupled datasets, limiting its applicability\nto anime illustrations. We propose the User-aware Multi-modal Animation\nIllustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle\nthese gaps. In the feature extract phase, for image features, we are the first\nto combine image painting style features with semantic features to construct a\ndual-output image encoder for enhancing representation. For text features, we\nobtain text embeddings based on fine-tuning Sentence-Transformers by\nincorporating domain knowledge that composes a variety of domain text pairs\nfrom multilingual mappings, entity relationships, and term explanation\nperspectives, respectively. In the multi-modal fusion phase, we novelly propose\na user-aware multi-modal contribution measurement mechanism to weight\nmulti-modal features dynamically according to user features at the interaction\nlevel and employ the DCN-V2 module to model bounded-degree multi-modal crosses\neffectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large\nreal-world datasets, demonstrating substantial performance enhancements.",
        "translated": ""
    },
    {
        "title": "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge",
        "url": "http://arxiv.org/abs/2402.12352v1",
        "pub_date": "2024-02-19",
        "summary": "Large language models (LLMs) are transforming the way information is\nretrieved with vast amounts of knowledge being summarized and presented via\nnatural language conversations. Yet, LLMs are prone to highlight the most\nfrequently seen pieces of information from the training set and to neglect the\nrare ones. In the field of biomedical research, latest discoveries are key to\nacademic and industrial actors and are obscured by the abundance of an\never-increasing literature corpus (the information overload problem). Surfacing\nnew associations between biomedical entities, e.g., drugs, genes, diseases,\nwith LLMs becomes a challenge of capturing the long-tail knowledge of the\nbiomedical scientific production. To overcome this challenge, Retrieval\nAugmented Generation (RAG) has been proposed to alleviate some of the\nshortcomings of LLMs by augmenting the prompts with context retrieved from\nexternal datasets. RAG methods typically select the context via maximum\nsimilarity search over text embeddings. In this study, we show that RAG methods\nleave out a significant proportion of relevant information due to clusters of\nover-represented concepts in the biomedical literature. We introduce a novel\ninformation-retrieval method that leverages a knowledge graph to downsample\nthese clusters and mitigate the information overload problem. Its retrieval\nperformance is about twice better than embedding similarity alternatives on\nboth precision and recall. Finally, we demonstrate that both embedding\nsimilarity and knowledge graph retrieval methods can be advantageously combined\ninto a hybrid model that outperforms both, enabling potential improvements to\nbiomedical question-answering models.",
        "translated": ""
    },
    {
        "title": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural\n  Language Explanations from Large Language Models",
        "url": "http://arxiv.org/abs/2402.12276v1",
        "pub_date": "2024-02-19",
        "summary": "The process of scale calibration in ranking systems involves adjusting the\noutputs of rankers to correspond with significant qualities like click-through\nrates or relevance, crucial for mirroring real-world value and thereby boosting\nthe system's effectiveness and reliability. Although there has been research on\ncalibrated ranking losses within learning-to-rank models, the particular issue\nof adjusting the scale for neural rankers, which excel in handling textual\ninformation, has not been thoroughly examined. Neural ranking models are adept\nat processing text data, yet the application of existing scale calibration\ntechniques to these models poses significant challenges due to their complexity\nand the intensive training they require, often resulting in suboptimal\noutcomes.\n  This study delves into the potential of large language models (LLMs) to\nprovide uncertainty measurements for a query and document pair that correlate\nwith the scale-calibrated scores. By employing Monte Carlo sampling to gauge\nrelevance probabilities from LLMs and incorporating natural language\nexplanations (NLEs) to articulate this uncertainty, we carry out comprehensive\ntests on two major document ranking datasets. Our findings reveal that the\napproach leveraging NLEs outperforms existing calibration methods under various\ntraining scenarios, leading to better calibrated neural rankers.",
        "translated": ""
    },
    {
        "title": "Analysis of Persian News Agencies on Instagram, A Words Co-occurrence\n  Graph-based Approach",
        "url": "http://arxiv.org/abs/2402.12272v1",
        "pub_date": "2024-02-19",
        "summary": "The rise of the Internet and the exponential increase in data have made\nmanual data summarization and analysis a challenging task. Instagram social\nnetwork is a prominent social network widely utilized in Iran for information\nsharing and communication across various age groups. The inherent structure of\nInstagram, characterized by its text-rich content and graph-like data\nrepresentation, enables the utilization of text and graph processing techniques\nfor data analysis purposes. The degree distributions of these networks exhibit\nscale-free characteristics, indicating non-random growth patterns. Recently,\nword co-occurrence has gained attention from researchers across multiple\ndisciplines due to its simplicity and practicality. Keyword extraction is a\ncrucial task in natural language processing. In this study, we demonstrated\nthat high-precision extraction of keywords from Instagram posts in the Persian\nlanguage can be achieved using unsupervised word co-occurrence methods without\nresorting to conventional techniques such as clustering or pre-trained models.\nAfter graph visualization and community detection, it was observed that the top\ntopics covered by news agencies are represented by these graphs. This approach\nis generalizable to new and diverse datasets and can provide acceptable outputs\nfor new data. To the author's knowledge, this method has not been employed in\nthe Persian language before on Instagram social network. The new crawled data\nhas been publicly released on GitHub for exploration by other researchers. By\nemploying this method, it is possible to use other graph-based algorithms, such\nas community detections. The results help us to identify the key role of\ndifferent news agencies in information diffusion among the public, identify\nhidden communities, and discover latent patterns among a massive amount of\ndata.",
        "translated": ""
    },
    {
        "title": "Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid\n  Federated Approach",
        "url": "http://arxiv.org/abs/2402.12202v1",
        "pub_date": "2024-02-19",
        "summary": "In the era of modern education, addressing cross-school learner diversity is\ncrucial, especially in personalized recommender systems for elective course\nselection. However, privacy concerns often limit cross-school data sharing,\nwhich hinders existing methods' ability to model sparse data and address\nheterogeneity effectively, ultimately leading to suboptimal recommendations. In\nresponse, we propose HFRec, a heterogeneity-aware hybrid federated recommender\nsystem designed for cross-school elective course recommendations. The proposed\nmodel constructs heterogeneous graphs for each school, incorporating various\ninteractions and historical behaviors between students to integrate context and\ncontent information. We design an attention mechanism to capture\nheterogeneity-aware representations. Moreover, under a federated scheme, we\ntrain individual school-based models with adaptive learning settings to\nrecommend tailored electives. Our HFRec model demonstrates its effectiveness in\nproviding personalized elective recommendations while maintaining privacy, as\nit outperforms state-of-the-art models on both open-source and real-world\ndatasets.",
        "translated": ""
    },
    {
        "title": "A Survey on Extractive Knowledge Graph Summarization: Applications,\n  Approaches, Evaluation, and Future Directions",
        "url": "http://arxiv.org/abs/2402.12001v1",
        "pub_date": "2024-02-19",
        "summary": "With the continuous growth of large Knowledge Graphs (KGs), extractive KG\nsummarization becomes a trending task. Aiming at distilling a compact subgraph\nwith condensed information, it facilitates various downstream KG-based tasks.\nIn this survey paper, we are among the first to provide a systematic overview\nof its applications and define a taxonomy for existing methods from its\ninterdisciplinary studies. Future directions are also laid out based on our\nextensive and comparative review.",
        "translated": ""
    },
    {
        "title": "FeB4RAG: Evaluating Federated Search in the Context of Retrieval\n  Augmented Generation",
        "url": "http://arxiv.org/abs/2402.11891v1",
        "pub_date": "2024-02-19",
        "summary": "Federated search systems aggregate results from multiple search engines,\nselecting appropriate sources to enhance result quality and align with user\nintent. With the increasing uptake of Retrieval-Augmented Generation (RAG)\npipelines, federated search can play a pivotal role in sourcing relevant\ninformation across heterogeneous data sources to generate informed responses.\nHowever, existing datasets, such as those developed in the past TREC FedWeb\ntracks, predate the RAG paradigm shift and lack representation of modern\ninformation retrieval challenges. To bridge this gap, we present FeB4RAG, a\nnovel dataset specifically designed for federated search within RAG frameworks.\nThis dataset, derived from 16 sub-collections of the widely used \\beir\nbenchmarking collection, includes 790 information requests (akin to\nconversational queries) tailored for chatbot applications, along with top\nresults returned by each resource and associated LLM-derived relevance\njudgements. Additionally, to support the need for this collection, we\ndemonstrate the impact on response generation of a high quality federated\nsearch system for RAG compared to a naive approach to federated search. We do\nso by comparing answers generated through the RAG pipeline through a\nqualitative side-by-side comparison. Our collection fosters and supports the\ndevelopment and evaluation of new federated search methods, especially in the\ncontext of RAG pipelines.",
        "translated": ""
    },
    {
        "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
        "url": "http://arxiv.org/abs/2402.11855v1",
        "pub_date": "2024-02-19",
        "summary": "Negative sampling stands as a pivotal technique in dense retrieval, essential\nfor training effective retrieval models and significantly impacting retrieval\nperformance. While existing negative sampling methods have made commendable\nprogress by leveraging hard negatives, a comprehensive guiding principle for\nconstructing negative candidates and designing negative sampling distributions\nis still lacking. To bridge this gap, we embark on a theoretical analysis of\nnegative sampling in dense retrieval. This exploration culminates in the\nunveiling of the quasi-triangular principle, a novel framework that elucidates\nthe triangular-like interplay between query, positive document, and negative\ndocument. Fueled by this guiding principle, we introduce TriSampler, a\nstraightforward yet highly effective negative sampling method. The keypoint of\nTriSampler lies in its ability to selectively sample more informative negatives\nwithin a prescribed constrained region. Experimental evaluation show that\nTriSampler consistently attains superior retrieval performance across a diverse\nof representative retrieval models.",
        "translated": ""
    },
    {
        "title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's\n  Preference in Conversational Search",
        "url": "http://arxiv.org/abs/2402.11827v1",
        "pub_date": "2024-02-19",
        "summary": "Conversational search, unlike single-turn retrieval tasks, requires\nunderstanding the current question within a dialogue context. The common\napproach of rewrite-then-retrieve aims to decontextualize questions to be\nself-sufficient for off-the-shelf retrievers, but most existing methods produce\nsub-optimal query rewrites due to the limited ability to incorporate signals\nfrom the retrieval results. To overcome this limitation, we present a novel\nframework RetPO (Retriever's Preference Optimization), which is designed to\noptimize a language model (LM) for reformulating search queries in line with\nthe preferences of the target retrieval systems. The process begins by\nprompting a large LM to produce various potential rewrites and then collects\nretrieval performance for these rewrites as the retrievers' preferences.\nThrough the process, we construct a large-scale dataset called RF collection,\ncontaining Retrievers' Feedback on over 410K query rewrites across 12K\nconversations. Furthermore, we fine-tune a smaller LM using this dataset to\nalign it with the retrievers' preferences as feedback. The resulting model\nachieves state-of-the-art performance on two recent conversational search\nbenchmarks, significantly outperforming existing baselines, including GPT-3.5.",
        "translated": ""
    },
    {
        "title": "Microstructures and Accuracy of Graph Recall by Large Language Models",
        "url": "http://arxiv.org/abs/2402.11821v1",
        "pub_date": "2024-02-19",
        "summary": "Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall by\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.",
        "translated": ""
    },
    {
        "title": "Unveiling the Magic: Investigating Attention Distillation in\n  Retrieval-augmented Generation",
        "url": "http://arxiv.org/abs/2402.11794v1",
        "pub_date": "2024-02-19",
        "summary": "Retrieval-augmented generation framework can address the limitations of large\nlanguage models by enabling real-time knowledge updates for more accurate\nanswers. An efficient way in the training phase of retrieval-augmented models\nis attention distillation, which uses attention scores as a supervision signal\ninstead of manually annotated query-document pairs. Despite its growing\npopularity, the detailed mechanisms behind the success of attention\ndistillation remain unexplored, particularly the specific patterns it leverages\nto benefit training. In this paper, we address this gap by conducting a\ncomprehensive review of attention distillation workflow and identifying key\nfactors influencing the learning quality of retrieval-augmented language\nmodels. We further propose indicators for optimizing models' training methods\nand avoiding ineffective training.",
        "translated": ""
    },
    {
        "title": "Unlocking Insights: Semantic Search in Jupyter Notebooks",
        "url": "http://arxiv.org/abs/2402.13234v1",
        "pub_date": "2024-02-20",
        "summary": "Semantic search, a process aimed at delivering highly relevant search results\nby comprehending the searcher's intent and the contextual meaning of terms\nwithin a searchable dataspace, plays a pivotal role in information retrieval.\nIn this paper, we investigate the application of large language models to\nenhance semantic search capabilities, specifically tailored for the domain of\nJupyter Notebooks. Our objective is to retrieve generated outputs, such as\nfigures or tables, associated functions and methods, and other pertinent\ninformation.\n  We demonstrate a semantic search framework that achieves a comprehensive\nsemantic understanding of the entire notebook's contents, enabling it to\neffectively handle various types of user queries. Key components of this\nframework include:\n  1). A data preprocessor is designed to handle diverse types of cells within\nJupyter Notebooks, encompassing both markdown and code cells. 2). An innovative\nmethodology is devised to address token size limitations that arise with\ncode-type cells. We implement a finer-grained approach to data input,\ntransitioning from the cell level to the function level, effectively resolving\nthese issues.",
        "translated": ""
    },
    {
        "title": "Mode Estimation with Partial Feedback",
        "url": "http://arxiv.org/abs/2402.13079v1",
        "pub_date": "2024-02-20",
        "summary": "The combination of lightly supervised pre-training and online fine-tuning has\nplayed a key role in recent AI developments. These new learning pipelines call\nfor new theoretical frameworks. In this paper, we formalize core aspects of\nweakly supervised and active learning with a simple problem: the estimation of\nthe mode of a distribution using partial feedback. We show how entropy coding\nallows for optimal information acquisition from partial feedback, develop\ncoarse sufficient statistics for mode identification, and adapt bandit\nalgorithms to our new setting. Finally, we combine those contributions into a\nstatistically and computationally efficient solution to our problem.",
        "translated": ""
    },
    {
        "title": "Enhancing Real-World Complex Network Representations with Hyperedge\n  Augmentation",
        "url": "http://arxiv.org/abs/2402.13033v1",
        "pub_date": "2024-02-20",
        "summary": "Graph augmentation methods play a crucial role in improving the performance\nand enhancing generalisation capabilities in Graph Neural Networks (GNNs).\nExisting graph augmentation methods mainly perturb the graph structures and are\nusually limited to pairwise node relations. These methods cannot fully address\nthe complexities of real-world large-scale networks that often involve\nhigher-order node relations beyond only being pairwise. Meanwhile, real-world\ngraph datasets are predominantly modelled as simple graphs, due to the scarcity\nof data that can be used to form higher-order edges. Therefore, reconfiguring\nthe higher-order edges as an integration into graph augmentation strategies\nlights up a promising research path to address the aforementioned issues. In\nthis paper, we present Hyperedge Augmentation (HyperAug), a novel graph\naugmentation method that constructs virtual hyperedges directly form the raw\ndata, and produces auxiliary node features by extracting from the virtual\nhyperedge information, which are used for enhancing GNN performances on\ndownstream tasks. We design three diverse virtual hyperedge construction\nstrategies to accompany the augmentation scheme: (1) via graph statistics, (2)\nfrom multiple data perspectives, and (3) utilising multi-modality. Furthermore,\nto facilitate HyperAug evaluation, we provide 23 novel real-world graph\ndatasets across various domains including social media, biology, and\ne-commerce. Our empirical study shows that HyperAug consistently and\nsignificantly outperforms GNN baselines and other graph augmentation methods,\nacross a variety of application contexts, which clearly indicates that it can\neffectively incorporate higher-order node relations into graph augmentation\nmethods for real-world complex networks.",
        "translated": ""
    },
    {
        "title": "Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism",
        "url": "http://arxiv.org/abs/2402.12997v1",
        "pub_date": "2024-02-20",
        "summary": "Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.",
        "translated": ""
    },
    {
        "title": "Distributionally Robust Graph-based Recommendation System",
        "url": "http://arxiv.org/abs/2402.12994v1",
        "pub_date": "2024-02-20",
        "summary": "With the capacity to capture high-order collaborative signals, Graph Neural\nNetworks (GNNs) have emerged as powerful methods in Recommender Systems (RS).\nHowever, their efficacy often hinges on the assumption that training and\ntesting data share the same distribution (a.k.a. IID assumption), and exhibits\nsignificant declines under distribution shifts. Distribution shifts commonly\narises in RS, often attributed to the dynamic nature of user preferences or\nubiquitous biases during data collection in RS. Despite its significance,\nresearches on GNN-based recommendation against distribution shift are still\nsparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN)\nthat incorporates Distributional Robust Optimization (DRO) into the GNN-based\nrecommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater\nto graph data intertwined with GNN, we reinterpret GNN as a graph smoothing\nregularizer, thereby facilitating the nuanced application of DRO; 2) Given the\ntypically sparse nature of recommendation data, which might impede robust\noptimization, we introduce slight perturbations in the training distribution to\nexpand its support. Notably, while DR-GNN involves complex optimization, it can\nbe implemented easily and efficiently. Our extensive experiments validate the\neffectiveness of DR-GNN against three typical distribution shifts. The code is\navailable at https://github.com/WANGBohaO-jpg/DR-GNN .",
        "translated": ""
    },
    {
        "title": "An Autonomous Large Language Model Agent for Chemical Literature Data\n  Mining",
        "url": "http://arxiv.org/abs/2402.12993v1",
        "pub_date": "2024-02-20",
        "summary": "Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.",
        "translated": ""
    },
    {
        "title": "Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval\n  Systems",
        "url": "http://arxiv.org/abs/2402.12784v1",
        "pub_date": "2024-02-20",
        "summary": "The introduction of Vec2Text, a technique for inverting text embeddings, has\nraised serious privacy concerns within dense retrieval systems utilizing text\nembeddings, including those provided by OpenAI and Cohere. This threat comes\nfrom the ability for a malicious attacker with access to text embeddings to\nreconstruct the original text.\n  In this paper, we investigate various aspects of embedding models that could\ninfluence the recoverability of text using Vec2Text. Our exploration involves\nfactors such as distance metrics, pooling functions, bottleneck pre-training,\ntraining with noise addition, embedding quantization, and embedding dimensions\n-- aspects not previously addressed in the original Vec2Text paper. Through a\nthorough analysis of these factors, our aim is to gain a deeper understanding\nof the critical elements impacting the trade-offs between text recoverability\nand retrieval effectiveness in dense retrieval systems. This analysis provides\nvaluable insights for practitioners involved in designing privacy-aware dense\nretrieval systems. Additionally, we propose a straightforward fix for embedding\ntransformation that ensures equal ranking effectiveness while mitigating the\nrisk of text recoverability.\n  Furthermore, we extend the application of Vec2Text to the separate task of\ncorpus poisoning, where, theoretically, Vec2Text presents a more potent threat\ncompared to previous attack methods. Notably, Vec2Text does not require access\nto the dense retriever's model parameters and can efficiently generate numerous\nadversarial passages.\n  In summary, this study highlights the potential threat posed by Vec2Text to\nexisting dense retrieval systems, while also presenting effective methods to\npatch and strengthen such systems against such risks.",
        "translated": ""
    },
    {
        "title": "Interpreting Conversational Dense Retrieval by Rewriting-Enhanced\n  Inversion of Session Embedding",
        "url": "http://arxiv.org/abs/2402.12774v1",
        "pub_date": "2024-02-20",
        "summary": "Conversational dense retrieval has shown to be effective in conversational\nsearch. However, a major limitation of conversational dense retrieval is their\nlack of interpretability, hindering intuitive understanding of model behaviors\nfor targeted improvements. This paper presents CONVINV, a simple yet effective\napproach to shed light on interpretable conversational dense retrieval models.\nCONVINV transforms opaque conversational session embeddings into explicitly\ninterpretable text while faithfully maintaining their original retrieval\nperformance as much as possible. Such transformation is achieved by training a\nrecently proposed Vec2Text model based on the ad-hoc query encoder, leveraging\nthe fact that the session and query embeddings share the same space in existing\nconversational dense retrieval. To further enhance interpretability, we propose\nto incorporate external interpretable query rewrites into the transformation\nprocess. Extensive evaluations on three conversational search benchmarks\ndemonstrate that CONVINV can yield more interpretable text and faithfully\npreserve original retrieval performance than baselines. Our work connects\nopaque session embeddings with transparent query rewriting, paving the way\ntoward trustworthy conversational search.",
        "translated": ""
    },
    {
        "title": "BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation",
        "url": "http://arxiv.org/abs/2402.12733v1",
        "pub_date": "2024-02-20",
        "summary": "In real recommendation scenarios, users often have different types of\nbehaviors, such as clicking and buying. Existing research methods show that it\nis possible to capture the heterogeneous interests of users through different\ntypes of behaviors. However, most multi-behavior approaches have limitations in\nlearning the relationship between different behaviors. In this paper, we\npropose a novel multilayer perceptron (MLP)-based heterogeneous sequential\nrecommendation method, namely behavior-aware multilayer perceptron (BMLP).\nSpecifically, it has two main modules, including a heterogeneous interest\nperception (HIP) module, which models behaviors at multiple granularities\nthrough behavior types and transition relationships, and a purchase intent\nperception (PIP) module, which adaptively fuses subsequences of auxiliary\nbehaviors to capture users' purchase intent. Compared with mainstream sequence\nmodels, MLP is competitive in terms of accuracy and has unique advantages in\nsimplicity and efficiency. Extensive experiments show that BMLP achieves\nsignificant improvement over state-of-the-art algorithms on four public\ndatasets. In addition, its pure MLP architecture leads to a linear time\ncomplexity.",
        "translated": ""
    },
    {
        "title": "Modality-Aware Integration with Large Language Models for\n  Knowledge-based Visual Question Answering",
        "url": "http://arxiv.org/abs/2402.12728v1",
        "pub_date": "2024-02-20",
        "summary": "Knowledge-based visual question answering (KVQA) has been extensively studied\nto answer visual questions with external knowledge, e.g., knowledge graphs\n(KGs). While several attempts have been proposed to leverage large language\nmodels (LLMs) as an implicit knowledge source, it remains challenging since\nLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,\nimages, KGs and LLMs, cannot be readily aligned for complex scenarios. To\ntackle these, we present a novel modality-aware integration with LLMs for KVQA\n(MAIL). It carefully leverages multimodal knowledge for both image\nunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stage\nprompting strategy with LLMs to densely embody the image into a scene graph\nwith detailed visual features; (ii) We construct a coupled concept graph by\nlinking the mentioned entities with external facts. (iii) A tailored\npseudo-siamese graph medium fusion is designed for sufficient multimodal\nfusion. We utilize the shared mentioned entities in two graphs as mediums to\nbridge a tight inter-modal exchange, while maximally preserving insightful\nintra-modal learning by constraining the fusion within mediums. Extensive\nexperiments on two benchmark datasets show the superiority of MAIL with 24x\nless resources.",
        "translated": ""
    },
    {
        "title": "Linear-Time Graph Neural Networks for Scalable Recommendations",
        "url": "http://arxiv.org/abs/2402.13973v1",
        "pub_date": "2024-02-21",
        "summary": "In an era of information explosion, recommender systems are vital tools to\ndeliver personalized recommendations for users. The key of recommender systems\nis to forecast users' future behaviors based on previous user-item\ninteractions. Due to their strong expressive power of capturing high-order\nconnectivities in user-item interaction data, recent years have witnessed a\nrising interest in leveraging Graph Neural Networks (GNNs) to boost the\nprediction performance of recommender systems. Nonetheless, classic Matrix\nFactorization (MF) and Deep Neural Network (DNN) approaches still play an\nimportant role in real-world large-scale recommender systems due to their\nscalability advantages. Despite the existence of GNN-acceleration solutions, it\nremains an open question whether GNN-based recommender systems can scale as\nefficiently as classic MF and DNN methods. In this paper, we propose a\nLinear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender\nsystems to achieve comparable scalability as classic MF approaches while\nmaintaining GNNs' powerful expressiveness for superior prediction accuracy.\nExtensive experiments and ablation studies are presented to validate the\neffectiveness and scalability of the proposed algorithm. Our implementation\nbased on PyTorch is available.",
        "translated": ""
    },
    {
        "title": "Retention Induced Biases in a Recommendation System with Heterogeneous\n  Users",
        "url": "http://arxiv.org/abs/2402.13959v1",
        "pub_date": "2024-02-21",
        "summary": "I examine a conceptual model of a recommendation system (RS) with user inflow\nand churn dynamics. When inflow and churn balance out, the user distribution\nreaches a steady state. Changing the recommendation algorithm alters the steady\nstate and creates a transition period. During this period, the RS behaves\ndifferently from its new steady state. In particular, A/B experiment metrics\nobtained in transition periods are biased indicators of the RS's long term\nperformance. Scholars and practitioners, however, often conduct A/B tests\nshortly after introducing new algorithms to validate their effectiveness. This\nA/B experiment paradigm, widely regarded as the gold standard for assessing RS\nimprovements, may consequently yield false conclusions. I also briefly discuss\nthe data bias caused by the user retention dynamics.",
        "translated": ""
    },
    {
        "title": "Science Checker Reloaded: A Bidirectional Paradigm for Transparency and\n  Logical Reasoning",
        "url": "http://arxiv.org/abs/2402.13897v1",
        "pub_date": "2024-02-21",
        "summary": "Information retrieval is a rapidly evolving field. However it still faces\nsignificant limitations in the scientific and industrial vast amounts of\ninformation, such as semantic divergence and vocabulary gaps in sparse\nretrieval, low precision and lack of interpretability in semantic search, or\nhallucination and outdated information in generative models. In this paper, we\nintroduce a two-block approach to tackle these hurdles for long documents. The\nfirst block enhances language understanding in sparse retrieval by query\nexpansion to retrieve relevant documents. The second block deepens the result\nby providing comprehensive and informative answers to the complex question\nusing only the information spread in the long document, enabling bidirectional\nengagement. At various stages of the pipeline, intermediate results are\npresented to users to facilitate understanding of the system's reasoning. We\nbelieve this bidirectional approach brings significant advancements in terms of\ntransparency, logical thinking, and comprehensive understanding in the field of\nscientific information retrieval.",
        "translated": ""
    },
    {
        "title": "Diversity-Aware $k$-Maximum Inner Product Search Revisited",
        "url": "http://arxiv.org/abs/2402.13858v1",
        "pub_date": "2024-02-21",
        "summary": "The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational\ncomponent in recommender systems and various data mining tasks. However, while\nmost existing $k$MIPS approaches prioritize the efficient retrieval of highly\nrelevant items for users, they often neglect an equally pivotal facet of search\nresults: \\emph{diversity}. To bridge this gap, we revisit and refine the\ndiversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known\ndiversity objectives -- minimizing the average and maximum pairwise item\nsimilarities within the results -- into the original relevance objective. This\nenhancement, inspired by Maximal Marginal Relevance (MMR), offers users a\ncontrollable trade-off between relevance and diversity. We introduce\n\\textsc{Greedy} and \\textsc{DualGreedy}, two linear scan-based algorithms\ntailored for D$k$MIPS. They both achieve data-dependent approximations and,\nwhen aiming to minimize the average pairwise similarity, \\textsc{DualGreedy}\nattains an approximation ratio of $1/4$ with an additive term for\nregularization. To further improve query efficiency, we integrate a lightweight\nBall-Cone Tree (BC-Tree) index with the two algorithms. Finally, comprehensive\nexperiments on ten real-world data sets demonstrate the efficacy of our\nproposed methods, showcasing their capability to efficiently deliver diverse\nand relevant search results to users.",
        "translated": ""
    },
    {
        "title": "LLM4SBR: A Lightweight and Effective Framework for Integrating Large\n  Language Models in Session-based Recommendation",
        "url": "http://arxiv.org/abs/2402.13840v1",
        "pub_date": "2024-02-21",
        "summary": "Traditional session-based recommendation (SBR) utilizes session behavior\nsequences from anonymous users for recommendation. Although this strategy is\nhighly efficient, it sacrifices the inherent semantic information of the items,\nmaking it difficult for the model to understand the true intent of the session\nand resulting in a lack of interpretability in the recommended results.\nRecently, large language models (LLMs) have flourished across various domains,\noffering a glimpse of hope in addressing the aforementioned challenges.\nInspired by the impact of LLMs, research exploring the integration of LLMs with\nthe Recommender system (RS) has surged like mushrooms after rain. However,\nconstrained by high time and space costs, as well as the brief and anonymous\nnature of session data, the first LLM recommendation framework suitable for\nindustrial deployment has yet to emerge in the field of SBR. To address the\naforementioned challenges, we have proposed the LLM Integration Framework for\nSBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR\nadopts a two-step strategy. Firstly, we transform session data into a bimodal\nform of text and behavior. In the first step, leveraging the inferential\ncapabilities of LLMs, we conduct inference on session text data from different\nperspectives and design the component for auxiliary enhancement. In the second\nstep, the SBR model is trained on behavior data, aligning and averaging two\nmodal session representations from different perspectives. Finally, we fuse\nsession representations from different perspectives and modalities as the\nultimate session representation for recommendation. We conducted experiments on\ntwo real-world datasets, and the results demonstrate that LLM4SBR significantly\nimproves the performance of traditional SBR models and is highly lightweight\nand efficient, making it suitable for industrial deployment.",
        "translated": ""
    },
    {
        "title": "Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks",
        "url": "http://arxiv.org/abs/2402.13787v1",
        "pub_date": "2024-02-21",
        "summary": "In this paper, we investigate the conditions under which link analysis\nalgorithms prevent minority groups from reaching high ranking slots. We find\nthat the most common link-based algorithms using centrality metrics, such as\nPageRank and HITS, can reproduce and even amplify bias against minority groups\nin networks. Yet, their behavior differs: one one hand, we empirically show\nthat PageRank mirrors the degree distribution for most of the ranking positions\nand it can equalize representation of minorities among the top ranked nodes; on\nthe other hand, we find that HITS amplifies pre-existing bias in homophilic\nnetworks through a novel theoretical analysis, supported by empirical results.\nWe find the root cause of bias amplification in HITS to be the level of\nhomophily present in the network, modeled through an evolving network model\nwith two communities. We illustrate our theoretical analysis on both synthetic\nand real datasets and we present directions for future work.",
        "translated": ""
    },
    {
        "title": "General Debiasing for Graph-based Collaborative Filtering via\n  Adversarial Graph Dropout",
        "url": "http://arxiv.org/abs/2402.13769v1",
        "pub_date": "2024-02-21",
        "summary": "Graph neural networks (GNNs) have shown impressive performance in recommender\nsystems, particularly in collaborative filtering (CF). The key lies in\naggregating neighborhood information on a user-item interaction graph to\nenhance user/item representations. However, we have discovered that this\naggregation mechanism comes with a drawback, which amplifies biases present in\nthe interaction graph. For instance, a user's interactions with items can be\ndriven by both unbiased true interest and various biased factors like item\npopularity or exposure. However, the current aggregation approach combines all\ninformation, both biased and unbiased, leading to biased representation\nlearning. Consequently, graph-based recommenders can learn distorted views of\nusers/items, hindering the modeling of their true preferences and\ngeneralizations. To address this issue, we introduce a novel framework called\nAdversarial Graph Dropout (AdvDrop). It differentiates between unbiased and\nbiased interactions, enabling unbiased representation learning. For each\nuser/item, AdvDrop employs adversarial learning to split the neighborhood into\ntwo views: one with bias-mitigated interactions and the other with bias-aware\ninteractions. After view-specific aggregation, AdvDrop ensures that the\nbias-mitigated and bias-aware representations remain invariant, shielding them\nfrom the influence of bias. We validate AdvDrop's effectiveness on five public\ndatasets that cover both general and specific biases, demonstrating significant\nimprovements. Furthermore, our method exhibits meaningful separation of\nsubgraphs and achieves unbiased representations for graph-based CF models, as\nrevealed by in-depth analysis. Our code is publicly available at\nhttps://github.com/Arthurma71/AdvDrop.",
        "translated": ""
    },
    {
        "title": "Breaking the Barrier: Utilizing Large Language Models for Industrial\n  Recommendation Systems through an Inferential Knowledge Graph",
        "url": "http://arxiv.org/abs/2402.13750v1",
        "pub_date": "2024-02-21",
        "summary": "Recommendation systems are widely used in e-commerce websites and online\nplatforms to address information overload. However, existing systems primarily\nrely on historical data and user feedback, making it difficult to capture user\nintent transitions. Recently, Knowledge Base (KB)-based models are proposed to\nincorporate expert knowledge, but it struggle to adapt to new items and the\nevolving e-commerce environment. To address these challenges, we propose a\nnovel Large Language Model based Complementary Knowledge Enhanced\nRecommendation System (LLM-KERec). It introduces an entity extractor that\nextracts unified concept terms from item and user information. To provide\ncost-effective and reliable prior knowledge, entity pairs are generated based\non entity popularity and specific strategies. The large language model\ndetermines complementary relationships in each entity pair, constructing a\ncomplementary knowledge graph. Furthermore, a new complementary recall module\nand an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of\nthe ranking model using real complementary exposure-click samples. Extensive\nexperiments conducted on three industry datasets demonstrate the significant\nperformance improvement of our model compared to existing approaches.\nAdditionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm\nfor consumption by recommending complementary items. In summary, LLM-KERec\naddresses the limitations of traditional recommendation systems by\nincorporating complementary knowledge and utilizing a large language model to\ncapture user intent transitions, adapt to new items, and enhance recommendation\nefficiency in the evolving e-commerce landscape.",
        "translated": ""
    },
    {
        "title": "Improving Video Corpus Moment Retrieval with Partial Relevance\n  Enhancement",
        "url": "http://arxiv.org/abs/2402.13576v1",
        "pub_date": "2024-02-21",
        "summary": "Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at\nretrieving a relevant moment from a large corpus of untrimmed videos using a\nnatural language text as query. The relevance between the video and query is\npartial, mainly evident in two aspects: (1) Scope: The untrimmed video contains\ninformation-rich frames, and not all are relevant to the query. Strong\ncorrelation is typically observed only within the relevant moment, emphasizing\nthe importance of capturing key content. (2) Modality: The relevance of query\nto different modalities varies; action descriptions align more with the visual\nelements, while character conversations are more related to textual\ninformation. Recognizing and addressing these modality-specific nuances is\ncrucial for effective retrieval in VCMR. However, existing methods often treat\nall video contents equally, leading to sub-optimal moment retrieval. We argue\nthat effectively capturing the partial relevance between the query and video is\nessential for the VCMR task. To this end, we propose a Partial Relevance\nEnhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video\nretrieval and moment localization. To align with their distinct objectives, we\nimplement specialized partial relevance enhancement strategies. For video\nretrieval, we introduce a multi-modal collaborative video retriever, generating\ndistinct query representations tailored for different modalities by\nmodality-specific pooling, ensuring a more effective match. For moment\nlocalization, we propose the focus-then-fuse moment localizer, utilizing\nmodality-specific gates to capture essential content, followed by fusing\nmulti-modal information for moment localization. Experimental results on TVR\nand DiDeMo datasets show that the proposed model outperforms the baselines,\nachieving a new state-of-the-art of VCMR.",
        "translated": ""
    },
    {
        "title": "Event-aware Video Corpus Moment Retrieval",
        "url": "http://arxiv.org/abs/2402.13566v1",
        "pub_date": "2024-02-21",
        "summary": "Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task\nfocused on identifying a specific moment within a vast corpus of untrimmed\nvideos using the natural language query. Existing methods for VCMR typically\nrely on frame-aware video retrieval, calculating similarities between the query\nand video frames to rank videos based on maximum frame similarity.However, this\napproach overlooks the semantic structure embedded within the information\nbetween frames, namely, the event, a crucial element for human comprehension of\nvideos. Motivated by this, we propose EventFormer, a model that explicitly\nutilizes events within videos as fundamental units for video retrieval. The\nmodel extracts event representations through event reasoning and hierarchical\nevent encoding. The event reasoning module groups consecutive and visually\nsimilar frame representations into events, while the hierarchical event\nencoding encodes information at both the frame and event levels. We also\nintroduce anchor multi-head self-attenion to encourage Transformer to capture\nthe relevance of adjacent content in the video. The training of EventFormer is\nconducted by two-branch contrastive learning and dual optimization for two\nsub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo\nbenchmarks show the effectiveness and efficiency of EventFormer in VCMR,\nachieving new state-of-the-art results. Additionally, the effectiveness of\nEventFormer is also validated on partially relevant video retrieval task.",
        "translated": ""
    },
    {
        "title": "Link Prediction under Heterophily: A Physics-Inspired Graph Neural\n  Network Approach",
        "url": "http://arxiv.org/abs/2402.14802v1",
        "pub_date": "2024-02-22",
        "summary": "In the past years, Graph Neural Networks (GNNs) have become the `de facto'\nstandard in various deep learning domains, thanks to their flexibility in\nmodeling real-world phenomena represented as graphs. However, the\nmessage-passing mechanism of GNNs faces challenges in learnability and\nexpressivity, hindering high performance on heterophilic graphs, where adjacent\nnodes frequently have different labels. Most existing solutions addressing\nthese challenges are primarily confined to specific benchmarks focused on node\nclassification tasks. This narrow focus restricts the potential impact that\nlink prediction under heterophily could offer in several applications,\nincluding recommender systems. For example, in social networks, two users may\nbe connected for some latent reason, making it challenging to predict such\nconnections in advance. Physics-Inspired GNNs such as GRAFF provided a\nsignificant contribution to enhance node classification performance under\nheterophily, thanks to the adoption of physics biases in the message-passing.\nDrawing inspiration from these findings, we advocate that the methodology\nemployed by GRAFF can improve link prediction performance as well. To further\nexplore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link\nprediction. We evaluate its efficacy within a recent collection of heterophilic\ngraphs, establishing a new benchmark for link prediction under heterophily. Our\napproach surpasses previous methods, in most of the datasets, showcasing a\nstrong flexibility in different contexts, and achieving relative AUROC\nimprovements of up to 26.7%.",
        "translated": ""
    },
    {
        "title": "IEPile: Unearthing Large-Scale Schema-Based Information Extraction\n  Corpus",
        "url": "http://arxiv.org/abs/2402.14710v1",
        "pub_date": "2024-02-22",
        "summary": "Large Language Models (LLMs) demonstrate remarkable potential across various\ndomains; however, they exhibit a significant performance gap in Information\nExtraction (IE). Note that high-quality instruction data is the vital key for\nenhancing the specific capabilities of LLMs, while current IE datasets tend to\nbe small in scale, fragmented, and lack standardized schema. To this end, we\nintroduce IEPile, a comprehensive bilingual (English and Chinese) IE\ninstruction corpus, which contains approximately 0.32B tokens. We construct\nIEPile by collecting and cleaning 33 existing IE datasets, and introduce\nschema-based instruction generation to unearth a large-scale corpus.\nExperimental results on LLaMA and Baichuan demonstrate that using IEPile can\nenhance the performance of LLMs for IE, especially the zero-shot\ngeneralization. We open-source the resource and pre-trained models, hoping to\nprovide valuable support to the NLP community.",
        "translated": ""
    },
    {
        "title": "From Keywords to Structured Summaries: Streamlining Scholarly Knowledge\n  Access",
        "url": "http://arxiv.org/abs/2402.14622v1",
        "pub_date": "2024-02-22",
        "summary": "This short paper highlights the growing importance of information retrieval\n(IR) engines in the scientific community, addressing the inefficiency of\ntraditional keyword-based search engines due to the rising volume of\npublications. The proposed solution involves structured records, underpinning\nadvanced information technology (IT) tools, including visualization dashboards,\nto revolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the ``reproductive number estimate of infectious diseases''\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation IR method accessible at\nhttps://orkg.org/usecases/r0-estimates.",
        "translated": ""
    },
    {
        "title": "Transforming Norm-based To Graph-based Spatial Representation for\n  Spatio-Temporal Epidemiological Models",
        "url": "http://arxiv.org/abs/2402.14539v1",
        "pub_date": "2024-02-22",
        "summary": "Pandemics, with their profound societal and economic impacts, pose\nsignificant threats to global health, mortality rates, economic stability, and\npolitical landscapes. In response to the persistent challenges posed by\nemerging and reemerging pandemics, numerous studies have employed\nspatio-temporal models to enhance our understanding and management of these\ncomplex phenomena. These spatio-temporal models can be roughly divided into two\nmain spatial categories: norm-based and graph-based trade-offering between\naccuracy, computational burden, and representational feasibility. In this\nstudy, we explore the ability to transform from norm-based to graph-based\nspatial representation for these models. We introduce a novel framework for\nthis task together with twelve possible implementations using a wide range of\nheuristic optimization approaches. Our findings show that by leveraging\nagent-based simulations and heuristic algorithms for the graph node's location\nand population's spatial walk dynamics approximation one can use graph-based\nspatial representation without losing much of the model's accuracy and\nexpressiveness. For three real-world cases, the best-performing algorithmic\nconfiguration archives 94\\% accuracy presence, on average. Moreover, an\nanalysis of synthetic cases shows the proposed framework is relatively robust,\nas fluctuation in both spatial and temporal dynamics is not badly reflected by\nthe framework's performance.",
        "translated": ""
    },
    {
        "title": "Personalized Behavior-Aware Transformer for Multi-Behavior Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2402.14473v1",
        "pub_date": "2024-02-22",
        "summary": "Sequential Recommendation (SR) captures users' dynamic preferences by\nmodeling how users transit among items. However, SR models that utilize only\nsingle type of behavior interaction data encounter performance degradation when\nthe sequences are short. To tackle this problem, we focus on Multi-Behavior\nSequential Recommendation (MBSR) in this paper, which aims to leverage\ntime-evolving heterogeneous behavioral dependencies for better exploring users'\npotential intents on the target behavior. Solving MBSR is challenging. On the\none hand, users exhibit diverse multi-behavior patterns due to personal\ncharacteristics. On the other hand, there exists comprehensive co-influence\nbetween behavior correlations and item collaborations, the intensity of which\nis deeply affected by temporal factors. To tackle these challenges, we propose\na Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem,\nwhich models personalized patterns and multifaceted sequential collaborations\nin a novel way to boost recommendation performance. First, PBAT develops a\npersonalized behavior pattern generator in the representation layer, which\nextracts dynamic and discriminative behavior patterns for sequential learning.\nSecond, PBAT reforms the self-attention layer with a behavior-aware\ncollaboration extractor, which introduces a fused behavior-aware attention\nmechanism for incorporating both behavioral and temporal impacts into\ncollaborative transitions. We conduct experiments on three benchmark datasets\nand the results demonstrate the effectiveness and interpretability of our\nframework. Our implementation code is released at\nhttps://github.com/TiliaceaeSU/PBAT.",
        "translated": ""
    },
    {
        "title": "Recommender for Its Purpose: Repeat and Exploration in Food Delivery\n  Recommendations",
        "url": "http://arxiv.org/abs/2402.14440v1",
        "pub_date": "2024-02-22",
        "summary": "Recommender systems have been widely used for various scenarios, such as\ne-commerce, news, and music, providing online contents to help and enrich\nusers' daily life. Different scenarios hold distinct and unique\ncharacteristics, calling for domain-specific investigations and corresponding\ndesigned recommender systems. Therefore, in this paper, we focus on food\ndelivery recommendations to unveil unique features in this domain, where users\norder food online and enjoy their meals shortly after delivery. We first\nconduct an in-depth analysis on food delivery datasets. The analysis shows that\nrepeat orders are prevalent for both users and stores, and situations'\ndifferently influence repeat and exploration consumption in the food delivery\nrecommender systems. Moreover, we revisit the ability of existing\nsituation-aware methods for repeat and exploration recommendations\nrespectively, and find them unable to effectively solve both tasks\nsimultaneously. Based on the analysis and experiments, we have designed two\nseparate recommendation models -- ReRec for repeat orders and ExpRec for\nexploration orders; both are simple in their design and computation. We conduct\nexperiments on three real-world food delivery datasets, and our proposed models\noutperform various types of baselines on repeat, exploration, and combined\nrecommendation tasks. This paper emphasizes the importance of dedicated\nanalyses and methods for domain-specific characteristics for the recommender\nsystem studies.",
        "translated": ""
    },
    {
        "title": "Tug-of-War Between Knowledge: Exploring and Resolving Knowledge\n  Conflicts in Retrieval-Augmented Language Models",
        "url": "http://arxiv.org/abs/2402.14409v1",
        "pub_date": "2024-02-22",
        "summary": "Retrieval-augmented language models (RALMs) have demonstrated significant\npotential in refining and expanding their internal memory by retrieving\nevidence from external sources. However, RALMs will inevitably encounter\nknowledge conflicts when integrating their internal memory with external\nsources. Knowledge conflicts can ensnare RALMs in a tug-of-war between\nknowledge, limiting their practical applicability. In this paper, we focus on\nexploring and resolving knowledge conflicts in RALMs. First, we present an\nevaluation framework for assessing knowledge conflicts across various\ndimensions. Then, we investigate the behavior and preference of RALMs from the\nfollowing two perspectives: (1) Conflicts between internal memory and external\nsources: We find that stronger RALMs emerge with the Dunning-Kruger effect,\npersistently favoring their faulty internal memory even when correct evidence\nis provided. Besides, RALMs exhibit an availability bias towards common\nknowledge; (2) Conflicts between truthful, irrelevant and misleading evidence:\nWe reveal that RALMs follow the principle of majority rule, leaning towards\nplacing trust in evidence that appears more frequently. Moreover, we find that\nRALMs exhibit confirmation bias, and are more willing to choose evidence that\nis consistent with their internal memory. To solve the challenge of knowledge\nconflicts, we propose a method called Conflict-Disentangle Contrastive Decoding\n(CD2) to better calibrate the model's confidence. Experimental results\ndemonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.",
        "translated": ""
    },
    {
        "title": "Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream\n  Paradigm for Live Streaming Recommendation",
        "url": "http://arxiv.org/abs/2402.14399v1",
        "pub_date": "2024-02-22",
        "summary": "Live streaming recommender system is specifically designed to recommend\nreal-time live streaming of interest to users. Due to the dynamic changes of\nlive content, improving the timeliness of the live streaming recommender system\nis a critical problem. Intuitively, the timeliness of the data determines the\nupper bound of the timeliness that models can learn. However, none of the\nprevious works addresses the timeliness problem of the live streaming\nrecommender system from the perspective of data stream design. Employing the\nconventional fixed window data stream paradigm introduces a trade-off dilemma\nbetween labeling accuracy and timeliness. In this paper, we propose a new data\nstream design paradigm, dubbed Sliver, that addresses the timeliness and\naccuracy problem of labels by reducing the window size and implementing a\nsliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco\nstrategy reducing the latency between request and impression to improve the\ntimeliness of the recommendation service and features by periodically\nrequesting the recommendation service. To demonstrate the effectiveness of our\napproach, we conduct offline experiments on a multi-task live streaming dataset\nwith labeling timestamps collected from the Kuaishou live streaming platform.\nExperimental results demonstrate that Sliver outperforms two fixed-window data\nstreams with varying window sizes across all targets in four typical multi-task\nrecommendation models. Furthermore, we deployed Sliver on the Kuaishou live\nstreaming platform. Results of the online A/B test show a significant\nimprovement in click-through rate (CTR), and new follow number (NFN), further\nvalidating the effectiveness of Sliver.",
        "translated": ""
    },
    {
        "title": "Scalable and Provably Fair Exposure Control for Large-Scale Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2402.14369v1",
        "pub_date": "2024-02-22",
        "summary": "Typical recommendation and ranking methods aim to optimize the satisfaction\nof users, but they are often oblivious to their impact on the items (e.g.,\nproducts, jobs, news, video) and their providers. However, there has been a\ngrowing understanding that the latter is crucial to consider for a wide range\nof applications, since it determines the utility of those being recommended.\nPrior approaches to fairness-aware recommendation optimize a regularized\nobjective to balance user satisfaction and item fairness based on some notion\nsuch as exposure fairness. These existing methods have been shown to be\neffective in controlling fairness, however, most of them are computationally\ninefficient, limiting their applications to only unrealistically small-scale\nsituations. This indeed implies that the literature does not yet provide a\nsolution to enable a flexible control of exposure in the industry-scale\nrecommender systems where millions of users and items exist. To enable a\ncomputationally efficient exposure control even for such large-scale systems,\nthis work develops a scalable, fast, and fair method called\n\\emph{\\textbf{ex}posure-aware \\textbf{ADMM} (\\textbf{exADMM})}. exADMM is based\non implicit alternating least squares (iALS), a conventional scalable algorithm\nfor collaborative filtering, but optimizes a regularized objective to achieve a\nflexible control of accuracy-fairness tradeoff. A particular technical\nchallenge in developing exADMM is the fact that the fairness regularizer\ndestroys the separability of optimization subproblems for users and items,\nwhich is an essential property to ensure the scalability of iALS. Therefore, we\ndevelop a set of optimization tools to enable yet scalable fairness control\nwith provable convergence guarantees as a basis of our algorithm.",
        "translated": ""
    },
    {
        "title": "Towards Efficient Pareto-optimal Utility-Fairness between Groups in\n  Repeated Rankings",
        "url": "http://arxiv.org/abs/2402.14305v1",
        "pub_date": "2024-02-22",
        "summary": "In this paper, we tackle the problem of computing a sequence of rankings with\nthe guarantee of the Pareto-optimal balance between (1) maximizing the utility\nof the consumers and (2) minimizing unfairness between producers of the items.\nSuch a multi-objective optimization problem is typically solved using a\ncombination of a scalarization method and linear programming on bi-stochastic\nmatrices, representing the distribution of possible rankings of items. However,\nthe above-mentioned approach relies on Birkhoff-von Neumann (BvN)\ndecomposition, of which the computational complexity is $\\mathcal{O}(n^5)$ with\n$n$ being the number of items, making it impractical for large-scale systems.\nTo address this drawback, we introduce a novel approach to the above problem by\nusing the Expohedron - a permutahedron whose points represent all achievable\nexposures of items. On the Expohedron, we profile the Pareto curve which\ncaptures the trade-off between group fairness and user utility by identifying a\nfinite number of Pareto optimal solutions. We further propose an efficient\nmethod by relaxing our optimization problem on the Expohedron's circumscribed\n$n$-sphere, which significantly improve the running time. Moreover, the\napproximate Pareto curve is asymptotically close to the real Pareto optimal\ncurve as the number of substantial solutions increases. Our methods are\napplicable with different ranking merits that are non-decreasing functions of\nitem relevance. The effectiveness of our methods are validated through\nexperiments on both synthetic and real-world datasets.",
        "translated": ""
    },
    {
        "title": "Faithful Temporal Question Answering over Heterogeneous Sources",
        "url": "http://arxiv.org/abs/2402.15400v1",
        "pub_date": "2024-02-23",
        "summary": "Temporal question answering (QA) involves time constraints, with phrases such\nas \"... in 2019\" or \"... before COVID\". In the former, time is an explicit\ncondition, in the latter it is implicit. State-of-the-art methods have\nlimitations along three dimensions. First, with neural inference, time\nconstraints are merely soft-matched, giving room to invalid or inexplicable\nanswers. Second, questions with implicit time are poorly supported. Third,\nanswers come from a single source: either a knowledge base (KB) or a text\ncorpus. We propose a temporal QA system that addresses these shortcomings.\nFirst, it enforces temporal constraints for faithful answering with tangible\nevidence. Second, it properly handles implicit questions. Third, it operates\nover heterogeneous sources, covering KB, text and web tables in a unified\nmanner. The method has three stages: (i) understanding the question and its\ntemporal conditions, (ii) retrieving evidence from all sources, and (iii)\nfaithfully answering the question. As implicit questions are sparse in prior\nbenchmarks, we introduce a principled method for generating diverse questions.\nExperiments show superior performance over a suite of baselines.",
        "translated": ""
    },
    {
        "title": "Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale\n  Libraries",
        "url": "http://arxiv.org/abs/2402.15276v1",
        "pub_date": "2024-02-23",
        "summary": "Text-to-image retrieval plays a crucial role across various applications,\nincluding digital libraries, e-commerce platforms, and multimedia databases, by\nenabling the search for images using text queries. Despite the advancements in\nMultimodal Large Language Models (MLLMs), which offer leading-edge performance,\ntheir applicability in large-scale, varied, and ambiguous retrieval scenarios\nis constrained by significant computational demands and the generation of\ninjective embeddings. This paper introduces the Text2Pic Swift framework,\ntailored for efficient and robust retrieval of images corresponding to\nextensive textual descriptions in sizable datasets. The framework employs a\ntwo-tier approach: the initial Entity-based Ranking (ER) stage addresses the\nambiguity inherent in lengthy text queries through a\nmultiple-queries-to-multiple-targets strategy, effectively narrowing down\npotential candidates for subsequent analysis. Following this, the Summary-based\nRe-ranking (SR) stage further refines these selections based on concise query\nsummaries. Additionally, we present a novel Decoupling-BEiT-3 encoder,\nspecifically designed to tackle the challenges of ambiguous queries and to\nfacilitate both stages of the retrieval process, thereby significantly\nimproving computational efficiency via vector-based similarity assessments. Our\nevaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift\noutperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000,\nalongside reductions in training and retrieval durations by 68.75% and 99.79%,\nrespectively.",
        "translated": ""
    },
    {
        "title": "Countries pushing the boundaries of knowledge: the US dominance, China\n  rise, and the EU stagnation",
        "url": "http://arxiv.org/abs/2402.15263v1",
        "pub_date": "2024-02-23",
        "summary": "Knowing which countries contribute the most to pushing the boundaries of\nknowledge in science and technology has social and political importance.\nHowever, common citation metrics do not adequately measure this contribution.\nThis measure requires more stringent metrics appropriate for the highly\ninfluential breakthrough papers that push the boundaries of knowledge, which\nare very highly cited but very rare. Here I used the recently described Rk\nindex, specifically designed to address this issue. I applied this index to 25\ncountries and the EU across 10 key research topics, five technological and five\nbiomedical, studying domestic and international collaborative papers\nindependently. In technological topics, the Rk indices of domestic papers show\nthat overall, the USA, China, and the EU are leaders; other countries are\nclearly behind. The USA is notably ahead of China, and the EU is far behind\nChina. The same approach to biomedical topics shows an overwhelming dominance\nof the USA and that the EU is ahead of China. The analysis of internationally\ncollaborative papers further demonstrates the US dominance. These results\nconflict with current country rankings based on less stringent indicators.",
        "translated": ""
    },
    {
        "title": "Multi-Agent Collaboration Framework for Recommender Systems",
        "url": "http://arxiv.org/abs/2402.15235v1",
        "pub_date": "2024-02-23",
        "summary": "LLM-based agents have gained considerable attention for their decision-making\nskills and ability to handle complex tasks. Recognizing the current gap in\nleveraging agent capabilities for multi-agent collaboration in recommendation\nsystems, we introduce MACRec, a novel framework designed to enhance\nrecommendation systems through multi-agent collaboration. Unlike existing work\non using agents for user/item simulation, we aim to deploy multi-agents to\ntackle recommendation tasks directly. In our framework, recommendation tasks\nare addressed through the collaborative efforts of various specialized agents,\nincluding Manager, User/Item Analyst, Reflector, Searcher, and Task\nInterpreter, with different working flows. Furthermore, we provide application\nexamples of how developers can easily use MACRec on various recommendation\ntasks, including rating prediction, sequential recommendation, conversational\nrecommendation, and explanation generation of recommendation results. The\nframework and demonstration video are publicly available at\nhttps://github.com/wzf2000/MACRec.",
        "translated": ""
    },
    {
        "title": "Item-side Fairness of Large Language Model-based Recommendation System",
        "url": "http://arxiv.org/abs/2402.15215v1",
        "pub_date": "2024-02-23",
        "summary": "Recommendation systems for Web content distribution intricately connect to\nthe information access and exposure opportunities for vulnerable populations.\nThe emergence of Large Language Models-based Recommendation System (LRS) may\nintroduce additional societal challenges to recommendation systems due to the\ninherent biases in Large Language Models (LLMs). From the perspective of\nitem-side fairness, there remains a lack of comprehensive investigation into\nthe item-side fairness of LRS given the unique characteristics of LRS compared\nto conventional recommendation systems. To bridge this gap, this study examines\nthe property of LRS with respect to item-side fairness and reveals the\ninfluencing factors of both historical users' interactions and inherent\nsemantic biases of LLMs, shedding light on the need to extend conventional\nitem-side fairness methods for LRS. Towards this goal, we develop a concise and\neffective framework called IFairLRS to enhance the item-side fairness of an\nLRS. IFairLRS covers the main stages of building an LRS with specifically\nadapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS\nto fine-tune LLaMA, a representative LLM, on \\textit{MovieLens} and\n\\textit{Steam} datasets, and observe significant item-side fairness\nimprovements. The code can be found in\nhttps://github.com/JiangM-C/IFairLRS.git.",
        "translated": ""
    },
    {
        "title": "EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning\n  Based Recommender Systems",
        "url": "http://arxiv.org/abs/2402.15164v1",
        "pub_date": "2024-02-23",
        "summary": "Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly\nrecognized for their ability to improve long-term user engagement. Yet, the\nfield grapples with challenges such as the absence of accessible frameworks,\ninconsistent evaluation standards, and the complexity of replicating prior\nwork. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and\nefficient library tailored for RL-based RSs. EasyRL4Rec features lightweight,\ndiverse RL environments built on five widely-used public datasets, and is\nequipped with comprehensive core modules that offer rich options to ease the\ndevelopment of models. It establishes consistent evaluation criteria with a\nfocus on long-term impacts and introduces customized solutions for state\nmodeling and action representation tailored to recommender systems.\nAdditionally, we share valuable insights gained from extensive experiments with\ncurrent methods. EasyRL4Rec aims to facilitate the model development and\nexperimental process in the domain of RL-based RSs. The library is openly\naccessible at https://github.com/chongminggao/EasyRL4Rec.",
        "translated": ""
    },
    {
        "title": "Trajectory-wise Iterative Reinforcement Learning Framework for\n  Auto-bidding",
        "url": "http://arxiv.org/abs/2402.15102v1",
        "pub_date": "2024-02-23",
        "summary": "In online advertising, advertisers participate in ad auctions to acquire ad\nopportunities, often by utilizing auto-bidding tools provided by demand-side\nplatforms (DSPs). The current auto-bidding algorithms typically employ\nreinforcement learning (RL). However, due to safety concerns, most RL-based\nauto-bidding policies are trained in simulation, leading to a performance\ndegradation when deployed in online environments. To narrow this gap, we can\ndeploy multiple auto-bidding agents in parallel to collect a large interaction\ndataset. Offline RL algorithms can then be utilized to train a new policy. The\ntrained policy can subsequently be deployed for further data collection,\nresulting in an iterative training framework, which we refer to as iterative\noffline RL. In this work, we identify the performance bottleneck of this\niterative offline RL framework, which originates from the ineffective\nexploration and exploitation caused by the inherent conservatism of offline RL\nalgorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration\nand Exploitation (TEE), which introduces a novel data collecting and data\nutilization method for iterative offline RL from a trajectory perspective.\nFurthermore, to ensure the safety of online exploration while preserving the\ndataset quality for TEE, we propose Safe Exploration by Adaptive Action\nSelection (SEAS). Both offline experiments and real-world experiments on\nAlibaba display advertising platform demonstrate the effectiveness of our\nproposed method.",
        "translated": ""
    },
    {
        "title": "ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot\n  Multilingual Information Retrieval",
        "url": "http://arxiv.org/abs/2402.15059v1",
        "pub_date": "2024-02-23",
        "summary": "State-of-the-art neural retrievers predominantly focus on high-resource\nlanguages like English, which impedes their adoption in retrieval scenarios\ninvolving other languages. Current approaches circumvent the lack of\nhigh-quality labeled data in non-English languages by leveraging multilingual\npretrained language models capable of cross-lingual transfer. However, these\nmodels require substantial task-specific fine-tuning across multiple languages,\noften perform poorly in languages with minimal representation in the\npretraining corpus, and struggle to incorporate new languages after the\npretraining phase. In this work, we present a novel modular dense retrieval\nmodel that learns from the rich data of a single high-resource language and\neffectively zero-shot transfers to a wide array of languages, thereby\neliminating the need for language-specific labeled data. Our model, ColBERT-XM,\ndemonstrates competitive performance against existing state-of-the-art\nmultilingual retrievers trained on more extensive datasets in various\nlanguages. Further analysis reveals that our modular approach is highly\ndata-efficient, effectively adapts to out-of-distribution data, and\nsignificantly reduces energy consumption and carbon emissions. By demonstrating\nits proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more\nsustainable and inclusive retrieval systems, enabling effective information\naccessibility in numerous languages. We publicly release our code and models\nfor the community.",
        "translated": ""
    },
    {
        "title": "Filter Bubble or Homogenization? Disentangling the Long-Term Effects of\n  Recommendations on User Consumption Patterns",
        "url": "http://arxiv.org/abs/2402.15013v1",
        "pub_date": "2024-02-22",
        "summary": "Recommendation algorithms play a pivotal role in shaping our media choices,\nwhich makes it crucial to comprehend their long-term impact on user behavior.\nThese algorithms are often linked to two critical outcomes: homogenization,\nwherein users consume similar content despite disparate underlying preferences,\nand the filter bubble effect, wherein individuals with differing preferences\nonly consume content aligned with their preferences (without much overlap with\nother users). Prior research assumes a trade-off between homogenization and\nfilter bubble effects and then shows that personalized recommendations mitigate\nfilter bubbles by fostering homogenization. However, because of this assumption\nof a tradeoff between these two effects, prior work cannot develop a more\nnuanced view of how recommendation systems may independently impact\nhomogenization and filter bubble effects. We develop a more refined definition\nof homogenization and the filter bubble effect by decomposing them into two key\nmetrics: how different the average consumption is between users (inter-user\ndiversity) and how varied an individual's consumption is (intra-user\ndiversity). We then use a novel agent-based simulation framework that enables a\nholistic view of the impact of recommendation systems on homogenization and\nfilter bubble effects. Our simulations show that traditional recommendation\nalgorithms (based on past behavior) mainly reduce filter bubbles by affecting\ninter-user diversity without significantly impacting intra-user diversity.\nBuilding on these findings, we introduce two new recommendation algorithms that\ntake a more nuanced approach by accounting for both types of diversity.",
        "translated": ""
    },
    {
        "title": "Pre-training Cross-lingual Open Domain Question Answering with\n  Large-scale Synthetic Supervision",
        "url": "http://arxiv.org/abs/2402.16508v1",
        "pub_date": "2024-02-26",
        "summary": "Cross-lingual question answering (CLQA) is a complex problem, comprising\ncross-lingual retrieval from a multilingual knowledge base, followed by answer\ngeneration either in English or the query language. Both steps are usually\ntackled by separate models, requiring substantial annotated datasets, and\ntypically auxiliary resources, like machine translation systems to bridge\nbetween languages. In this paper, we show that CLQA can be addressed using a\nsingle encoder-decoder model. To effectively train this model, we propose a\nself-supervised method based on exploiting the cross-lingual link structure\nwithin Wikipedia. We demonstrate how linked Wikipedia pages can be used to\nsynthesise supervisory signals for cross-lingual retrieval, through a form of\ncloze query, and generate more natural queries to supervise answer generation.\nTogether, we show our approach, \\texttt{CLASS}, outperforms comparable methods\non both supervised and zero-shot language adaptation settings, including those\nusing machine translation.",
        "translated": ""
    },
    {
        "title": "Retrouver l'inventeur-auteur : la lev{é}e d'homonymies d'autorat entre\n  les brevets et les publications scientifiques",
        "url": "http://arxiv.org/abs/2402.16440v1",
        "pub_date": "2024-02-26",
        "summary": "Patents and scientific papers provide an essential source for measuring\nscience and technology output, to be used as a basis for the most varied\nscientometric analyzes. Authors' and inventors' names are the key identifiers\nto carry out these analyses, which however, run up against the issue of\ndisambiguation. By extension identifying inventors who are also academic\nauthors is a non-trivial challenge. We propose a method using the International\nPatent Classification (IPC) and the IPCCAT API to assess the degree of\nsimilarity of patents and papers abstracts of a given inventor, in order to\nmatch both types of documents. The method is developed and manually qualified\nbased on three corpora of patents extracted from the international EPO database\nEspacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501\nauthors. The proposed algorithm solves the general problem of disambiguation\nwith an error rate lower than 5%.",
        "translated": ""
    },
    {
        "title": "Effect of utterance duration and phonetic content on speaker\n  identification using second-order statistical methods",
        "url": "http://arxiv.org/abs/2402.16429v1",
        "pub_date": "2024-02-26",
        "summary": "Second-order statistical methods show very good results for automatic speaker\nidentification in controlled recording conditions. These approaches are\ngenerally used on the entire speech material available. In this paper, we study\nthe influence of the content of the test speech material on the performances of\nsuch methods, i.e. under a more analytical approach. The goal is to investigate\non the kind of information which is used by these methods, and where it is\nlocated in the speech signal. Liquids and glides together, vowels, and more\nparticularly nasal vowels and nasal consonants, are found to be particularly\nspeaker specific: test utterances of 1 second, composed in majority of acoustic\nmaterial from one of these classes provide better speaker identification\nresults than phonetically balanced test utterances, even though the training is\ndone, in both cases, with 15 seconds of phonetically balanced speech.\nNevertheless, results with other phoneme classes are never dramatically poor.\nThese results tend to show that the speaker-dependent information captured by\nlong-term second-order statistics is consistently common to all phonetic\nclasses, and that the homogeneity of the test material may improve the quality\nof the estimates.",
        "translated": ""
    },
    {
        "title": "An Integrated Data Processing Framework for Pretraining Foundation\n  Models",
        "url": "http://arxiv.org/abs/2402.16358v1",
        "pub_date": "2024-02-26",
        "summary": "The ability of the foundation models heavily relies on large-scale, diverse,\nand high-quality pretraining data. In order to improve data quality,\nresearchers and practitioners often have to manually curate datasets from\ndifference sources and develop dedicated data cleansing pipeline for each data\nrepository. Lacking a unified data processing framework, this process is\nrepetitive and cumbersome. To mitigate this issue, we propose a data processing\nframework that integrates a Processing Module which consists of a series of\noperators at different granularity levels, and an Analyzing Module which\nsupports probing and evaluation of the refined data. The proposed framework is\neasy to use and highly flexible. In this demo paper, we first introduce how to\nuse this framework with some example use cases and then demonstrate its\neffectiveness in improving the data quality with an automated evaluation with\nChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code\nand demonstration videos are accessible on GitHub.",
        "translated": ""
    },
    {
        "title": "Deep Rating Elicitation for New Users in Collaborative Filtering",
        "url": "http://arxiv.org/abs/2402.16327v1",
        "pub_date": "2024-02-26",
        "summary": "Recent recommender systems started to use rating elicitation, which asks new\nusers to rate a small seed itemset for inferring their preferences, to improve\nthe quality of initial recommendations. The key challenge of the rating\nelicitation is to choose the seed items which can best infer the new users'\npreference. This paper proposes a novel end-to-end Deep learning framework for\nRating Elicitation (DRE), that chooses all the seed items at a time with\nconsideration of the non-linear interactions. To this end, it first defines\ncategorical distributions to sample seed items from the entire itemset, then it\ntrains both the categorical distributions and a neural reconstruction network\nto infer users' preferences on the remaining items from CF information of the\nsampled seed items. Through the end-to-end training, the categorical\ndistributions are learned to select the most representative seed items while\nreflecting the complex non-linear interactions. Experimental results show that\nDRE outperforms the state-of-the-art approaches in the recommendation quality\nby accurately inferring the new users' preferences and its seed itemset better\nrepresents the latent space than the seed itemset obtained by the other\nmethods.",
        "translated": ""
    },
    {
        "title": "Confidence Calibration for Recommender Systems and Its Applications",
        "url": "http://arxiv.org/abs/2402.16325v1",
        "pub_date": "2024-02-26",
        "summary": "Despite the importance of having a measure of confidence in recommendation\nresults, it has been surprisingly overlooked in the literature compared to the\naccuracy of the recommendation. In this dissertation, I propose a model\ncalibration framework for recommender systems for estimating accurate\nconfidence in recommendation results based on the learned ranking scores.\nMoreover, I subsequently introduce two real-world applications of confidence on\nrecommendations: (1) Training a small student model by treating the confidence\nof a big teacher model as additional learning guidance, (2) Adjusting the\nnumber of presented items based on the expected user utility estimated with\ncalibrated probability.",
        "translated": ""
    },
    {
        "title": "Top-Personalized-K Recommendation",
        "url": "http://arxiv.org/abs/2402.16304v1",
        "pub_date": "2024-02-26",
        "summary": "The conventional top-K recommendation, which presents the top-K items with\nthe highest ranking scores, is a common practice for generating personalized\nranking lists. However, is this fixed-size top-K recommendation the optimal\napproach for every user's satisfaction? Not necessarily. We point out that\nproviding fixed-size recommendations without taking into account user utility\ncan be suboptimal, as it may unavoidably include irrelevant items or limit the\nexposure to relevant ones. To address this issue, we introduce\nTop-Personalized-K Recommendation, a new recommendation task aimed at\ngenerating a personalized-sized ranking list to maximize individual user\nsatisfaction. As a solution to the proposed task, we develop a model-agnostic\nframework named PerK. PerK estimates the expected user utility by leveraging\ncalibrated interaction probabilities, subsequently selecting the recommendation\nsize that maximizes this expected utility. Through extensive experiments on\nreal-world datasets, we demonstrate the superiority of PerK in\nTop-Personalized-K recommendation task. We expect that Top-Personalized-K\nrecommendation has the potential to offer enhanced solutions for various\nreal-world recommendation scenarios, based on its great compatibility with\nexisting models.",
        "translated": ""
    },
    {
        "title": "Against Filter Bubbles: Diversified Music Recommendation via Weighted\n  Hypergraph Embedding Learning",
        "url": "http://arxiv.org/abs/2402.16299v1",
        "pub_date": "2024-02-26",
        "summary": "Recommender systems serve a dual purpose for users: sifting out inappropriate\nor mismatched information while accurately identifying items that align with\ntheir preferences. Numerous recommendation algorithms are designed to provide\nusers with a personalized array of information tailored to their preferences.\nNevertheless, excessive personalization can confine users within a \"filter\nbubble\". Consequently, achieving the right balance between accuracy and\ndiversity in recommendations is a pressing concern. To address this challenge,\nexemplified by music recommendation, we introduce the Diversified Weighted\nHypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm,\nthe initial connections between users and listened tracks are represented by a\nweighted hypergraph. Simultaneously, associations between artists, albums and\ntags with tracks are also appended to the hypergraph. To explore users' latent\npreferences, a hypergraph-based random walk embedding method is applied to the\nconstructed hypergraph. In our investigation, accuracy is gauged by the\nalignment between the user and the track, whereas the array of recommended\ntrack types measures diversity. We rigorously compared DWHRec against seven\nstate-of-the-art recommendation algorithms using two real-world music datasets.\nThe experimental results validate DWHRec as a solution that adeptly harmonizes\naccuracy and diversity, delivering a more enriched musical experience. Beyond\nmusic recommendation, DWHRec can be extended to cater to other scenarios with\nsimilar data structures.",
        "translated": ""
    },
    {
        "title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification,\n  Retrieval, and Synthesis in Question Answering",
        "url": "http://arxiv.org/abs/2402.16288v1",
        "pub_date": "2024-02-26",
        "summary": "Long-term memory plays a critical role in personal interaction, considering\nlong-term memory can better leverage world knowledge, historical information,\nand preferences in dialogues. Our research introduces PerLTQA, an innovative QA\ndataset that combines semantic and episodic memories, including world\nknowledge, profiles, social relationships, events, and dialogues. This dataset\nis collected to investigate the use of personalized memories, focusing on\nsocial interactions and events in the QA task. PerLTQA features two types of\nmemory and a comprehensive benchmark of 8,593 questions for 30 characters,\nfacilitating the exploration and application of personalized memories in Large\nLanguage Models (LLMs). Based on PerLTQA, we propose a novel framework for\nmemory integration and generation, consisting of three main components: Memory\nClassification, Memory Retrieval, and Memory Synthesis. We evaluate this\nframework using five LLMs and three retrievers. Experimental results\ndemonstrate that BERT-based classification models significantly outperform LLMs\nsuch as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,\nour study highlights the importance of effective memory integration in the QA\ntask.",
        "translated": ""
    },
    {
        "title": "UniRetriever: Multi-task Candidates Selection for Various\n  Context-Adaptive Conversational Retrieval",
        "url": "http://arxiv.org/abs/2402.16261v1",
        "pub_date": "2024-02-26",
        "summary": "Conversational retrieval refers to an information retrieval system that\noperates in an iterative and interactive manner, requiring the retrieval of\nvarious external resources, such as persona, knowledge, and even response, to\neffectively engage with the user and successfully complete the dialogue.\nHowever, most previous work trained independent retrievers for each specific\nresource, resulting in sub-optimal performance and low efficiency. Thus, we\npropose a multi-task framework function as a universal retriever for three\ndominant retrieval tasks during the conversation: persona selection, knowledge\nselection, and response selection. To this end, we design a dual-encoder\narchitecture consisting of a context-adaptive dialogue encoder and a candidate\nencoder, aiming to attention to the relevant context from the long dialogue and\nretrieve suitable candidates by simply a dot product. Furthermore, we introduce\ntwo loss constraints to capture the subtle relationship between dialogue\ncontext and different candidates by regarding historically selected candidates\nas hard negatives. Extensive experiments and analysis establish\nstate-of-the-art retrieval quality both within and outside its training domain,\nrevealing the promising potential and generalization capability of our model to\nserve as a universal retriever for different candidate selection tasks\nsimultaneously.",
        "translated": ""
    },
    {
        "title": "Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control",
        "url": "http://arxiv.org/abs/2402.17535v1",
        "pub_date": "2024-02-27",
        "summary": "Learned sparse retrieval (LSR) is a family of neural methods that encode\nqueries and documents into sparse lexical vectors that can be indexed and\nretrieved efficiently with an inverted index. We explore the application of LSR\nto the multi-modal domain, with a focus on text-image retrieval. While LSR has\nseen success in text retrieval, its application in multimodal retrieval remains\nunderexplored. Current approaches like LexLIP and STAIR require complex\nmulti-step training on massive datasets. Our proposed approach efficiently\ntransforms dense vectors from a frozen dense model into sparse lexical vectors.\nWe address issues of high dimension co-activation and semantic deviation\nthrough a new training algorithm, using Bernoulli random variables to control\nquery expansion. Experiments with two dense models (BLIP, ALBEF) and two\ndatasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively\nreduces co-activation and semantic deviation. Our best-performing sparsified\nmodel outperforms state-of-the-art text-image LSR models with a shorter\ntraining time and lower GPU memory requirements. Our approach offers an\neffective solution for training LSR retrieval models in multimodal settings.\nOur code and model checkpoints are available at\ngithub.com/thongnt99/lsr-multimodal",
        "translated": ""
    },
    {
        "title": "BASES: Large-scale Web Search User Simulation with Large Language Model\n  based Agents",
        "url": "http://arxiv.org/abs/2402.17505v1",
        "pub_date": "2024-02-27",
        "summary": "Due to the excellent capacities of large language models (LLMs), it becomes\nfeasible to develop LLM-based agents for reliable user simulation. Considering\nthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,\nwe conduct large-scale user simulation for web search, to improve the analysis\nand modeling of user search behavior. Specially, we propose BASES, a novel user\nsimulation framework with LLM-based agents, designed to facilitate\ncomprehensive simulations of web search user behaviors. Our simulation\nframework can generate unique user profiles at scale, which subsequently leads\nto diverse search behaviors. To demonstrate the effectiveness of BASES, we\nconduct evaluation experiments based on two human benchmarks in both Chinese\nand English, demonstrating that BASES can effectively simulate large-scale\nhuman-like search behaviors. To further accommodate the research on web search,\nwe develop WARRIORS, a new large-scale dataset encompassing web search user\nbehaviors, including both Chinese and English versions, which can greatly\nbolster research in the field of information retrieval. Our code and data will\nbe publicly released soon.",
        "translated": ""
    },
    {
        "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering",
        "url": "http://arxiv.org/abs/2402.17497v1",
        "pub_date": "2024-02-27",
        "summary": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (i.e., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness of source relevance for LLMs, so as to adaptively utilize\nexternal knowledge in RAG systems. Specially, we develop a new architecture for\nLLM based RAG system, by incorporating a specially designed rank head that\nprecisely assesses the relevance of retrieved documents. Furthermore, we\npropose an improved training method based on bi-granularity relevance fusion\nand noise-resistant training. By combining the improvements in both\narchitecture and training, our proposed REAR can better utilize external\nknowledge by effectively perceiving the relevance of retrieved documents.\nExperiments on four open-domain QA tasks show that REAR significantly\noutperforms previous a number of competitive RAG approaches. Our code and data\ncan be accessed at https://github.com/RUCAIBox/REAR.",
        "translated": ""
    },
    {
        "title": "Natural Language Processing Methods for Symbolic Music Generation and\n  Information Retrieval: a Survey",
        "url": "http://arxiv.org/abs/2402.17467v1",
        "pub_date": "2024-02-27",
        "summary": "Several adaptations of Transformers models have been developed in various\ndomains since its breakthrough in Natural Language Processing (NLP). This trend\nhas spread into the field of Music Information Retrieval (MIR), including\nstudies processing music data. However, the practice of leveraging NLP tools\nfor symbolic music data is not novel in MIR. Music has been frequently compared\nto language, as they share several similarities, including sequential\nrepresentations of text and music. These analogies are also reflected through\nsimilar tasks in MIR and NLP. This survey reviews NLP methods applied to\nsymbolic music generation and information retrieval studies following two axes.\nWe first propose an overview of representations of symbolic music adapted from\nnatural language sequential representations. Such representations are designed\nby considering the specificities of symbolic music. These representations are\nthen processed by models. Such models, possibly originally developed for text\nand adapted for symbolic music, are trained on various tasks. We describe these\nmodels, in particular deep learning models, through different prisms,\nhighlighting music-specialized mechanisms. We finally present a discussion\nsurrounding the effective use of NLP tools for symbolic music data. This\nincludes technical issues regarding NLP methods and fundamental differences\nbetween text and music, which may open several doors for further research into\nmore effectively adapting NLP tools to symbolic MIR.",
        "translated": ""
    },
    {
        "title": "Deep Learning Based Named Entity Recognition Models for Recipes",
        "url": "http://arxiv.org/abs/2402.17447v1",
        "pub_date": "2024-02-27",
        "summary": "Food touches our lives through various endeavors, including flavor,\nnourishment, health, and sustainability. Recipes are cultural capsules\ntransmitted across generations via unstructured text. Automated protocols for\nrecognizing named entities, the building blocks of recipe text, are of immense\nvalue for various applications ranging from information extraction to novel\nrecipe generation. Named entity recognition is a technique for extracting\ninformation from unstructured or semi-structured data with known labels.\nStarting with manually-annotated data of 6,611 ingredient phrases, we created\nan augmented dataset of 26,445 phrases cumulatively. Simultaneously, we\nsystematically cleaned and analyzed ingredient phrases from RecipeDB, the\ngold-standard recipe data repository, and annotated them using the Stanford\nNER. Based on the analysis, we sampled a subset of 88,526 phrases using a\nclustering-based approach while preserving the diversity to create the\nmachine-annotated dataset. A thorough investigation of NER approaches on these\nthree datasets involving statistical, fine-tuning of deep learning-based\nlanguage models and few-shot prompting on large language models (LLMs) provides\ndeep insights. We conclude that few-shot prompting on LLMs has abysmal\nperformance, whereas the fine-tuned spaCy-transformer emerges as the best model\nwith macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,\naugmented, and machine-annotated datasets, respectively.",
        "translated": ""
    },
    {
        "title": "BiVRec: Bidirectional View-based Multimodal Sequential Recommendation",
        "url": "http://arxiv.org/abs/2402.17334v1",
        "pub_date": "2024-02-27",
        "summary": "The integration of multimodal information into sequential recommender systems\nhas attracted significant attention in recent research. In the initial stages\nof multimodal sequential recommendation models, the mainstream paradigm was\nID-dominant recommendations, wherein multimodal information was fused as side\ninformation. However, due to their limitations in terms of transferability and\ninformation intrusion, another paradigm emerged, wherein multimodal features\nwere employed directly for recommendation, enabling recommendation across\ndatasets. Nonetheless, it overlooked user ID information, resulting in low\ninformation utilization and high training costs. To this end, we propose an\ninnovative framework, BivRec, that jointly trains the recommendation tasks in\nboth ID and multimodal views, leveraging their synergistic relationship to\nenhance recommendation performance bidirectionally. To tackle the information\nheterogeneity issue, we first construct structured user interest\nrepresentations and then learn the synergistic relationship between them.\nSpecifically, BivRec comprises three modules: Multi-scale Interest Embedding,\ncomprehensively modeling user interests by expanding user interaction sequences\nwith multi-scale patching; Intra-View Interest Decomposition, constructing\nhighly structured interest representations using carefully designed Gaussian\nattention and Cluster attention; and Cross-View Interest Learning, learning the\nsynergistic relationship between the two recommendation views through\ncoarse-grained overall semantic similarity and fine-grained interest allocation\nsimilarity BiVRec achieves state-of-the-art performance on five datasets and\nshowcases various practical advantages.",
        "translated": ""
    },
    {
        "title": "DiFashion: Towards Personalized Outfit Generation",
        "url": "http://arxiv.org/abs/2402.17279v1",
        "pub_date": "2024-02-27",
        "summary": "The evolution of Outfit Recommendation (OR) in the realm of fashion has\nprogressed through two distinct phases: Pre-defined Outfit Recommendation and\nPersonalized Outfit Composition. Despite these advancements, both phases face\nlimitations imposed by existing fashion products, hindering their effectiveness\nin meeting users' diverse fashion needs. The emergence of AI-generated content\nhas paved the way for OR to overcome these constraints, demonstrating the\npotential for personalized outfit generation.\n  In pursuit of this, we introduce an innovative task named Generative Outfit\nRecommendation (GOR), with the goal of synthesizing a set of fashion images and\nassembling them to form visually harmonious outfits customized to individual\nusers. The primary objectives of GOR revolve around achieving high fidelity,\ncompatibility, and personalization of the generated outfits. To accomplish\nthese, we propose DiFashion, a generative outfit recommender model that\nharnesses exceptional diffusion models for the simultaneous generation of\nmultiple fashion images. To ensure the fulfillment of these objectives, three\ntypes of conditions are designed to guide the parallel generation process and\nClassifier-Free-Guidance are employed to enhance the alignment between\ngenerated images and conditions. DiFashion is applied to both personalized\nFill-In-The-Blank and GOR tasks, and extensive experiments are conducted on the\niFashion and Polyvore-U datasets. The results of quantitative and\nhuman-involved qualitative evaluations highlight the superiority of DiFashion\nover competitive baselines.",
        "translated": ""
    },
    {
        "title": "PromptMM: Multi-Modal Knowledge Distillation for Recommendation with\n  Prompt-Tuning",
        "url": "http://arxiv.org/abs/2402.17188v1",
        "pub_date": "2024-02-27",
        "summary": "Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited\nfrom the incorporation of multimedia (e.g., visual, textual, and acoustic)\ncontent into their personal recommender systems. These modalities provide\nintuitive semantics that facilitate modality-aware user preference modeling.\nHowever, two key challenges in multi-modal recommenders remain unresolved: i)\nThe introduction of multi-modal encoders with a large number of additional\nparameters causes overfitting, given high-dimensional multi-modal features\nprovided by extractors (e.g., ViT, BERT). ii) Side information inevitably\nintroduces inaccuracies and redundancies, which skew the modality-interaction\ndependency from reflecting true user preference. To tackle these problems, we\npropose to simplify and empower recommenders through Multi-modal Knowledge\nDistillation (PromptMM) with the prompt-tuning that enables adaptive quality\ndistillation. Specifically, PromptMM conducts model compression through\ndistilling u-i edge relationship and multi-modal node content from cumbersome\nteachers to relieve students from the additional feature reduction parameters.\nTo bridge the semantic gap between multi-modal context and collaborative\nsignals for empowering the overfitting teacher, soft prompt-tuning is\nintroduced to perform student task-adaptive. Additionally, to adjust the impact\nof inaccuracies in multimedia data, a disentangled multi-modal list-wise\ndistillation is developed with modality-aware re-weighting mechanism.\nExperiments on real-world data demonstrate PromptMM's superiority over existing\ntechniques. Ablation tests confirm the effectiveness of key components.\nAdditional tests show the efficiency and effectiveness.",
        "translated": ""
    },
    {
        "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential\n  Transducers for Generative Recommendations",
        "url": "http://arxiv.org/abs/2402.17152v1",
        "pub_date": "2024-02-27",
        "summary": "Large-scale recommendation systems are characterized by their reliance on\nhigh cardinality, heterogeneous features and the need to handle tens of\nbillions of user actions on a daily basis. Despite being trained on huge volume\nof data with thousands of features, most Deep Learning Recommendation Models\n(DLRMs) in industry fail to scale with compute.\n  Inspired by success achieved by Transformers in language and vision domains,\nwe revisit fundamental design choices in recommendation systems. We reformulate\nrecommendation problems as sequential transduction tasks within a generative\nmodeling framework (``Generative Recommenders''), and propose a new\narchitecture, HSTU, designed for high cardinality, non-stationary streaming\nrecommendation data.\n  HSTU outperforms baselines over synthetic and public datasets by up to 65.8\\%\nin NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on\n8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion\nparameters, improve metrics in online A/B tests by 12.4\\% and have been\ndeployed on multiple surfaces of a large internet platform with billions of\nusers. More importantly, the model quality of Generative Recommenders\nempirically scales as a power-law of training compute across three orders of\nmagnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for\nfuture model developments, and further paves the way for the first foundational\nmodels in recommendations.",
        "translated": ""
    },
    {
        "title": "Side Information-Driven Session-based Recommendation: A Survey",
        "url": "http://arxiv.org/abs/2402.17129v1",
        "pub_date": "2024-02-27",
        "summary": "The session-based recommendation (SBR) garners increasing attention due to\nits ability to predict anonymous user intents within limited interactions.\nEmerging efforts incorporate various kinds of side information into their\nmethods for enhancing task performance. In this survey, we thoroughly review\nthe side information-driven session-based recommendation from a data-centric\nperspective. Our survey commences with an illustration of the motivation and\nnecessity behind this research topic. This is followed by a detailed\nexploration of various benchmarks rich in side information, pivotal for\nadvancing research in this field. Moreover, we delve into how these diverse\ntypes of side information enhance SBR, underscoring their characteristics and\nutility. A systematic review of research progress is then presented, offering\nan analysis of the most recent and representative developments within this\ntopic. Finally, we present the future prospects of this vibrant topic.",
        "translated": ""
    },
    {
        "title": "A Categorization of Complexity Classes for Information Retrieval and\n  Synthesis Using Natural Logic",
        "url": "http://arxiv.org/abs/2402.18566v1",
        "pub_date": "2024-02-28",
        "summary": "Given the emergent reasoning abilities of large language models, information\nretrieval is becoming more complex. Rather than just retrieve a document,\nmodern information retrieval systems advertise that they can synthesize an\nanswer based on potentially many different documents, conflicting data sources,\nand using reasoning. But, different kinds of questions have different answers,\nand different answers have different complexities. In this paper, we introduce\na novel framework for analyzing the complexity of a question answer based on\nthe natural deduction calculus as presented in Prawitz (1965). Our framework is\nnovel both in that no one to our knowledge has used this logic as a basis for\ncomplexity classes, and also in that no other existing complexity classes to\nthese have been delineated using any analogous methods either. We identify\nthree decidable fragments in particular called the forward, query and planning\nfragments, and we compare this to what would be needed to do proofs for the\ncomplete first-order calculus, for which theorem-proving is long known to be\nundecidable.",
        "translated": ""
    },
    {
        "title": "Approaching Human-Level Forecasting with Language Models",
        "url": "http://arxiv.org/abs/2402.18563v1",
        "pub_date": "2024-02-28",
        "summary": "Forecasting future events is important for policy and decision making. In\nthis work, we study whether language models (LMs) can forecast at the level of\ncompetitive human forecasters. Towards this goal, we develop a\nretrieval-augmented LM system designed to automatically search for relevant\ninformation, generate forecasts, and aggregate predictions. To facilitate our\nstudy, we collect a large dataset of questions from competitive forecasting\nplatforms. Under a test set published after the knowledge cut-offs of our LMs,\nwe evaluate the end-to-end performance of our system against the aggregates of\nhuman forecasts. On average, the system nears the crowd aggregate of\ncompetitive forecasters, and in some settings surpasses it. Our work suggests\nthat using LMs to forecast the future could provide accurate predictions at\nscale and help to inform institutional decision making.",
        "translated": ""
    },
    {
        "title": "Graph Regularized Encoder Training for Extreme Classification",
        "url": "http://arxiv.org/abs/2402.18434v1",
        "pub_date": "2024-02-28",
        "summary": "Deep extreme classification (XC) aims to train an encoder architecture and an\naccompanying classifier architecture to tag a data point with the most relevant\nsubset of labels from a very large universe of labels. XC applications in\nranking, recommendation and tagging routinely encounter tail labels for which\nthe amount of training data is exceedingly small. Graph convolutional networks\n(GCN) present a convenient but computationally expensive way to leverage task\nmetadata and enhance model accuracies in these settings. This paper formally\nestablishes that in several use cases, the steep computational cost of GCNs is\nentirely avoidable by replacing GCNs with non-GCN architectures. The paper\nnotices that in these settings, it is much more effective to use graph data to\nregularize encoder training than to implement a GCN. Based on these insights,\nan alternative paradigm RAMEN is presented to utilize graph metadata in XC\nsettings that offers significant performance boosts with zero increase in\ninference computational costs. RAMEN scales to datasets with up to 1M labels\nand offers prediction accuracy up to 15% higher on benchmark datasets than\nstate of the art methods, including those that use graph metadata to train\nGCNs. RAMEN also offers 10% higher accuracy over the best baseline on a\nproprietary recommendation dataset sourced from click logs of a popular search\nengine. Code for RAMEN will be released publicly.",
        "translated": ""
    },
    {
        "title": "DynaWarp -- Efficient, large-scale log storage and retrieval",
        "url": "http://arxiv.org/abs/2402.18355v1",
        "pub_date": "2024-02-28",
        "summary": "Modern, large scale monitoring systems have to process and store vast amounts\nof log data in near real-time. At query time the systems have to find relevant\nlogs based on the content of the log message using support structures that can\nscale to these amounts of data while still being efficient to use. We present\nour novel DynaWarp membership sketch, capable of answering Multi-Set\nMulti-Membership-Queries, that can be used as an alternative to existing\nindexing structures for streamed log data. In our experiments, DynaWarp\nrequired up to 93% less storage space than the tested state-of-the-art inverted\nindex and had up to four orders of magnitude less false-positives than the\ntested state-of-the-art membership sketch. Additionally, DynaWarp achieved up\nto 250 times higher query throughput than the tested inverted index and up to\n240 times higher query throughput than the tested membership sketch.",
        "translated": ""
    },
    {
        "title": "Detecting Anti-vaccine Content on Twitter using Multiple Message-Based\n  Network Representations",
        "url": "http://arxiv.org/abs/2402.18335v1",
        "pub_date": "2024-02-28",
        "summary": "Social media platforms such as Twitter have a fundamental role in\nfacilitating the spread and discussion of ideas online through the concept of\nretweeting and replying. However, these features also contribute to the spread\nof mis/disinformation during the vaccine rollout of the COVID-19 pandemic.\nUsing COVID-19 vaccines as a case study, we analyse multiple social network\nrepresentation derived from three message-based interactions on Twitter (quote\nretweets, mentions and replies) based upon a set of known anti-vax hashtags and\nkeywords. Each network represents a certain hashtag or keyword which were\nlabelled as \"controversial\" and \"non-controversial\" according to a small group\nof participants. For each network, we extract a combination of global and local\nnetwork-based metrics which are used as feature vectors for binary\nclassification. Our results suggest that it is possible to detect controversial\nfrom non-controversial terms with high accuracy using simple network-based\nmetrics. Furthermore, these results demonstrate the potential of network\nrepresentations as language-agnostic models for detecting mis/disinformation at\nscale, irrespective of content and across multiple social media platforms.",
        "translated": ""
    },
    {
        "title": "Prospect Personalized Recommendation on Large Language Model-based Agent\n  Platform",
        "url": "http://arxiv.org/abs/2402.18240v1",
        "pub_date": "2024-02-28",
        "summary": "The new kind of Agent-oriented information system, exemplified by GPTs, urges\nus to inspect the information system infrastructure to support Agent-level\ninformation processing and to adapt to the characteristics of Large Language\nModel (LLM)-based Agents, such as interactivity. In this work, we envisage the\nprospect of the recommender system on LLM-based Agent platforms and introduce a\nnovel recommendation paradigm called Rec4Agentverse, comprised of Agent Items\nand Agent Recommender. Rec4Agentverse emphasizes the collaboration between\nAgent Items and Agent Recommender, thereby promoting personalized information\nservices and enhancing the exchange of information beyond the traditional\nuser-recommender feedback loop. Additionally, we prospect the evolution of\nRec4Agentverse and conceptualize it into three stages based on the enhancement\nof the interaction and information exchange among Agent Items, Agent\nRecommender, and the user. A preliminary study involving several cases of\nRec4Agentverse validates its significant potential for application. Lastly, we\ndiscuss potential issues and promising directions for future research.",
        "translated": ""
    },
    {
        "title": "Sequence-level Semantic Representation Fusion for Recommender Systems",
        "url": "http://arxiv.org/abs/2402.18166v1",
        "pub_date": "2024-02-28",
        "summary": "With the rapid development of recommender systems, there is increasing side\ninformation that can be employed to improve the recommendation performance.\nSpecially, we focus on the utilization of the associated \\emph{textual data} of\nitems (eg product title) and study how text features can be effectively fused\nwith ID features in sequential recommendation. However, there exists distinct\ndata characteristics for the two kinds of item features, making a direct fusion\nmethod (eg adding text and ID embeddings as item representation) become less\neffective. To address this issue, we propose a novel {\\ul \\emph{Te}}xt-I{\\ul\n\\emph{D}} semantic fusion approach for sequential {\\ul \\emph{Rec}}ommendation,\nnamely \\textbf{\\our}. The core idea of our approach is to conduct a\nsequence-level semantic fusion approach by better integrating global contexts.\nThe key strategy lies in that we transform the text embeddings and ID\nembeddings by Fourier Transform from \\emph{time domain} to \\emph{frequency\ndomain}. In the frequency domain, the global sequential characteristics of the\noriginal sequences are inherently aggregated into the transformed\nrepresentations, so that we can employ simple multiplicative operations to\neffectively fuse the two kinds of item features. Our fusion approach can be\nproved to have the same effects of contextual convolution, so as to achieving\nsequence-level semantic fusion. In order to further improve the fusion\nperformance, we propose to enhance the discriminability of the text embeddings\nfrom the text encoder, by adaptively injecting positional information via a\nmixture-of-experts~(MoE) modulation method. Our implementation is available at\nthis repository: \\textcolor{magenta}{\\url{https://github.com/RUCAIBox/TedRec}}.",
        "translated": ""
    },
    {
        "title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and\n  Mitigating Knowledge Conflicts in Language Models",
        "url": "http://arxiv.org/abs/2402.18154v1",
        "pub_date": "2024-02-28",
        "summary": "Recently, retrieval augmentation and tool augmentation have demonstrated a\nremarkable capability to expand the internal memory boundaries of language\nmodels (LMs) by providing external context. However, internal memory and\nexternal context inevitably clash, leading to knowledge conflicts within LMs.\nIn this paper, we aim to interpret the mechanism of knowledge conflicts through\nthe lens of information flow, and then mitigate conflicts by precise\ninterventions at the pivotal point. We find there are some attention heads with\nopposite effects in the later layers, where memory heads can recall knowledge\nfrom internal memory, and context heads can retrieve knowledge from external\ncontext. Moreover, we reveal that the pivotal point at which knowledge\nconflicts emerge in LMs is the integration of inconsistent information flows by\nmemory heads and context heads. Inspired by the insights, we propose a novel\nmethod called Pruning Head via PatH PatcHing (PH3), which can efficiently\nmitigate knowledge conflicts by pruning conflicting attention heads without\nupdating model parameters. PH3 can flexibly control eight LMs to use internal\nmemory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3\ncan also improve the performance of LMs on open-domain QA tasks. We also\nconduct extensive experiments to demonstrate the cross-model, cross-relation,\nand cross-format generalization of our method.",
        "translated": ""
    },
    {
        "title": "Unsupervised Information Refinement Training of Large Language Models\n  for Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2402.18150v1",
        "pub_date": "2024-02-28",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.",
        "translated": ""
    },
    {
        "title": "Corpus-Steered Query Expansion with Large Language Models",
        "url": "http://arxiv.org/abs/2402.18031v1",
        "pub_date": "2024-02-28",
        "summary": "Recent studies demonstrate that query expansions generated by large language\nmodels (LLMs) can considerably enhance information retrieval systems by\ngenerating hypothetical documents that answer the queries as expansions.\nHowever, challenges arise from misalignments between the expansions and the\nretrieval corpus, resulting in issues like hallucinations and outdated\ninformation due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo\nRelevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to\npromote the incorporation of knowledge embedded within the corpus. CSQE\nutilizes the relevance assessing capability of LLMs to systematically identify\npivotal sentences in the initially-retrieved documents. These corpus-originated\ntexts are subsequently used to expand the query together with LLM-knowledge\nempowered expansions, improving the relevance prediction between the query and\nthe target documents. Extensive experiments reveal that CSQE exhibits strong\nperformance without necessitating any training, especially with queries for\nwhich LLMs lack knowledge.",
        "translated": ""
    },
    {
        "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based\n  Search Engines",
        "url": "http://arxiv.org/abs/2402.19421v1",
        "pub_date": "2024-02-29",
        "summary": "In the domain of digital information dissemination, search engines act as\npivotal conduits linking information seekers with providers. The advent of\nchat-based search engines utilizing Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary\nleap in the search ecosystem. They demonstrate metacognitive abilities in\ninterpreting web information and crafting responses with human-like\nunderstanding and creativity. Nonetheless, the intricate nature of LLMs renders\ntheir \"cognitive\" processes opaque, challenging even their designers'\nunderstanding. This research aims to dissect the mechanisms through which an\nLLM-powered chat-based search engine, specifically Bing Chat, selects\ninformation sources for its responses. To this end, an extensive dataset has\nbeen compiled through engagements with New Bing, documenting the websites it\ncites alongside those listed by the conventional search engine. Employing\nnatural language processing (NLP) techniques, the research reveals that Bing\nChat exhibits a preference for content that is not only readable and formally\nstructured, but also demonstrates lower perplexity levels, indicating a unique\ninclination towards text that is predictable by the underlying LLM. Further\nenriching our analysis, we procure an additional dataset through interactions\nwith the GPT-4 based knowledge retrieval API, unveiling a congruent text\npreference between the RAG API and Bing Chat. This consensus suggests that\nthese text preferences intrinsically emerge from the underlying language\nmodels, rather than being explicitly crafted by Bing Chat's developers.\nMoreover, our investigation documents a greater similarity among websites cited\nby RAG technologies compared to those ranked highest by conventional search\nengines.",
        "translated": ""
    },
    {
        "title": "PaECTER: Patent-level Representation Learning using Citation-informed\n  Transformers",
        "url": "http://arxiv.org/abs/2402.19411v1",
        "pub_date": "2024-02-29",
        "summary": "PaECTER is a publicly available, open-source document-level encoder specific\nfor patents. We fine-tune BERT for Patents with examiner-added citation\ninformation to generate numerical representations for patent documents. PaECTER\nperforms better in similarity tasks than current state-of-the-art models used\nin the patent domain. More specifically, our model outperforms the next-best\npatent specific pre-trained language model (BERT for Patents) on our patent\ncitation prediction test dataset on two different rank evaluation metrics.\nPaECTER predicts at least one most similar patent at a rank of 1.32 on average\nwhen compared against 25 irrelevant patents. Numerical representations\ngenerated by PaECTER from patent text can be used for downstream tasks such as\nclassification, tracing knowledge flows, or semantic similarity search.\nSemantic similarity search is especially relevant in the context of prior art\nsearch for both inventors and patent examiners. PaECTER is available on Hugging\nFace.",
        "translated": ""
    },
    {
        "title": "MENTOR: Multi-level Self-supervised Learning for Multimodal\n  Recommendation",
        "url": "http://arxiv.org/abs/2402.19407v1",
        "pub_date": "2024-02-29",
        "summary": "With the increasing multimedia information, multimodal recommendation has\nreceived extensive attention. It utilizes multimodal information to alleviate\nthe data sparsity problem in recommendation systems, thus improving\nrecommendation accuracy. However, the reliance on labeled data severely limits\nthe performance of multimodal recommendation models. Recently, self-supervised\nlearning has been used in multimodal recommendations to mitigate the label\nsparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the\nmodality noise when aligning multimodal information due to the large\ndifferences in the distributions of different modalities. To this end, we\npropose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation\n(MENTOR) method to address the label sparsity problem and the modality\nalignment problem. Specifically, MENTOR first enhances the specific features of\neach modality using the graph convolutional network (GCN) and fuses the visual\nand textual modalities. It then enhances the item representation via the item\nsemantic graph for all modalities, including the fused modality. Then, it\nintroduces two multilevel self-supervised tasks: the multilevel cross-modal\nalignment task and the general feature enhancement task. The multilevel\ncross-modal alignment task aligns each modality under the guidance of the ID\nembedding from multiple levels while maintaining the historical interaction\ninformation. The general feature enhancement task enhances the general feature\nfrom both the graph and feature perspectives to improve the robustness of our\nmodel. Extensive experiments on three publicly available datasets demonstrate\nthe effectiveness of our method. Our code is publicly available at\nhttps://github.com/Jinfeng-Xu/MENTOR.",
        "translated": ""
    },
    {
        "title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical\n  question-answering with open-source large language models",
        "url": "http://arxiv.org/abs/2402.19371v1",
        "pub_date": "2024-02-29",
        "summary": "LLMs have become increasingly capable at accomplishing a range of\nspecialized-tasks and can be utilized to expand equitable access to medical\nknowledge. Most medical LLMs have involved extensive fine-tuning, leveraging\nspecialized medical data and significant, thus costly, amounts of computational\npower. Many of the top performing LLMs are proprietary and their access is\nlimited to very few research groups. However, open-source (OS) models represent\na key area of growth for medical LLMs due to significant improvements in\nperformance and an inherent ability to provide the transparency and compliance\nrequired in healthcare. We present OpenMedLM, a prompting platform which\ndelivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.\nWe evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks\n(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of\nprompting strategies, including zero-shot, few-shot, chain-of-thought (random\nselection and kNN selection), and ensemble/self-consistency voting. We found\nthat OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,\nsurpassing the previous best performing OS models that leveraged\ncomputationally costly extensive fine-tuning. The model delivers a 72.6%\naccuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and\nachieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the\nfirst OS LLM to surpass 80% accuracy on this benchmark. Our results highlight\nmedical-specific emergent properties in OS LLMs which have not yet been\ndocumented to date elsewhere, and showcase the benefits of further leveraging\nprompt engineering to improve the performance of accessible LLMs for medical\napplications.",
        "translated": ""
    },
    {
        "title": "A SOUND APPROACH: Using Large Language Models to generate audio\n  descriptions for egocentric text-audio retrieval",
        "url": "http://arxiv.org/abs/2402.19106v1",
        "pub_date": "2024-02-29",
        "summary": "Video databases from the internet are a valuable source of text-audio\nretrieval datasets. However, given that sound and vision streams represent\ndifferent \"views\" of the data, treating visual descriptions as audio\ndescriptions is far from optimal. Even if audio class labels are present, they\ncommonly are not very detailed, making them unsuited for text-audio retrieval.\nTo exploit relevant audio information from video-text datasets, we introduce a\nmethodology for generating audio-centric descriptions using Large Language\nModels (LLMs). In this work, we consider the egocentric video setting and\npropose three new text-audio retrieval benchmarks based on the EpicMIR and\nEgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining\naudio-centric descriptions gives significantly higher zero-shot performance\nthan using the original visual-centric descriptions. Furthermore, we show that\nusing the same prompts, we can successfully employ LLMs to improve the\nretrieval on EpicSounds, compared to using the original audio class labels of\nthe dataset. Finally, we confirm that LLMs can be used to determine the\ndifficulty of identifying the action associated with a sound.",
        "translated": ""
    },
    {
        "title": "Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain\n  Recommendation",
        "url": "http://arxiv.org/abs/2402.19101v1",
        "pub_date": "2024-02-29",
        "summary": "In recent years, the recommendation content on e-commerce platforms has\nbecome increasingly rich -- a single user feed may contain multiple entities,\nsuch as selling products, short videos, and content posts. To deal with the\nmulti-entity recommendation problem, an intuitive solution is to adopt the\nshared-network-based architecture for joint training. The idea is to transfer\nthe extracted knowledge from one type of entity (source entity) to another\n(target entity). However, different from the conventional same-entity\ncross-domain recommendation, multi-entity knowledge transfer encounters several\nimportant issues: (1) data distributions of the source entity and target entity\nare naturally different, making the shared-network-based joint training\nsusceptible to the negative transfer issue, (2) more importantly, the\ncorresponding feature schema of each entity is not exactly aligned (e.g., price\nis an essential feature for selling product while missing for content posts),\nmaking the existing methods no longer appropriate. Recent researchers have also\nexperimented with the pre-training and fine-tuning paradigm. Again, they only\nconsider the scenarios with the same entity type and feature systems, which is\ninappropriate in our case. To this end, we design a pre-training &amp; fine-tuning\nbased Multi-entity Knowledge Transfer framework called MKT. MKT utilizes a\nmulti-entity pre-training module to extract transferable knowledge across\ndifferent entities. In particular, a feature alignment module is first applied\nto scale and align different feature schemas. Afterward, a couple of knowledge\nextractors are employed to extract the common and entity-specific knowledge. In\nthe end, the extracted common knowledge is adopted for target entity model\ntraining. Through extensive offline and online experiments, we demonstrated the\nsuperiority of MKT over multiple State-Of-The-Art methods.",
        "translated": ""
    },
    {
        "title": "Stop Relying on No-Choice and Do not Repeat the Moves: Optimal,\n  Efficient and Practical Algorithms for Assortment Optimization",
        "url": "http://arxiv.org/abs/2402.18917v1",
        "pub_date": "2024-02-29",
        "summary": "We address the problem of active online assortment optimization problem with\npreference feedback, which is a framework for modeling user choices and\nsubsetwise utility maximization. The framework is useful in various real-world\napplications including ad placement, online retail, recommender systems,\nfine-tuning language models, amongst many. The problem, although has been\nstudied in the past, lacks an intuitive and practical solution approach with\nsimultaneously efficient algorithm and optimal regret guarantee. E.g.,\npopularly used assortment selection algorithms often require the presence of a\n`strong reference' which is always included in the choice sets, further they\nare also designed to offer the same assortments repeatedly until the reference\nitem gets selected -- all such requirements are quite unrealistic for practical\napplications. In this paper, we designed efficient algorithms for the problem\nof regret minimization in assortment selection with \\emph{Plackett Luce} (PL)\nbased user choices. We designed a novel concentration guarantee for estimating\nthe score parameters of the PL model using `\\emph{Pairwise Rank-Breaking}',\nwhich builds the foundation of our proposed algorithms. Moreover, our methods\nare practical, provably optimal, and devoid of the aforementioned limitations\nof the existing methods. Empirical evaluations corroborate our findings and\noutperform the existing baselines.",
        "translated": ""
    },
    {
        "title": "Aligning Language Models for Versatile Text-based Item Retrieval",
        "url": "http://arxiv.org/abs/2402.18899v1",
        "pub_date": "2024-02-29",
        "summary": "This paper addresses the gap between general-purpose text embeddings and the\nspecific demands of item retrieval tasks. We demonstrate the shortcomings of\nexisting models in capturing the nuances necessary for zero-shot performance on\nitem retrieval tasks. To overcome these limitations, we propose generate\nin-domain dataset from ten tasks tailored to unlocking models' representation\nability for item retrieval. Our empirical studies demonstrate that fine-tuning\nembedding models on the dataset leads to remarkable improvements in a variety\nof retrieval tasks. We also illustrate the practical application of our refined\nmodel in a conversational setting, where it enhances the capabilities of\nLLM-based Recommender Agents like Chat-Rec. Our code is available at\nhttps://github.com/microsoft/RecAI.",
        "translated": ""
    },
    {
        "title": "LM4OPT: Unveiling the Potential of Large Language Models in Formulating\n  Mathematical Optimization Problems",
        "url": "http://arxiv.org/abs/2403.01342v1",
        "pub_date": "2024-03-02",
        "summary": "In the rapidly evolving field of natural language processing, the translation\nof linguistic descriptions into mathematical formulation of optimization\nproblems presents a formidable challenge, demanding intricate understanding and\nprocessing capabilities from Large Language Models (LLMs). This study compares\nprominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and\none-shot settings for this task. Our findings show GPT-4's superior\nperformance, particularly in the one-shot scenario. A central part of this\nresearch is the introduction of `LM4OPT,' a progressive fine-tuning framework\nfor Llama-2-7b that utilizes noisy embeddings and specialized datasets.\nHowever, this research highlights a notable gap in the contextual understanding\ncapabilities of smaller models such as Llama-2-7b compared to larger\ncounterparts, especially in processing lengthy and complex input contexts. Our\nempirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4\nsurpasses the baseline performance established by previous research, achieving\nan F1-score of 0.63, solely based on the problem description in natural\nlanguage, and without relying on any additional named entity information.\nGPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These\nfindings not only benchmark the current capabilities of LLMs in a novel\napplication area but also lay the groundwork for future improvements in\nmathematical formulation of optimization problems from natural language input.",
        "translated": ""
    },
    {
        "title": "Supplier Recommendation in Online Procurement",
        "url": "http://arxiv.org/abs/2403.01301v1",
        "pub_date": "2024-03-02",
        "summary": "Supply chain optimization is key to a healthy and profitable business. Many\ncompanies use online procurement systems to agree contracts with suppliers. It\nis vital that the most competitive suppliers are invited to bid for such\ncontracts. In this work, we propose a recommender system to assist with\nsupplier discovery in road freight online procurement. Our system is able to\nprovide personalized supplier recommendations, taking into account customer\nneeds and preferences. This is a novel application of recommender systems,\ncalling for design choices that fit the unique requirements of online\nprocurement. Our preliminary results, using real-world data, are promising.",
        "translated": ""
    },
    {
        "title": "COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for\n  Traffic Forecasting",
        "url": "http://arxiv.org/abs/2403.01091v1",
        "pub_date": "2024-03-02",
        "summary": "This paper investigates traffic forecasting, which attempts to forecast the\nfuture state of traffic based on historical situations. This problem has\nreceived ever-increasing attention in various scenarios and facilitated the\ndevelopment of numerous downstream applications such as urban planning and\ntransportation management. However, the efficacy of existing methods remains\nsub-optimal due to their tendency to model temporal and spatial relationships\nindependently, thereby inadequately accounting for complex high-order\ninteractions of both worlds. Moreover, the diversity of transitional patterns\nin traffic forecasting makes them challenging to capture for existing\napproaches, warranting a deeper exploration of their diversity. Toward this\nend, this paper proposes Conjoint Spatio-Temporal graph neural network\n(abbreviated as COOL), which models heterogeneous graphs from prior and\nposterior information to conjointly capture high-order spatio-temporal\nrelationships. On the one hand, heterogeneous graphs connecting sequential\nobservation are constructed to extract composite spatio-temporal relationships\nvia prior message passing. On the other hand, we model dynamic relationships\nusing constructed affinity and penalty graphs, which guide posterior message\npassing to incorporate complementary semantic information into node\nrepresentations. Moreover, to capture diverse transitional properties to\nenhance traffic forecasting, we propose a conjoint self-attention decoder that\nmodels diverse temporal patterns from both multi-rank and multi-scale views.\nExperimental results on four popular benchmark datasets demonstrate that our\nproposed COOL provides state-of-the-art performance compared with the\ncompetitive baselines.",
        "translated": ""
    },
    {
        "title": "BasedAI: A decentralized P2P network for Zero Knowledge Large Language\n  Models (ZK-LLMs)",
        "url": "http://arxiv.org/abs/2403.01008v1",
        "pub_date": "2024-03-01",
        "summary": "BasedAI is a distributed network of machines which introduces decentralized\ninfrastructure capable of integrating Fully Homomorphic Encryption (FHE) with\nany large language model (LLM) connected to its network. The proposed framework\nembeds a default mechanism, called \"Cerberus Squeezing\", into the mining\nprocess which enables the transformation of a standard LLMs into encrypted\nzero-knowledge LLMs, or \"ZK-LLMs\", leveraging insights from generative\nadversarial networks for data privacy. This novel quantization mechanism\nempowers BasedAI miners to process and respond to prompts derived from User\ninteraction with LLMs without the need for decrypting ei- ther the queries or\ntheir corresponding responses. The introduction of Cerberus Squeezing\nsignificantly improves performance degradation caused by quantized functions in\ncurrent FHE-compliant computing environments by proactively optimizing calls\nbetween users, miners, and validators.",
        "translated": ""
    },
    {
        "title": "An Interpretable Ensemble of Graph and Language Models for Improving\n  Search Relevance in E-Commerce",
        "url": "http://arxiv.org/abs/2403.00923v1",
        "pub_date": "2024-03-01",
        "summary": "The problem of search relevance in the E-commerce domain is a challenging one\nsince it involves understanding the intent of a user's short nuanced query and\nmatching it with the appropriate products in the catalog. This problem has\ntraditionally been addressed using language models (LMs) and graph neural\nnetworks (GNNs) to capture semantic and inter-product behavior signals,\nrespectively. However, the rapid development of new architectures has created a\ngap between research and the practical adoption of these techniques. Evaluating\nthe generalizability of these models for deployment requires extensive\nexperimentation on complex, real-world datasets, which can be non-trivial and\nexpensive. Furthermore, such models often operate on latent space\nrepresentations that are incomprehensible to humans, making it difficult to\nevaluate and compare the effectiveness of different models. This lack of\ninterpretability hinders the development and adoption of new techniques in the\nfield. To bridge this gap, we propose Plug and Play Graph LAnguage Model\n(PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a\nmodular framework with uniform data processing pipelines. It employs additive\nexplanation metrics to independently decide whether to include (i) language\nmodel candidates, (ii) GNN model candidates, and (iii) inter-product behavioral\nsignals. For the task of search relevance, we show that PP-GLAM outperforms\nseveral state-of-the-art baselines as well as a proprietary model on real-world\nmultilingual, multi-regional e-commerce datasets. To promote better model\ncomprehensibility and adoption, we also provide an analysis of the\nexplainability and computational complexity of our model. We also provide the\npublic codebase and provide a deployment strategy for practical implementation.",
        "translated": ""
    },
    {
        "title": "End-to-end Graph-Sequential Representation Learning for Accurate\n  Recommendations",
        "url": "http://arxiv.org/abs/2403.00895v1",
        "pub_date": "2024-03-01",
        "summary": "Many recent advancements in recommender systems have focused on developing\nsequence-based and graph-based approaches. Both approaches proved useful in\nmodeling intricate relationships within behavioral data, leading to promising\noutcomes in personalized ranking and next-item recommendation tasks while\nmaintaining good scalability. However, they capture very different signals from\ndata. While the former approach represents users directly through ordered\ninteractions with recent items, the latter one aims to capture indirect\ndependencies across the interactions graph. This paper presents a novel\nmulti-representational learning framework that exploits the synergies between\nthese two paradigms. Our empirical evaluation on several datasets demonstrates\nthat mutual training of sequential and graph components with the proposed\nframework significantly improves recommendations performance.",
        "translated": ""
    },
    {
        "title": "Open Assistant Toolkit -- version 2",
        "url": "http://arxiv.org/abs/2403.00586v1",
        "pub_date": "2024-03-01",
        "summary": "We present the second version of the Open Assistant Toolkit (OAT-v2), an\nopen-source task-oriented conversational system for composing generative neural\nmodels. OAT-v2 is a scalable and flexible assistant platform supporting\nmultiple domains and modalities of user interaction. It splits processing a\nuser utterance into modular system components, including submodules such as\naction code generation, multimodal content retrieval, and knowledge-augmented\nresponse generation. Developed over multiple years of the Alexa TaskBot\nchallenge, OAT-v2 is a proven system that enables scalable and robust\nexperimentation in experimental and real-world deployment. OAT-v2 provides open\nmodels and software for research and commercial applications to enable the\nfuture of multimodal virtual assistants across diverse applications and types\nof rich interaction.",
        "translated": ""
    },
    {
        "title": "Generalized User Representations for Transfer Learning",
        "url": "http://arxiv.org/abs/2403.00584v1",
        "pub_date": "2024-03-01",
        "summary": "We present a novel framework for user representation in large-scale\nrecommender systems, aiming at effectively representing diverse user taste in a\ngeneralized manner. Our approach employs a two-stage methodology combining\nrepresentation learning and transfer learning. The representation learning\nmodel uses an autoencoder that compresses various user features into a\nrepresentation space. In the second stage, downstream task-specific models\nleverage user representations via transfer learning instead of curating user\nfeatures individually. We further augment this methodology on the\nrepresentation's input features to increase flexibility and enable reaction to\nuser events, including new user experiences, in Near-Real Time. Additionally,\nwe propose a novel solution to manage deployment of this framework in\nproduction models, allowing downstream models to work independently. We\nvalidate the performance of our framework through rigorous offline and online\nexperiments within a large-scale system, showcasing its remarkable efficacy\nacross multiple evaluation tasks. Finally, we show how the proposed framework\ncan significantly reduce infrastructure costs compared to alternative\napproaches.",
        "translated": ""
    },
    {
        "title": "IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural\n  Components and Transparent User Modeling",
        "url": "http://arxiv.org/abs/2403.00520v1",
        "pub_date": "2024-03-01",
        "summary": "While interest in conversational recommender systems has been on the rise,\noperational systems suitable for serving as research platforms for\ncomprehensive studies are currently lacking. This paper introduces an enhanced\nversion of the IAI MovieBot conversational movie recommender system, aiming to\nevolve it into a robust and adaptable platform for conducting user-facing\nexperiments. The key highlights of this enhancement include the addition of\ntrainable neural components for natural language understanding and dialogue\npolicy, transparent and explainable modeling of user preferences, along with\nimprovements in the user interface and research infrastructure.",
        "translated": ""
    },
    {
        "title": "Text classification of column headers with a controlled vocabulary:\n  leveraging LLMs for metadata enrichment",
        "url": "http://arxiv.org/abs/2403.00884v1",
        "pub_date": "2024-03-01",
        "summary": "Traditional dataset retrieval systems index on metadata information rather\nthan on the data values. Thus relying primarily on manual annotations and\nhigh-quality metadata, processes known to be labour-intensive and challenging\nto automate. We propose a method to support metadata enrichment with topic\nannotations of column headers using three Large Language Models (LLMs):\nChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to\nclassify column headers based on domain-specific topics from a controlled\nvocabulary. We evaluate our approach by assessing the internal consistency of\nthe LLMs, the inter-machine alignment, and the human-machine agreement for the\ntopic classification task. Additionally, we investigate the impact of\ncontextual information (i.e. dataset description) on the classification\noutcomes. Our results suggest that ChatGPT and GoogleGemini outperform\nGoogleBard for internal consistency as well as LLM-human-alignment.\nInterestingly, we found that context had no impact on the LLMs performances.\nThis work proposes a novel approach that leverages LLMs for text classification\nusing a controlled topic vocabulary, which has the potential to facilitate\nautomated metadata enrichment, thereby enhancing dataset retrieval and the\nFindability, Accessibility, Interoperability and Reusability (FAIR) of research\ndata on the Web.",
        "translated": ""
    },
    {
        "title": "Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples",
        "url": "http://arxiv.org/abs/2403.02875v1",
        "pub_date": "2024-03-05",
        "summary": "Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.",
        "translated": ""
    },
    {
        "title": "Contrastive Pre-training for Deep Session Data Understanding",
        "url": "http://arxiv.org/abs/2403.02825v1",
        "pub_date": "2024-03-05",
        "summary": "Session data has been widely used for understanding user's behavior in\ne-commerce. Researchers are trying to leverage session data for different\ntasks, such as purchase intention prediction, remaining length prediction,\nrecommendation, etc., as it provides context clues about the user's dynamic\ninterests. However, online shopping session data is semi-structured and complex\nin nature, which contains both unstructured textual data about the products,\nsearch queries, and structured user action sequences. Most existing works focus\non leveraging the coarse-grained item sequences for specific tasks, while\nlargely ignore the fine-grained information from text and user action details.\nIn this work, we delve into deep session data understanding via scrutinizing\nthe various clues inside the rich information in user sessions. Specifically,\nwe propose to pre-train a general-purpose User Behavior Model (UBM) over\nlarge-scale session data with rich details, such as product title, attributes\nand various kinds of user actions. A two-stage pre-training scheme is\nintroduced to encourage the model to self-learn from various augmentations with\ncontrastive learning objectives, which spans different granularity levels of\nsession data. Then the well-trained session understanding model can be easily\nfine-tuned for various downstream tasks. Extensive experiments show that UBM\nbetter captures the complex intra-item semantic relations, inter-item\nconnections and inter-interaction dependencies, leading to large performance\ngains as compared to the baselines on several downstream tasks. And it also\ndemonstrates strong robustness when data is sparse.",
        "translated": ""
    },
    {
        "title": "A Distance Metric Learning Model Based On Variational Information\n  Bottleneck",
        "url": "http://arxiv.org/abs/2403.02794v1",
        "pub_date": "2024-03-05",
        "summary": "In recent years, personalized recommendation technology has flourished and\nbecome one of the hot research directions. The matrix factorization model and\nthe metric learning model which proposed successively have been widely studied\nand applied. The latter uses the Euclidean distance instead of the dot product\nused by the former to measure the latent space vector. While avoiding the\nshortcomings of the dot product, the assumption of Euclidean distance is\nneglected, resulting in limited recommendation quality of the model. In order\nto solve this problem, this paper combines the Variationl Information\nBottleneck with metric learning model for the first time, and proposes a new\nmetric learning model VIB-DML (Variational Information Bottleneck Distance\nMetric Learning) for rating prediction, which limits the mutual information of\nthe latent space feature vector to improve the robustness of the model and\nsatisfiy the assumption of Euclidean distance by decoupling the latent space\nfeature vector. In this paper, the experimental results are compared with the\nroot mean square error (RMSE) on the three public datasets. The results show\nthat the generalization ability of VIB-DML is excellent. Compared with the\ngeneral metric learning model MetricF, the prediction error is reduced by\n7.29%. Finally, the paper proves the strong robustness of VIBDML through\nexperiments.",
        "translated": ""
    },
    {
        "title": "Learning to Ask Critical Questions for Assisting Product Search",
        "url": "http://arxiv.org/abs/2403.02754v1",
        "pub_date": "2024-03-05",
        "summary": "Product search plays an essential role in eCommerce. It was treated as a\nspecial type of information retrieval problem. Most existing works make use of\nhistorical data to improve the search performance, which do not take the\nopportunity to ask for user's current interest directly. Some session-aware\nmethods take the user's clicks within the session as implicit feedback, but it\nis still just a guess on user's preference. To address this problem, recent\nconversational or question-based search models interact with users directly for\nunderstanding the user's interest explicitly. However, most users do not have a\nclear picture on what to buy at the initial stage. Asking critical attributes\nthat the user is looking for after they explored for a while should be a more\nefficient way to help them searching for the target items. In this paper, we\npropose a dual-learning model that hybrids the best from both implicit session\nfeedback and proactively clarifying with users on the most critical questions.\nWe first establish a novel utility score to measure whether a clicked item\nprovides useful information for finding the target. Then we develop the dual\nSelection Net and Ranking Net for choosing the critical questions and ranking\nthe items. It innovatively links traditional click-stream data and text-based\nquestions together. To verify our proposal, we did extensive experiments on a\npublic dataset, and our model largely outperformed other state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "Uplift Modeling for Target User Attacks on Recommender Systems",
        "url": "http://arxiv.org/abs/2403.02692v1",
        "pub_date": "2024-03-05",
        "summary": "Recommender systems are vulnerable to injective attacks, which inject limited\nfake users into the platforms to manipulate the exposure of target items to all\nusers. In this work, we identify that conventional injective attackers overlook\nthe fact that each item has its unique potential audience, and meanwhile, the\nattack difficulty across different users varies. Blindly attacking all users\nwill result in a waste of fake user budgets and inferior attack performance. To\naddress these issues, we focus on an under-explored attack task called target\nuser attacks, aiming at promoting target items to a particular user group. In\naddition, we formulate the varying attack difficulty as heterogeneous treatment\neffects through a causal lens and propose an Uplift-guided Budget Allocation\n(UBA) framework. UBA estimates the treatment effect on each target user and\noptimizes the allocation of fake user budgets to maximize the attack\nperformance. Theoretical and empirical analysis demonstrates the rationality of\ntreatment effect estimation methods of UBA. By instantiating UBA on multiple\nattackers, we conduct extensive experiments on three datasets under various\nsettings with different target items, target users, fake user budgets, victim\nmodels, and defense models, validating the effectiveness and robustness of UBA.",
        "translated": ""
    },
    {
        "title": "FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal\n  Decoupling",
        "url": "http://arxiv.org/abs/2403.02630v1",
        "pub_date": "2024-03-05",
        "summary": "In recent years, Cross-Domain Recommendation (CDR) has drawn significant\nattention, which utilizes user data from multiple domains to enhance the\nrecommendation performance. However, current CDR methods require sharing user\ndata across domains, thereby violating the General Data Protection Regulation\n(GDPR). Consequently, numerous approaches have been proposed for Federated\nCross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity\nacross different domains inevitably influences the overall performance of\nfederated learning. In this study, we propose FedHCDR, a novel Federated\nCross-Domain Recommendation framework with Hypergraph signal decoupling.\nSpecifically, to address the data heterogeneity across domains, we introduce an\napproach called hypergraph signal decoupling (HSD) to decouple the user\nfeatures into domain-exclusive and domain-shared features. The approach employs\nhigh-pass and low-pass hypergraph filters to decouple domain-exclusive and\ndomain-shared user representations, which are trained by the local-global\nbi-directional transfer algorithm. In addition, a hypergraph contrastive\nlearning (HCL) module is devised to enhance the learning of domain-shared user\nrelationship information by perturbing the user hypergraph. Extensive\nexperiments conducted on three real-world scenarios demonstrate that FedHCDR\noutperforms existing baselines significantly.",
        "translated": ""
    },
    {
        "title": "Search Intenion Network for Personalized Query Auto-Completion in\n  E-Commerce",
        "url": "http://arxiv.org/abs/2403.02609v1",
        "pub_date": "2024-03-05",
        "summary": "Query Auto-Completion(QAC), as an important part of the modern search engine,\nplays a key role in complementing user queries and helping them refine their\nsearch intentions.Today's QAC systems in real-world scenarios face two major\nchallenges:1)intention equivocality(IE): during the user's typing process,the\nprefix often contains a combination of characters and subwords, which makes the\ncurrent intention ambiguous and difficult to model.2)intention transfer\n(IT):previous works make personalized recommendations based on users'\nhistorical sequences, but ignore the search intention transfer.However, the\ncurrent intention extracted from prefix may be contrary to the historical\npreferences.",
        "translated": ""
    },
    {
        "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative\n  Literature Summary",
        "url": "http://arxiv.org/abs/2403.02574v1",
        "pub_date": "2024-03-05",
        "summary": "The literature review is an indispensable step in the research process. It\nprovides the benefit of comprehending the research problem and understanding\nthe current research situation while conducting a comparative analysis of prior\nworks. However, literature summary is challenging and time consuming. The\nprevious LLM-based studies on literature review mainly focused on the complete\nprocess, including literature retrieval, screening, and summarization. However,\nfor the summarization step, simple CoT method often lacks the ability to\nprovide extensive comparative summary. In this work, we firstly focus on the\nindependent literature summarization step and introduce ChatCite, an LLM agent\nwith human workflow guidance for comparative literature summary. This agent, by\nmimicking the human workflow, first extracts key elements from relevant\nliterature and then generates summaries using a Reflective Incremental\nMechanism. In order to better evaluate the quality of the generated summaries,\nwe devised a LLM-based automatic evaluation metric, G-Score, in refer to the\nhuman evaluation criteria. The ChatCite agent outperformed other models in\nvarious dimensions in the experiments. The literature summaries generated by\nChatCite can also be directly used for drafting literature reviews.",
        "translated": ""
    },
    {
        "title": "Magnetic Localization for In-body Nano-communication Medical Systems",
        "url": "http://arxiv.org/abs/2403.02497v1",
        "pub_date": "2024-03-04",
        "summary": "Nano-machines circulating inside the human body, collecting data on tissue\nconditions, represent a vital part of next-generation medical diagnostic\nsystems. However, for these devices to operate effectively, they need to relay\nnot only their medical measurements but also their positions. This paper\nintroduces a novel localization method for in-body nano-machines based on the\nmagnetic field, leveraging the advantageous magnetic permeability of all human\ntissues. The entire proposed localization system is described, starting from\n10x10 ${\\mu}m^2$ magnetometers to be integrated into the nano-machines, to a\nset of external wires generating the magnetic field. Mathematical equations for\nthe localization algorithm are also provided, assuming the nano-machines do not\nexecute the computations themselves, but transmit their magnetic field\nmeasurements together with medical data outside of the body. The whole system\nis validated with computer simulations that capture the measurement error of\nthe magnetometers, the error induced by the Earth magnetic field, and a human\nbody model assuming different possible positions of nano-machines. The results\nshow a very high system accuracy with localization errors even below 1 cm.",
        "translated": ""
    },
    {
        "title": "CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation\n  towards Automatic Compliance Checking",
        "url": "http://arxiv.org/abs/2403.02231v1",
        "pub_date": "2024-03-04",
        "summary": "Automatic Compliance Checking (ACC) within the Architecture, Engineering, and\nConstruction (AEC) sector necessitates automating the interpretation of\nbuilding regulations to achieve its full potential. However, extracting\ninformation from textual rules to convert them to a machine-readable format has\nbeen a challenge due to the complexities associated with natural language and\nthe limited resources that can support advanced machine-learning techniques. To\naddress this challenge, we introduce CODE-ACCORD, a unique dataset compiled\nunder the EU Horizon ACCORD project. CODE-ACCORD comprises 862 self-contained\nsentences extracted from the building regulations of England and Finland.\nAligned with our core objective of facilitating information extraction from\ntext for machine-readable rule generation, each sentence was annotated with\nentities and relations. Entities represent specific components such as \"window\"\nand \"smoke detectors\", while relations denote semantic associations between\nthese entities, collectively capturing the conveyed ideas in natural language.\nWe manually annotated all the sentences using a group of 12 annotators. Each\nsentence underwent annotations by multiple annotators and subsequently careful\ndata curation to finalise annotations, ensuring their accuracy and reliability,\nthereby establishing the dataset as a solid ground truth. CODE-ACCORD offers a\nrich resource for diverse machine learning and natural language processing\n(NLP) related tasks in ACC, including text classification, entity recognition\nand relation extraction. To the best of our knowledge, this is the first entity\nand relation-annotated dataset in compliance checking, which is also publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Backtracing: Retrieving the Cause of the Query",
        "url": "http://arxiv.org/abs/2403.03956v1",
        "pub_date": "2024-03-06",
        "summary": "Many online content portals allow users to ask questions to supplement their\nunderstanding (e.g., of lectures). While information retrieval (IR) systems may\nprovide answers for such user queries, they do not directly assist content\ncreators -- such as lecturers who want to improve their content -- identify\nsegments that _caused_ a user to ask those questions. We introduce the task of\nbacktracing, in which systems retrieve the text segment that most likely caused\na user query. We formalize three real-world domains for which backtracing is\nimportant in improving content delivery and communication: understanding the\ncause of (a) student confusion in the Lecture domain, (b) reader curiosity in\nthe News Article domain, and (c) user emotion in the Conversation domain. We\nevaluate the zero-shot performance of popular information retrieval methods and\nlanguage modeling methods, including bi-encoder, re-ranking and\nlikelihood-based methods and ChatGPT. While traditional IR systems retrieve\nsemantically relevant information (e.g., details on \"projection matrices\" for a\nquery \"does projecting multiple times still lead to the same point?\"), they\noften miss the causally relevant context (e.g., the lecturer states \"projecting\ntwice gets me the same answer as one projection\"). Our results show that there\nis room for improvement on backtracing and it requires new retrieval\napproaches. We hope our benchmark serves to improve future retrieval systems\nfor backtracing, spawning systems that refine content generation and identify\nlinguistic triggers influencing user queries. Our code and data are\nopen-sourced: https://github.com/rosewang2008/backtracing.",
        "translated": ""
    },
    {
        "title": "Bridging Language and Items for Retrieval and Recommendation",
        "url": "http://arxiv.org/abs/2403.03952v1",
        "pub_date": "2024-03-06",
        "summary": "This paper introduces BLaIR, a series of pretrained sentence embedding models\nspecialized for recommendation scenarios. BLaIR is trained to learn\ncorrelations between item metadata and potential natural language context,\nwhich is useful for retrieving and recommending items. To pretrain BLaIR, we\ncollect Amazon Reviews 2023, a new dataset comprising over 570 million reviews\nand 48 million items from 33 categories, significantly expanding beyond the\nscope of previous versions. We evaluate the generalization ability of BLaIR\nacross multiple domains and tasks, including a new task named complex product\nsearch, referring to retrieving relevant items given long, complex natural\nlanguage contexts. Leveraging large language models like ChatGPT, we\ncorrespondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical\nresults on the new task, as well as conventional retrieval and recommendation\ntasks, demonstrate that BLaIR exhibit strong text and item representation\ncapacity. Our datasets, code, and checkpoints are available at:\nhttps://github.com/hyp1231/AmazonReviews2023.",
        "translated": ""
    },
    {
        "title": "Mamba4Rec: Towards Efficient Sequential Recommendation with Selective\n  State Space Models",
        "url": "http://arxiv.org/abs/2403.03900v1",
        "pub_date": "2024-03-06",
        "summary": "Sequential recommendation aims to estimate the dynamic user preferences and\nsequential dependencies among historical user behaviors. Although\nTransformer-based models have proven to be effective for sequential\nrecommendation, they suffer from the inference inefficiency problem stemming\nfrom the quadratic computational complexity of attention operators, especially\nfor long-range behavior sequences. Inspired by the recent success of state\nspace models (SSMs), we propose Mamba4Rec, which is the first work to explore\nthe potential of selective SSMs for efficient sequential recommendation. Built\nupon the basic Mamba block which is a selective SSM with an efficient\nhardware-aware parallel algorithm, we incorporate a series of sequential\nmodeling techniques to further promote the model performance and meanwhile\nmaintain the inference efficiency. Experiments on two public datasets\ndemonstrate that Mamba4Rec is able to well address the effectiveness-efficiency\ndilemma, and defeat both RNN- and attention-based baselines in terms of both\neffectiveness and efficiency.",
        "translated": ""
    },
    {
        "title": "Cobweb: An Incremental and Hierarchical Model of Human-Like Category\n  Learning",
        "url": "http://arxiv.org/abs/2403.03835v1",
        "pub_date": "2024-03-06",
        "summary": "Cobweb, a human like category learning system, differs from other incremental\ncategorization models in constructing hierarchically organized cognitive\ntree-like structures using the category utility measure. Prior studies have\nshown that Cobweb can capture psychological effects such as the basic level,\ntypicality, and fan effects. However, a broader evaluation of Cobweb as a model\nof human categorization remains lacking. The current study addresses this gap.\nIt establishes Cobweb's alignment with classical human category learning\neffects. It also explores Cobweb's flexibility to exhibit both exemplar and\nprototype like learning within a single model. These findings set the stage for\nfuture research on Cobweb as a comprehensive model of human category learning.",
        "translated": ""
    },
    {
        "title": "Intent-aware Recommendation via Disentangled Graph Contrastive Learning",
        "url": "http://arxiv.org/abs/2403.03714v1",
        "pub_date": "2024-03-06",
        "summary": "Graph neural network (GNN) based recommender systems have become one of the\nmainstream trends due to the powerful learning ability from user behavior data.\nUnderstanding the user intents from behavior data is the key to recommender\nsystems, which poses two basic requirements for GNN-based recommender systems.\nOne is how to learn complex and diverse intents especially when the user\nbehavior is usually inadequate in reality. The other is different behaviors\nhave different intent distributions, so how to establish their relations for a\nmore explainable recommender system. In this paper, we present the Intent-aware\nRecommendation via Disentangled Graph Contrastive Learning (IDCL), which\nsimultaneously learns interpretable intents and behavior distributions over\nthose intents. Specifically, we first model the user behavior data as a\nuser-item-concept graph, and design a GNN based behavior disentangling module\nto learn the different intents. Then we propose the intent-wise contrastive\nlearning to enhance the intent disentangling and meanwhile infer the behavior\ndistributions. Finally, the coding rate reduction regularization is introduced\nto make the behaviors of different intents orthogonal. Extensive experiments\ndemonstrate the effectiveness of IDCL in terms of substantial improvement and\nthe interpretability.",
        "translated": ""
    },
    {
        "title": "Towards Efficient and Effective Unlearning of Large Language Models for\n  Recommendation",
        "url": "http://arxiv.org/abs/2403.03536v1",
        "pub_date": "2024-03-06",
        "summary": "The significant advancements in large language models (LLMs) give rise to a\npromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).\nThe efficacy of LLMRec arises from the open-world knowledge and reasoning\ncapabilities inherent in LLMs. LLMRec acquires the recommendation capabilities\nthrough instruction tuning based on user interaction data. However, in order to\nprotect user privacy and optimize utility, it is also crucial for LLMRec to\nintentionally forget specific user data, which is generally referred to as\nrecommendation unlearning. In the era of LLMs, recommendation unlearning poses\nnew challenges for LLMRec in terms of \\textit{inefficiency} and\n\\textit{ineffectiveness}. Existing unlearning methods require updating billions\nof parameters in LLMRec, which is costly and time-consuming. Besides, they\nalways impact the model utility during the unlearning process. To this end, we\npropose \\textbf{E2URec}, the first \\underline{E}fficient and\n\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our\nproposed E2URec enhances the unlearning efficiency by updating only a few\nadditional LoRA parameters, and improves the unlearning effectiveness by\nemploying a teacher-student framework, where we maintain multiple teacher\nnetworks to guide the unlearning process. Extensive experiments show that\nE2URec outperforms state-of-the-art baselines on two real-world datasets.\nSpecifically, E2URec can efficiently forget specific data without affecting\nrecommendation performance. The source code is at\n\\url{https://github.com/justarter/E2URec}.",
        "translated": ""
    },
    {
        "title": "Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling",
        "url": "http://arxiv.org/abs/2403.03516v1",
        "pub_date": "2024-03-06",
        "summary": "Dense retrieval methods have demonstrated promising performance in\nmultilingual information retrieval, where queries and documents can be in\ndifferent languages. However, dense retrievers typically require a substantial\namount of paired data, which poses even greater challenges in multilingual\nscenarios. This paper introduces UMR, an Unsupervised Multilingual dense\nRetriever trained without any paired data. Our approach leverages the sequence\nlikelihood estimation capabilities of multilingual language models to acquire\npseudo labels for training dense retrievers. We propose a two-stage framework\nwhich iteratively improves the performance of multilingual dense retrievers.\nExperimental results on two benchmark datasets show that UMR outperforms\nsupervised baselines, showcasing the potential of training multilingual\nretrievers without paired data, thereby enhancing their practicality. Our\nsource code, data, and models are publicly available at\nhttps://github.com/MiuLab/UMR",
        "translated": ""
    },
    {
        "title": "Generative News Recommendation",
        "url": "http://arxiv.org/abs/2403.03424v1",
        "pub_date": "2024-03-06",
        "summary": "Most existing news recommendation methods tackle this task by conducting\nsemantic matching between candidate news and user representation produced by\nhistorical clicked news. However, they overlook the high-level connections\namong different news articles and also ignore the profound relationship between\nthese news articles and users. And the definition of these methods dictates\nthat they can only deliver news articles as-is. On the contrary, integrating\nseveral relevant news articles into a coherent narrative would assist users in\ngaining a quicker and more comprehensive understanding of events. In this\npaper, we propose a novel generative news recommendation paradigm that includes\ntwo steps: (1) Leveraging the internal knowledge and reasoning capabilities of\nthe Large Language Model (LLM) to perform high-level matching between candidate\nnews and user representation; (2) Generating a coherent and logically\nstructured narrative based on the associations between related news and user\ninterests, thus engaging users in further reading of the news. Specifically, we\npropose GNR to implement the generative news recommendation paradigm. First, we\ncompose the dual-level representation of news and users by leveraging LLM to\ngenerate theme-level representations and combine them with semantic-level\nrepresentations. Next, in order to generate a coherent narrative, we explore\nthe news relation and filter the related news according to the user preference.\nFinally, we propose a novel training method named UIFT to train the LLM to fuse\nmultiple news articles in a coherent narrative. Extensive experiments show that\nGNR can improve recommendation accuracy and eventually generate more\npersonalized and factually consistent narratives.",
        "translated": ""
    },
    {
        "title": "Benchmarking News Recommendation in the Era of Green AI",
        "url": "http://arxiv.org/abs/2403.04736v1",
        "pub_date": "2024-03-07",
        "summary": "Over recent years, news recommender systems have gained significant attention\nin both academia and industry, emphasizing the need for a standardized\nbenchmark to evaluate and compare the performance of these systems.\nConcurrently, Green AI advocates for reducing the energy consumption and\nenvironmental impact of machine learning. To address these concerns, we\nintroduce the first Green AI benchmarking framework for news recommendation,\nknown as GreenRec, and propose a metric for assessing the tradeoff between\nrecommendation accuracy and efficiency. Our benchmark encompasses 30 base\nmodels and their variants, covering traditional end-to-end training paradigms\nas well as our proposed efficient only-encode-once (OLEO) paradigm. Through\nexperiments consuming 2000 GPU hours, we observe that the OLEO paradigm\nachieves competitive accuracy compared to state-of-the-art end-to-end paradigms\nand delivers up to a 2992\\% improvement in sustainability metrics.",
        "translated": ""
    },
    {
        "title": "Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing\n  Framework for Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2403.04503v1",
        "pub_date": "2024-03-07",
        "summary": "In this work, we introduce Ducho 2.0, the latest stable version of our\nframework. Differently from Ducho, Ducho 2.0 offers a more personalized user\nexperience with the definition and import of custom extraction models\nfine-tuned on specific tasks and datasets. Moreover, the new version is capable\nof extracting and processing features through multimodal-by-design large\nmodels. Notably, all these new features are supported by optimized data loading\nand storing to the local memory. To showcase the capabilities of Ducho 2.0, we\ndemonstrate a complete multimodal recommendation pipeline, from the\nextraction/processing to the final recommendation. The idea is to provide\npractitioners and experienced scholars with a ready-to-use tool that, put on\ntop of any multimodal recommendation framework, may permit them to run\nextensive benchmarking analyses. All materials are accessible at:\n\\url{https://github.com/sisinflab/Ducho}.",
        "translated": ""
    },
    {
        "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise,\n  Privacy and OOD Challenges",
        "url": "http://arxiv.org/abs/2403.04468v1",
        "pub_date": "2024-03-07",
        "summary": "Graph-structured data exhibits universality and widespread applicability\nacross diverse domains, such as social network analysis, biochemistry,\nfinancial fraud detection, and network security. Significant strides have been\nmade in leveraging Graph Neural Networks (GNNs) to achieve remarkable success\nin these areas. However, in real-world scenarios, the training environment for\nmodels is often far from ideal, leading to substantial performance degradation\nof GNN models due to various unfavorable factors, including imbalance in data\ndistribution, the presence of noise in erroneous data, privacy protection of\nsensitive information, and generalization capability for out-of-distribution\n(OOD) scenarios. To tackle these issues, substantial efforts have been devoted\nto improving the performance of GNN models in practical real-world scenarios,\nas well as enhancing their reliability and robustness. In this paper, we\npresent a comprehensive survey that systematically reviews existing GNN models,\nfocusing on solutions to the four mentioned real-world challenges including\nimbalance, noise, privacy, and OOD in practical scenarios that many existing\nreviews have not considered. Specifically, we first highlight the four key\nchallenges faced by existing GNNs, paving the way for our exploration of\nreal-world GNN models. Subsequently, we provide detailed discussions on these\nfour aspects, dissecting how these solutions contribute to enhancing the\nreliability and robustness of GNN models. Last but not least, we outline\npromising directions and offer future perspectives in the field.",
        "translated": ""
    },
    {
        "title": "The 2nd Workshop on Recommendation with Generative Models",
        "url": "http://arxiv.org/abs/2403.04399v1",
        "pub_date": "2024-03-07",
        "summary": "The rise of generative models has driven significant advancements in\nrecommender systems, leaving unique opportunities for enhancing users'\npersonalized recommendations. This workshop serves as a platform for\nresearchers to explore and exchange innovative concepts related to the\nintegration of generative models into recommender systems. It primarily focuses\non five key perspectives: (i) improving recommender algorithms, (ii) generating\npersonalized content, (iii) evolving the user-system interaction paradigm, (iv)\nenhancing trustworthiness checks, and (v) refining evaluation methodologies for\ngenerative recommendations. With generative models advancing rapidly, an\nincreasing body of research is emerging in these domains, underscoring the\ntimeliness and critical importance of this workshop. The related research will\nintroduce innovative technologies to recommender systems and contribute to\nfresh challenges in both academia and industry. In the long term, this research\ndirection has the potential to revolutionize the traditional recommender\nparadigms and foster the development of next-generation recommender systems.",
        "translated": ""
    },
    {
        "title": "ALTO: An Efficient Network Orchestrator for Compound AI Systems",
        "url": "http://arxiv.org/abs/2403.04311v1",
        "pub_date": "2024-03-07",
        "summary": "We present ALTO, a network orchestrator for efficiently serving compound AI\nsystems such as pipelines of language models. ALTO achieves high throughput and\nlow latency by taking advantage of an optimization opportunity specific to\ngenerative language models: streaming intermediate outputs. As language models\nproduce outputs token by token, ALTO exposes opportunities to stream\nintermediate outputs between stages when possible. We highlight two new\nchallenges of correctness and load balancing which emerge when streaming\nintermediate data across distributed pipeline stage instances. We also motivate\nthe need for an aggregation-aware routing interface and distributed\nprompt-aware scheduling to address these challenges. We demonstrate the impact\nof ALTO's partial output streaming on a complex chatbot verification pipeline,\nincreasing throughput by up to 3x for a fixed latency target of 4 seconds /\nrequest while also reducing tail latency by 1.8x compared to a baseline serving\napproach.",
        "translated": ""
    },
    {
        "title": "DGR: A General Graph Desmoothing Framework for Recommendation via Global\n  and Local Perspectives",
        "url": "http://arxiv.org/abs/2403.04287v1",
        "pub_date": "2024-03-07",
        "summary": "Graph Convolutional Networks (GCNs) have become pivotal in recommendation\nsystems for learning user and item embeddings by leveraging the user-item\ninteraction graph's node information and topology. However, these models often\nface the famous over-smoothing issue, leading to indistinct user and item\nembeddings and reduced personalization. Traditional desmoothing methods in\nGCN-based systems are model-specific, lacking a universal solution. This paper\nintroduces a novel, model-agnostic approach named \\textbf{D}esmoothing\nFramework for \\textbf{G}CN-based \\textbf{R}ecommendation Systems\n(\\textbf{DGR}). It effectively addresses over-smoothing on general GCN-based\nrecommendation models by considering both global and local perspectives.\nSpecifically, we first introduce vector perturbations during each message\npassing layer to penalize the tendency of node embeddings approximating overly\nto be similar with the guidance of the global topological structure. Meanwhile,\nwe further develop a tailored-design loss term for the readout embeddings to\npreserve the local collaborative relations between users and their neighboring\nitems. In particular, items that exhibit a high correlation with neighboring\nitems are also incorporated to enhance the local topological information. To\nvalidate our approach, we conduct extensive experiments on 5 benchmark datasets\nbased on 5 well-known GCN-based recommendation models, demonstrating the\neffectiveness and generalization of our proposed framework.",
        "translated": ""
    },
    {
        "title": "SSDRec: Self-Augmented Sequence Denoising for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2403.04278v1",
        "pub_date": "2024-03-07",
        "summary": "Traditional sequential recommendation methods assume that users' sequence\ndata is clean enough to learn accurate sequence representations to reflect user\npreferences. In practice, users' sequences inevitably contain noise (e.g.,\naccidental interactions), leading to incorrect reflections of user preferences.\nConsequently, some pioneer studies have explored modeling sequentiality and\ncorrelations in sequences to implicitly or explicitly reduce noise's influence.\nHowever, relying on only available intra-sequence information (i.e.,\nsequentiality and correlations in a sequence) is insufficient and may result in\nover-denoising and under-denoising problems (OUPs), especially for short\nsequences. To improve reliability, we propose to augment sequences by inserting\nitems before denoising. However, due to the data sparsity issue and\ncomputational costs, it is challenging to select proper items from the entire\nitem universe to insert into proper positions in a target sequence. Motivated\nby the above observation, we propose a novel framework--Self-augmented Sequence\nDenoising for sequential Recommendation (SSDRec) with a three-stage learning\nparadigm to solve the above challenges. In the first stage, we empower SSDRec\nby a global relation encoder to learn multi-faceted inter-sequence relations in\na data-driven manner. These relations serve as prior knowledge to guide\nsubsequent stages. In the second stage, we devise a self-augmentation module to\naugment sequences to alleviate OUPs. Finally, we employ a hierarchical\ndenoising module in the third stage to reduce the risk of false augmentations\nand pinpoint all noise in raw sequences. Extensive experiments on five\nreal-world datasets demonstrate the superiority of \\model over state-of-the-art\ndenoising methods and its flexible applications to mainstream sequential\nrecommendation models. The source code is available at\nhttps://github.com/zc-97/SSDRec.",
        "translated": ""
    },
    {
        "title": "Can Small Language Models be Good Reasoners for Sequential\n  Recommendation?",
        "url": "http://arxiv.org/abs/2403.04260v1",
        "pub_date": "2024-03-07",
        "summary": "Large language models (LLMs) open up new horizons for sequential\nrecommendations, owing to their remarkable language comprehension and\ngeneration capabilities. However, there are still numerous challenges that\nshould be addressed to successfully implement sequential recommendations\nempowered by LLMs. Firstly, user behavior patterns are often complex, and\nrelying solely on one-step reasoning from LLMs may lead to incorrect or\ntask-irrelevant responses. Secondly, the prohibitively resource requirements of\nLLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real\nsequential recommender systems. In this paper, we propose a novel Step-by-step\nknowLedge dIstillation fraMework for recommendation (SLIM), paving a promising\npath for sequential recommenders to enjoy the exceptional reasoning\ncapabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner. We\nintroduce CoT prompting based on user behavior sequences for the larger teacher\nmodel. The rationales generated by the teacher model are then utilized as\nlabels to distill the downstream smaller student model (e.g., LLaMA2-7B). In\nthis way, the student model acquires the step-by-step reasoning capabilities in\nrecommendation tasks. We encode the generated rationales from the student model\ninto a dense vector, which empowers recommendation in both ID-based and\nID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of\nSLIM over state-of-the-art baselines, and further analysis showcasing its\nability to generate meaningful recommendation reasoning at affordable costs.",
        "translated": ""
    },
    {
        "title": "Towards Robustness Analysis of E-Commerce Ranking System",
        "url": "http://arxiv.org/abs/2403.04257v1",
        "pub_date": "2024-03-07",
        "summary": "Information retrieval (IR) is a pivotal component in various applications.\nRecent advances in machine learning (ML) have enabled the integration of ML\nalgorithms into IR, particularly in ranking systems. While there is a plethora\nof research on the robustness of ML-based ranking systems, these studies\nlargely neglect commercial e-commerce systems and fail to establish a\nconnection between real-world and manipulated query relevance. In this paper,\nwe present the first systematic measurement study on the robustness of\ne-commerce ranking systems. We define robustness as the consistency of ranking\noutcomes for semantically identical queries. To quantitatively analyze\nrobustness, we propose a novel metric that considers both ranking position and\nitem-specific information that are absent in existing metrics. Our large-scale\nmeasurement study with real-world data from e-commerce retailers reveals an\nopen opportunity to measure and improve robustness since semantically identical\nqueries often yield inconsistent ranking results. Based on our observations, we\npropose several solution directions to enhance robustness, such as the use of\nLarge Language Models. Note that the issue of robustness discussed herein does\nnot constitute an error or oversight. Rather, in scenarios where there exists a\nvast array of choices, it is feasible to present a multitude of products in\nvarious permutations, all of which could be equally appealing. However, this\nextensive selection may lead to customer confusion. As e-commerce retailers use\nvarious techniques to improve the quality of search results, we hope that this\nresearch offers valuable guidance for measuring the robustness of the ranking\nsystems.",
        "translated": ""
    },
    {
        "title": "Federated Recommendation via Hybrid Retrieval Augmented Generation",
        "url": "http://arxiv.org/abs/2403.04256v1",
        "pub_date": "2024-03-07",
        "summary": "Federated Recommendation (FR) emerges as a novel paradigm that enables\nprivacy-preserving recommendations. However, traditional FR systems usually\nrepresent users/items with discrete identities (IDs), suffering from\nperformance degradation due to the data sparsity and heterogeneity in FR. On\nthe other hand, Large Language Models (LLMs) as recommenders have proven\neffective across various recommendation scenarios. Yet, LLM-based recommenders\nencounter challenges such as low inference efficiency and potential\nhallucination, compromising their performance in real-world scenarios. To this\nend, we propose GPT-FedRec, a federated recommendation framework leveraging\nChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.\nGPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval\nprocess, mining ID-based user patterns and text-based item features. Next, the\nretrieved results are converted into text prompts and fed into GPT for\nre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims\nto extract generalized features from data and exploit pretrained knowledge\nwithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, the\nRAG approach also prevents LLM hallucination, improving the recommendation\nperformance for real-world users. Experimental results on diverse benchmark\ndatasets demonstrate the superior performance of GPT-FedRec against\nstate-of-the-art baseline methods.",
        "translated": ""
    },
    {
        "title": "Is Cosine-Similarity of Embeddings Really About Similarity?",
        "url": "http://arxiv.org/abs/2403.05440v1",
        "pub_date": "2024-03-08",
        "summary": "Cosine-similarity is the cosine of the angle between two vectors, or\nequivalently the dot product between their normalizations. A popular\napplication is to quantify semantic similarity between high-dimensional objects\nby applying cosine-similarity to a learned low-dimensional feature embedding.\nThis can work better but sometimes also worse than the unnormalized dot-product\nbetween embedded vectors in practice. To gain insight into this empirical\nobservation, we study embeddings derived from regularized linear models, where\nclosed-form solutions facilitate analytical insights. We derive analytically\nhow cosine-similarity can yield arbitrary and therefore meaningless\n`similarities.' For some linear models the similarities are not even unique,\nwhile for others they are implicitly controlled by the regularization. We\ndiscuss implications beyond linear models: a combination of different\nregularizations are employed when learning deep models; these have implicit and\nunintended effects when taking cosine-similarities of the resulting embeddings,\nrendering results opaque and possibly arbitrary. Based on these insights, we\ncaution against blindly using cosine-similarity and outline alternatives.",
        "translated": ""
    },
    {
        "title": "Harnessing Multi-Role Capabilities of Large Language Models for\n  Open-Domain Question Answering",
        "url": "http://arxiv.org/abs/2403.05217v1",
        "pub_date": "2024-03-08",
        "summary": "Open-domain question answering (ODQA) has emerged as a pivotal research\nspotlight in information systems. Existing methods follow two main paradigms to\ncollect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves\npertinent documents from an external corpus; and (2) the\n\\textit{generate-then-read} paradigm employs large language models (LLMs) to\ngenerate relevant documents. However, neither can fully address multifaceted\nrequirements for evidence. To this end, we propose LLMQA, a generalized\nframework that formulates the ODQA process into three basic steps: query\nexpansion, document selection, and answer generation, combining the superiority\nof both retrieval-based and generation-based evidence. Since LLMs exhibit their\nexcellent capabilities to accomplish various tasks, we instruct LLMs to play\nmultiple roles as generators, rerankers, and evaluators within our framework,\nintegrating them to collaborate in the ODQA process. Furthermore, we introduce\na novel prompt optimization algorithm to refine role-playing prompts and steer\nLLMs to produce higher-quality evidence and answers. Extensive experimental\nresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that\nLLMQA achieves the best performance in terms of both answer accuracy and\nevidence quality, showcasing its potential for advancing ODQA research and\napplications.",
        "translated": ""
    },
    {
        "title": "Personalized Audiobook Recommendations at Spotify Through Graph Neural\n  Networks",
        "url": "http://arxiv.org/abs/2403.05185v1",
        "pub_date": "2024-03-08",
        "summary": "In the ever-evolving digital audio landscape, Spotify, well-known for its\nmusic and talk content, has recently introduced audiobooks to its vast user\nbase. While promising, this move presents significant challenges for\npersonalized recommendations. Unlike music and podcasts, audiobooks, initially\navailable for a fee, cannot be easily skimmed before purchase, posing higher\nstakes for the relevance of recommendations. Furthermore, introducing a new\ncontent type into an existing platform confronts extreme data sparsity, as most\nusers are unfamiliar with this new content type. Lastly, recommending content\nto millions of users requires the model to react fast and be scalable. To\naddress these challenges, we leverage podcast and music user preferences and\nintroduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous\nGraph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach\nuncovers nuanced item relationships while ensuring low latency and complexity.\nWe decouple users from the HGNN graph and propose an innovative multi-link\nneighbor sampler. These choices, together with the 2T component, significantly\nreduce the complexity of the HGNN model. Empirical evaluations involving\nmillions of users show significant improvement in the quality of personalized\nrecommendations, resulting in a +46% increase in new audiobooks start rate and\na +23% boost in streaming rates. Intriguingly, our model's impact extends\nbeyond audiobooks, benefiting established products like podcasts.",
        "translated": ""
    },
    {
        "title": "Multi-Tower Multi-Interest Recommendation with User Representation Repel",
        "url": "http://arxiv.org/abs/2403.05122v1",
        "pub_date": "2024-03-08",
        "summary": "In the era of information overload, the value of recommender systems has been\nprofoundly recognized in academia and industry alike. Multi-interest sequential\nrecommendation, in particular, is a subfield that has been receiving increasing\nattention in recent years. By generating multiple-user representations,\nmulti-interest learning models demonstrate superior expressiveness than\nsingle-user representation models, both theoretically and empirically. Despite\nmajor advancements in the field, three major issues continue to plague the\nperformance and adoptability of multi-interest learning methods, the difference\nbetween training and deployment objectives, the inability to access item\ninformation, and the difficulty of industrial adoption due to its single-tower\narchitecture. We address these challenges by proposing a novel multi-tower\nmulti-interest framework with user representation repel. Experimental results\nacross multiple large-scale industrial datasets proved the effectiveness and\ngeneralizability of our proposed framework.",
        "translated": ""
    },
    {
        "title": "Aligning Large Language Models for Controllable Recommendations",
        "url": "http://arxiv.org/abs/2403.05063v1",
        "pub_date": "2024-03-08",
        "summary": "Inspired by the exceptional general intelligence of Large Language Models\n(LLMs), researchers have begun to explore their application in pioneering the\nnext generation of recommender systems - systems that are conversational,\nexplainable, and controllable. However, existing literature primarily\nconcentrates on integrating domain-specific knowledge into LLMs to enhance\naccuracy, often neglecting the ability to follow instructions. To address this\ngap, we initially introduce a collection of supervised learning tasks,\naugmented with labels derived from a conventional recommender model, aimed at\nexplicitly improving LLMs' proficiency in adhering to recommendation-specific\ninstructions. Subsequently, we develop a reinforcement learning-based alignment\nprocedure to further strengthen LLMs' aptitude in responding to users'\nintentions and mitigating formatting errors. Through extensive experiments on\ntwo real-world datasets, our method markedly advances the capability of LLMs to\ncomply with instructions within recommender systems, while sustaining a high\nlevel of accuracy performance.",
        "translated": ""
    },
    {
        "title": "Can't Remember Details in Long Documents? You Need Some R&amp;R",
        "url": "http://arxiv.org/abs/2403.05004v1",
        "pub_date": "2024-03-08",
        "summary": "Long-context large language models (LLMs) hold promise for tasks such as\nquestion-answering (QA) over long documents, but they tend to miss important\ninformation in the middle of context documents (arXiv:2307.03172v3). Here, we\nintroduce $\\textit{R&amp;R}$ -- a combination of two novel prompt-based methods\ncalled $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to\nalleviate this effect in document-based QA. In reprompting, we repeat the\nprompt instructions periodically throughout the context document to remind the\nLLM of its original task. In ICR, rather than instructing the LLM to answer the\nquestion directly, we instruct it to retrieve the top $k$ passage numbers most\nrelevant to the given question, which are then used as an abbreviated context\nin a second QA prompt. We test R&amp;R with GPT-4 Turbo and Claude-2.1 on documents\nup to 80k tokens in length and observe a 16-point boost in QA accuracy on\naverage. Our further analysis suggests that R&amp;R improves performance on long\ndocument-based QA because it reduces the distance between relevant context and\nthe instructions. Finally, we show that compared to short-context chunkwise\nmethods, R&amp;R enables the use of larger chunks that cost fewer LLM calls and\noutput tokens, while minimizing the drop in accuracy.",
        "translated": ""
    },
    {
        "title": "Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning",
        "url": "http://arxiv.org/abs/2403.04875v1",
        "pub_date": "2024-03-07",
        "summary": "Adaptations of Transformer models, such as BERT4Rec and SASRec, achieve\nstate-of-the-art performance in the sequential recommendation task according to\naccuracy-based metrics, such as NDCG. These models treat items as tokens and\nthen utilise a score-and-rank approach (Top-K strategy), where the model first\ncomputes item scores and then ranks them according to this score. While this\napproach works well for accuracy-based metrics, it is hard to use it for\noptimising more complex beyond-accuracy metrics such as diversity. Recently,\nthe GPTRec model, which uses a different Next-K strategy, has been proposed as\nan alternative to the Top-K models. In contrast with traditional Top-K\nrecommendations, Next-K generates recommendations item-by-item and, therefore,\ncan account for complex item-to-item interdependencies important for the\nbeyond-accuracy measures. However, the original GPTRec paper focused only on\naccuracy in experiments and needed to address how to optimise the model for\ncomplex beyond-accuracy metrics. Indeed, training GPTRec for beyond-accuracy\ngoals is challenging because the interaction training data available for\ntraining recommender systems typically needs to be aligned with beyond-accuracy\nrecommendation goals. To solve the misalignment problem, we train GPTRec using\na 2-stage approach: in the first stage, we use a teacher-student approach to\ntrain GPTRec, mimicking the behaviour of traditional Top-K models; in the\nsecond stage, we use Reinforcement Learning to align the model for\nbeyond-accuracy goals. In particular, we experiment with increasing\nrecommendation diversity and reducing popularity bias. Our experiments on two\ndatasets show that in 3 out of 4 cases, GPTRec's Next-K generation approach\noffers a better tradeoff between accuracy and secondary metrics than classic\ngreedy re-ranking techniques.",
        "translated": ""
    },
    {
        "title": "ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings\n  and Structured Data",
        "url": "http://arxiv.org/abs/2403.04871v1",
        "pub_date": "2024-03-07",
        "summary": "Applications increasingly leverage mixed-modality data, and must jointly\nsearch over vector data, such as embedded images, text and video, as well as\nstructured data, such as attributes and keywords. Proposed methods for this\nhybrid search setting either suffer from poor performance or support a severely\nrestricted set of search predicates (e.g., only small sets of equality\npredicates), making them impractical for many applications. To address this, we\npresent ACORN, an approach for performant and predicate-agnostic hybrid search.\nACORN builds on Hierarchical Navigable Small Worlds (HNSW), a state-of-the-art\ngraph-based approximate nearest neighbor index, and can be implemented\nefficiently by extending existing HNSW libraries. ACORN introduces the idea of\npredicate subgraph traversal to emulate a theoretically ideal, but impractical,\nhybrid search strategy. ACORN's predicate-agnostic construction algorithm is\ndesigned to enable this effective search strategy, while supporting a wide\narray of predicate sets and query semantics. We systematically evaluate ACORN\non both prior benchmark datasets, with simple, low-cardinality predicate sets,\nand complex multi-modal datasets not supported by prior methods. We show that\nACORN achieves state-of-the-art performance on all datasets, outperforming\nprior methods with 2-1,000x higher throughput at a fixed recall.",
        "translated": ""
    },
    {
        "title": "SPLADE-v3: New baselines for SPLADE",
        "url": "http://arxiv.org/abs/2403.06789v1",
        "pub_date": "2024-03-11",
        "summary": "A companion to the release of the latest version of the SPLADE library. We\ndescribe changes to the training structure and present our latest series of\nmodels -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as\nre-rankers, and showcase its effectiveness via a meta-analysis over more than\n40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is\nstatistically significantly more effective than both BM25 and SPLADE++, while\ncomparing well to cross-encoder re-rankers. Specifically, it gets more than 40\nMRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on\nthe BEIR benchmark.",
        "translated": ""
    },
    {
        "title": "MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation",
        "url": "http://arxiv.org/abs/2403.06747v1",
        "pub_date": "2024-03-11",
        "summary": "Compared to business-to-consumer (B2C) e-commerce systems,\nconsumer-to-consumer (C2C) e-commerce platforms usually encounter the\nlimited-stock problem, that is, a product can only be sold one time in a C2C\nsystem. This poses several unique challenges for click-through rate (CTR)\nprediction. Due to limited user interactions for each product (i.e. item), the\ncorresponding item embedding in the CTR model may not easily converge. This\nmakes the conventional sequence modeling based approaches cannot effectively\nutilize user history information since historical user behaviors contain a\nmixture of items with different volume of stocks. Particularly, the attention\nmechanism in a sequence model tends to assign higher score to products with\nmore accumulated user interactions, making limited-stock products being ignored\nand contribute less to the final output. To this end, we propose the Meta-Split\nNetwork (MSN) to split user history sequence regarding to the volume of stock\nfor each product, and adopt differentiated modeling approaches for different\nsequences. As for the limited-stock products, a meta-learning approach is\napplied to address the problem of inconvergence, which is achieved by designing\nmeta scaling and shifting networks with ID and side information. In addition,\ntraditional approach can hardly update item embedding once the product is\nconsumed. Thereby, we propose an auxiliary loss that makes the parameters\nupdatable even when the product is no longer in distribution. To the best of\nour knowledge, this is the first solution addressing the recommendation of\nlimited-stock product. Experimental results on the production dataset and\nonline A/B testing demonstrate the effectiveness of our proposed method.",
        "translated": ""
    },
    {
        "title": "Post-Training Attribute Unlearning in Recommender Systems",
        "url": "http://arxiv.org/abs/2403.06737v1",
        "pub_date": "2024-03-11",
        "summary": "With the growing privacy concerns in recommender systems, recommendation\nunlearning is getting increasing attention. Existing studies predominantly use\ntraining data, i.e., model inputs, as unlearning target. However, attackers can\nextract private information from the model even if it has not been explicitly\nencountered during training. We name this unseen information as\n\\textit{attribute} and treat it as unlearning target. To protect the sensitive\nattribute of users, Attribute Unlearning (AU) aims to make target attributes\nindistinguishable. In this paper, we focus on a strict but practical setting of\nAU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can\nonly be performed after the training of the recommendation model is completed.\nTo address the PoT-AU problem in recommender systems, we propose a\ntwo-component loss function. The first component is distinguishability loss,\nwhere we design a distribution-based measurement to make attribute labels\nindistinguishable from attackers. We further extend this measurement to handle\nmulti-class attribute cases with efficient computational overhead. The second\ncomponent is regularization loss, where we explore a function-space measurement\nthat effectively maintains recommendation performance compared to\nparameter-space regularization. We use stochastic gradient descent algorithm to\noptimize our proposed loss. Extensive experiments on four real-world datasets\ndemonstrate the effectiveness of our proposed methods.",
        "translated": ""
    },
    {
        "title": "Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based\n  Method for Dynamic Observation Processing in Spatially Distributed\n  Emergencies",
        "url": "http://arxiv.org/abs/2403.06716v1",
        "pub_date": "2024-03-11",
        "summary": "In emergencies, high stake decisions often have to be made under time\npressure and strain. In order to support such decisions, information from\nvarious sources needs to be collected and processed rapidly. The information\navailable tends to be temporally and spatially variable, uncertain, and\nsometimes conflicting, leading to potential biases in decisions. Currently,\nthere is a lack of systematic approaches for information processing and\nsituation assessment which meet the particular demands of emergency situations.\nTo address this gap, we present a Bayesian network-based method called ERIMap\nthat is tailored to the complex information-scape during emergencies. The\nmethod enables the systematic and rapid processing of heterogeneous and\npotentially uncertain observations and draws inferences about key variables of\nan emergency. It thereby reduces complexity and cognitive load for decision\nmakers. The output of the ERIMap method is a dynamically evolving and spatially\nresolved map of beliefs about key variables of an emergency that is updated\neach time a new observation becomes available. The method is illustrated in a\ncase study in which an emergency response is triggered by an accident causing a\ngas leakage on a chemical plant site.",
        "translated": ""
    },
    {
        "title": "KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation",
        "url": "http://arxiv.org/abs/2403.06642v1",
        "pub_date": "2024-03-11",
        "summary": "The utilization of semantic information is an important research problem in\nthe field of recommender systems, which aims to complement the missing parts of\nmainstream ID-based approaches. With the rise of LLM, its ability to act as a\nknowledge base and its reasoning capability have opened up new possibilities\nfor this research area, making LLM-based recommendation an emerging research\ndirection. However, directly using LLM to process semantic information for\nrecommendation scenarios is unreliable and sub-optimal due to several problems\nsuch as hallucination. A promising way to cope with this is to use external\nknowledge to aid LLM in generating truthful and usable text. Inspired by the\nabove motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to\nusing external knowledge in prompts, the proposed method also includes a\nknowledge-based contrastive learning scheme for training. Experiments on public\ndatasets and in-enterprise datasets validate the effectiveness of the proposed\nmethod.",
        "translated": ""
    },
    {
        "title": "Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology",
        "url": "http://arxiv.org/abs/2403.06567v1",
        "pub_date": "2024-03-11",
        "summary": "Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.",
        "translated": ""
    },
    {
        "title": "ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval",
        "url": "http://arxiv.org/abs/2403.06551v1",
        "pub_date": "2024-03-11",
        "summary": "Tool learning aims to extend the capabilities of large language models (LLMs)\nwith external tools. A major challenge in tool learning is how to support a\nlarge number of tools, including unseen tools. To address this challenge,\nprevious studies have proposed retrieving suitable tools for the LLM based on\nthe user query. However, previously proposed methods do not consider the\ndifferences between seen and unseen tools, nor do they take the hierarchy of\nthe tool library into account, which may lead to suboptimal performance for\ntool retrieval. Therefore, to address the aforementioned issues, we propose\nToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval\nto further refine the retrieval results. Specifically, our proposed ToolRerank\nincludes Adaptive Truncation, which truncates the retrieval results related to\nseen and unseen tools at different positions, and Hierarchy-Aware Reranking,\nwhich makes retrieval results more concentrated for single-tool queries and\nmore diverse for multi-tool queries. Experimental results show that ToolRerank\ncan improve the quality of the retrieval results, leading to better execution\nresults generated by the LLM.",
        "translated": ""
    },
    {
        "title": "RecAI: Leveraging Large Language Models for Next-Generation Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2403.06465v1",
        "pub_date": "2024-03-11",
        "summary": "This paper introduces RecAI, a practical toolkit designed to augment or even\nrevolutionize recommender systems with the advanced capabilities of Large\nLanguage Models (LLMs). RecAI provides a suite of tools, including Recommender\nAI Agent, Recommendation-oriented Language Models, Knowledge Plugin,\nRecExplainer, and Evaluator, to facilitate the integration of LLMs into\nrecommender systems from multifaceted perspectives. The new generation of\nrecommender systems, empowered by LLMs, are expected to be more versatile,\nexplainable, conversational, and controllable, paving the way for more\nintelligent and user-centric recommendation experiences. We hope the\nopen-source of RecAI can help accelerate evolution of new advanced recommender\nsystems. The source code of RecAI is available at\n\\url{https://github.com/microsoft/RecAI}.",
        "translated": ""
    },
    {
        "title": "CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve\n  Long-tail Recommendation",
        "url": "http://arxiv.org/abs/2403.06447v1",
        "pub_date": "2024-03-11",
        "summary": "The long-tail recommendation is a challenging task for traditional\nrecommender systems, due to data sparsity and data imbalance issues. The recent\ndevelopment of large language models (LLMs) has shown their abilities in\ncomplex reasoning, which can help to deduce users' preferences based on very\nfew previous interactions. However, since most LLM-based systems rely on items'\nsemantic meaning as the sole evidence for reasoning, the collaborative\ninformation of user-item interactions is neglected, which can cause the LLM's\nreasoning to be misaligned with task-specific collaborative information of the\ndataset. To further align LLMs' reasoning to task-specific user-item\ninteraction knowledge, we introduce collaborative retrieval-augmented LLMs,\nCoRAL, which directly incorporate collaborative evidence into the prompts.\nBased on the retrieved user-item interactions, the LLM can analyze shared and\ndistinct preferences among users, and summarize the patterns indicating which\ntypes of users would be attracted by certain items. The retrieved collaborative\nevidence prompts the LLM to align its reasoning with the user-item interaction\npatterns in the dataset. However, since the capacity of the input prompt is\nlimited, finding the minimally-sufficient collaborative information for\nrecommendation tasks can be challenging. We propose to find the optimal\ninteraction set through a sequential decision-making process and develop a\nretrieval policy learned through a reinforcement learning (RL) framework,\nCoRAL. Our experimental results show that CoRAL can significantly improve LLMs'\nreasoning abilities on specific recommendation tasks. Our analysis also reveals\nthat CoRAL can more efficiently explore collaborative information through\nreinforcement learning.",
        "translated": ""
    },
    {
        "title": "Repeated Padding as Data Augmentation for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2403.06372v1",
        "pub_date": "2024-03-11",
        "summary": "Sequential recommendation aims to provide users with personalized suggestions\nbased on their historical interactions. When training sequential models,\npadding is a widely adopted technique for two main reasons: 1) The vast\nmajority of models can only handle fixed-length sequences; 2) Batching-based\ntraining needs to ensure that the sequences in each batch have the same length.\nThe special value \\emph{0} is usually used as the padding content, which does\nnot contain the actual information and is ignored in the model calculations.\nThis common-sense padding strategy leads us to a problem that has never been\nexplored before: \\emph{Can we fully utilize this idle input space by padding\nother content to further improve model performance and training efficiency?}\n  In this paper, we propose a simple yet effective padding method called\n\\textbf{Rep}eated \\textbf{Pad}ding (\\textbf{RepPad}). Specifically, we use the\noriginal interaction sequences as the padding content and fill it to the\npadding positions during model training. This operation can be performed a\nfinite number of times or repeated until the input sequences' length reaches\nthe maximum limit. Our RepPad can be viewed as a sequence-level data\naugmentation strategy. Unlike most existing works, our method contains no\ntrainable parameters or hyperparameters and is a plug-and-play data\naugmentation operation. Extensive experiments on various categories of\nsequential models and five real-world datasets demonstrate the effectiveness\nand efficiency of our approach. The average recommendation performance\nimprovement is up to 60.3\\% on GRU4Rec and 24.3\\% on SASRec. We also provide\nin-depth analysis and explanation of what makes RepPad effective from multiple\nperspectives. The source code will be released to ensure the reproducibility of\nour experiments.",
        "translated": ""
    },
    {
        "title": "DESERE: The 1st Workshop on Decentralised Search and Recommendation",
        "url": "http://arxiv.org/abs/2403.07732v1",
        "pub_date": "2024-03-12",
        "summary": "The DESERE Workshop, our First Workshop on Decentralised Search and\nRecommendation, offers a platform for researchers to explore and share\ninnovative ideas on decentralised web services, mainly focusing on three major\ntopics: (i) societal impact of decentralised systems: their effect on privacy,\npolicy, and regulation; (ii) decentralising applications: algorithmic and\nperformance challenges that arise from decentralisation; and (iii)\ninfrastructure to support decentralised systems and services: peer-to-peer\nnetworks, routing, and performance evaluation tools",
        "translated": ""
    },
    {
        "title": "Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models",
        "url": "http://arxiv.org/abs/2403.07654v1",
        "pub_date": "2024-03-12",
        "summary": "Modern sequence-to-sequence relevance models like monoT5 can effectively\ncapture complex textual interactions between queries and documents through\ncross-encoding. However, the use of natural language tokens in prompts, such as\nQuery, Document, and Relevant for monoT5, opens an attack vector for malicious\ndocuments to manipulate their relevance score through prompt injection, e.g.,\nby adding target words such as true. Since such possibilities have not yet been\nconsidered in retrieval evaluation, we analyze the impact of query-independent\nprompt injection via manually constructed templates and LLM-based rewriting of\ndocuments on several existing relevance models. Our experiments on the TREC\nDeep Learning track show that adversarial documents can easily manipulate\ndifferent sequence-to-sequence relevance models, while BM25 (as a typical\nlexical model) is not affected. Remarkably, the attacks also affect\nencoder-only relevance models (which do not rely on natural language prompt\ntokens), albeit to a lesser extent.",
        "translated": ""
    },
    {
        "title": "Empowering Sequential Recommendation from Collaborative Signals and\n  Semantic Relatedness",
        "url": "http://arxiv.org/abs/2403.07623v1",
        "pub_date": "2024-03-12",
        "summary": "Sequential recommender systems (SRS) could capture dynamic user preferences\nby modeling historical behaviors ordered in time. Despite effectiveness,\nfocusing only on the \\textit{collaborative signals} from behaviors does not\nfully grasp user interests. It is also significant to model the\n\\textit{semantic relatedness} reflected in content features, e.g., images and\ntext. Towards that end, in this paper, we aim to enhance the SRS tasks by\neffectively unifying collaborative signals and semantic relatedness together.\nNotably, we empirically point out that it is nontrivial to achieve such a goal\ndue to semantic gap issues. Thus, we propose an end-to-end two-stream\narchitecture for sequential recommendation, named TSSR, to learn user\npreferences from ID-based and content-based sequence. Specifically, we first\npresent novel hierarchical contrasting module, including coarse user-grained\nand fine item-grained terms, to align the representations of inter-modality.\nFurthermore, we also design a two-stream architecture to learn the dependence\nof intra-modality sequence and the complex interactions of inter-modality\nsequence, which can yield more expressive capacity in understanding user\ninterests. We conduct extensive experiments on five public datasets. The\nexperimental results show that the TSSR could yield superior performance than\ncompetitive baselines. We also make our experimental codes publicly available\nat https://anonymous.4open.science/r/TSSR-2A27/.",
        "translated": ""
    },
    {
        "title": "Proactive Recommendation with Iterative Preference Guidance",
        "url": "http://arxiv.org/abs/2403.07571v1",
        "pub_date": "2024-03-12",
        "summary": "Recommender systems mainly tailor personalized recommendations according to\nuser interests learned from user feedback. However, such recommender systems\npassively cater to user interests and even reinforce existing interests in the\nfeedback loop, leading to problems like filter bubbles and opinion\npolarization. To counteract this, proactive recommendation actively steers\nusers towards developing new interests in a target item or topic by\nstrategically modulating recommendation sequences. Existing work for proactive\nrecommendation faces significant hurdles: 1) overlooking the user feedback in\nthe guidance process; 2) lacking explicit modeling of the guiding objective;\nand 3) insufficient flexibility for integration into existing industrial\nrecommender systems. To address these issues, we introduce an Iterative\nPreference Guidance (IPG) framework. IPG performs proactive recommendation in a\nflexible post-processing manner by ranking items according to their IPG scores\nthat consider both interaction probability and guiding value. These scores are\nexplicitly estimated with iteratively updated user representation that\nconsiders the most recent user interactions. Extensive experiments validate\nthat IPG can effectively guide user interests toward target interests with a\nreasonable trade-off in recommender accuracy. The code is available at\nhttps://github.com/GabyUSTC/IPG-Rec.",
        "translated": ""
    },
    {
        "title": "The future of document indexing: GPT and Donut revolutionize table of\n  content processing",
        "url": "http://arxiv.org/abs/2403.07553v1",
        "pub_date": "2024-03-12",
        "summary": "Industrial projects rely heavily on lengthy, complex specification documents,\nmaking tedious manual extraction of structured information a major bottleneck.\nThis paper introduces an innovative approach to automate this process,\nleveraging the capabilities of two cutting-edge AI models: Donut, a model that\nextracts information directly from scanned documents without OCR, and OpenAI\nGPT-3.5 Turbo, a robust large language model. The proposed methodology is\ninitiated by acquiring the table of contents (ToCs) from construction\nspecification documents and subsequently structuring the ToCs text into JSON\ndata. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5\nTurbo reaching 89% in effectively organizing the ToCs. This landmark\nachievement represents a significant leap forward in document indexing,\ndemonstrating the immense potential of AI to automate information extraction\ntasks across diverse document types, boosting efficiency and liberating\ncritical resources in various industries.",
        "translated": ""
    },
    {
        "title": "Towards Graph Foundation Models for Personalization",
        "url": "http://arxiv.org/abs/2403.07478v1",
        "pub_date": "2024-03-12",
        "summary": "In the realm of personalization, integrating diverse information sources such\nas consumption signals and content-based representations is becoming\nincreasingly critical to build state-of-the-art solutions. In this regard, two\nof the biggest trends in research around this subject are Graph Neural Networks\n(GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in\nindustry for powering personalization at scale, FMs have only recently caught\nattention for their promising performance in personalization tasks like ranking\nand retrieval. In this paper, we present a graph-based foundation modeling\napproach tailored to personalization. Central to this approach is a\nHeterogeneous GNN (HGNN) designed to capture multi-hop content and consumption\nrelationships across a range of recommendable item types. To ensure the\ngenerality required from a Foundation Model, we employ a Large Language Model\n(LLM) text-based featurization of nodes that accommodates all item types, and\nconstruct the graph using co-interaction signals, which inherently transcend\ncontent specificity. To facilitate practical generalization, we further couple\nthe HGNN with an adaptation mechanism based on a two-tower (2T) architecture,\nwhich also operates agnostically to content type. This multi-stage approach\nensures high scalability; while the HGNN produces general purpose embeddings,\nthe 2T component models in a continuous space the sheer size of user-item\ninteraction data. Our comprehensive approach has been rigorously tested and\nproven effective in delivering recommendations across a diverse array of\nproducts within a real-world, industrial audio streaming platform.",
        "translated": ""
    },
    {
        "title": "LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial\n  Keyword Queries",
        "url": "http://arxiv.org/abs/2403.07331v1",
        "pub_date": "2024-03-12",
        "summary": "With the proliferation of spatio-textual data, Top-k KNN spatial keyword\nqueries (TkQs), which return a list of objects based on a ranking function that\nevaluates both spatial and textual relevance, have found many real-life\napplications. Existing geo-textual indexes for TkQs use traditional retrieval\nmodels like BM25 to compute text relevance and usually exploit a simple linear\nfunction to compute spatial relevance, but its effectiveness is limited. To\nimprove effectiveness, several deep learning models have recently been\nproposed, but they suffer severe efficiency issues. To the best of our\nknowledge, there are no efficient indexes specifically designed to accelerate\nthe top-k search process for these deep learning models.\n  To tackle these issues, we propose a novel technique, which Learns to Index\nthe Spatio-Textual data for answering embedding based spatial keyword queries\n(called LIST). LIST is featured with two novel components. Firstly, we propose\na lightweight and effective relevance model that is capable of learning both\ntextual and spatial relevance. Secondly, we introduce a novel machine learning\nbased Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new\nlearning-to-cluster technique to group relevant queries and objects together\nwhile separating irrelevant queries and objects. Two key challenges in building\nan effective and efficient index are the absence of high-quality labels and\nunbalanced clustering results. We develop a novel pseudo-label generation\ntechnique to address the two challenges. Experimental results show that LIST\nsignificantly outperforms state-of-the-art methods on effectiveness, with\nimprovements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is\nthree orders of magnitude faster than the most effective baseline.",
        "translated": ""
    },
    {
        "title": "Self-supervised Contrastive Learning for Implicit Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2403.07265v1",
        "pub_date": "2024-03-12",
        "summary": "Contrastive learning-based recommendation algorithms have significantly\nadvanced the field of self-supervised recommendation, particularly with BPR as\na representative ranking prediction task that dominates implicit collaborative\nfiltering. However, the presence of false-positive and false-negative examples\nin recommendation systems hampers accurate preference learning. In this study,\nwe propose a simple self-supervised contrastive learning framework that\nleverages positive feature augmentation and negative label augmentation to\nimprove the self-supervisory signal. Theoretical analysis demonstrates that our\nlearning method is equivalent to maximizing the likelihood estimation with\nlatent variables representing user interest centers. Additionally, we establish\nan efficient negative label augmentation technique that samples unlabeled\nexamples with a probability linearly dependent on their relative ranking\npositions, enabling efficient augmentation in constant time complexity. Through\nvalidation on multiple datasets, we illustrate the significant improvements our\nmethod achieves over the widely used BPR optimization objective while\nmaintaining comparable runtime.",
        "translated": ""
    },
    {
        "title": "Time Series Analysis of Key Societal Events as Reflected in Complex\n  Social Media Data Streams",
        "url": "http://arxiv.org/abs/2403.07090v1",
        "pub_date": "2024-03-11",
        "summary": "Social media platforms hold valuable insights, yet extracting essential\ninformation can be challenging. Traditional top-down approaches often struggle\nto capture critical signals in rapidly changing events. As global events evolve\nswiftly, social media narratives, including instances of disinformation, become\nsignificant sources of insights. To address the need for an inductive strategy,\nwe explore a niche social media platform GAB and an established messaging\nservice Telegram, to develop methodologies applicable on a broader scale. This\nstudy investigates narrative evolution on these platforms using quantitative\ncorpus-based discourse analysis techniques. Our approach is a novel mode to\nstudy multiple social media domains to distil key information which may be\nobscured otherwise, allowing for useful and actionable insights. The paper\ndetails the technical and methodological aspects of gathering and preprocessing\nGAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying\ncrucial nouns and verbs for deeper exploration. Empirically, this approach is\napplied to a case study of a well defined event that had global impact: the\n2023 Wagner mutiny. The main findings are: (1) the time line can be\ndeconstructed to provide useful data features allowing for improved\ninterpretation; (2) a methodology is applied which provides a basis for\ngeneralization. The key contribution is an approach, that in some cases,\nprovides the ability to capture the dynamic narrative shifts over time with\nelevated confidence. The approach can augment near-real-time assessment of key\nsocial movements, allowing for informed governance choices. This research is\nimportant because it lays out a useful methodology for time series relevant\ninfo-culling, which can enable proactive modes for positive social engagement.",
        "translated": ""
    },
    {
        "title": "ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation",
        "url": "http://arxiv.org/abs/2403.08737v1",
        "pub_date": "2024-03-13",
        "summary": "Existing Machine Learning approaches for local citation recommendation\ndirectly map or translate a query, which is typically a claim or an entity\nmention, to citation-worthy research papers. Within such a formulation, it is\nchallenging to pinpoint why one should cite a specific research paper for a\nparticular query, leading to limited recommendation interpretability. To\nalleviate this, we introduce the evidence-grounded local citation\nrecommendation task, where the target latent space comprises evidence spans for\nrecommending specific papers. Using a distantly-supervised evidence retrieval\nand multi-step re-ranking framework, our proposed system, ILCiteR, recommends\npapers to cite for a query grounded on similar evidence spans extracted from\nthe existing research literature. Unlike past formulations that simply output\nrecommendations, ILCiteR retrieves ranked lists of evidence span and\nrecommended paper pairs. Secondly, previously proposed neural models for\ncitation recommendation require expensive training on massive labeled data,\nideally after every significant update to the pool of candidate papers. In\ncontrast, ILCiteR relies solely on distant supervision from a dynamic evidence\ndatabase and pre-trained Transformer-based Language Models without any model\ntraining. We contribute a novel dataset for the evidence-grounded local\ncitation recommendation task and demonstrate the efficacy of our proposed\nconditional neural rank-ensembling approach for re-ranking evidence spans.",
        "translated": ""
    },
    {
        "title": "NLQxform-UI: A Natural Language Interface for Querying DBLP\n  Interactively",
        "url": "http://arxiv.org/abs/2403.08475v1",
        "pub_date": "2024-03-13",
        "summary": "In recent years, the DBLP computer science bibliography has been prominently\nused for searching scholarly information, such as publications, scholars, and\nvenues. However, its current search service lacks the capability to handle\ncomplex queries, which limits the usability of DBLP. In this paper, we present\nNLQxform-UI, a web-based natural language interface that enables users to query\nDBLP directly with complex natural language questions. NLQxform-UI\nautomatically translates given questions into SPARQL queries and executes the\nqueries over the DBLP knowledge graph to retrieve answers. The querying process\nis presented to users in an interactive manner, which improves the transparency\nof the system and helps examine the returned answers. Also, intermediate\nresults in the querying process can be previewed and manually altered to\nimprove the accuracy of the system. NLQxform-UI has been completely\nopen-sourced: https://github.com/ruijie-wang-uzh/NLQxform-UI.",
        "translated": ""
    },
    {
        "title": "Knowledge Conflicts for LLMs: A Survey",
        "url": "http://arxiv.org/abs/2403.08319v1",
        "pub_date": "2024-03-13",
        "summary": "This survey provides an in-depth analysis of knowledge conflicts for large\nlanguage models (LLMs), highlighting the complex challenges they encounter when\nblending contextual and parametric knowledge. Our focus is on three categories\nof knowledge conflicts: context-memory, inter-context, and intra-memory\nconflict. These conflicts can significantly impact the trustworthiness and\nperformance of LLMs, especially in real-world applications where noise and\nmisinformation are common. By categorizing these conflicts, exploring the\ncauses, examining the behaviors of LLMs under such conflicts, and reviewing\navailable solutions, this survey aims to shed light on strategies for improving\nthe robustness of LLMs, thereby serving as a valuable resource for advancing\nresearch in this evolving area.",
        "translated": ""
    },
    {
        "title": "Towards Unified Modeling for Positive and Negative Preferences in\n  Sign-Aware Recommendation",
        "url": "http://arxiv.org/abs/2403.08246v1",
        "pub_date": "2024-03-13",
        "summary": "Recently, sign-aware graph recommendation has drawn much attention as it will\nlearn users' negative preferences besides positive ones from both positive and\nnegative interactions (i.e., links in a graph) with items. To accommodate the\ndifferent semantics of negative and positive links, existing works utilize two\nindependent encoders to model users' positive and negative preferences,\nrespectively. However, these approaches cannot learn the negative preferences\nfrom high-order heterogeneous interactions between users and items formed by\nmultiple links with different signs, resulting in inaccurate and incomplete\nnegative user preferences. To cope with these intractable issues, we propose a\nnovel \\textbf{L}ight \\textbf{S}igned \\textbf{G}raph Convolution Network\nspecifically for \\textbf{Rec}ommendation (\\textbf{LSGRec}), which adopts a\nunified modeling approach to simultaneously model high-order users' positive\nand negative preferences on a signed user-item interaction graph. Specifically,\nfor the negative preferences within high-order heterogeneous interactions,\nfirst-order negative preferences are captured by the negative links, while\nhigh-order negative preferences are propagated along positive edges. Then,\nrecommendation results are generated based on positive preferences and\noptimized with negative ones. Finally, we train representations of users and\nitems through different auxiliary tasks. Extensive experiments on three\nreal-world datasets demonstrate that our method outperforms existing baselines\nregarding performance and computational efficiency. Our code is available at\n\\url{https://anonymous.4open.science/r/LSGRec-BB95}.",
        "translated": ""
    },
    {
        "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
        "url": "http://arxiv.org/abs/2403.08206v1",
        "pub_date": "2024-03-13",
        "summary": "Incorporating item content information into click-through rate (CTR)\nprediction models remains a challenge, especially with the time and space\nconstraints of industrial scenarios. The content-encoding paradigm, which\nintegrates user and item encoders directly into CTR models, prioritizes space\nover time. In contrast, the embedding-based paradigm transforms item and user\nsemantics into latent embeddings and then caches them, prioritizes space over\ntime. In this paper, we introduce a new semantic-token paradigm and propose a\ndiscrete semantic tokenization approach, namely UIST, for user and item\nrepresentation. UIST facilitates swift training and inference while maintaining\na conservative memory footprint. Specifically, UIST quantizes dense embedding\nvectors into discrete tokens with shorter lengths and employs a hierarchical\nmixture inference module to weigh the contribution of each user--item token\npair. Our experimental results on news recommendation showcase the\neffectiveness and efficiency (about 200-fold space compression) of UIST for CTR\nprediction.",
        "translated": ""
    },
    {
        "title": "Logical Discrete Graphical Models Must Supplement Large Language Models\n  for Information Synthesis",
        "url": "http://arxiv.org/abs/2403.09599v1",
        "pub_date": "2024-03-14",
        "summary": "Given the emergent reasoning abilities of large language models, information\nretrieval is becoming more complex. Rather than just retrieve a document,\nmodern information retrieval systems advertise that they can synthesize an\nanswer based on potentially many different documents, conflicting data sources,\nand using reasoning. We review recent literature and argue that the large\nlanguage model has crucial flaws that prevent it from on its own ever\nconstituting general intelligence, or answering general information synthesis\nrequests. This review shows that the following are problems for large language\nmodels: hallucinations, complex reasoning, planning under uncertainty, and\ncomplex calculations. We outline how logical discrete graphical models can\nsolve all of these problems, and outline a method of training a logical\ndiscrete model from unlabeled text.",
        "translated": ""
    },
    {
        "title": "More than words: Advancements and challenges in speech recognition for\n  singing",
        "url": "http://arxiv.org/abs/2403.09298v1",
        "pub_date": "2024-03-14",
        "summary": "This paper addresses the challenges and advancements in speech recognition\nfor singing, a domain distinctly different from standard speech recognition.\nSinging encompasses unique challenges, including extensive pitch variations,\ndiverse vocal styles, and background music interference. We explore key areas\nsuch as phoneme recognition, language identification in songs, keyword\nspotting, and full lyrics transcription. I will describe some of my own\nexperiences when performing research on these tasks just as they were starting\nto gain traction, but will also show how recent developments in deep learning\nand large-scale datasets have propelled progress in this field. My goal is to\nilluminate the complexities of applying speech recognition to singing, evaluate\ncurrent capabilities, and outline future research directions.",
        "translated": ""
    },
    {
        "title": "Seed-based information retrieval in networks of research publications:\n  Evaluation of direct citations, bibliographic coupling, co-citations and\n  PubMed related article score",
        "url": "http://arxiv.org/abs/2403.09295v1",
        "pub_date": "2024-03-14",
        "summary": "In this contribution, we deal with seed-based information retrieval in\nnetworks of research publications. Using systematic reviews as a baseline, and\npublication data from the NIH Open Citation Collection, we compare the\nperformance of the three citation-based approaches direct citation,\nco-citation, and bibliographic coupling with respect to recall and precision\nmeasures. In addition, we include the PubMed Related Article score as well as\ncombined approaches in the comparison. We also provide a fairly comprehensive\nreview of earlier research in which citation relations have been used for\ninformation retrieval purposes. The results show an advantage for co-citation\nover bibliographic coupling and direct citation. However, combining the three\napproaches outperforms the exclusive use of co-citation in the study. The\nresults further indicate, in line with previous research, that combining\ncitation-based approaches with textual approaches enhances the performance of\nseed-based information retrieval. The results from the study may guide\napproaches combining citation-based and textual approaches in their choice of\ncitation similarity measures. We suggest that future research use more\nstructured approaches to evaluate methods for seed-based retrieval of\npublications, including comparative approaches as well as the elaboration of\ncommon data sets and baselines for evaluation.",
        "translated": ""
    },
    {
        "title": "Online and Offline Evaluation in Search Clarification",
        "url": "http://arxiv.org/abs/2403.09180v1",
        "pub_date": "2024-03-14",
        "summary": "The effectiveness of clarification question models in engaging users within\nsearch systems is currently constrained, casting doubt on their overall\nusefulness. To improve the performance of these models, it is crucial to employ\nassessment approaches that encompass both real-time feedback from users (online\nevaluation) and the characteristics of clarification questions evaluated\nthrough human assessment (offline evaluation). However, the relationship\nbetween online and offline evaluations has been debated in information\nretrieval. This study aims to investigate how this discordance holds in search\nclarification. We use user engagement as ground truth and employ several\noffline labels to investigate to what extent the offline ranked lists of\nclarification resemble the ideal ranked lists based on online user engagement.",
        "translated": ""
    },
    {
        "title": "Evaluating LLMs for Gender Disparities in Notable Persons",
        "url": "http://arxiv.org/abs/2403.09148v1",
        "pub_date": "2024-03-14",
        "summary": "This study examines the use of Large Language Models (LLMs) for retrieving\nfactual information, addressing concerns over their propensity to produce\nfactually incorrect \"hallucinated\" responses or to altogether decline to even\nanswer prompt at all. Specifically, it investigates the presence of\ngender-based biases in LLMs' responses to factual inquiries. This paper takes a\nmulti-pronged approach to evaluating GPT models by evaluating fairness across\nmultiple dimensions of recall, hallucinations and declinations. Our findings\nreveal discernible gender disparities in the responses generated by GPT-3.5.\nWhile advancements in GPT-4 have led to improvements in performance, they have\nnot fully eradicated these gender disparities, notably in instances where\nresponses are declined. The study further explores the origins of these\ndisparities by examining the influence of gender associations in prompts and\nthe homogeneity in the responses.",
        "translated": ""
    },
    {
        "title": "USimAgent: Large Language Models for Simulating Search Users",
        "url": "http://arxiv.org/abs/2403.09142v1",
        "pub_date": "2024-03-14",
        "summary": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators.",
        "translated": ""
    },
    {
        "title": "Projected Gradient Descent for Spectral Compressed Sensing via Symmetric\n  Hankel Factorization",
        "url": "http://arxiv.org/abs/2403.09031v1",
        "pub_date": "2024-03-14",
        "summary": "Current spectral compressed sensing methods via Hankel matrix completion\nemploy symmetric factorization to demonstrate the low-rank property of the\nHankel matrix. However, previous non-convex gradient methods only utilize\nasymmetric factorization to achieve spectral compressed sensing. In this paper,\nwe propose a novel nonconvex projected gradient descent method for spectral\ncompressed sensing via symmetric factorization named Symmetric Hankel Projected\nGradient Descent (SHGD), which updates only one matrix and avoids a balancing\nregularization term. SHGD reduces about half of the computation and storage\ncosts compared to the prior gradient method based on asymmetric factorization.\n{Besides, the symmetric factorization employed in our work is completely novel\nto the prior low-rank factorization model, introducing a new factorization\nambiguity under complex orthogonal transformation}. Novel distance metrics are\ndesigned for our factorization method and a linear convergence guarantee to the\ndesired signal is established with $O(r^2\\log(n))$ observations. Numerical\nsimulations demonstrate the superior performance of the proposed SHGD method in\nphase transitions and computation efficiency compared to state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval\n  through Self-Supervision by Meticulous Pseudo-Relevance Labeling",
        "url": "http://arxiv.org/abs/2403.08970v1",
        "pub_date": "2024-03-13",
        "summary": "Recent studies have demonstrated that the ability of dense retrieval models\nto generalize to target domains with different distributions is limited, which\ncontrasts with the results obtained with interaction-based models. Prior\nattempts to mitigate this challenge involved leveraging adversarial learning\nand query generation approaches, but both approaches nevertheless resulted in\nlimited improvements. In this paper, we propose to combine the query-generation\napproach with a self-supervision approach in which pseudo-relevance labels are\nautomatically generated on the target domain. To accomplish this, a T5-3B model\nis utilized for pseudo-positive labeling, and meticulous hard negatives are\nchosen. We also apply this strategy on conversational dense retrieval model for\nconversational search. A similar pseudo-labeling approach is used, but with the\naddition of a query-rewriting module to rewrite conversational queries for\nsubsequent labeling. This proposed approach enables a model's domain adaptation\nwith real queries and documents from the target dataset. Experiments on\nstandard dense retrieval and conversational dense retrieval models both\ndemonstrate improvements on baseline models when they are fine-tuned on the\npseudo-relevance labeled data.",
        "translated": ""
    },
    {
        "title": "PAPERCLIP: Associating Astronomical Observations and Natural Language\n  with Multi-Modal Models",
        "url": "http://arxiv.org/abs/2403.08851v1",
        "pub_date": "2024-03-13",
        "summary": "We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation\nfor Contrastive Language-Image Pre-training), a method which associates\nastronomical observations imaged by telescopes with natural language using a\nneural network model. The model is fine-tuned from a pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) model using successful observing proposal\nabstracts and corresponding downstream observations, with the abstracts\noptionally summarized via guided generation using large language models (LLMs).\nUsing observations from the Hubble Space Telescope (HST) as an example, we show\nthat the fine-tuned model embodies a meaningful joint representation between\nobservations and natural language through tests targeting image retrieval\n(i.e., finding the most relevant observations using natural language queries)\nand description retrieval (i.e., querying for astrophysical object classes and\nuse cases most relevant to a given observation). Our study demonstrates the\npotential for using generalist foundation models rather than task-specific\nmodels for interacting with astronomical data by leveraging text as an\ninterface.",
        "translated": ""
    },
    {
        "title": "Dual-Channel Multiplex Graph Neural Networks for Recommendation",
        "url": "http://arxiv.org/abs/2403.11624v1",
        "pub_date": "2024-03-18",
        "summary": "Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.",
        "translated": ""
    },
    {
        "title": "ConvSDG: Session Data Generation for Conversational Search",
        "url": "http://arxiv.org/abs/2403.11335v1",
        "pub_date": "2024-03-17",
        "summary": "Conversational search provides a more convenient interface for users to\nsearch by allowing multi-turn interaction with the search engine. However, the\neffectiveness of the conversational dense retrieval methods is limited by the\nscarcity of training data required for their fine-tuning. Thus, generating more\ntraining conversational sessions with relevant labels could potentially improve\nsearch performance. Based on the promising capabilities of large language\nmodels (LLMs) on text generation, we propose ConvSDG, a simple yet effective\nframework to explore the feasibility of boosting conversational search by using\nLLM for session data generation. Within this framework, we design\ndialogue/session-level and query-level data generation with unsupervised and\nsemi-supervised learning, according to the availability of relevance judgments.\nThe generated data are used to fine-tune the conversational dense retriever.\nExtensive experiments on four widely used datasets demonstrate the\neffectiveness and broad applicability of our ConvSDG framework compared with\nseveral strong baselines.",
        "translated": ""
    },
    {
        "title": "Is Contrastive Learning Necessary? A Study of Data Augmentation vs\n  Contrastive Learning in Sequential Recommendation",
        "url": "http://arxiv.org/abs/2403.11136v1",
        "pub_date": "2024-03-17",
        "summary": "Sequential recommender systems (SRS) are designed to predict users' future\nbehaviors based on their historical interaction data. Recent research has\nincreasingly utilized contrastive learning (CL) to leverage unsupervised\nsignals to alleviate the data sparsity issue in SRS. In general, CL-based SRS\nfirst augments the raw sequential interaction data by using data augmentation\nstrategies and employs a contrastive training scheme to enforce the\nrepresentations of those sequences from the same raw interaction data to be\nsimilar. Despite the growing popularity of CL, data augmentation, as a basic\ncomponent of CL, has not received sufficient attention. This raises the\nquestion: Is it possible to achieve superior recommendation results solely\nthrough data augmentation? To answer this question, we benchmark eight widely\nused data augmentation strategies, as well as state-of-the-art CL-based SRS\nmethods, on four real-world datasets under both warm- and cold-start settings.\nIntriguingly, the conclusion drawn from our study is that, certain data\naugmentation strategies can achieve similar or even superior performance\ncompared with some CL-based methods, demonstrating the potential to\nsignificantly alleviate the data sparsity issue with fewer computational\noverhead. We hope that our study can further inspire more fundamental studies\non the key functional components of complex CL techniques. Our processed\ndatasets and codes are available at https://github.com/AIM-SE/DA4Rec.",
        "translated": ""
    },
    {
        "title": "Entity Alignment with Unlabeled Dangling Cases",
        "url": "http://arxiv.org/abs/2403.10978v1",
        "pub_date": "2024-03-16",
        "summary": "We investigate the entity alignment problem with unlabeled dangling cases,\nmeaning that there are entities in the source or target graph having no\ncounterparts in the other, and those entities remain unlabeled. The problem\narises when the source and target graphs are of different scales, and it is\nmuch cheaper to label the matchable pairs than the dangling entities. To solve\nthe issue, we propose a novel GNN-based dangling detection and entity alignment\nframework. While the two tasks share the same GNN and are trained together, the\ndetected dangling entities are removed in the alignment. Our framework is\nfeatured by a designed entity and relation attention mechanism for selective\nneighborhood aggregation in representation learning, as well as a\npositive-unlabeled learning loss for an unbiased estimation of dangling\nentities. Experimental results have shown that each component of our design\ncontributes to the overall alignment performance which is comparable or\nsuperior to baselines, even if the baselines additionally have 30\\% of the\ndangling entities labeled as training data.",
        "translated": ""
    },
    {
        "title": "Improving the Robustness of Dense Retrievers Against Typos via\n  Multi-Positive Contrastive Learning",
        "url": "http://arxiv.org/abs/2403.10939v1",
        "pub_date": "2024-03-16",
        "summary": "Dense retrieval has become the new paradigm in passage retrieval. Despite its\neffectiveness on typo-free queries, it is not robust when dealing with queries\nthat contain typos. Current works on improving the typo-robustness of dense\nretrievers combine (i) data augmentation to obtain the typoed queries during\ntraining time with (ii) additional robustifying subtasks that aim to align the\noriginal, typo-free queries with their typoed variants. Even though multiple\ntypoed variants are available as positive samples per query, some methods\nassume a single positive sample and a set of negative ones per anchor and\ntackle the robustifying subtask with contrastive learning; therefore, making\ninsufficient use of the multiple positives (typoed queries). In contrast, in\nthis work, we argue that all available positives can be used at the same time\nand employ contrastive learning that supports multiple positives\n(multi-positive). Experimental results on two datasets show that our proposed\napproach of leveraging all positives simultaneously and employing\nmulti-positive contrastive learning on the robustifying subtask yields\nimprovements in robustness against using contrastive learning with a single\npositive.",
        "translated": ""
    },
    {
        "title": "The Impact Of Bug Localization Based on Crash Report Mining: A\n  Developers' Perspective",
        "url": "http://arxiv.org/abs/2403.10753v1",
        "pub_date": "2024-03-16",
        "summary": "Developers often use crash reports to understand the root cause of bugs.\nHowever, locating the buggy source code snippet from such information is a\nchallenging task, mainly when the log database contains many crash reports. To\nmitigate this issue, recent research has proposed and evaluated approaches for\ngrouping crash report data and using stack trace information to locate bugs.\nThe effectiveness of such approaches has been evaluated by mainly comparing the\ncandidate buggy code snippets with the actual changed code in bug-fix commits\n-- which happens in the context of retrospective repository mining studies.\nTherefore, the existing literature still lacks discussing the use of such\napproaches in the daily life of a software company, which could explain the\ndevelopers' perceptions on the use of these approaches. In this paper, we\nreport our experience of using an approach for grouping crash reports and\nfinding buggy code on a weekly basis for 18 months, within three development\nteams in a software company. We grouped over 750,000 crash reports, opened over\n130 issues, and collected feedback from 18 developers and team leaders. Among\nother results, we observe that the amount of system logs related to a crash\nreport group is not the only criteria developers use to choose a candidate bug\nto be analyzed. Instead, other factors were considered, such as the need to\ndeliver customer-prioritized features and the difficulty of solving complex\ncrash reports (e.g., architectural debts), to cite some. The approach\ninvestigated in this study correctly suggested the buggy file most of the time\n-- the approach's precision was around 80%. In this study, the developers also\nshared their perspectives on the usefulness of the suspicious files and methods\nextracted from crash reports to fix related bugs.",
        "translated": ""
    },
    {
        "title": "Towards Unified Multi-Modal Personalization: Large Vision-Language\n  Models for Generative Recommendation and Beyond",
        "url": "http://arxiv.org/abs/2403.10667v1",
        "pub_date": "2024-03-15",
        "summary": "Developing a universal model that can effectively harness heterogeneous\nresources and respond to a wide range of personalized needs has been a\nlongstanding community aspiration. Our daily choices, especially in domains\nlike fashion and retail, are substantially shaped by multi-modal data, such as\npictures and textual descriptions. These modalities not only offer intuitive\nguidance but also cater to personalized user preferences. However, the\npredominant personalization approaches mainly focus on the ID or text-based\nrecommendation problem, failing to comprehend the information spanning various\ntasks or modalities. In this paper, our goal is to establish a Unified paradigm\nfor Multi-modal Personalization systems (UniMP), which effectively leverages\nmulti-modal data while eliminating the complexities associated with task- and\nmodality-specific customization. We argue that the advancements in foundational\ngenerative modeling have provided the flexibility and effectiveness necessary\nto achieve the objective. In light of this, we develop a generic and extensible\npersonalization generative framework, that can handle a wide range of\npersonalized needs including item recommendation, product search, preference\nprediction, explanation generation, and further user-guided image generation.\nOur methodology enhances the capabilities of foundational language models for\npersonalized tasks by seamlessly ingesting interleaved cross-modal user history\ninformation, ensuring a more precise and customized experience for users. To\ntrain and evaluate the proposed multi-modal personalized tasks, we also\nintroduce a novel and comprehensive benchmark covering a variety of user\nrequirements. Our experiments on the real-world benchmark showcase the model's\npotential, outperforming competitive methods specialized for each task.",
        "translated": ""
    },
    {
        "title": "VideoAgent: Long-form Video Understanding with Large Language Model as\n  Agent",
        "url": "http://arxiv.org/abs/2403.10517v1",
        "pub_date": "2024-03-15",
        "summary": "Long-form video understanding represents a significant challenge within\ncomputer vision, demanding a model capable of reasoning over long multi-modal\nsequences. Motivated by the human cognitive process for long-form video\nunderstanding, we emphasize interactive reasoning and planning over the ability\nto process lengthy visual inputs. We introduce a novel agent-based system,\nVideoAgent, that employs a large language model as a central agent to\niteratively identify and compile crucial information to answer a question, with\nvision-language foundation models serving as tools to translate and retrieve\nvisual information. Evaluated on the challenging EgoSchema and NExT-QA\nbenchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only\n8.4 and 8.2 frames used on average. These results demonstrate superior\neffectiveness and efficiency of our method over the current state-of-the-art\nmethods, highlighting the potential of agent-based approaches in advancing\nlong-form video understanding.",
        "translated": ""
    },
    {
        "title": "FeatUp: A Model-Agnostic Framework for Features at Any Resolution",
        "url": "http://arxiv.org/abs/2403.10516v1",
        "pub_date": "2024-03-15",
        "summary": "Deep features are a cornerstone of computer vision research, capturing image\nsemantics and enabling the community to solve downstream tasks even in the\nzero- or few-shot regime. However, these features often lack the spatial\nresolution to directly perform dense prediction tasks like segmentation and\ndepth prediction because models aggressively pool information over large areas.\nIn this work, we introduce FeatUp, a task- and model-agnostic framework to\nrestore lost spatial information in deep features. We introduce two variants of\nFeatUp: one that guides features with high-resolution signal in a single\nforward pass, and one that fits an implicit model to a single image to\nreconstruct features at any resolution. Both approaches use a multi-view\nconsistency loss with deep analogies to NeRFs. Our features retain their\noriginal semantics and can be swapped into existing applications to yield\nresolution and performance gains even without re-training. We show that FeatUp\nsignificantly outperforms other feature upsampling and image super-resolution\napproaches in class activation map generation, transfer learning for\nsegmentation and depth prediction, and end-to-end training for semantic\nsegmentation.",
        "translated": ""
    },
    {
        "title": "SocialGenPod: Privacy-Friendly Generative AI Social Web Applications\n  with Decentralised Personal Data Stores",
        "url": "http://arxiv.org/abs/2403.10408v1",
        "pub_date": "2024-03-15",
        "summary": "We present SocialGenPod, a decentralised and privacy-friendly way of\ndeploying generative AI Web applications. Unlike centralised Web and data\narchitectures that keep user data tied to application and service providers, we\nshow how one can use Solid -- a decentralised Web specification -- to decouple\nuser data from generative AI applications. We demonstrate SocialGenPod using a\nprototype that allows users to converse with different Large Language Models,\noptionally leveraging Retrieval Augmented Generation to generate answers\ngrounded in private documents stored in any Solid Pod that the user is allowed\nto access, directly or indirectly. SocialGenPod makes use of Solid access\ncontrol mechanisms to give users full control of determining who has access to\ndata stored in their Pods. SocialGenPod keeps all user data (chat history, app\nconfiguration, personal documents, etc) securely in the user's personal Pod;\nseparate from specific model or application providers. Besides better privacy\ncontrols, this approach also enables portability across different services and\napplications. Finally, we discuss challenges, posed by the large compute\nrequirements of state-of-the-art models, that future research in this area\nshould address. Our prototype is open-source and available at:\nhttps://github.com/Vidminas/socialgenpod/.",
        "translated": ""
    },
    {
        "title": "ERASE: Benchmarking Feature Selection Methods for Deep Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2403.12660v1",
        "pub_date": "2024-03-19",
        "summary": "Deep Recommender Systems (DRS) are increasingly dependent on a large number\nof feature fields for more precise recommendations. Effective feature selection\nmethods are consequently becoming critical for further enhancing the accuracy\nand optimizing storage efficiencies to align with the deployment demands. This\nresearch area, particularly in the context of DRS, is nascent and faces three\ncore challenges. Firstly, variant experimental setups across research papers\noften yield unfair comparisons, obscuring practical insights. Secondly, the\nexisting literature's lack of detailed analysis on selection attributes, based\non large-scale datasets and a thorough comparison among selection techniques\nand DRS backbones, restricts the generalizability of findings and impedes\ndeployment on DRS. Lastly, research often focuses on comparing the peak\nperformance achievable by feature selection methods, an approach that is\ntypically computationally infeasible for identifying the optimal\nhyperparameters and overlooks evaluating the robustness and stability of these\nmethods. To bridge these gaps, this paper presents ERASE, a comprehensive\nbEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation\nof eleven feature selection methods, covering both traditional and deep\nlearning approaches, across four public datasets, private industrial datasets,\nand a real-world commercial platform, achieving significant enhancement. Our\ncode is available online for ease of reproduction.",
        "translated": ""
    },
    {
        "title": "InBox: Recommendation with Knowledge Graph using Interest Box Embedding",
        "url": "http://arxiv.org/abs/2403.12649v1",
        "pub_date": "2024-03-19",
        "summary": "Knowledge graphs (KGs) have become vitally important in modern recommender\nsystems, effectively improving performance and interpretability. Fundamentally,\nrecommender systems aim to identify user interests based on historical\ninteractions and recommend suitable items. However, existing works overlook two\nkey challenges: (1) an interest corresponds to a potentially large set of\nrelated items, and (2) the lack of explicit, fine-grained exploitation of KG\ninformation and interest connectivity. This leads to an inability to reflect\ndistinctions between entities and interests when modeling them in a single way.\nAdditionally, the granularity of concepts in the knowledge graphs used for\nrecommendations tends to be coarse, failing to match the fine-grained nature of\nuser interests. This homogenization limits the precise exploitation of\nknowledge graph data and interest connectivity. To address these limitations,\nwe introduce a novel embedding-based model called InBox. Specifically, various\nknowledge graph entities and relations are embedded as points or boxes, while\nuser interests are modeled as boxes encompassing interaction history.\nRepresenting interests as boxes enables containing collections of item points\nrelated to that interest. We further propose that an interest comprises diverse\nbasic concepts, and box intersection naturally supports concept combination.\nAcross three training steps, InBox significantly outperforms state-of-the-art\nmethods like HAKG and KGIN on recommendation tasks. Further analysis provides\nmeaningful insights into the variable value of different KG data for\nrecommendations. In summary, InBox advances recommender systems through\nbox-based interest and concept modeling for sophisticated knowledge graph\nexploitation.",
        "translated": ""
    },
    {
        "title": "Context-based Fast Recommendation Strategy for Long User Behavior\n  Sequence in Meituan Waimai",
        "url": "http://arxiv.org/abs/2403.12566v1",
        "pub_date": "2024-03-19",
        "summary": "In the recommender system of Meituan Waimai, we are dealing with\never-lengthening user behavior sequences, which pose an increasing challenge to\nmodeling user preference effectively. Existing sequential recommendation models\noften fail to capture long-term dependencies or are too complex, complicating\nthe fulfillment of Meituan Waimai's unique business needs. To better model user\ninterests, we consider selecting relevant sub-sequences from users' extensive\nhistorical behaviors based on their preferences. In this specific scenario,\nwe've noticed that the contexts in which users interact have a significant\nimpact on their preferences. For this purpose, we introduce a novel method\ncalled Context-based Fast Recommendation Strategy to tackle the issue of long\nsequences. We first identify contexts that share similar user preferences with\nthe target context and then locate the corresponding PoIs based on these\nidentified contexts. This approach eliminates the necessity to select a\nsub-sequence for every candidate PoI, thereby avoiding high time complexity.\nSpecifically, we implement a prototype-based approach to pinpoint contexts that\nmirror similar user preferences. To amplify accuracy and interpretability, we\nemploy JS divergence of PoI attributes such as categories and prices as a\nmeasure of similarity between contexts. A temporal graph integrating both\nprototype and context nodes helps incorporate temporal information. We then\nidentify appropriate prototypes considering both target contexts and short-term\nuser preferences. Following this, we utilize contexts aligned with these\nprototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores\nwith target attention. Since its inception in 2023, this strategy has been\nadopted in Meituan Waimai's display recommender system, leading to a 4.6% surge\nin CTR and a 4.2% boost in GMV.",
        "translated": ""
    },
    {
        "title": "Listwise Generative Retrieval Models via a Sequential Learning Process",
        "url": "http://arxiv.org/abs/2403.12499v1",
        "pub_date": "2024-03-19",
        "summary": "Recently, a novel generative retrieval (GR) paradigm has been proposed, where\na single sequence-to-sequence model is learned to directly generate a list of\nrelevant document identifiers (docids) given a query. Existing GR models\ncommonly employ maximum likelihood estimation (MLE) for optimization: this\ninvolves maximizing the likelihood of a single relevant docid given an input\nquery, with the assumption that the likelihood for each docid is independent of\nthe other docids in the list. We refer to these models as the pointwise\napproach in this paper. While the pointwise approach has been shown to be\neffective in the context of GR, it is considered sub-optimal due to its\ndisregard for the fundamental principle that ranking involves making\npredictions about lists. In this paper, we address this limitation by\nintroducing an alternative listwise approach, which empowers the GR model to\noptimize the relevance at the docid list level. Specifically, we view the\ngeneration of a ranked docid list as a sequence learning process: at each step\nwe learn a subset of parameters that maximizes the corresponding generation\nlikelihood of the $i$-th docid given the (preceding) top $i-1$ docids. To\nformalize the sequence learning process, we design a positional conditional\nprobability for GR. To alleviate the potential impact of beam search on the\ngeneration quality during inference, we perform relevance calibration on the\ngeneration likelihood of model-generated docids according to relevance grades.\nWe conduct extensive experiments on representative binary and multi-graded\nrelevance datasets. Our empirical results demonstrate that our method\noutperforms state-of-the-art GR baselines in terms of retrieval performance.",
        "translated": ""
    },
    {
        "title": "Interpretable User Satisfaction Estimation for Conversational Systems\n  with Large Language Models",
        "url": "http://arxiv.org/abs/2403.12388v1",
        "pub_date": "2024-03-19",
        "summary": "Accurate and interpretable user satisfaction estimation (USE) is critical for\nunderstanding, evaluating, and continuously improving conversational systems.\nUsers express their satisfaction or dissatisfaction with diverse conversational\npatterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented\n(customer service chatbot) conversational systems. Existing approaches based on\nfeaturized ML models or text embeddings fall short in extracting generalizable\npatterns and are hard to interpret. In this work, we show that LLMs can extract\ninterpretable signals of user satisfaction from their natural language\nutterances more effectively than embedding-based approaches. Moreover, an LLM\ncan be tailored for USE via an iterative prompting framework using supervision\nfrom labeled examples. The resulting method, Supervised Prompting for User\nsatisfaction Rubrics (SPUR), not only has higher accuracy but is more\ninterpretable as it scores user satisfaction via learned rubrics with a\ndetailed breakdown.",
        "translated": ""
    },
    {
        "title": "An Aligning and Training Framework for Multimodal Recommendations",
        "url": "http://arxiv.org/abs/2403.12384v1",
        "pub_date": "2024-03-19",
        "summary": "With the development of multimedia applications, multimodal recommendations\nare playing an essential role, as they can leverage rich contexts beyond user\ninteractions. Existing methods mainly regard multimodal information as an\nauxiliary, using them to help learn ID features; however, there exist semantic\ngaps among multimodal content features and ID features, for which directly\nusing multimodal information as an auxiliary would lead to misalignment in\nrepresentations of users and items. In this paper, we first systematically\ninvestigate the misalignment issue in multimodal recommendations, and propose a\nsolution named AlignRec. In AlignRec, the recommendation objective is\ndecomposed into three alignments, namely alignment within contents, alignment\nbetween content and categorical ID, and alignment between users and items. Each\nalignment is characterized by a specific objective function and is integrated\ninto our multimodal recommendation framework. To effectively train our\nAlignRec, we propose starting from pre-training the first alignment to obtain\nunified multimodal features and subsequently training the following two\nalignments together with these features as input. As it is essential to analyze\nwhether each multimodal feature helps in training, we design three new classes\nof metrics to evaluate intermediate performance. Our extensive experiments on\nthree real-world datasets consistently verify the superiority of AlignRec\ncompared to nine baselines. We also find that the multimodal features generated\nby AlignRec are better than currently used ones, which are to be open-sourced.",
        "translated": ""
    },
    {
        "title": "Methods for Generating Drift in Text Streams",
        "url": "http://arxiv.org/abs/2403.12328v1",
        "pub_date": "2024-03-18",
        "summary": "Systems and individuals produce data continuously. On the Internet, people\nshare their knowledge, sentiments, and opinions, provide reviews about services\nand products, and so on. Automatically learning from these textual data can\nprovide insights to organizations and institutions, thus preventing financial\nimpacts, for example. To learn from textual data over time, the machine\nlearning system must account for concept drift. Concept drift is a frequent\nphenomenon in real-world datasets and corresponds to changes in data\ndistribution over time. For instance, a concept drift occurs when sentiments\nchange or a word's meaning is adjusted over time. Although concept drift is\nfrequent in real-world applications, benchmark datasets with labeled drifts are\nrare in the literature. To bridge this gap, this paper provides four textual\ndrift generation methods to ease the production of datasets with labeled\ndrifts. These methods were applied to Yelp and Airbnb datasets and tested using\nincremental classifiers respecting the stream mining paradigm to evaluate their\nability to recover from the drifts. Results show that all methods have their\nperformance degraded right after the drifts, and the incremental SVM is the\nfastest to run and recover the previous performance levels regarding accuracy\nand Macro F1-Score.",
        "translated": ""
    },
    {
        "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
        "url": "http://arxiv.org/abs/2403.12173v1",
        "pub_date": "2024-03-18",
        "summary": "Transforming unstructured text into structured and meaningful forms,\norganized by useful category labels, is a fundamental step in text mining for\ndownstream analysis and application. However, most existing methods for\nproducing label taxonomies and building text-based label classifiers still rely\nheavily on domain expertise and manual curation, making the process expensive\nand time-consuming. This is particularly challenging when the label space is\nunder-specified and large-scale data annotations are unavailable. In this\npaper, we address these challenges with Large Language Models (LLMs), whose\nprompt-based interface facilitates the induction and use of large-scale pseudo\nlabels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate\nthe process of end-to-end label generation and assignment with minimal human\neffort for any given use-case. In the first phase, we introduce a zero-shot,\nmulti-stage reasoning approach which enables LLMs to produce and refine a label\ntaxonomy iteratively. In the second phase, LLMs are used as data labelers that\nyield training samples so that lightweight supervised classifiers can be\nreliably built, deployed, and served at scale. We apply TnT-LLM to the analysis\nof user intent and conversational domain for Bing Copilot (formerly Bing Chat),\nan open-domain chat-based search engine. Extensive experiments using both human\nand automatic evaluation metrics demonstrate that TnT-LLM generates more\naccurate and relevant label taxonomies when compared against state-of-the-art\nbaselines, and achieves a favorable balance between accuracy and efficiency for\nclassification at scale. We also share our practical experiences and insights\non the challenges and opportunities of using LLMs for large-scale text mining\nin real-world applications.",
        "translated": ""
    },
    {
        "title": "Leveraging High-Resolution Features for Improved Deep Hashing-based\n  Image Retrieval",
        "url": "http://arxiv.org/abs/2403.13747v1",
        "pub_date": "2024-03-20",
        "summary": "Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task",
        "translated": ""
    },
    {
        "title": "No more optimization rules: LLM-enabled policy-based multi-modal query\n  optimizer (version 1)",
        "url": "http://arxiv.org/abs/2403.13597v1",
        "pub_date": "2024-03-20",
        "summary": "Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.",
        "translated": ""
    },
    {
        "title": "A Large Language Model Enhanced Sequential Recommender for Joint Video\n  and Comment Recommendation",
        "url": "http://arxiv.org/abs/2403.13574v1",
        "pub_date": "2024-03-20",
        "summary": "In online video platforms, reading or writing comments on interesting videos\nhas become an essential part of the video watching experience. However,\nexisting video recommender systems mainly model users' interaction behaviors\nwith videos, lacking consideration of comments in user behavior modeling. In\nthis paper, we propose a novel recommendation approach called LSVCR by\nleveraging user interaction histories with both videos and comments, so as to\njointly conduct personalized video and comment recommendation. Specifically,\nour approach consists of two key components, namely sequential recommendation\n(SR) model and supplemental large language model (LLM) recommender. The SR\nmodel serves as the primary recommendation backbone (retained in deployment) of\nour approach, allowing for efficient user preference modeling. Meanwhile, we\nleverage the LLM recommender as a supplemental component (discarded in\ndeployment) to better capture underlying user preferences from heterogeneous\ninteraction behaviors. In order to integrate the merits of the SR model and the\nsupplemental LLM recommender, we design a twostage training paradigm. The first\nstage is personalized preference alignment, which aims to align the preference\nrepresentations from both components, thereby enhancing the semantics of the SR\nmodel. The second stage is recommendation-oriented fine-tuning, in which the\nalignment-enhanced SR model is fine-tuned according to specific objectives.\nExtensive experiments in both video and comment recommendation tasks\ndemonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the\nKuaiShou platform verifies the actual benefits brought by our approach. In\nparticular, we achieve a significant overall gain of 4.13% in comment watch\ntime.",
        "translated": ""
    },
    {
        "title": "A Unified Optimal Transport Framework for Cross-Modal Retrieval with\n  Noisy Labels",
        "url": "http://arxiv.org/abs/2403.13480v1",
        "pub_date": "2024-03-20",
        "summary": "Cross-modal retrieval (CMR) aims to establish interaction between different\nmodalities, among which supervised CMR is emerging due to its flexibility in\nlearning semantic category discrimination. Despite the remarkable performance\nof previous supervised CMR methods, much of their success can be attributed to\nthe well-annotated data. However, even for unimodal data, precise annotation is\nexpensive and time-consuming, and it becomes more challenging with the\nmultimodal scenario. In practice, massive multimodal data are collected from\nthe Internet with coarse annotation, which inevitably introduces noisy labels.\nTraining with such misleading labels would bring two key challenges --\nenforcing the multimodal samples to \\emph{align incorrect semantics} and\n\\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To\ntackle these challenges, this work proposes UOT-RCL, a Unified framework based\non Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a\nsemantic alignment based on partial OT to progressively correct the noisy\nlabels, where a novel cross-modal consistent cost function is designed to blend\ndifferent modalities and provide precise transport cost. Second, to narrow the\ndiscrepancy in multi-modal data, an OT-based relation alignment is proposed to\ninfer the semantic-level cross-modal matching. Both of these two components\nleverage the inherent correlation among multi-modal data to facilitate\neffective cost function. The experiments on three widely-used cross-modal\nretrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art\napproaches and significantly improves the robustness against noisy labels.",
        "translated": ""
    },
    {
        "title": "DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using\n  Mixture-of-Experts",
        "url": "http://arxiv.org/abs/2403.13468v1",
        "pub_date": "2024-03-20",
        "summary": "Open-domain question answering requires retrieval systems able to cope with\nthe diverse and varied nature of questions, providing accurate answers across a\nbroad spectrum of query types and topics. To deal with such topic heterogeneity\nthrough a unique model, we propose DESIRE-ME, a neural information retrieval\nmodel that leverages the Mixture-of-Experts framework to combine multiple\nspecialized neural models. We rely on Wikipedia data to train an effective\nneural gating mechanism that classifies the incoming query and that weighs the\npredictions of the different domain-specific experts correspondingly. This\nallows DESIRE-ME to specialize adaptively in multiple domains. Through\nextensive experiments on publicly available datasets, we show that our proposal\ncan effectively generalize domain-enhanced neural models. DESIRE-ME excels in\nhandling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and\n22% in P@1, the underlying state-of-the-art dense retrieval model.",
        "translated": ""
    },
    {
        "title": "USE: Dynamic User Modeling with Stateful Sequence Models",
        "url": "http://arxiv.org/abs/2403.13344v1",
        "pub_date": "2024-03-20",
        "summary": "User embeddings play a crucial role in user engagement forecasting and\npersonalized services. Recent advances in sequence modeling have sparked\ninterest in learning user embeddings from behavioral data. Yet behavior-based\nuser embedding learning faces the unique challenge of dynamic user modeling. As\nusers continuously interact with the apps, user embeddings should be\nperiodically updated to account for users' recent and long-term behavior\npatterns. Existing methods highly rely on stateless sequence models that lack\nmemory of historical behavior. They have to either discard historical data and\nuse only the most recent data or reprocess the old and new data jointly. Both\ncases incur substantial computational overhead. To address this limitation, we\nintroduce User Stateful Embedding (USE). USE generates user embeddings and\nreflects users' evolving behaviors without the need for exhaustive reprocessing\nby storing previous model states and revisiting them in the future.\nFurthermore, we introduce a novel training objective named future W-behavior\nprediction to transcend the limitations of next-token prediction by forecasting\na broader horizon of upcoming user behaviors. By combining it with the Same\nUser Prediction, a contrastive learning-based objective that predicts whether\ndifferent segments of behavior sequences belong to the same user, we further\nimprove the embeddings' distinctiveness and representativeness. We conducted\nexperiments on 8 downstream tasks using Snapchat users' behavioral logs in both\nstatic (i.e., fixed user behavior sequences) and dynamic (i.e., periodically\nupdated user behavior sequences) settings. We demonstrate USE's superior\nperformance over established baselines. The results underscore USE's\neffectiveness and efficiency in integrating historical and recent user behavior\nsequences into user embeddings in dynamic user modeling.",
        "translated": ""
    },
    {
        "title": "Harnessing Large Language Models for Text-Rich Sequential Recommendation",
        "url": "http://arxiv.org/abs/2403.13325v1",
        "pub_date": "2024-03-20",
        "summary": "Recent advances in Large Language Models (LLMs) have been changing the\nparadigm of Recommender Systems (RS). However, when items in the recommendation\nscenarios contain rich textual information, such as product descriptions in\nonline shopping or news headlines on social media, LLMs require longer texts to\ncomprehensively depict the historical user behavior sequence. This poses\nsignificant challenges to LLM-based recommenders, such as over-length\nlimitations, extensive time and space overheads, and suboptimal model\nperformance. To this end, in this paper, we design a novel framework for\nharnessing Large Language Models for Text-Rich Sequential Recommendation\n(LLM-TRSR). Specifically, we first propose to segment the user historical\nbehaviors and subsequently employ an LLM-based summarizer for summarizing these\nuser behavior blocks. Particularly, drawing inspiration from the successful\napplication of Convolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN) models in user modeling, we introduce two unique summarization\ntechniques in this paper, respectively hierarchical summarization and recurrent\nsummarization. Then, we construct a prompt text encompassing the user\npreference summary, recent user interactions, and candidate item information\ninto an LLM-based recommender, which is subsequently fine-tuned using\nSupervised Fine-Tuning (SFT) techniques to yield our final recommendation\nmodel. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient\nFine-Tuning (PEFT). We conduct experiments on two public datasets, and the\nresults clearly demonstrate the effectiveness of our approach.",
        "translated": ""
    },
    {
        "title": "Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image\n  Retrieval",
        "url": "http://arxiv.org/abs/2403.13317v1",
        "pub_date": "2024-03-20",
        "summary": "With the explosive growth of multi-modal information on the Internet,\nunimodal search cannot satisfy the requirement of Internet applications.\nText-image retrieval research is needed to realize high-quality and efficient\nretrieval between different modalities. Existing text-image retrieval research\nis mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K),\nin which the query utterance is rigid and unnatural (i.e. verbosity and\nformality). To overcome the shortcoming, we construct a new Compact and\nFragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image\nretrieval task considering multiple query content and style, including compact\nand fine-grained entity-relation corpus. We propose a novel query-enhanced\ntext-image retrieval method using prompt engineering based on LLM. Experiments\nshow that our proposed Flickr30-CFQ reveals the insufficiency of existing\nvision-language datasets in realistic text-image tasks. Our LLM-based\nQuery-enhanced method applied on different existing text-image retrieval models\nimproves query understanding performance both on public dataset and our\nchallenge set Flickr30-CFQ with over 0.9% and 2.4% respectively. Our project\ncan be available anonymously in https://sites.google.com/view/Flickr30K-cfq.",
        "translated": ""
    },
    {
        "title": "A Semantic Search Engine for Mathlib4",
        "url": "http://arxiv.org/abs/2403.13310v1",
        "pub_date": "2024-03-20",
        "summary": "The interactive theorem prover, Lean, enables the verification of formal\nmathematical proofs and is backed by an expanding community. Central to this\necosystem is its mathematical library, mathlib4, which lays the groundwork for\nthe formalization of an expanding range of mathematical theories. However,\nsearching for theorems in mathlib4 can be challenging. To successfully search\nin mathlib4, users often need to be familiar with its naming conventions or\ndocumentation strings. Therefore, creating a semantic search engine that can be\nused easily by individuals with varying familiarity with mathlib4 is very\nimportant. In this paper, we present a semantic search engine for mathlib4 that\naccepts informal queries and finds the relevant theorems. We also establish a\nbenchmark for assessing the performance of various search engines for mathlib4.",
        "translated": ""
    },
    {
        "title": "An Analysis on Matching Mechanisms and Token Pruning for\n  Late-interaction Models",
        "url": "http://arxiv.org/abs/2403.13291v1",
        "pub_date": "2024-03-20",
        "summary": "With the development of pre-trained language models, the dense retrieval\nmodels have become promising alternatives to the traditional retrieval models\nthat rely on exact match and sparse bag-of-words representations. Different\nfrom most dense retrieval models using a bi-encoder to encode each query or\ndocument into a dense vector, the recently proposed late-interaction\nmulti-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval\neffectiveness by using all token embeddings to represent documents and queries\nand modeling their relevance with a sum-of-max operation. However, these\nfine-grained representations may cause unacceptable storage overhead for\npractical search systems. In this study, we systematically analyze the matching\nmechanism of these late-interaction models and show that the sum-of-max\noperation heavily relies on the co-occurrence signals and some important words\nin the document. Based on these findings, we then propose several simple\ndocument pruning methods to reduce the storage overhead and compare the\neffectiveness of different pruning methods on different late-interaction\nmodels. We also leverage query pruning methods to further reduce the retrieval\nlatency. We conduct extensive experiments on both in-domain and out-domain\ndatasets and show that some of the used pruning methods can significantly\nimprove the efficiency of these late-interaction models without substantially\nhurting their retrieval effectiveness.",
        "translated": ""
    },
    {
        "title": "Knowledge-Enhanced Recommendation with User-Centric Subgraph Network",
        "url": "http://arxiv.org/abs/2403.14377v1",
        "pub_date": "2024-03-21",
        "summary": "Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.",
        "translated": ""
    },
    {
        "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
        "url": "http://arxiv.org/abs/2403.14374v1",
        "pub_date": "2024-03-21",
        "summary": "Due to the extraordinarily large number of parameters, fine-tuning Large\nLanguage Models (LLMs) to update long-tail or out-of-date knowledge is\nimpractical in lots of applications. To avoid fine-tuning, we can alternatively\ntreat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment\nit with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.\nRecently, black-box RAG has achieved success in knowledge-intensive tasks and\nhas gained much attention. Existing black-box RAG methods typically fine-tune\nthe retriever to cater to LLMs' preferences and concatenate all the retrieved\ndocuments as the input, which suffers from two issues: (1) Ignorance of Factual\nInformation. The LLM preferred documents may not contain the factual\ninformation for the given question, which can mislead the retriever and hurt\nthe effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating\nall the retrieved documents brings large amounts of unnecessary tokens for\nLLMs, which degenerates the efficiency of black-box RAG. To address these\nissues, this paper proposes a novel black-box RAG framework which utilizes the\nfactual information in the retrieval and reduces the number of tokens for\naugmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by\nconstructing a bi-label document scorer. Besides, it reduces the tokens by\nintroducing a self-knowledge recognizer and a sub-document-level token reducer.\nFIT-RAG achieves both superior effectiveness and efficiency, which is validated\nby extensive experiments across three open-domain question-answering datasets:\nTriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of\nLlama2-13B-Chat by 14.3\\% on TriviaQA, 19.9\\% on NQ and 27.5\\% on PopQA,\nrespectively. Furthermore, it can save approximately half of the tokens on\naverage across the three datasets.",
        "translated": ""
    },
    {
        "title": "Understanding the Ranking Loss for Recommendation with Sparse User\n  Feedback",
        "url": "http://arxiv.org/abs/2403.14144v1",
        "pub_date": "2024-03-21",
        "summary": "Click-through rate (CTR) prediction holds significant importance in the realm\nof online advertising. While many existing approaches treat it as a binary\nclassification problem and utilize binary cross entropy (BCE) as the\noptimization objective, recent advancements have indicated that combining BCE\nloss with ranking loss yields substantial performance improvements. However,\nthe full efficacy of this combination loss remains incompletely understood. In\nthis paper, we uncover a new challenge associated with BCE loss in scenarios\nwith sparse positive feedback, such as CTR prediction: the gradient vanishing\nfor negative samples. Subsequently, we introduce a novel perspective on the\neffectiveness of ranking loss in CTR prediction, highlighting its ability to\ngenerate larger gradients on negative samples, thereby mitigating their\noptimization issues and resulting in improved classification ability. Our\nperspective is supported by extensive theoretical analysis and empirical\nevaluation conducted on publicly available datasets. Furthermore, we\nsuccessfully deployed the ranking loss in Tencent's online advertising system,\nachieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for\ntwo main scenarios. The code for our approach is openly accessible at the\nfollowing GitHub repository:\nhttps://github.com/SkylerLinn/Understanding-the-Ranking-Loss.",
        "translated": ""
    },
    {
        "title": "M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain\n  Multi-Hop Dense Sentence Retrieval",
        "url": "http://arxiv.org/abs/2403.14074v1",
        "pub_date": "2024-03-21",
        "summary": "In recent research, contrastive learning has proven to be a highly effective\nmethod for representation learning and is widely used for dense retrieval.\nHowever, we identify that relying solely on contrastive learning can lead to\nsuboptimal retrieval performance. On the other hand, despite many retrieval\ndatasets supporting various learning objectives beyond contrastive learning,\ncombining them efficiently in multi-task learning scenarios can be challenging.\nIn this paper, we introduce M3, an advanced recursive Multi-hop dense sentence\nretrieval system built upon a novel Multi-task Mixed-objective approach for\ndense text representation learning, addressing the aforementioned challenges.\nOur approach yields state-of-the-art performance on a large-scale open-domain\nfact verification benchmark dataset, FEVER. Code and data are available at:\nhttps://github.com/TonyBY/M3",
        "translated": ""
    },
    {
        "title": "gTBLS: Generating Tables from Text by Conditional Question Answering",
        "url": "http://arxiv.org/abs/2403.14457v1",
        "pub_date": "2024-03-21",
        "summary": "Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.",
        "translated": ""
    },
    {
        "title": "Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions",
        "url": "http://arxiv.org/abs/2403.15279v1",
        "pub_date": "2024-03-22",
        "summary": "This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.",
        "translated": ""
    },
    {
        "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions",
        "url": "http://arxiv.org/abs/2403.15246v1",
        "pub_date": "2024-03-22",
        "summary": "Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.",
        "translated": ""
    },
    {
        "title": "Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2403.15075v1",
        "pub_date": "2024-03-22",
        "summary": "Recent methods utilize graph contrastive Learning within graph-structured\nuser-item interaction data for collaborative filtering and have demonstrated\ntheir efficacy in recommendation tasks. However, they ignore that the\ndifference relation density of nodes between the user- and item-side causes the\nadaptability of graphs on bilateral nodes to be different after multi-hop graph\ninteraction calculation, which limits existing models to achieve ideal results.\nTo solve this issue, we propose a novel framework for recommendation tasks\ncalled Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that\nconsider the bilateral unsymmetry on user-item node relation density for sliced\nuser and item graph reasoning better with bilateral slicing contrastive\ntraining. Especially, taking into account the aggregation ability of\nhypergraph-based graph convolutional network (GCN) in digging implicit\nsimilarities is more suitable for user nodes, embeddings generated from three\ndifferent modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into\ntwo subviews by the user- and item-side respectively, and selectively combined\ninto subview pairs bilaterally based on the characteristics of inter-node\nrelation structure. Furthermore, to align the distribution of user and item\nembeddings after aggregation, a dispersing loss is leveraged to adjust the\nmutual distance between all embeddings for maintaining learning ability.\nComprehensive experiments on two public datasets have proved the superiority of\nBusGCL in comparison to various recommendation methods. Other models can simply\nutilize our bilateral slicing contrastive learning to enhance recommending\nperformance without incurring extra expenses.",
        "translated": ""
    },
    {
        "title": "ProCQA: A Large-scale Community-based Programming Question Answering\n  Dataset for Code Search",
        "url": "http://arxiv.org/abs/2403.16702v1",
        "pub_date": "2024-03-25",
        "summary": "Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.",
        "translated": ""
    },
    {
        "title": "A comparative analysis of embedding models for patent similarity",
        "url": "http://arxiv.org/abs/2403.16630v1",
        "pub_date": "2024-03-25",
        "summary": "This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.",
        "translated": ""
    },
    {
        "title": "LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent\n  Classification",
        "url": "http://arxiv.org/abs/2403.16504v1",
        "pub_date": "2024-03-25",
        "summary": "Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.",
        "translated": ""
    },
    {
        "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2403.16435v1",
        "pub_date": "2024-03-25",
        "summary": "This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR",
        "translated": ""
    },
    {
        "title": "An Experiment with the Use of ChatGPT for LCSH Subject Assignment on\n  Electronic Theses and Dissertations",
        "url": "http://arxiv.org/abs/2403.16424v1",
        "pub_date": "2024-03-25",
        "summary": "This study delves into the potential use of Large Language Models (LLMs) for\ngenerating Library of Congress Subject Headings (LCSH). The authors employed\nChatGPT to generate subject headings for electronic theses and dissertations\n(ETDs) based on their titles and summaries. The results revealed that although\nsome generated subject headings were valid, there were issues regarding\nspecificity and exhaustiveness. The study showcases that LLMs can serve as a\nstrategic response to the backlog of items awaiting cataloging in academic\nlibraries, while also offering a cost-effective approach for promptly\ngenerating LCSH. Nonetheless, human catalogers remain essential for verifying\nand enhancing the validity, exhaustiveness, and specificity of LCSH generated\nby LLMs.",
        "translated": ""
    },
    {
        "title": "Play to Your Strengths: Collaborative Intelligence of Conventional\n  Recommender Models and Large Language Models",
        "url": "http://arxiv.org/abs/2403.16378v1",
        "pub_date": "2024-03-25",
        "summary": "The rise of large language models (LLMs) has opened new opportunities in\nRecommender Systems (RSs) by enhancing user behavior modeling and content\nunderstanding. However, current approaches that integrate LLMs into RSs solely\nutilize either LLM or conventional recommender model (CRM) to generate final\nrecommendations, without considering which data segments LLM or CRM excel in.\nTo fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books\ndatasets, and compare the performance of a representative CRM (DCNv2) and an\nLLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that\nLLMs excel in data segments where CRMs exhibit lower confidence and precision,\nwhile samples where CRM excels are relatively challenging for LLM, requiring\nsubstantial training data and a long training time for comparable performance.\nThis suggests potential synergies in the combination between LLM and CRM.\nMotivated by these insights, we propose Collaborative Recommendation with\nconventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}). In\nthis framework, we first jointly train LLM and CRM and address the issue of\ndecision boundary shifts through alignment loss. Then, the resource-efficient\nCRM, with a shorter inference time, handles simple and moderate samples, while\nLLM processes the small subset of challenging samples for CRM. Our experimental\nresults demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM\nmethods significantly, underscoring its effectiveness in recommendation tasks.",
        "translated": ""
    },
    {
        "title": "Uncovering Selective State Space Model's Capabilities in Lifelong\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2403.16371v1",
        "pub_date": "2024-03-25",
        "summary": "Sequential Recommenders have been widely applied in various online services,\naiming to model users' dynamic interests from their sequential interactions.\nWith users increasingly engaging with online platforms, vast amounts of\nlifelong user behavioral sequences have been generated. However, existing\nsequential recommender models often struggle to handle such lifelong sequences.\nThe primary challenges stem from computational complexity and the ability to\ncapture long-range dependencies within the sequence. Recently, a state space\nmodel featuring a selective mechanism (i.e., Mamba) has emerged. In this work,\nwe investigate the performance of Mamba for lifelong sequential recommendation\n(i.e., length&gt;=2k). More specifically, we leverage the Mamba block to model\nlifelong user sequences selectively. We conduct extensive experiments to\nevaluate the performance of representative sequential recommendation models in\nthe setting of lifelong sequences. Experiments on two real-world datasets\ndemonstrate the superiority of Mamba. We found that RecMamba achieves\nperformance comparable to the representative model while significantly reducing\ntraining duration by approximately 70% and memory costs by 80%. Codes and data\nare available at \\url{https://github.com/nancheng58/RecMamba}.",
        "translated": ""
    },
    {
        "title": "Enhanced Facet Generation with LLM Editing",
        "url": "http://arxiv.org/abs/2403.16345v1",
        "pub_date": "2024-03-25",
        "summary": "In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.",
        "translated": ""
    },
    {
        "title": "Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior\n  Detection",
        "url": "http://arxiv.org/abs/2403.16151v1",
        "pub_date": "2024-03-24",
        "summary": "The online community has increasingly been inundated by a toxic wave of\nharmful comments. In response to this growing challenge, we introduce a\ntwo-stage ultra-low-cost multimodal harmful behavior detection method designed\nto identify harmful comments and images with high precision and recall rates.\nWe first utilize the CLIP-ViT model to transform tweets and images into\nembeddings, effectively capturing the intricate interplay of semantic meaning\nand subtle contextual clues within texts and images. Then in the second stage,\nthe system feeds these embeddings into a conventional machine learning\nclassifier like SVM or logistic regression, enabling the system to be trained\nrapidly and to perform inference at an ultra-low cost. By converting tweets\ninto rich multimodal embeddings through the CLIP-ViT model and utilizing them\nto train conventional machine learning classifiers, our system is not only\ncapable of detecting harmful textual information with near-perfect performance,\nachieving precision and recall rates above 99\\% but also demonstrates the\nability to zero-shot harmful images without additional training, thanks to its\nmultimodal embedding input. This capability empowers our system to identify\nunseen harmful images without requiring extensive and costly image datasets.\nAdditionally, our system quickly adapts to new harmful content; if a new\nharmful content pattern is identified, we can fine-tune the classifier with the\ncorresponding tweets' embeddings to promptly update the system. This makes it\nwell suited to addressing the ever-evolving nature of online harmfulness,\nproviding online communities with a robust, generalizable, and cost-effective\ntool to safeguard their communities.",
        "translated": ""
    },
    {
        "title": "Complementary Recommendation in E-commerce: Definition, Approaches, and\n  Future Directions",
        "url": "http://arxiv.org/abs/2403.16135v1",
        "pub_date": "2024-03-24",
        "summary": "In recent years, complementary recommendation has received extensive\nattention in the e-commerce domain. In this paper, we comprehensively summarize\nand compare 34 representative studies conducted between 2009 and 2024. Firstly,\nwe compare the data and methods used for modeling complementary relationships\nbetween products, including simple complementarity and more complex scenarios\nsuch as asymmetric complementarity, the coexistence of substitution and\ncomplementarity relationships between products, and varying degrees of\ncomplementarity between different pairs of products. Next, we classify and\ncompare the models based on the research problems of complementary\nrecommendation, such as diversity, personalization, and cold-start.\nFurthermore, we provide a comparative analysis of experimental results from\ndifferent studies conducted on the same dataset, which helps identify the\nstrengths and weaknesses of the research. Compared to previous surveys, this\npaper provides a more updated and comprehensive summary of the research,\ndiscusses future research directions, and contributes to the advancement of\nthis field.",
        "translated": ""
    },
    {
        "title": "Search and Society: Reimagining Information Access for Radical Futures",
        "url": "http://arxiv.org/abs/2403.17901v1",
        "pub_date": "2024-03-26",
        "summary": "Information retrieval (IR) technologies and research are undergoing\ntransformative changes. It is our perspective that the community should accept\nthis opportunity to re-center our research agendas on societal needs while\ndismantling the artificial separation between the work on fairness,\naccountability, transparency, and ethics in IR and the rest of IR research.\nInstead of adopting a reactionary strategy of trying to mitigate potential\nsocial harms from emerging technologies, the community should aim to\nproactively set the research agenda for the kinds of systems we should build\ninspired by diverse explicitly stated sociotechnical imaginaries. The\nsociotechnical imaginaries that underpin the design and development of\ninformation access technologies needs to be explicitly articulated, and we need\nto develop theories of change in context of these diverse perspectives. Our\nguiding future imaginaries must be informed by other academic fields, such as\ndemocratic theory and critical theory, and should be co-developed with social\nscience scholars, legal scholars, civil rights and social justice activists,\nand artists, among others. In this perspective paper, we motivate why the\ncommunity must consider this radical shift in how we do research and what we\nwork on, and sketch a path forward towards this transformation.",
        "translated": ""
    },
    {
        "title": "MIND Your Language: A Multilingual Dataset for Cross-lingual News\n  Recommendation",
        "url": "http://arxiv.org/abs/2403.17876v1",
        "pub_date": "2024-03-26",
        "summary": "Digital news platforms use news recommenders as the main instrument to cater\nto the individual information needs of readers. Despite an increasingly\nlanguage-diverse online community, in which many Internet users consume news in\nmultiple languages, the majority of news recommendation focuses on major,\nresource-rich languages, and English in particular. Moreover, nearly all news\nrecommendation efforts assume monolingual news consumption, whereas more and\nmore users tend to consume information in at least two languages. Accordingly,\nthe existing body of work on news recommendation suffers from a lack of\npublicly available multilingual benchmarks that would catalyze development of\nnews recommenders effective in multilingual settings and for low-resource\nlanguages. Aiming to fill this gap, we introduce xMIND, an open, multilingual\nnews recommendation dataset derived from the English MIND dataset using machine\ntranslation, covering a set of 14 linguistically and geographically diverse\nlanguages, with digital footprints of varying sizes. Using xMIND, we\nsystematically benchmark several state-of-the-art content-based neural news\nrecommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT)\ncross-lingual transfer scenarios, considering both monolingual and bilingual\nnews consumption patterns. Our findings reveal that (i) current NNRs, even when\nbased on a multilingual language model, suffer from substantial performance\nlosses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT\ntraining has limited benefits, particularly when combined with a bilingual news\nconsumption. Our findings thus warrant a broader research effort in\nmultilingual and cross-lingual news recommendation. The xMIND dataset is\navailable at https://github.com/andreeaiana/xMIND.",
        "translated": ""
    },
    {
        "title": "ArabicaQA: A Comprehensive Dataset for Arabic Question Answering",
        "url": "http://arxiv.org/abs/2403.17848v1",
        "pub_date": "2024-03-26",
        "summary": "In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.",
        "translated": ""
    },
    {
        "title": "CaseLink: Inductive Graph Learning for Legal Case Retrieval",
        "url": "http://arxiv.org/abs/2403.17780v1",
        "pub_date": "2024-03-26",
        "summary": "In case law, the precedents are the relevant cases that are used to support\nthe decisions made by the judges and the opinions of lawyers towards a given\ncase. This relevance is referred to as the case-to-case reference relation. To\nefficiently find relevant cases from a large case pool, retrieval tools are\nwidely used by legal practitioners. Existing legal case retrieval models mainly\nwork by comparing the text representations of individual cases. Although they\nobtain a decent retrieval accuracy, the intrinsic case connectivity\nrelationships among cases have not been well exploited for case encoding,\ntherefore limiting the further improvement of retrieval performance. In a case\npool, there are three types of case connectivity relationships: the case\nreference relationship, the case semantic relationship, and the case legal\ncharge relationship. Due to the inductive manner in the task of legal case\nretrieval, using case reference as input is not applicable for testing. Thus,\nin this paper, a CaseLink model based on inductive graph learning is proposed\nto utilise the intrinsic case connectivity for legal case retrieval, a novel\nGlobal Case Graph is incorporated to represent both the case semantic\nrelationship and the case legal charge relationship. A novel contrastive\nobjective with a regularisation on the degree of case nodes is proposed to\nleverage the information carried by the case reference relationship to optimise\nthe model. Extensive experiments have been conducted on two benchmark datasets,\nwhich demonstrate the state-of-the-art performance of CaseLink. The code has\nbeen released on https://github.com/yanran-tang/CaseLink.",
        "translated": ""
    },
    {
        "title": "TWOLAR: a TWO-step LLM-Augmented distillation method for passage\n  Reranking",
        "url": "http://arxiv.org/abs/2403.17759v1",
        "pub_date": "2024-03-26",
        "summary": "In this paper, we present TWOLAR: a two-stage pipeline for passage reranking\nbased on the distillation of knowledge from Large Language Models (LLM). TWOLAR\nintroduces a new scoring strategy and a distillation process consisting in the\ncreation of a novel and diverse training dataset. The dataset consists of 20K\nqueries, each associated with a set of documents retrieved via four distinct\nretrieval methods to ensure diversity, and then reranked by exploiting the\nzero-shot reranking capabilities of an LLM. Our ablation studies demonstrate\nthe contribution of each new component we introduced. Our experimental results\nshow that TWOLAR significantly enhances the document reranking ability of the\nunderlying model, matching and in some cases even outperforming\nstate-of-the-art models with three orders of magnitude more parameters on the\nTREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate\nfuture work we release our data set, finetuned models, and code.",
        "translated": ""
    },
    {
        "title": "All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating\n  Prediction",
        "url": "http://arxiv.org/abs/2403.17740v1",
        "pub_date": "2024-03-26",
        "summary": "Cold-start rating prediction is a fundamental problem in recommender systems\nthat has been extensively studied. Many methods have been proposed that exploit\nexplicit relations among existing data, such as collaborative filtering, social\nrecommendations and heterogeneous information network, to alleviate the data\ninsufficiency issue for cold-start users and items. However, the explicit\nrelations constructed based on data between different roles may be unreliable\nand irrelevant, which limits the performance ceiling of the specific\nrecommendation task. Motivated by this, in this paper, we propose a flexible\nframework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not\nsolely rely on the pre-defined interaction pattern or the manually constructed\nheterogeneous information network. Instead, we devise a Heterogeneous\nInteraction Module (HIM) to jointly model the heterogeneous interactions and\ndirectly infer the important interactions via the observed data. In the\nexperiments, we evaluate our model under three cold-start settings on three\nreal-world datasets. The experimental results show that HIRE outperforms other\nbaselines by a large margin. Furthermore, we visualize the inferred\ninteractions of HIRE to confirm the contribution of our model.",
        "translated": ""
    },
    {
        "title": "EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention",
        "url": "http://arxiv.org/abs/2403.17729v1",
        "pub_date": "2024-03-26",
        "summary": "To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe anisotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.",
        "translated": ""
    },
    {
        "title": "Large Language Models Enhanced Collaborative Filtering",
        "url": "http://arxiv.org/abs/2403.17688v1",
        "pub_date": "2024-03-26",
        "summary": "Recent advancements in Large Language Models (LLMs) have attracted\nconsiderable interest among researchers to leverage these models to enhance\nRecommender Systems (RSs). Existing work predominantly utilizes LLMs to\ngenerate knowledge-rich texts or utilizes LLM-derived embeddings as features to\nimprove RSs. Al- though the extensive world knowledge embedded in LLMs\ngenerally benefits RSs, the application can only take limited number of users\nand items as inputs, without adequately exploiting collaborative filtering\ninformation. Considering its crucial role in RSs, one key challenge in\nenhancing RSs with LLMs lies in providing better collaborative filtering\ninformation through LLMs. In this paper, drawing inspiration from the\nin-context learning and chain of thought reasoning in LLMs, we propose the\nLarge Language Models enhanced Collaborative Filtering (LLM-CF) framework,\nwhich distils the world knowledge and reasoning capabilities of LLMs into\ncollaborative filtering. We also explored a concise and efficient\ninstruction-tuning method, which improves the recommendation capabilities of\nLLMs while preserving their general functionalities (e.g., not decreasing on\nthe LLM benchmark). Comprehensive experiments on three real-world datasets\ndemonstrate that LLM-CF significantly enhances several backbone recommendation\nmodels and consistently outperforms competitive baselines, showcasing its\neffectiveness in distilling the world knowledge and reasoning capabilities of\nLLM into collaborative filtering.",
        "translated": ""
    },
    {
        "title": "S+t-SNE - Bringing dimensionality reduction to data streams",
        "url": "http://arxiv.org/abs/2403.17643v1",
        "pub_date": "2024-03-26",
        "summary": "We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.",
        "translated": ""
    },
    {
        "title": "Retentive Decision Transformer with Adaptive Masking for Reinforcement\n  Learning based Recommendation Systems",
        "url": "http://arxiv.org/abs/2403.17634v1",
        "pub_date": "2024-03-26",
        "summary": "Reinforcement Learning-based Recommender Systems (RLRS) have shown promise\nacross a spectrum of applications, from e-commerce platforms to streaming\nservices. Yet, they grapple with challenges, notably in crafting reward\nfunctions and harnessing large pre-existing datasets within the RL framework.\nRecent advancements in offline RLRS provide a solution for how to address these\ntwo challenges. However, existing methods mainly rely on the transformer\narchitecture, which, as sequence lengths increase, can introduce challenges\nassociated with computational resources and training costs. Additionally, the\nprevalent methods employ fixed-length input trajectories, restricting their\ncapacity to capture evolving user preferences. In this study, we introduce a\nnew offline RLRS method to deal with the above problems. We reinterpret the\nRLRS challenge by modeling sequential decision-making as an inference task,\nleveraging adaptive masking configurations. This adaptive approach selectively\nmasks input tokens, transforming the recommendation task into an inference\nchallenge based on varying token subsets, thereby enhancing the agent's ability\nto infer across diverse trajectory lengths. Furthermore, we incorporate a\nmulti-scale segmented retention mechanism that facilitates efficient modeling\nof long sequences, significantly enhancing computational efficiency. Our\nexperimental analysis, conducted on both online simulator and offline datasets,\nclearly demonstrates the advantages of our proposed method.",
        "translated": ""
    },
    {
        "title": "Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means",
        "url": "http://arxiv.org/abs/2403.18766v1",
        "pub_date": "2024-03-27",
        "summary": "This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.",
        "translated": ""
    },
    {
        "title": "Scaling Laws For Dense Retrieval",
        "url": "http://arxiv.org/abs/2403.18684v1",
        "pub_date": "2024-03-27",
        "summary": "Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.",
        "translated": ""
    },
    {
        "title": "Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users",
        "url": "http://arxiv.org/abs/2403.18667v1",
        "pub_date": "2024-03-27",
        "summary": "Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.",
        "translated": ""
    },
    {
        "title": "To Recommend or Not: Recommendability Identification in Conversations\n  with Pre-trained Language Models",
        "url": "http://arxiv.org/abs/2403.18628v1",
        "pub_date": "2024-03-27",
        "summary": "Most current recommender systems primarily focus on what to recommend,\nassuming users always require personalized recommendations. However, with the\nwidely spread of ChatGPT and other chatbots, a more crucial problem in the\ncontext of conversational systems is how to minimize user disruption when we\nprovide recommendation services for users. While previous research has\nextensively explored different user intents in dialogue systems, fewer efforts\nare made to investigate whether recommendations should be provided. In this\npaper, we formally define the recommendability identification problem, which\naims to determine whether recommendations are necessary in a specific scenario.\nFirst, we propose and define the recommendability identification task, which\ninvestigates the need for recommendations in the current conversational\ncontext. A new dataset is constructed. Subsequently, we discuss and evaluate\nthe feasibility of leveraging pre-trained language models (PLMs) for\nrecommendability identification. Finally, through comparative experiments, we\ndemonstrate that directly employing PLMs with zero-shot results falls short of\nmeeting the task requirements. Besides, fine-tuning or utilizing soft prompt\ntechniques yields comparable results to traditional classification methods. Our\nwork is the first to study recommendability before recommendation and provides\npreliminary ways to make it a fundamental component of the future\nrecommendation system.",
        "translated": ""
    },
    {
        "title": "Antitrust, Amazon, and Algorithmic Auditing",
        "url": "http://arxiv.org/abs/2403.18623v1",
        "pub_date": "2024-03-27",
        "summary": "In digital markets, antitrust law and special regulations aim to ensure that\nmarkets remain competitive despite the dominating role that digital platforms\nplay today in everyone's life. Unlike traditional markets, market participant\nbehavior is easily observable in these markets. We present a series of\nempirical investigations into the extent to which Amazon engages in practices\nthat are typically described as self-preferencing. We discuss how the computer\nscience tools used in this paper can be used in a regulatory environment that\nis based on algorithmic auditing and requires regulating digital markets at\nscale.",
        "translated": ""
    },
    {
        "title": "Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity,\n  and Seasonality into Tourism Recommender Systems",
        "url": "http://arxiv.org/abs/2403.18604v1",
        "pub_date": "2024-03-27",
        "summary": "In an era of information overload and complex decision-making processes,\nRecommender Systems (RS) have emerged as indispensable tools across diverse\ndomains, particularly travel and tourism. These systems simplify trip planning\nby offering personalized recommendations that consider individual preferences\nand address broader challenges like seasonality, travel regulations, and\ncapacity constraints. The intricacies of the tourism domain, characterized by\nmultiple stakeholders, including consumers, item providers, platforms, and\nsociety, underscore the complexity of achieving balance among diverse\ninterests. Although previous research has focused on fairness in Tourism\nRecommender Systems (TRS) from a multistakeholder perspective, limited work has\nfocused on generating sustainable recommendations.\n  Our paper introduces a novel approach for assigning a sustainability\nindicator (SF index) for city trips accessible from the users' starting point,\nintegrating Co2e analysis, destination popularity, and seasonal demand. Our\nmethodology involves comprehensive data gathering on transportation modes and\nemissions, complemented by analyses of destination popularity and seasonal\ndemand. A user study validates our index, showcasing its practicality and\nefficacy in providing well-rounded and sustainable city trip recommendations.\nOur findings contribute significantly to the evolution of responsible tourism\nstrategies, harmonizing the interests of tourists, local communities, and the\nenvironment while paving the way for future research in responsible and\nequitable tourism practices.",
        "translated": ""
    },
    {
        "title": "A Novel Behavior-Based Recommendation System for E-commerce",
        "url": "http://arxiv.org/abs/2403.18536v1",
        "pub_date": "2024-03-27",
        "summary": "The majority of existing recommender systems rely on user ratings, which are\nlimited by the lack of user collaboration and the sparsity problem. To address\nthese issues, this study proposes a behavior-based recommender system that\nleverages customers' natural behaviors, such as browsing and clicking, on\ne-commerce platforms. The proposed recommendation system involves clustering\nactive customers, determining neighborhoods, collecting similar users,\ncalculating product reputation based on similar users, and recommending\nhigh-reputation products. To overcome the complexity of customer behaviors and\ntraditional clustering methods, an unsupervised clustering approach based on\nproduct categories is developed to enhance the recommendation methodology. This\nstudy makes notable contributions in several aspects. Firstly, a groundbreaking\nbehavior-based recommendation methodology is developed, incorporating customer\nbehavior to generate accurate and tailored recommendations leading to improved\ncustomer satisfaction and engagement. Secondly, an original unsupervised\nclustering method, focusing on product categories, enables more precise\nclustering and facilitates accurate recommendations. Finally, an approach to\ndetermine neighborhoods for active customers within clusters is established,\nensuring grouping of customers with similar behavioral patterns to enhance\nrecommendation accuracy and relevance. The proposed recommendation methodology\nand clustering method contribute to improved recommendation performance,\noffering valuable insights for researchers and practitioners in the field of\ne-commerce recommendation systems. Additionally, the proposed method\noutperforms benchmark methods in experiments conducted using a behavior dataset\nfrom the well-known e-commerce site Alibaba.",
        "translated": ""
    },
    {
        "title": "Enhanced Generative Recommendation via Content and Collaboration\n  Integration",
        "url": "http://arxiv.org/abs/2403.18480v1",
        "pub_date": "2024-03-27",
        "summary": "Generative recommendation has emerged as a promising paradigm aimed at\naugmenting recommender systems with recent advancements in generative\nartificial intelligence. This task has been formulated as a\nsequence-to-sequence generation process, wherein the input sequence encompasses\ndata pertaining to the user's previously interacted items, and the output\nsequence denotes the generative identifier for the suggested item. However,\nexisting generative recommendation approaches still encounter challenges in (i)\neffectively integrating user-item collaborative signals and item content\ninformation within a unified generative framework, and (ii) executing an\nefficient alignment between content information and collaborative signals.\n  In this paper, we introduce content-based collaborative generation for\nrecommender systems, denoted as ColaRec. To capture collaborative signals, the\ngenerative item identifiers are derived from a pretrained collaborative\nfiltering model, while the user is represented through the aggregation of\ninteracted items' content. Subsequently, the aggregated textual description of\nitems is fed into a language model to encapsulate content information. This\nintegration enables ColaRec to amalgamate collaborative signals and content\ninformation within an end-to-end framework. Regarding the alignment, we propose\nan item indexing task to facilitate the mapping between the content-based\nsemantic space and the interaction-based collaborative space. Additionally, a\ncontrastive loss is introduced to ensure that items with similar collaborative\nGIDs possess comparable content representations, thereby enhancing alignment.\nTo validate the efficacy of ColaRec, we conduct experiments on three benchmark\ndatasets. Empirical results substantiate the superior performance of ColaRec.",
        "translated": ""
    },
    {
        "title": "Lightweight Embeddings for Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2403.18479v1",
        "pub_date": "2024-03-27",
        "summary": "Graph neural networks (GNNs) are currently one of the most performant\ncollaborative filtering methods. Meanwhile, owing to the use of an embedding\ntable to represent each user/item as a distinct vector, GNN-based recommenders\nhave inherited the long-standing defect of parameter inefficiency. As a common\npractice for scalable embeddings, parameter sharing enables the use of fewer\nembedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most\nexisting methods are a heuristically designed, predefined mapping from each\nuser's/item's ID to the corresponding meta-embedding indexes, thus simplifying\nthe optimization problem into learning only the meta-embeddings. However, in\nthe context of GNN-based collaborative filtering, such a fixed mapping omits\nthe semantic correlations between entities that are evident in the user-item\ninteraction graph, leading to suboptimal recommendation performance. To this\nend, we propose Lightweight Embeddings for Graph Collaborative Filtering\n(LEGCF), a parameter-efficient embedding framework dedicated to GNN-based\nrecommenders. LEGCF innovatively introduces an assignment matrix as an extra\nlearnable component on top of meta-embeddings. To jointly optimize these two\nheavily entangled components, aside from learning the meta-embeddings by\nminimizing the recommendation loss, LEGCF further performs efficient assignment\nupdate by enforcing a novel semantic similarity constraint and finding its\nclosed-form solution based on matrix pseudo-inverse. The meta-embeddings and\nassignment matrix are alternately updated, where the latter is sparsified on\nthe fly to ensure negligible storage overhead. Extensive experiments on three\nbenchmark datasets have verified LEGCF's smallest trade-off between size and\nperformance, with consistent accuracy gain over state-of-the-art baselines. The\ncodebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.",
        "translated": ""
    },
    {
        "title": "Decoy Effect In Search Interaction: Understanding User Behavior and\n  Measuring System Vulnerability",
        "url": "http://arxiv.org/abs/2403.18462v1",
        "pub_date": "2024-03-27",
        "summary": "This study examines the decoy effect's underexplored influence on user search\ninteractions and methods for measuring information retrieval (IR) systems'\nvulnerability to this effect. It explores how decoy results alter users'\ninteractions on search engine result pages, focusing on metrics like\nclick-through likelihood, browsing time, and perceived document usefulness. By\nanalyzing user interaction logs from multiple datasets, the study demonstrates\nthat decoy results significantly affect users' behavior and perceptions.\nFurthermore, it investigates how different levels of task difficulty and user\nknowledge modify the decoy effect's impact, finding that easier tasks and lower\nknowledge levels lead to higher engagement with target documents. In terms of\nIR system evaluation, the study introduces the DEJA-VU metric to assess\nsystems' susceptibility to the decoy effect, testing it on specific retrieval\ntasks. The results show differences in systems' effectiveness and\nvulnerability, contributing to our understanding of cognitive biases in search\nbehavior and suggesting pathways for creating more balanced and bias-aware IR\nevaluations.",
        "translated": ""
    },
    {
        "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
        "url": "http://arxiv.org/abs/2403.19651v1",
        "pub_date": "2024-03-28",
        "summary": "Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.",
        "translated": ""
    },
    {
        "title": "Croissant: A Metadata Format for ML-Ready Datasets",
        "url": "http://arxiv.org/abs/2403.19546v1",
        "pub_date": "2024-03-28",
        "summary": "Data is a critical resource for Machine Learning (ML), yet working with data\nremains a key friction point. This paper introduces Croissant, a metadata\nformat for datasets that simplifies how data is used by ML tools and\nframeworks. Croissant makes datasets more discoverable, portable and\ninteroperable, thereby addressing significant challenges in ML data management\nand responsible AI. Croissant is already supported by several popular dataset\nrepositories, spanning hundreds of thousands of datasets, ready to be loaded\ninto the most popular ML frameworks.",
        "translated": ""
    },
    {
        "title": "Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual\n  User Behaviors",
        "url": "http://arxiv.org/abs/2403.19347v1",
        "pub_date": "2024-03-28",
        "summary": "With the rise of large language models (LLMs), recent works have leveraged\nLLMs to improve the performance of click-through rate (CTR) prediction.\nHowever, we argue that a critical obstacle remains in deploying LLMs for\npractical use: the efficiency of LLMs when processing long textual user\nbehaviors. As user sequences grow longer, the current efficiency of LLMs is\ninadequate for training on billions of users and items. To break through the\nefficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical\nEncoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.\nSpecifically, BAHE proposes a novel hierarchical architecture that decouples\nthe encoding of user behaviors from inter-behavior interactions. Firstly, to\nprevent computational redundancy from repeated encoding of identical user\nbehaviors, BAHE employs the LLM's pre-trained shallow layers to extract\nembeddings of the most granular, atomic user behaviors from extensive user\nsequences and stores them in the offline database. Subsequently, the deeper,\ntrainable layers of the LLM facilitate intricate inter-behavior interactions,\nthereby generating comprehensive user embeddings. This separation allows the\nlearning of high-level user representations to be independent of low-level\nbehavior encoding, significantly reducing computational complexity. Finally,\nthese refined user embeddings, in conjunction with correspondingly processed\nitem embeddings, are incorporated into the CTR model to compute the CTR scores.\nExtensive experimental results show that BAHE reduces training time and memory\nby five times for CTR models using LLMs, especially with longer user sequences.\nBAHE has been deployed in a real-world system, allowing for daily updates of 50\nmillion CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR\nprediction.",
        "translated": ""
    },
    {
        "title": "Intelligent Classification and Personalized Recommendation of E-commerce\n  Products Based on Machine Learning",
        "url": "http://arxiv.org/abs/2403.19345v1",
        "pub_date": "2024-03-28",
        "summary": "With the rapid evolution of the Internet and the exponential proliferation of\ninformation, users encounter information overload and the conundrum of choice.\nPersonalized recommendation systems play a pivotal role in alleviating this\nburden by aiding users in filtering and selecting information tailored to their\npreferences and requirements. Such systems not only enhance user experience and\nsatisfaction but also furnish opportunities for businesses and platforms to\naugment user engagement, sales, and advertising efficacy.This paper undertakes\na comparative analysis between the operational mechanisms of traditional\ne-commerce commodity classification systems and personalized recommendation\nsystems. It delineates the significance and application of personalized\nrecommendation systems across e-commerce, content information, and media\ndomains. Furthermore, it delves into the challenges confronting personalized\nrecommendation systems in e-commerce, including data privacy, algorithmic bias,\nscalability, and the cold start problem. Strategies to address these challenges\nare elucidated.Subsequently, the paper outlines a personalized recommendation\nsystem leveraging the BERT model and nearest neighbor algorithm, specifically\ntailored to address the exigencies of the eBay e-commerce platform. The\nefficacy of this recommendation system is substantiated through manual\nevaluation, and a practical application operational guide and structured output\nrecommendation results are furnished to ensure the system's operability and\nscalability.",
        "translated": ""
    },
    {
        "title": "Generate then Retrieve: Conversational Response Retrieval Using LLMs as\n  Answer and Query Generators",
        "url": "http://arxiv.org/abs/2403.19302v1",
        "pub_date": "2024-03-28",
        "summary": "CIS is a prominent area in IR that focuses on developing interactive\nknowledge assistants. These systems must adeptly comprehend the user's\ninformation requirements within the conversational context and retrieve the\nrelevant information. To this aim, the existing approaches model the user's\ninformation needs with one query called rewritten query and use this query for\npassage retrieval. In this paper, we propose three different methods for\ngenerating multiple queries to enhance the retrieval. In these methods, we\nleverage the capabilities of large language models (LLMs) in understanding the\nuser's information need and generating an appropriate response, to generate\nmultiple queries. We implement and evaluate the proposed models utilizing\nvarious LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot\nsettings. In addition, we propose a new benchmark for TREC iKAT based on gpt\n3.5 judgments. Our experiments reveal the effectiveness of our proposed models\non the TREC iKAT dataset.",
        "translated": ""
    },
    {
        "title": "Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling\n  in Recommender Systems",
        "url": "http://arxiv.org/abs/2403.19276v1",
        "pub_date": "2024-03-28",
        "summary": "In implicit collaborative filtering, hard negative mining techniques are\ndeveloped to accelerate and enhance the recommendation model learning. However,\nthe inadvertent selection of false negatives remains a major concern in hard\nnegative sampling, as these false negatives can provide incorrect information\nand mislead the model learning. To date, only a small number of studies have\nbeen committed to solve the false negative problem, primarily focusing on\ndesigning sophisticated sampling algorithms to filter false negatives. In\ncontrast, this paper shifts its focus to refining the loss function. We find\nthat the original Bayesian Personalized Ranking (BPR), initially designed for\nuniform negative sampling, is inadequate in adapting to hard sampling\nscenarios. Hence, we introduce an enhanced Bayesian Personalized Ranking\nobjective, named as Hard-BPR, which is specifically crafted for dynamic hard\nnegative sampling to mitigate the influence of false negatives. This method is\nsimple yet efficient for real-world deployment. Extensive experiments conducted\non three real-world datasets demonstrate the effectiveness and robustness of\nour approach, along with the enhanced ability to distinguish false negatives.",
        "translated": ""
    },
    {
        "title": "Are Large Language Models Good at Utility Judgments?",
        "url": "http://arxiv.org/abs/2403.19216v1",
        "pub_date": "2024-03-28",
        "summary": "Retrieval-augmented generation (RAG) is considered to be a promising approach\nto alleviate the hallucination issue of large language models (LLMs), and it\nhas received widespread attention from researchers recently. Due to the\nlimitation in the semantic understanding of retrieval models, the success of\nRAG heavily lies on the ability of LLMs to identify passages with utility.\nRecent efforts have explored the ability of LLMs to assess the relevance of\npassages in retrieval, but there has been limited work on evaluating the\nutility of passages in supporting question answering. In this work, we conduct\na comprehensive study about the capabilities of LLMs in utility evaluation for\nopen-domain QA. Specifically, we introduce a benchmarking procedure and\ncollection of candidate passages with different characteristics, facilitating a\nseries of experiments with five representative LLMs. Our experiments reveal\nthat: (i) well-instructed LLMs can distinguish between relevance and utility,\nand that LLMs are highly receptive to newly generated counterfactual passages.\nMoreover, (ii) we scrutinize key factors that affect utility judgments in the\ninstruction design. And finally, (iii) to verify the efficacy of utility\njudgments in practical retrieval augmentation applications, we delve into LLMs'\nQA capabilities using the evidence judged with utility and direct dense\nretrieval results. (iv) We propose a k-sampling, listwise approach to reduce\nthe dependency of LLMs on the sequence of input passages, thereby facilitating\nsubsequent answer generation. We believe that the way we formalize and study\nthe problem along with our findings contributes to a critical assessment of\nretrieval-augmented LLMs. Our code and benchmark can be found at\n\\url{https://github.com/ict-bigdatalab/utility_judgments}.",
        "translated": ""
    },
    {
        "title": "Make Large Language Model a Better Ranker",
        "url": "http://arxiv.org/abs/2403.19181v1",
        "pub_date": "2024-03-28",
        "summary": "The evolution of Large Language Models (LLMs) has significantly enhanced\ncapabilities across various fields, leading to a paradigm shift in how\nRecommender Systems (RSs) are conceptualized and developed. However, existing\nresearch primarily focuses on point-wise and pair-wise recommendation\nparadigms. These approaches prove inefficient in LLM-based recommenders due to\nthe high computational cost of utilizing Large Language Models. While some\nstudies have delved into list-wise approaches, they fall short in ranking\ntasks. This shortfall is attributed to the misalignment between the objectives\nof ranking and language generation. To this end, this paper introduces the\nLanguage Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO\nis designed to bridge the gap between the capabilities of LLMs and the nuanced\nrequirements of ranking tasks within recommender systems. A key feature of ALRO\nis the introduction of soft lambda loss, an adaptation of lambda loss tailored\nto suit language generation tasks. Additionally, ALRO incorporates a\npermutation-sensitive learning mechanism that addresses position bias, a\nprevalent issue in generative models, without imposing additional computational\nburdens during inference. Our evaluative studies reveal that ALRO outperforms\nexisting embedding-based recommendation methods and the existing LLM-based\nrecommendation baselines, highlighting its efficacy.",
        "translated": ""
    },
    {
        "title": "Instruction-based Hypergraph Pretraining",
        "url": "http://arxiv.org/abs/2403.19063v1",
        "pub_date": "2024-03-28",
        "summary": "Pretraining has been widely explored to augment the adaptability of graph\nlearning models to transfer knowledge from large datasets to a downstream task,\nsuch as link prediction or classification. However, the gap between training\nobjectives and the discrepancy between data distributions in pretraining and\ndownstream tasks hinders the transfer of the pretrained knowledge. Inspired by\ninstruction-based prompts widely used in pretrained language models, we\nintroduce instructions into graph pretraining. In this paper, we propose a\nnovel pretraining framework named Instruction-based Hypergraph Pretraining. To\novercome the discrepancy between pretraining and downstream tasks, text-based\ninstructions are applied to provide explicit guidance on specific tasks for\nrepresentation learning. Compared to learnable prompts, whose effectiveness\ndepends on the quality and the diversity of training data, text-based\ninstructions intrinsically encapsulate task information and support the model\nto generalize beyond the structure seen during pretraining. To capture\nhigh-order relations with task information in a context-aware manner, a novel\nprompting hypergraph convolution layer is devised to integrate instructions\ninto information propagation in hypergraphs. Extensive experiments conducted on\nthree public datasets verify the superiority of IHP in various scenarios.",
        "translated": ""
    },
    {
        "title": "Towards LLM-RecSys Alignment with Textual ID Learning",
        "url": "http://arxiv.org/abs/2403.19021v1",
        "pub_date": "2024-03-27",
        "summary": "Generative recommendation based on Large Language Models (LLMs) have\ntransformed the traditional ranking-based recommendation style into a\ntext-to-text generation paradigm. However, in contrast to standard NLP tasks\nthat inherently operate on human vocabulary, current research in generative\nrecommendations struggles to effectively encode recommendation items within the\ntext-to-text framework using concise yet meaningful ID representations. To\nbetter align LLMs with recommendation needs, we propose IDGen, representing\neach item as a unique, concise, semantically rich, platform-agnostic textual ID\nusing human language tokens. This is achieved by training a textual ID\ngenerator alongside the LLM-based recommender, enabling seamless integration of\npersonalized recommendations into natural language generation. Notably, as user\nhistory is expressed in natural language and decoupled from the original\ndataset, our approach suggests the potential for a foundational generative\nrecommendation model. Experiments show that our framework consistently\nsurpasses existing models in sequential recommendation under standard\nexperimental setting. Then, we explore the possibility of training a foundation\nrecommendation model with the proposed method on data collected from 19\ndifferent datasets and tested its recommendation performance on 6 unseen\ndatasets across different platforms under a completely zero-shot setting. The\nresults show that the zero-shot performance of the pre-trained foundation model\nis comparable to or even better than some traditional recommendation models\nbased on supervised training, showing the potential of the IDGen paradigm\nserving as the foundation model for generative recommendation. Code and data\nare open-sourced at https://github.com/agiresearch/IDGenRec.",
        "translated": ""
    },
    {
        "title": "Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and\n  Hierarchy-Aware Domain Disentanglement",
        "url": "http://arxiv.org/abs/2403.20298v1",
        "pub_date": "2024-03-29",
        "summary": "The issue of data sparsity poses a significant challenge to recommender\nsystems. In response to this, algorithms that leverage side information such as\nreview texts have been proposed. Furthermore, Cross-Domain Recommendation\n(CDR), which captures domain-shareable knowledge and transfers it from a richer\ndomain (source) to a sparser one (target), has received notable attention.\nNevertheless, the majority of existing methodologies assume a Euclidean\nembedding space, encountering difficulties in accurately representing richer\ntext information and managing complex interactions between users and items.\nThis paper advocates a hyperbolic CDR approach based on review texts for\nmodeling user-item relationships. We first emphasize that conventional\ndistance-based domain alignment techniques may cause problems because small\nmodifications in hyperbolic geometry result in magnified perturbations,\nultimately leading to the collapse of hierarchical structures. To address this\nchallenge, we propose hierarchy-aware embedding and domain alignment schemes\nthat adjust the scale to extract domain-shareable information without\ndisrupting structural forms. The process involves the initial embedding of\nreview texts in hyperbolic space, followed by feature extraction incorporating\ndegree-based normalization and structure alignment. We conducted extensive\nexperiments to substantiate the efficiency, robustness, and scalability of our\nproposed model in comparison to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "Aiming at the Target: Filter Collaborative Information for Cross-Domain\n  Recommendation",
        "url": "http://arxiv.org/abs/2403.20296v1",
        "pub_date": "2024-03-29",
        "summary": "Cross-domain recommender (CDR) systems aim to enhance the performance of the\ntarget domain by utilizing data from other related domains. However, irrelevant\ninformation from the source domain may instead degrade target domain\nperformance, which is known as the negative transfer problem. There have been\nsome attempts to address this problem, mostly by designing adaptive\nrepresentations for overlapped users. Whereas, representation adaptions solely\nrely on the expressive capacity of the CDR model, lacking explicit constraint\nto filter the irrelevant source-domain collaborative information for the target\ndomain.\n  In this paper, we propose a novel Collaborative information regularized User\nTransformation (CUT) framework to tackle the negative transfer problem by\ndirectly filtering users' collaborative information. In CUT, user similarity in\nthe target domain is adopted as a constraint for user transformation learning\nto filter the user collaborative information from the source domain. CUT first\nlearns user similarity relationships from the target domain. Then,\nsource-target information transfer is guided by the user similarity, where we\ndesign a user transformation layer to learn target-domain user representations\nand a contrastive loss to supervise the user collaborative information\ntransferred. The results show significant performance improvement of CUT\ncompared with SOTA single and cross-domain methods. Further analysis of the\ntarget-domain results illustrates that CUT can effectively alleviate the\nnegative transfer problem.",
        "translated": ""
    },
    {
        "title": "Shallow Cross-Encoders for Low-Latency Retrieval",
        "url": "http://arxiv.org/abs/2403.20222v1",
        "pub_date": "2024-03-29",
        "summary": "Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in\ntext retrieval. However, Cross-Encoders based on large transformer models (such\nas BERT or T5) are computationally expensive and allow for scoring only a small\nnumber of documents within a reasonably small latency window. However, keeping\nsearch latencies low is important for user satisfaction and energy usage. In\nthis paper, we show that weaker shallow transformer models (i.e., transformers\nwith a limited number of layers) actually perform better than full-scale models\nwhen constrained to these practical low-latency settings since they can\nestimate the relevance of more documents in the same time budget. We further\nshow that shallow transformers may benefit from the generalized Binary\nCross-Entropy (gBCE) training scheme, which has recently demonstrated success\nfor recommendation tasks. Our experiments with TREC Deep Learning passage\nranking query sets demonstrate significant improvements in shallow and\nfull-scale models in low-latency scenarios. For example, when the latency limit\nis 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT\nmodel) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while\nTinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches\nNDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallow\nCross-Encoders are effective even when used without a GPU (e.g., with CPU\ninference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms\nlatency), which makes Cross-Encoders practical to run even without specialized\nhardware acceleration.",
        "translated": ""
    },
    {
        "title": "Dual Simplex Volume Maximization for Simplex-Structured Matrix\n  Factorization",
        "url": "http://arxiv.org/abs/2403.20197v1",
        "pub_date": "2024-03-29",
        "summary": "Simplex-structured matrix factorization (SSMF) is a generalization of\nnonnegative matrix factorization, a fundamental interpretable data analysis\nmodel, and has applications in hyperspectral unmixing and topic modeling. To\nobtain identifiable solutions, a standard approach is to find minimum-volume\nsolutions. By taking advantage of the duality/polarity concept for polytopes,\nwe convert minimum-volume SSMF in the primal space to a maximum-volume problem\nin the dual space. We first prove the identifiability of this maximum-volume\ndual problem. Then, we use this dual formulation to provide a novel\noptimization approach which bridges the gap between two existing families of\nalgorithms for SSMF, namely volume minimization and facet identification.\nNumerical experiments show that the proposed approach performs favorably\ncompared to the state-of-the-art SSMF algorithms.",
        "translated": ""
    },
    {
        "title": "Robust Federated Contrastive Recommender System against Model Poisoning\n  Attack",
        "url": "http://arxiv.org/abs/2403.20107v1",
        "pub_date": "2024-03-29",
        "summary": "Federated Recommender Systems (FedRecs) have garnered increasing attention\nrecently, thanks to their privacy-preserving benefits. However, the\ndecentralized and open characteristics of current FedRecs present two dilemmas.\nFirst, the performance of FedRecs is compromised due to highly sparse on-device\ndata for each client. Second, the system's robustness is undermined by the\nvulnerability to model poisoning attacks launched by malicious users. In this\npaper, we introduce a novel contrastive learning framework designed to fully\nleverage the client's sparse data through embedding augmentation, referred to\nas CL4FedRec. Unlike previous contrastive learning approaches in FedRecs that\nnecessitate clients to share their private parameters, our CL4FedRec aligns\nwith the basic FedRec learning protocol, ensuring compatibility with most\nexisting FedRec implementations. We then evaluate the robustness of FedRecs\nequipped with CL4FedRec by subjecting it to several state-of-the-art model\npoisoning attacks. Surprisingly, our observations reveal that contrastive\nlearning tends to exacerbate the vulnerability of FedRecs to these attacks.\nThis is attributed to the enhanced embedding uniformity, making the polluted\ntarget item embedding easily proximate to popular items. Based on this insight,\nwe propose an enhanced and robust version of CL4FedRec (rCL4FedRec) by\nintroducing a regularizer to maintain the distance among item embeddings with\ndifferent popularity levels. Extensive experiments conducted on four commonly\nused recommendation datasets demonstrate that CL4FedRec significantly enhances\nboth the model's performance and the robustness of FedRecs.",
        "translated": ""
    },
    {
        "title": "KGUF: Simple Knowledge-aware Graph-based Recommender with User-based\n  Semantic Features Filtering",
        "url": "http://arxiv.org/abs/2403.20095v1",
        "pub_date": "2024-03-29",
        "summary": "The recent integration of Graph Neural Networks (GNNs) into recommendation\nhas led to a novel family of Collaborative Filtering (CF) approaches, namely\nGraph Collaborative Filtering (GCF). Following the same GNNs wave, recommender\nsystems exploiting Knowledge Graphs (KGs) have also been successfully empowered\nby the GCF rationale to combine the representational power of GNNs with the\nsemantics conveyed by KGs, giving rise to Knowledge-aware Graph Collaborative\nFiltering (KGCF), which use KGs to mine hidden user intent. Nevertheless,\nempirical evidence suggests that computing and combining user-level intent\nmight not always be necessary, as simpler approaches can yield comparable or\nsuperior results while keeping explicit semantic features. Under this\nperspective, user historical preferences become essential to refine the KG and\nretain the most discriminating features, thus leading to concise item\nrepresentation. Driven by the assumptions above, we propose KGUF, a KGCF model\nthat learns latent representations of semantic features in the KG to better\ndefine the item profile. By leveraging user profiles through decision trees,\nKGUF effectively retains only those features relevant to users. Results on\nthree datasets justify KGUF's rationale, as our approach is able to reach\nperformance comparable or superior to SOTA methods while maintaining a simpler\nformalization. Link to the repository: https://github.com/sisinflab/KGUF.",
        "translated": ""
    },
    {
        "title": "Inclusive Design Insights from a Preliminary Image-Based Conversational\n  Search Systems Evaluation",
        "url": "http://arxiv.org/abs/2403.19899v1",
        "pub_date": "2024-03-29",
        "summary": "The digital realm has witnessed the rise of various search modalities, among\nwhich the Image-Based Conversational Search System stands out. This research\ndelves into the design, implementation, and evaluation of this specific system,\njuxtaposing it against its text-based and mixed counterparts. A diverse\nparticipant cohort ensures a broad evaluation spectrum. Advanced tools\nfacilitate emotion analysis, capturing user sentiments during interactions,\nwhile structured feedback sessions offer qualitative insights. Results indicate\nthat while the text-based system minimizes user confusion, the image-based\nsystem presents challenges in direct information interpretation. However, the\nmixed system achieves the highest engagement, suggesting an optimal blend of\nvisual and textual information. Notably, the potential of these systems,\nespecially the image-based modality, to assist individuals with intellectual\ndisabilities is highlighted. The study concludes that the Image-Based\nConversational Search System, though challenging in some aspects, holds\npromise, especially when integrated into a mixed system, offering both clarity\nand engagement.",
        "translated": ""
    },
    {
        "title": "Towards a Robust Retrieval-Based Summarization System",
        "url": "http://arxiv.org/abs/2403.19889v1",
        "pub_date": "2024-03-29",
        "summary": "This paper describes an investigation of the robustness of large language\nmodels (LLMs) for retrieval augmented generation (RAG)-based summarization\ntasks. While LLMs provide summarization capabilities, their performance in\ncomplex, real-world scenarios remains under-explored. Our first contribution is\nLogicSumm, an innovative evaluation framework incorporating realistic scenarios\nto assess LLM robustness during RAG-based summarization. Based on limitations\nidentified by LogiSumm, we then developed SummRAG, a comprehensive system to\ncreate training dialogues and fine-tune a model to enhance robustness within\nLogicSumm's scenarios. SummRAG is an example of our goal of defining structured\nmethods to test the capabilities of an LLM, rather than addressing issues in a\none-off fashion. Experimental results confirm the power of SummRAG, showcasing\nimproved logical coherence and summarization quality. Data, corresponding model\nweights, and Python code are available online.",
        "translated": ""
    },
    {
        "title": "Dealing with Missing Modalities in Multimodal Recommendation: a Feature\n  Propagation-based Approach",
        "url": "http://arxiv.org/abs/2403.19841v1",
        "pub_date": "2024-03-28",
        "summary": "Multimodal recommender systems work by augmenting the representation of the\nproducts in the catalogue through multimodal features extracted from images,\ntextual descriptions, or audio tracks characterising such products.\nNevertheless, in real-world applications, only a limited percentage of products\ncome with multimodal content to extract meaningful features from, making it\nhard to provide accurate recommendations. To the best of our knowledge, very\nfew attention has been put into the problem of missing modalities in multimodal\nrecommendation so far. To this end, our paper comes as a preliminary attempt to\nformalise and address such an issue. Inspired by the recent advances in graph\nrepresentation learning, we propose to re-sketch the missing modalities problem\nas a problem of missing graph node features to apply the state-of-the-art\nfeature propagation algorithm eventually. Technically, we first project the\nuser-item graph into an item-item one based on co-interactions. Then,\nleveraging the multimodal similarities among co-interacted items, we apply a\nmodified version of the feature propagation technique to impute the missing\nmultimodal features. Adopted as a pre-processing stage for two recent\nmultimodal recommender systems, our simple approach performs better than other\nshallower solutions on three popular datasets.",
        "translated": ""
    },
    {
        "title": "Rematch: Robust and Efficient Matching of Local Knowledge Graphs to\n  Improve Structural and Semantic Similarity",
        "url": "http://arxiv.org/abs/2404.02126v1",
        "pub_date": "2024-04-02",
        "summary": "Knowledge graphs play a pivotal role in various applications, such as\nquestion-answering and fact-checking. Abstract Meaning Representation (AMR)\nrepresents text as knowledge graphs. Evaluating the quality of these graphs\ninvolves matching them structurally to each other and semantically to the\nsource text. Existing AMR metrics are inefficient and struggle to capture\nsemantic similarity. We also lack a systematic evaluation benchmark for\nassessing structural similarity between AMR graphs. To overcome these\nlimitations, we introduce a novel AMR similarity metric, rematch, alongside a\nnew evaluation for structural similarity called RARE. Among state-of-the-art\nmetrics, rematch ranks second in structural similarity; and first in semantic\nsimilarity by 1--5 percentage points on the STS-B and SICK-R benchmarks.\nRematch is also five times faster than the next most efficient metric.",
        "translated": ""
    },
    {
        "title": "IISAN: Efficiently Adapting Multimodal Representation for Sequential\n  Recommendation with Decoupled PEFT",
        "url": "http://arxiv.org/abs/2404.02059v1",
        "pub_date": "2024-04-02",
        "summary": "Multimodal foundation models are transformative in sequential recommender\nsystems, leveraging powerful representation learning capabilities. While\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation\nmodels for recommendation tasks, most research prioritizes parameter\nefficiency, often overlooking critical factors like GPU memory efficiency and\ntraining speed. Addressing this gap, our paper introduces IISAN (Intra- and\nInter-modal Side Adapted Network for Multimodal Representation), a simple\nplug-and-play architecture using a Decoupled PEFT structure and exploiting both\nintra- and inter-modal adaptation.\n  IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art\nPEFT. More importantly, it significantly reduces GPU memory usage - from 47GB\nto just 3GB for multimodal sequential recommendation tasks. Additionally, it\naccelerates training time per epoch from 443s to 22s compared to FFT. This is\nalso a notable improvement over the Adapter and LoRA, which require 37-39 GB\nGPU memory and 350-380 seconds per epoch for training.\n  Furthermore, we propose a new composite efficiency metric, TPME\n(Training-time, Parameter, and GPU Memory Efficiency) to alleviate the\nprevalent misconception that \"parameter efficiency represents overall\nefficiency\". TPME provides more comprehensive insights into practical\nefficiency comparisons between different methods. Besides, we give an\naccessible efficiency analysis of all PEFT and FFT approaches, which\ndemonstrate the superiority of IISAN. We release our codes and other materials\nat https://github.com/jjGenAILab/IISAN.",
        "translated": ""
    },
    {
        "title": "Where to Move Next: Zero-shot Generalization of LLMs for Next POI\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.01855v1",
        "pub_date": "2024-04-02",
        "summary": "Next Point-of-interest (POI) recommendation provides valuable suggestions for\nusers to explore their surrounding environment. Existing studies rely on\nbuilding recommendation models from large-scale users' check-in data, which is\ntask-specific and needs extensive computational resources. Recently, the\npretrained large language models (LLMs) have achieved significant advancements\nin various NLP tasks and have also been investigated for recommendation\nscenarios. However, the generalization abilities of LLMs still are unexplored\nto address the next POI recommendations, where users' geographical movement\npatterns should be extracted. Although there are studies that leverage LLMs for\nnext-item recommendations, they fail to consider the geographical influence and\nsequential transitions. Hence, they cannot effectively solve the next POI\nrecommendation task. To this end, we design novel prompting strategies and\nconduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for\npredicting a user's next check-in. Specifically, we consider several essential\nfactors in human movement behaviors, including user geographical preference,\nspatial distance, and sequential transitions, and formulate the recommendation\ntask as a ranking problem. Through extensive experiments on two widely used\nreal-world datasets, we derive several key findings. Empirical evaluations\ndemonstrate that LLMs have promising zero-shot recommendation abilities and can\nprovide accurate and reasonable predictions. We also reveal that LLMs cannot\naccurately comprehend geographical context information and are sensitive to the\norder of presentation of candidate POIs, which shows the limitations of LLMs\nand necessitates further research on robust human mobility reasoning\nmechanisms.",
        "translated": ""
    },
    {
        "title": "CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling",
        "url": "http://arxiv.org/abs/2404.01735v1",
        "pub_date": "2024-04-02",
        "summary": "Product bundling has been a prevailing marketing strategy that is beneficial\nin the online shopping scenario. Effective product bundling methods depend on\nhigh-quality item representations, which need to capture both the individual\nitems' semantics and cross-item relations. However, previous item\nrepresentation learning methods, either feature fusion or graph learning,\nsuffer from inadequate cross-modal alignment and struggle to capture the\ncross-item relations for cold-start items. Multimodal pre-train models could be\nthe potential solutions given their promising performance on various multimodal\ndownstream tasks. However, the cross-item relations have been under-explored in\nthe current multimodal pre-train models. To bridge this gap, we propose a novel\nand simple framework Cross-Item Relational Pre-training (CIRP) for item\nrepresentation learning in product bundling. Specifically, we employ a\nmultimodal encoder to generate image and text representations. Then we leverage\nboth the cross-item contrastive loss (CIC) and individual item's image-text\ncontrastive loss (ITC) as the pre-train objectives. Our method seeks to\nintegrate cross-item relation modeling capability into the multimodal encoder,\nwhile preserving the in-depth aligned multimodal semantics. Therefore, even for\ncold-start items that have no relations, their representations are still\nrelation-aware. Furthermore, to eliminate the potential noise and reduce the\ncomputational cost, we harness a relation pruning module to remove the noisy\nand redundant relations. We apply the item representations extracted by CIRP to\nthe product bundling model ItemKNN, and experiments on three e-commerce\ndatasets demonstrate that CIRP outperforms various leading representation\nlearning methods.",
        "translated": ""
    },
    {
        "title": "Entity Disambiguation via Fusion Entity Decoding",
        "url": "http://arxiv.org/abs/2404.01626v1",
        "pub_date": "2024-04-02",
        "summary": "Entity disambiguation (ED), which links the mentions of ambiguous entities to\ntheir referent entities in a knowledge base, serves as a core component in\nentity linking (EL). Existing generative approaches demonstrate improved\naccuracy compared to classification approaches under the standardized ZELDA\nbenchmark. Nevertheless, generative approaches suffer from the need for\nlarge-scale pre-training and inefficient generation. Most importantly, entity\ndescriptions, which could contain crucial information to distinguish similar\nentities from each other, are often overlooked. We propose an encoder-decoder\nmodel to disambiguate entities with more detailed entity descriptions. Given\ntext and candidate entities, the encoder learns interactions between the text\nand each candidate entity, producing representations for each entity candidate.\nThe decoder then fuses the representations of entity candidates together and\nselects the correct entity. Our experiments, conducted on various entity\ndisambiguation benchmarks, demonstrate the strong and robust performance of\nthis model, particularly +1.5% in the ZELDA benchmark compared with GENRE.\nFurthermore, we integrate this approach into the retrieval/reader framework and\nobserve +1.5% improvements in end-to-end entity linking in the GERBIL benchmark\ncompared with EntQA.",
        "translated": ""
    },
    {
        "title": "Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems",
        "url": "http://arxiv.org/abs/2404.01616v1",
        "pub_date": "2024-04-02",
        "summary": "Large language models (LLMs) are trained on text-only data that go far beyond\nthe languages with paired speech and text data. At the same time, Dual Encoder\n(DE) based retrieval systems project queries and documents into the same\nembedding space and have demonstrated their success in retrieval and bi-text\nmining. To match speech and text in many languages, we propose using LLMs to\ninitialize multi-modal DE retrieval systems. Unlike traditional methods, our\nsystem doesn't require speech data during LLM pre-training and can exploit\nLLM's multilingual text understanding capabilities to match speech and text in\nlanguages unseen during retrieval training. Our multi-modal LLM-based retrieval\nsystem is capable of matching speech and text in 102 languages despite only\ntraining on 21 languages. Our system outperforms previous systems trained\nexplicitly on all 102 languages. We achieve a 10% absolute improvement in\nRecall@1 averaged across these languages. Additionally, our model demonstrates\ncross-lingual speech and text matching, which is further enhanced by readily\navailable machine translation data.",
        "translated": ""
    },
    {
        "title": "Multi-granular Adversarial Attacks against Black-box Neural Ranking\n  Models",
        "url": "http://arxiv.org/abs/2404.01574v1",
        "pub_date": "2024-04-02",
        "summary": "Adversarial ranking attacks have gained increasing attention due to their\nsuccess in probing vulnerabilities, and, hence, enhancing the robustness, of\nneural ranking models. Conventional attack methods employ perturbations at a\nsingle granularity, e.g., word-level or sentence-level, to a target document.\nHowever, limiting perturbations to a single level of granularity may reduce the\nflexibility of creating adversarial examples, thereby diminishing the potential\nthreat of the attack. Therefore, we focus on generating high-quality\nadversarial examples by incorporating multi-granular perturbations. Achieving\nthis objective involves tackling a combinatorial explosion problem, which\nrequires identifying an optimal combination of perturbations across all\npossible levels of granularity, positions, and textual pieces. To address this\nchallenge, we transform the multi-granular adversarial attack into a sequential\ndecision-making process, where perturbations in the next attack step are\ninfluenced by the perturbed document in the current attack step. Since the\nattack process can only access the final state without direct intermediate\nsignals, we use reinforcement learning to perform multi-granular attacks.\nDuring the reinforcement learning process, two agents work cooperatively to\nidentify multi-granular vulnerabilities as attack targets and organize\nperturbation candidates into a final perturbation sequence. Experimental\nresults show that our attack method surpasses prevailing baselines in both\nattack effectiveness and imperceptibility.",
        "translated": ""
    },
    {
        "title": "Set-Aligning Framework for Auto-Regressive Event Temporal Graph\n  Generation",
        "url": "http://arxiv.org/abs/2404.01532v1",
        "pub_date": "2024-04-01",
        "summary": "Event temporal graphs have been shown as convenient and effective\nrepresentations of complex temporal relations between events in text. Recent\nstudies, which employ pre-trained language models to auto-regressively generate\nlinearised graphs for constructing event temporal graphs, have shown promising\nresults. However, these methods have often led to suboptimal graph generation\nas the linearised graphs exhibit set characteristics which are instead treated\nsequentially by language models. This discrepancy stems from the conventional\ntext generation objectives, leading to erroneous penalisation of correct\npredictions caused by the misalignment of elements in target sequences. To\naddress these challenges, we reframe the task as a conditional set generation\nproblem, proposing a Set-aligning Framework tailored for the effective\nutilisation of Large Language Models (LLMs). The framework incorporates data\naugmentations and set-property regularisations designed to alleviate text\ngeneration loss penalties associated with the linearised graph edge sequences,\nthus encouraging the generation of more relation edges. Experimental results\nshow that our framework surpasses existing baselines for event temporal graph\ngeneration. Furthermore, under zero-shot settings, the structural knowledge\nintroduced through our framework notably improves model generalisation,\nparticularly when the training examples available are limited.",
        "translated": ""
    },
    {
        "title": "OpenChemIE: An Information Extraction Toolkit For Chemistry Literature",
        "url": "http://arxiv.org/abs/2404.01462v1",
        "pub_date": "2024-04-01",
        "summary": "Information extraction from chemistry literature is vital for constructing\nup-to-date reaction databases for data-driven chemistry. Complete extraction\nrequires combining information across text, tables, and figures, whereas prior\nwork has mainly investigated extracting reactions from single modalities. In\nthis paper, we present OpenChemIE to address this complex challenge and enable\nthe extraction of reaction data at the document level. OpenChemIE approaches\nthe problem in two steps: extracting relevant information from individual\nmodalities and then integrating the results to obtain a final list of\nreactions. For the first step, we employ specialized neural models that each\naddress a specific task for chemistry information extraction, such as parsing\nmolecules or reactions from text or figures. We then integrate the information\nfrom these modules using chemistry-informed algorithms, allowing for the\nextraction of fine-grained reaction data from reaction condition and substrate\nscope investigations. Our machine learning models attain state-of-the-art\nperformance when evaluated individually, and we meticulously annotate a\nchallenging dataset of reaction schemes with R-groups to evaluate our pipeline\nas a whole, achieving an F1 score of 69.5%. Additionally, the reaction\nextraction results of \\ours attain an accuracy score of 64.3% when directly\ncompared against the Reaxys chemical database. We provide OpenChemIE freely to\nthe public as an open-source package, as well as through a web interface.",
        "translated": ""
    },
    {
        "title": "Towards System Modelling to Support Diseases Data Extraction from the\n  Electronic Health Records for Physicians Research Activities",
        "url": "http://arxiv.org/abs/2404.01218v1",
        "pub_date": "2024-04-01",
        "summary": "The use of Electronic Health Records (EHRs) has increased dramatically in the\npast 15 years, as, it is considered an important source of managing data od\npatients. The EHRs are primary sources of disease diagnosis and demographic\ndata of patients worldwide. Therefore, the data can be utilized for secondary\ntasks such as research. This paper aims to make such data usable for research\nactivities such as monitoring disease statistics for a specific population. As\na result, the researchers can detect the disease causes for the behavior and\nlifestyle of the target group. One of the limitations of EHRs systems is that\nthe data is not available in the standard format but in various forms.\nTherefore, it is required to first convert the names of the diseases and\ndemographics data into one standardized form to make it usable for research\nactivities. There is a large amount of EHRs available, and solving the\nstandardizing issues requires some optimized techniques. We used a first-hand\nEHR dataset extracted from EHR systems. Our application uploads the dataset\nfrom the EHRs and converts it to the ICD-10 coding system to solve the\nstandardization problem. So, we first apply the steps of pre-processing,\nannotation, and transforming the data to convert it into the standard form. The\ndata pre-processing is applied to normalize demographic formats. In the\nannotation step, a machine learning model is used to recognize the diseases\nfrom the text. Furthermore, the transforming step converts the disease name to\nthe ICD-10 coding format. The model was evaluated manually by comparing its\nperformance in terms of disease recognition with an available dictionary-based\nsystem (MetaMap). The accuracy of the proposed machine learning model is 81%,\nthat outperformed MetaMap accuracy of 67%. This paper contributed to system\nmodelling for EHR data extraction to support research activities.",
        "translated": ""
    },
    {
        "title": "Generative-Contrastive Heterogeneous Graph Neural Network",
        "url": "http://arxiv.org/abs/2404.02810v1",
        "pub_date": "2024-04-03",
        "summary": "Heterogeneous Graphs (HGs) can effectively model complex relationships in the\nreal world by multi-type nodes and edges. In recent years, inspired by\nself-supervised learning, contrastive Heterogeneous Graphs Neural Networks\n(HGNNs) have shown great potential by utilizing data augmentation and\ndiscriminators for downstream tasks. However, data augmentation is still\nlimited due to the discrete and abstract nature of graphs. To tackle the above\nlimitations, we propose a novel \\textit{Generative-Contrastive Heterogeneous\nGraph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous\ngraph generative learning enhanced contrastive paradigm. This paradigm\nincludes: 1) A contrastive view augmentation strategy by using masked\nautoencoder. 2) Position-aware and semantics-aware positive sample sampling\nstrategy for generate hard negative samples. 3) A hierarchical contrastive\nlearning strategy for capturing local and global information. Furthermore, the\nhierarchical contrastive learning and sampling strategies aim to constitute an\nenhanced discriminator under the generative-contrastive perspective. Finally,\nwe compare our model with seventeen baselines on eight real-world datasets. Our\nmodel outperforms the latest contrastive and generative baselines on node\nclassification and link prediction tasks. To reproduce our work, we have\nopen-sourced our code at https://github.com/xxx.",
        "translated": ""
    },
    {
        "title": "Efficient Multi-Vector Dense Retrieval Using Bit Vectors",
        "url": "http://arxiv.org/abs/2404.02805v1",
        "pub_date": "2024-04-03",
        "summary": "Dense retrieval techniques employ pre-trained large language models to build\na high-dimensional representation of queries and passages. These\nrepresentations compute the relevance of a passage w.r.t. to a query using\nefficient similarity measures. In this line, multi-vector representations show\nimproved effectiveness at the expense of a one-order-of-magnitude increase in\nmemory footprint and query latency by encoding queries and documents on a\nper-token level. Recently, PLAID has tackled these problems by introducing a\ncentroid-based term representation to reduce the memory impact of multi-vector\nsystems. By exploiting a centroid interaction mechanism, PLAID filters out\nnon-relevant documents, thus reducing the cost of the successive ranking\nstages. This paper proposes ``Efficient Multi-Vector dense retrieval with Bit\nvectors'' (EMVB), a novel framework for efficient query processing in\nmulti-vector dense retrieval. First, EMVB employs a highly efficient\npre-filtering step of passages using optimized bit vectors. Second, the\ncomputation of the centroid interaction happens column-wise, exploiting SIMD\ninstructions, thus reducing its latency. Third, EMVB leverages Product\nQuantization (PQ) to reduce the memory footprint of storing vector\nrepresentations while jointly allowing for fast late interaction. Fourth, we\nintroduce a per-document term filtering method that further improves the\nefficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVB\nis up to 2.8x faster while reducing the memory footprint by 1.8x with no loss\nin retrieval accuracy compared to PLAID.",
        "translated": ""
    },
    {
        "title": "Improving Topic Relevance Model by Mix-structured Summarization and\n  LLM-based Data Augmentation",
        "url": "http://arxiv.org/abs/2404.02616v1",
        "pub_date": "2024-04-03",
        "summary": "Topic relevance between query and document is a very important part of social\nsearch, which can evaluate the degree of matching between document and user's\nrequirement. In most social search scenarios such as Dianping, modeling search\nrelevance always faces two challenges. One is that many documents in social\nsearch are very long and have much redundant information. The other is that the\ntraining data for search relevance model is difficult to get, especially for\nmulti-classification relevance model. To tackle above two problems, we first\ntake query concatenated with the query-based summary and the document summary\nwithout query as the input of topic relevance model, which can help model learn\nthe relevance degree between query and the core topic of document. Then, we\nutilize the language understanding and generation abilities of large language\nmodel (LLM) to rewrite and generate query from queries and documents in\nexisting training data, which can construct new query-document pairs as\ntraining data. Extensive offline experiments and online A/B tests show that the\nproposed approaches effectively improve the performance of relevance modeling.",
        "translated": ""
    },
    {
        "title": "The Surprising Effectiveness of Rankers Trained on Expanded Queries",
        "url": "http://arxiv.org/abs/2404.02587v1",
        "pub_date": "2024-04-03",
        "summary": "An important problem in text-ranking systems is handling the hard queries\nthat form the tail end of the query distribution. The difficulty may arise due\nto the presence of uncommon, underspecified, or incomplete queries. In this\nwork, we improve the ranking performance of hard or difficult queries without\ncompromising the performance of other queries. Firstly, we do LLM based query\nenrichment for training queries using relevant documents. Next, a specialized\nranker is fine-tuned only on the enriched hard queries instead of the original\nqueries. We combine the relevance scores from the specialized ranker and the\nbase ranker, along with a query performance score estimated for each query. Our\napproach departs from existing methods that usually employ a single ranker for\nall queries, which is biased towards easy queries, which form the majority of\nthe query distribution. In our extensive experiments on the DL-Hard dataset, we\nfind that a principled query performance based scoring method using base and\nspecialized ranker offers a significant improvement of up to 25% on the passage\nranking task and up to 48.4% on the document ranking task when compared to the\nbaseline performance of using original queries, even outperforming SOTA model.",
        "translated": ""
    },
    {
        "title": "Multi-Granularity Guided Fusion-in-Decoder",
        "url": "http://arxiv.org/abs/2404.02581v1",
        "pub_date": "2024-04-03",
        "summary": "In Open-domain Question Answering (ODQA), it is essential to discern relevant\ncontexts as evidence and avoid spurious ones among retrieved results. The model\narchitecture that uses concatenated multiple contexts in the decoding phase,\ni.e., Fusion-in-Decoder, demonstrates promising performance but generates\nincorrect outputs from seemingly plausible contexts. To address this problem,\nwe propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning\nevidence across multiple levels of granularity. Based on multi-task learning,\nMGFiD harmonizes passage re-ranking with sentence classification. It aggregates\nevident sentences into an anchor vector that instructs the decoder.\nAdditionally, it improves decoding efficiency by reusing the results of passage\nre-ranking for passage pruning. Through our experiments, MGFiD outperforms\nexisting models on the Natural Questions (NQ) and TriviaQA (TQA) datasets,\nhighlighting the benefits of its multi-granularity solution.",
        "translated": ""
    },
    {
        "title": "Unbiased Learning to Rank Meets Reality: Lessons from Baidu's\n  Large-Scale Search Dataset",
        "url": "http://arxiv.org/abs/2404.02543v1",
        "pub_date": "2024-04-03",
        "summary": "Unbiased learning-to-rank (ULTR) is a well-established framework for learning\nfrom user clicks, which are often biased by the ranker collecting the data.\nWhile theoretically justified and extensively tested in simulation, ULTR\ntechniques lack empirical validation, especially on modern search engines. The\ndataset released for the WSDM Cup 2023, collected from Baidu's search engine,\noffers a rare opportunity to assess the real-world performance of prominent\nULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the\nsubsequent NTCIR ULTRE-2 task, it remains unclear whether the observed\nimprovements stem from applying ULTR or other learning techniques. We revisit\nand extend the available experiments. We find that unbiased learning-to-rank\ntechniques do not bring clear performance improvements, especially compared to\nthe stark differences brought by the choice of ranking loss and query-document\nfeatures. Our experiments reveal that ULTR robustly improves click prediction.\nHowever, these gains in click prediction do not translate to enhanced ranking\nperformance on expert relevance annotations, implying that conclusions strongly\ndepend on how success is measured in this benchmark.",
        "translated": ""
    },
    {
        "title": "DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by\n  Diversifying Synthetic Query Generation",
        "url": "http://arxiv.org/abs/2404.02489v1",
        "pub_date": "2024-04-03",
        "summary": "State-of-the-art neural rankers pre-trained on large task-specific training\ndata such as MS-MARCO, have been shown to exhibit strong performance on various\nranking tasks without domain adaptation, also called zero-shot. However,\nzero-shot neural ranking may be sub-optimal, as it does not take advantage of\nthe target domain information. Unfortunately, acquiring sufficiently large and\nhigh quality target training data to improve a modern neural ranker can be\ncostly and time-consuming. To address this problem, we propose a new approach\nto unsupervised domain adaptation for ranking, DUQGen, which addresses a\ncritical gap in prior literature, namely how to automatically generate both\neffective and diverse synthetic training data to fine tune a modern neural\nranker for a new domain. Specifically, DUQGen produces a more effective\nrepresentation of the target domain by identifying clusters of similar\ndocuments; and generates a more diverse training dataset by probabilistic\nsampling over the resulting document clusters. Our extensive experiments, over\nthe standard BEIR collection, demonstrate that DUQGen consistently outperforms\nall zero-shot baselines and substantially outperforms the SOTA baselines on 16\nout of 18 datasets, for an average of 4% relative improvement across all\ndatasets. We complement our results with a thorough analysis for more in-depth\nunderstanding of the proposed method's performance and to identify promising\nareas for further improvements.",
        "translated": ""
    },
    {
        "title": "uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?",
        "url": "http://arxiv.org/abs/2404.02474v1",
        "pub_date": "2024-04-03",
        "summary": "Inspired by human cognition, Jiang et al.(2023c) create a benchmark for\nassessing LLMs' lateral thinking-thinking outside the box. Building upon this\nbenchmark, we investigate how different prompting methods enhance LLMs'\nperformance on this task to reveal their inherent power for outside-the-box\nthinking ability. Through participating in SemEval-2024, task 9, Sentence\nPuzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)\nand direct prompting, enhancing with informative descriptions, and employing\ncontextualizing prompts using a retrieval augmented generation (RAG) pipeline.\nOur experiments involve three LLMs including GPT-3.5, GPT-4, and\nZephyr-7B-beta. We generate a dataset of thinking paths between riddles and\noptions using GPT-4, validated by humans for quality. Findings indicate that\ncompressed informative prompts enhance performance. Dynamic in-context learning\nenhances model performance significantly. Furthermore, fine-tuning Zephyr on\nour dataset enhances performance across other commonsense datasets,\nunderscoring the value of innovative thinking.",
        "translated": ""
    },
    {
        "title": "Token Trails: Navigating Contextual Depths in Conversational AI with\n  ChatLLM",
        "url": "http://arxiv.org/abs/2404.02402v1",
        "pub_date": "2024-04-03",
        "summary": "Conversational modeling using Large Language Models (LLMs) requires a nuanced\nunderstanding of context to generate coherent and contextually relevant\nresponses. In this paper, we present Token Trails, a novel approach that\nleverages token-type embeddings to navigate the intricate contextual nuances\nwithin conversations. Our framework utilizes token-type embeddings to\ndistinguish between user utterances and bot responses, facilitating the\ngeneration of context-aware replies. Through comprehensive experimentation and\nevaluation, we demonstrate the effectiveness of Token Trails in improving\nconversational understanding and response generation, achieving\nstate-of-the-art performance. Our results highlight the significance of\ncontextual modeling in conversational AI and underscore the promising potential\nof Token Trails to advance the field, paving the way for more sophisticated and\ncontextually aware chatbot interactions.",
        "translated": ""
    },
    {
        "title": "A Survey of Web Content Control for Generative AI",
        "url": "http://arxiv.org/abs/2404.02309v1",
        "pub_date": "2024-04-02",
        "summary": "The groundbreaking advancements around generative AI have recently caused a\nwave of concern culminating in a row of lawsuits, including high-profile\nactions against Stability AI and OpenAI. This situation of legal uncertainty\nhas sparked a broad discussion on the rights of content creators and publishers\nto protect their intellectual property on the web. European as well as US law\nalready provides rough guidelines, setting a direction for technical solutions\nto regulate web data use. In this course, researchers and practitioners have\nworked on numerous web standards and opt-out formats that empower publishers to\nkeep their data out of the development of generative AI models. The emerging\nAI/ML opt-out protocols are valuable in regards to data sovereignty, but again,\nit creates an adverse situation for a site owners who are overwhelmed by the\nmultitude of recent ad hoc standards to consider. In our work, we want to\nsurvey the different proposals, ideas and initiatives, and provide a\ncomprehensive legal and technical background in the context of the current\ndiscussion on web publishers control.",
        "translated": ""
    },
    {
        "title": "Sequential Recommendation for Optimizing Both Immediate Feedback and\n  Long-term Retention",
        "url": "http://arxiv.org/abs/2404.03637v1",
        "pub_date": "2024-04-04",
        "summary": "In the landscape of Recommender System (RS) applications, reinforcement\nlearning (RL) has recently emerged as a powerful tool, primarily due to its\nproficiency in optimizing long-term rewards. Nevertheless, it suffers from\ninstability in the learning process, stemming from the intricate interactions\namong bootstrapping, off-policy training, and function approximation. Moreover,\nin multi-reward recommendation scenarios, designing a proper reward setting\nthat reconciles the inner dynamics of various tasks is quite intricate. In\nresponse to these challenges, we introduce DT4IER, an advanced decision\ntransformer-based recommendation model that is engineered to not only elevate\nthe effectiveness of recommendations but also to achieve a harmonious balance\nbetween immediate user engagement and long-term retention. The DT4IER applies\nan innovative multi-reward design that adeptly balances short and long-term\nrewards with user-specific attributes, which serve to enhance the contextual\nrichness of the reward sequence ensuring a more informed and personalized\nrecommendation process. To enhance its predictive capabilities, DT4IER\nincorporates a high-dimensional encoder, skillfully designed to identify and\nleverage the intricate interrelations across diverse tasks. Furthermore, we\nintegrate a contrastive learning approach within the action embedding\npredictions, a strategy that significantly boosts the model's overall\nperformance. Experiments on three real-world datasets demonstrate the\neffectiveness of DT4IER against state-of-the-art Sequential Recommender Systems\n(SRSs) and Multi-Task Learning (MTL) models in terms of both prediction\naccuracy and effectiveness in specific tasks. The source code is accessible\nonline to facilitate replication",
        "translated": ""
    },
    {
        "title": "Analyzing Musical Characteristics of National Anthems in Relation to\n  Global Indices",
        "url": "http://arxiv.org/abs/2404.03606v1",
        "pub_date": "2024-04-04",
        "summary": "Music plays a huge part in shaping peoples' psychology and behavioral\npatterns. This paper investigates the connection between national anthems and\ndifferent global indices with computational music analysis and statistical\ncorrelation analysis. We analyze national anthem musical data to determine\nwhether certain musical characteristics are associated with peace, happiness,\nsuicide rate, crime rate, etc. To achieve this, we collect national anthems\nfrom 169 countries and use computational music analysis techniques to extract\npitch, tempo, beat, and other pertinent audio features. We then compare these\nmusical characteristics with data on different global indices to ascertain\nwhether a significant correlation exists. Our findings indicate that there may\nbe a correlation between the musical characteristics of national anthems and\nthe indices we investigated. The implications of our findings for music\npsychology and policymakers interested in promoting social well-being are\ndiscussed. This paper emphasizes the potential of musical data analysis in\nsocial research and offers a novel perspective on the relationship between\nmusic and social indices. The source code and data are made open-access for\nreproducibility and future research endeavors. It can be accessed at\nhttp://bit.ly/na_code.",
        "translated": ""
    },
    {
        "title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
        "url": "http://arxiv.org/abs/2404.03528v1",
        "pub_date": "2024-04-04",
        "summary": "Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.",
        "translated": ""
    },
    {
        "title": "A Comprehensive Survey on Self-Supervised Learning for Recommendation",
        "url": "http://arxiv.org/abs/2404.03354v1",
        "pub_date": "2024-04-04",
        "summary": "Recommender systems play a crucial role in tackling the challenge of\ninformation overload by delivering personalized recommendations based on\nindividual user preferences. Deep learning techniques, such as RNNs, GNNs, and\nTransformer architectures, have significantly propelled the advancement of\nrecommender systems by enhancing their comprehension of user behaviors and\npreferences. However, supervised learning methods encounter challenges in\nreal-life scenarios due to data sparsity, resulting in limitations in their\nability to learn representations effectively. To address this, self-supervised\nlearning (SSL) techniques have emerged as a solution, leveraging inherent data\nstructures to generate supervision signals without relying solely on labeled\ndata. By leveraging unlabeled data and extracting meaningful representations,\nrecommender systems utilizing SSL can make accurate predictions and\nrecommendations even when confronted with data sparsity. In this paper, we\nprovide a comprehensive review of self-supervised learning frameworks designed\nfor recommender systems, encompassing a thorough analysis of over 170 papers.\nWe conduct an exploration of nine distinct scenarios, enabling a comprehensive\nunderstanding of SSL-enhanced recommenders in different contexts. For each\ndomain, we elaborate on different self-supervised learning paradigms, namely\ncontrastive learning, generative learning, and adversarial learning, so as to\npresent technical details of how SSL enhances recommender systems in various\ncontexts. We consistently maintain the related open-source materials at\nhttps://github.com/HKUDS/Awesome-SSLRec-Papers.",
        "translated": ""
    },
    {
        "title": "A Directional Diffusion Graph Transformer for Recommendation",
        "url": "http://arxiv.org/abs/2404.03326v1",
        "pub_date": "2024-04-04",
        "summary": "In real-world recommender systems, implicitly collected user feedback, while\nabundant, often includes noisy false-positive and false-negative interactions.\nThe possible misinterpretations of the user-item interactions pose a\nsignificant challenge for traditional graph neural recommenders. These\napproaches aggregate the users' or items' neighbours based on implicit\nuser-item interactions in order to accurately capture the users' profiles. To\naccount for and model possible noise in the users' interactions in graph neural\nrecommenders, we propose a novel Diffusion Graph Transformer (DiffGT) model for\ntop-k recommendation. Our DiffGT model employs a diffusion process, which\nincludes a forward phase for gradually introducing noise to implicit\ninteractions, followed by a reverse process to iteratively refine the\nrepresentations of the users' hidden preferences (i.e., a denoising process).\nIn our proposed approach, given the inherent anisotropic structure observed in\nthe user-item interaction graph, we specifically use anisotropic and\ndirectional Gaussian noises in the forward diffusion process. Our approach\ndiffers from the sole use of isotropic Gaussian noises in existing diffusion\nmodels. In the reverse diffusion process, to reverse the effect of noise added\nearlier and recover the true users' preferences, we integrate a graph\ntransformer architecture with a linear attention module to denoise the noisy\nuser/item embeddings in an effective and efficient manner. In addition, such a\nreverse diffusion process is further guided by personalised information (e.g.,\ninteracted items) to enable the accurate estimation of the users' preferences\non items. Our extensive experiments conclusively demonstrate the superiority of\nour proposed graph diffusion model over ten existing state-of-the-art\napproaches across three benchmark datasets.",
        "translated": ""
    },
    {
        "title": "To Search or to Recommend: Predicting Open-App Motivation with Neural\n  Hawkes Process",
        "url": "http://arxiv.org/abs/2404.03267v1",
        "pub_date": "2024-04-04",
        "summary": "Incorporating Search and Recommendation (S&amp;R) services within a singular\napplication is prevalent in online platforms, leading to a new task termed\nopen-app motivation prediction, which aims to predict whether users initiate\nthe application with the specific intent of information searching, or to\nexplore recommended content for entertainment. Studies have shown that\npredicting users' motivation to open an app can help to improve user engagement\nand enhance performance in various downstream tasks. However, accurately\npredicting open-app motivation is not trivial, as it is influenced by\nuser-specific factors, search queries, clicked items, as well as their temporal\noccurrences. Furthermore, these activities occur sequentially and exhibit\nintricate temporal dependencies. Inspired by the success of the Neural Hawkes\nProcess (NHP) in modeling temporal dependencies in sequences, this paper\nproposes a novel neural Hawkes process model to capture the temporal\ndependencies between historical user browsing and querying actions. The model,\nreferred to as Neural Hawkes Process-based Open-App Motivation prediction model\n(NHP-OAM), employs a hierarchical transformer and a novel intensity function to\nencode multiple factors, and open-app motivation prediction layer to integrate\ntime and user-specific information for predicting users' open-app motivations.\nTo demonstrate the superiority of our NHP-OAM model and construct a benchmark\nfor the Open-App Motivation Prediction task, we not only extend the public S&amp;R\ndataset ZhihuRec but also construct a new real-world Open-App Motivation\nDataset (OAMD). Experiments on these two datasets validate NHP-OAM's\nsuperiority over baseline models. Further downstream application experiments\ndemonstrate NHP-OAM's effectiveness in predicting users' Open-App Motivation,\nhighlighting the immense application value of NHP-OAM.",
        "translated": ""
    },
    {
        "title": "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness\n  of LLMs as Rankers",
        "url": "http://arxiv.org/abs/2404.03192v1",
        "pub_date": "2024-04-04",
        "summary": "The integration of Large Language Models (LLMs) in information retrieval has\nraised a critical reevaluation of fairness in the text-ranking models. LLMs,\nsuch as GPT models and Llama2, have shown effectiveness in natural language\nunderstanding tasks, and prior works (e.g., RankGPT) have also demonstrated\nthat the LLMs exhibit better performance than the traditional ranking models in\nthe ranking task. However, their fairness remains largely unexplored. This\npaper presents an empirical study evaluating these LLMs using the TREC Fair\nRanking dataset, focusing on the representation of binary protected attributes\nsuch as gender and geographic location, which are historically underrepresented\nin search outcomes. Our analysis delves into how these LLMs handle queries and\ndocuments related to these attributes, aiming to uncover biases in their\nranking algorithms. We assess fairness from both user and content perspectives,\ncontributing an empirical benchmark for evaluating LLMs as the fair ranker.",
        "translated": ""
    },
    {
        "title": "Does Knowledge Graph Really Matter for Recommender Systems?",
        "url": "http://arxiv.org/abs/2404.03164v1",
        "pub_date": "2024-04-04",
        "summary": "Recommender systems (RSs) are designed to provide personalized\nrecommendations to users. Recently, knowledge graphs (KGs) have been widely\nintroduced in RSs to improve recommendation accuracy. In this study, however,\nwe demonstrate that RSs do not necessarily perform worse even if the KG is\ndowngraded to the user-item interaction graph only (or removed). We propose an\nevaluation framework KG4RecEval to systematically evaluate how much a KG\ncontributes to the recommendation accuracy of a KG-based RS, using our defined\nmetric KGER (KG utilization efficiency in recommendation). We consider the\nscenarios where knowledge in a KG gets completely removed, randomly distorted\nand decreased, and also where recommendations are for cold-start users. Our\nextensive experiments on four commonly used datasets and a number of\nstate-of-the-art KG-based RSs reveal that: to remove, randomly distort or\ndecrease knowledge does not necessarily decrease recommendation accuracy, even\nfor cold-start users. These findings inspire us to rethink how to better\nutilize knowledge from existing KGs, whereby we discuss and provide insights\ninto what characteristics of datasets and KG-based RSs may help improve KG\nutilization efficiency.",
        "translated": ""
    },
    {
        "title": "Dwell in the Beginning: How Language Models Embed Long Documents for\n  Dense Retrieval",
        "url": "http://arxiv.org/abs/2404.04163v1",
        "pub_date": "2024-04-05",
        "summary": "This study investigates the existence of positional biases in\nTransformer-based models for text representation learning, particularly in the\ncontext of web document retrieval. We build on previous research that\ndemonstrated loss of information in the middle of input sequences for causal\nlanguage models, extending it to the domain of representation learning. We\nexamine positional biases at various stages of training for an encoder-decoder\nmodel, including language model pre-training, contrastive pre-training, and\ncontrastive fine-tuning. Experiments with the MS-MARCO document collection\nreveal that after contrastive pre-training the model already generates\nembeddings that better capture early contents of the input, with fine-tuning\nfurther aggravating this effect.",
        "translated": ""
    },
    {
        "title": "Large language models as oracles for instantiating ontologies with\n  domain-specific knowledge",
        "url": "http://arxiv.org/abs/2404.04108v1",
        "pub_date": "2024-04-05",
        "summary": "Background. Endowing intelligent systems with semantic data commonly requires\ndesigning and instantiating ontologies with domain-specific knowledge.\nEspecially in the early phases, those activities are typically performed\nmanually by human experts possibly leveraging on their own experience. The\nresulting process is therefore time-consuming, error-prone, and often biased by\nthe personal background of the ontology designer. Objective. To mitigate that\nissue, we propose a novel domain-independent approach to automatically\ninstantiate ontologies with domain-specific knowledge, by leveraging on large\nlanguage models (LLMs) as oracles. Method. Starting from (i) an initial schema\ncomposed by inter-related classes andproperties and (ii) a set of query\ntemplates, our method queries the LLM multi- ple times, and generates instances\nfor both classes and properties from its replies. Thus, the ontology is\nautomatically filled with domain-specific knowledge, compliant to the initial\nschema. As a result, the ontology is quickly and automatically enriched with\nmanifold instances, which experts may consider to keep, adjust, discard, or\ncomplement according to their own needs and expertise. Contribution. We\nformalise our method in general way and instantiate it over various LLMs, as\nwell as on a concrete case study. We report experiments rooted in the\nnutritional domain where an ontology of food meals and their ingredients is\nsemi-automatically instantiated from scratch, starting from a categorisation of\nmeals and their relationships. There, we analyse the quality of the generated\nontologies and compare ontologies attained by exploiting different LLMs.\nFinally, we provide a SWOT analysis of the proposed method.",
        "translated": ""
    },
    {
        "title": "A Comparison of Methods for Evaluating Generative IR",
        "url": "http://arxiv.org/abs/2404.04044v1",
        "pub_date": "2024-04-05",
        "summary": "Information retrieval systems increasingly incorporate generative components.\nFor example, in a retrieval augmented generation (RAG) system, a retrieval\ncomponent might provide a source of ground truth, while a generative component\nsummarizes and augments its responses. In other systems, a large language model\n(LLM) might directly generate responses without consulting a retrieval\ncomponent. While there are multiple definitions of generative information\nretrieval (Gen-IR) systems, in this paper we focus on those systems where the\nsystem's response is not drawn from a fixed collection of documents or\npassages. The response to a query may be entirely new text never. Since\ntraditional IR evaluation methods break down under this model, we explore\nvarious methods that extend traditional offline evaluation approaches to the\nGen-IR context. Offline IR evaluation traditionally employs paid human\nassessors, but increasingly LLMs are replacing human assessment, demonstrating\ncapabilities similar or superior to crowdsourced labels. Given that Gen-IR\nsystems do not generate responses from a fixed set, we assume that methods for\nGen-IR evaluation must largely depend on LLM-generated labels. Along with\nmethods based on binary and graded relevance, we explore methods based on\nexplicit subtopics, pairwise preferences, and embeddings. We first validate\nthese methods against human assessments on several TREC Deep Learning Track\ntasks; we then apply these methods to evaluate the output of several purely\ngenerative systems. For each method we consider both its ability to act\nautonomously, without the need for human labels or other input, and its ability\nto support human auditing. To trust these methods, we must be assured that\ntheir results align with human assessments. In order to do so, evaluation\ncriteria must be transparent, so that outcomes can be audited by human\nassessors.",
        "translated": ""
    },
    {
        "title": "Understanding Language Modeling Paradigm Adaptations in Recommender\n  Systems: Lessons Learned and Open Challenges",
        "url": "http://arxiv.org/abs/2404.03788v1",
        "pub_date": "2024-04-04",
        "summary": "The emergence of Large Language Models (LLMs) has achieved tremendous success\nin the field of Natural Language Processing owing to diverse training paradigms\nthat empower LLMs to effectively capture intricate linguistic patterns and\nsemantic representations. In particular, the recent \"pre-train, prompt and\npredict\" training paradigm has attracted significant attention as an approach\nfor learning generalizable models with limited labeled data. In line with this\nadvancement, these training paradigms have recently been adapted to the\nrecommendation domain and are seen as a promising direction in both academia\nand industry. This half-day tutorial aims to provide a thorough understanding\nof extracting and transferring knowledge from pre-trained models learned\nthrough different training paradigms to improve recommender systems from\nvarious perspectives, such as generality, sparsity, effectiveness and\ntrustworthiness. In this tutorial, we first introduce the basic concepts and a\ngeneric architecture of the language modeling paradigm for recommendation\npurposes. Then, we focus on recent advancements in adapting LLM-related\ntraining strategies and optimization objectives for different recommendation\ntasks. After that, we will systematically introduce ethical issues in LLM-based\nrecommender systems and discuss possible approaches to assessing and mitigating\nthem. We will also summarize the relevant datasets, evaluation metrics, and an\nempirical study on the recommendation performance of training paradigms.\nFinally, we will conclude the tutorial with a discussion of open challenges and\nfuture directions.",
        "translated": ""
    },
    {
        "title": "GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query\n  Reformulation",
        "url": "http://arxiv.org/abs/2404.03746v1",
        "pub_date": "2024-04-04",
        "summary": "Query Reformulation(QR) is a set of techniques used to transform a user's\noriginal search query to a text that better aligns with the user's intent and\nimproves their search experience. Recently, zero-shot QR has been shown to be a\npromising approach due to its ability to exploit knowledge inherent in large\nlanguage models. By taking inspiration from the success of ensemble prompting\nstrategies which have benefited many tasks, we investigate if they can help\nimprove query reformulation. In this context, we propose an ensemble based\nprompting technique, GenQREnsemble which leverages paraphrases of a zero-shot\ninstruction to generate multiple sets of keywords ultimately improving\nretrieval performance. We further introduce its post-retrieval variant,\nGenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over\nfour IR benchmarks, we find that GenQREnsemble generates better reformulations\nwith relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over\nthe previous zero-shot state-of-art. On the MSMarco Passage Ranking task,\nGenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback,\nand 9% nDCG@10 using relevant feedback documents.",
        "translated": ""
    },
    {
        "title": "Investigating the Robustness of Counterfactual Learning to Rank Models:\n  A Reproducibility Study",
        "url": "http://arxiv.org/abs/2404.03707v1",
        "pub_date": "2024-04-04",
        "summary": "Counterfactual learning to rank (CLTR) has attracted extensive attention in\nthe IR community for its ability to leverage massive logged user interaction\ndata to train ranking models. While the CLTR models can be theoretically\nunbiased when the user behavior assumption is correct and the propensity\nestimation is accurate, their effectiveness is usually empirically evaluated\nvia simulation-based experiments due to a lack of widely-available,\nlarge-scale, real click logs. However, the mainstream simulation-based\nexperiments are somewhat limited as they often feature a single, deterministic\nproduction ranker and simplified user simulation models to generate the\nsynthetic click logs. As a result, the robustness of CLTR models in complex and\ndiverse situations is largely unknown and needs further investigation.\n  To address this problem, in this paper, we aim to investigate the robustness\nof existing CLTR models in a reproducibility study with extensive\nsimulation-based experiments that (1) use both deterministic and stochastic\nproduction rankers, each with different ranking performance, and (2) leverage\nmultiple user simulation models with different user behavior assumptions. We\nfind that the DLA models and IPS-DCM show better robustness under various\nsimulation settings than IPS-PBM and PRS with offline propensity estimation.\nBesides, the existing CLTR models often fail to outperform the naive click\nbaselines when the production ranker has relatively high ranking performance or\ncertain randomness, which suggests an urgent need for developing new CLTR\nalgorithms that work for these settings.",
        "translated": ""
    },
    {
        "title": "MealRec$^+$: A Meal Recommendation Dataset with Meal-Course Affiliation\n  for Personalization and Healthiness",
        "url": "http://arxiv.org/abs/2404.05386v1",
        "pub_date": "2024-04-08",
        "summary": "Meal recommendation, as a typical health-related recommendation task,\ncontains complex relationships between users, courses, and meals. Among them,\nmeal-course affiliation associates user-meal and user-course interactions.\nHowever, an extensive literature review demonstrates that there is a lack of\npublicly available meal recommendation datasets including meal-course\naffiliation. Meal recommendation research has been constrained in exploring the\nimpact of cooperation between two levels of interaction on personalization and\nhealthiness. To pave the way for meal recommendation research, we introduce a\nnew benchmark dataset called MealRec$^+$. Due to constraints related to user\nhealth privacy and meal scenario characteristics, the collection of data that\nincludes both meal-course affiliation and two levels of interactions is\nimpeded. Therefore, a simulation method is adopted to derive meal-course\naffiliation and user-meal interaction from the user's dining sessions simulated\nbased on user-course interaction data. Then, two well-known nutritional\nstandards are used to calculate the healthiness scores of meals. Moreover, we\nexperiment with several baseline models, including separate and cooperative\ninteraction learning methods. Our experiment demonstrates that cooperating the\ntwo levels of interaction in appropriate ways is beneficial for meal\nrecommendations. Furthermore, in response to the less healthy recommendation\nphenomenon found in the experiment, we explore methods to enhance the\nhealthiness of meal recommendations. The dataset is available on GitHub\n(https://github.com/WUT-IDEA/MealRecPlus).",
        "translated": ""
    },
    {
        "title": "Beyond the Sequence: Statistics-Driven Pre-training for Stabilizing\n  Sequential Recommendation Model",
        "url": "http://arxiv.org/abs/2404.05342v1",
        "pub_date": "2024-04-08",
        "summary": "The sequential recommendation task aims to predict the item that user is\ninterested in according to his/her historical action sequence. However,\ninevitable random action, i.e. user randomly accesses an item among multiple\ncandidates or clicks several items at random order, cause the sequence fails to\nprovide stable and high-quality signals. To alleviate the issue, we propose the\nStatisTics-Driven Pre-traing framework (called STDP briefly). The main idea of\nthe work lies in the exploration of utilizing the statistics information along\nwith the pre-training paradigm to stabilize the optimization of recommendation\nmodel. Specifically, we derive two types of statistical information: item\nco-occurrence across sequence and attribute frequency within the sequence. And\nwe design the following pre-training tasks: 1) The co-occurred items prediction\ntask, which encourages the model to distribute its attention on multiple\nsuitable targets instead of just focusing on the next item that may be\nunstable. 2) We generate a paired sequence by replacing items with their\nco-occurred items and enforce its representation close with the original one,\nthus enhancing the model's robustness to the random noise. 3) To reduce the\nimpact of random on user's long-term preferences, we encourage the model to\ncapture sequence-level frequent attributes. The significant improvement over\nsix datasets demonstrates the effectiveness and superiority of the proposal,\nand further analysis verified the generalization of the STDP framework on other\nmodels.",
        "translated": ""
    },
    {
        "title": "HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large\n  Foundation Models",
        "url": "http://arxiv.org/abs/2404.05083v1",
        "pub_date": "2024-04-07",
        "summary": "While recent progress in video-text retrieval has been driven by the\nexploration of powerful model architectures and training strategies, the\nrepresentation learning ability of video-text retrieval models is still limited\ndue to low-quality and scarce training data annotations. To address this issue,\nwe present a novel video-text learning paradigm, HaVTR, which augments video\nand text data to learn more generalized features. Specifically, we first adopt\na simple augmentation method, which generates self-similar data by randomly\nduplicating or dropping subwords and frames. In addition, inspired by the\nrecent advancement in visual and language generative models, we propose a more\npowerful augmentation method through textual paraphrasing and video stylization\nusing large language models (LLMs) and visual generative models (VGMs).\nFurther, to bring richer information into video and text, we propose a\nhallucination-based augmentation method, where we use LLMs and VGMs to generate\nand add new relevant information to the original data. Benefiting from the\nenriched data, extensive experiments on several video-text retrieval benchmarks\ndemonstrate the superiority of HaVTR over existing methods.",
        "translated": ""
    },
    {
        "title": "Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval",
        "url": "http://arxiv.org/abs/2404.04998v1",
        "pub_date": "2024-04-07",
        "summary": "Deep quantization methods have shown high efficiency on large-scale image\nretrieval. However, current models heavily rely on ground-truth information,\nhindering the application of quantization in label-hungry scenarios. A more\nrealistic demand is to learn from inexhaustible uploaded images that are\nassociated with informal tags provided by amateur users. Though such sketchy\ntags do not obviously reveal the labels, they actually contain useful semantic\ninformation for supervising deep quantization. To this end, we propose\nWeakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first\nwork to learn deep quantization from weakly tagged images. Specifically, 1) we\nuse word embeddings to represent the tags and enhance their semantic\ninformation based on a tag correlation graph. 2) To better preserve semantic\ninformation in quantization codes and reduce quantization error, we jointly\nlearn semantics-preserving embeddings and supervised quantizer on hypersphere\nby employing a well-designed fusion layer and tailor-made loss functions.\nExtensive experiments show that WSDHQ can achieve state-of-art performance on\nweakly-supervised compact coding. Code is available at\nhttps://github.com/gimpong/AAAI21-WSDHQ.",
        "translated": ""
    },
    {
        "title": "Balancing Information Perception with Yin-Yang: Agent-Based Information\n  Neutrality Model for Recommendation Systems",
        "url": "http://arxiv.org/abs/2404.04906v1",
        "pub_date": "2024-04-07",
        "summary": "While preference-based recommendation algorithms effectively enhance user\nengagement by recommending personalized content, they often result in the\ncreation of ``filter bubbles''. These bubbles restrict the range of information\nusers interact with, inadvertently reinforcing their existing viewpoints.\nPrevious research has focused on modifying these underlying algorithms to\ntackle this issue. Yet, approaches that maintain the integrity of the original\nalgorithms remain largely unexplored. This paper introduces an Agent-based\nInformation Neutrality model grounded in the Yin-Yang theory, namely, AbIN.\nThis innovative approach targets the imbalance in information perception within\nexisting recommendation systems. It is designed to integrate with these\npreference-based systems, ensuring the delivery of recommendations with neutral\ninformation. Our empirical evaluation of this model proved its efficacy,\nshowcasing its capacity to expand information diversity while respecting user\npreferences. Consequently, AbIN emerges as an instrumental tool in mitigating\nthe negative impact of filter bubbles on information consumption.",
        "translated": ""
    },
    {
        "title": "Single-Server Pliable Private Information Retrieval with Identifiable\n  Side Information",
        "url": "http://arxiv.org/abs/2404.04820v1",
        "pub_date": "2024-04-07",
        "summary": "In Pliable Private Information Retrieval (PPIR) with a single server,\nmessages are partitioned into $\\Gamma$ non-overlapping classes \\cite{ref5}. The\nuser wants to retrieve a message from its desired class without revealing the\nidentity of the desired class to the server. In \\cite{ref6}, Obead et al.\nconsider the problem of PPIR with Side Information (PPIR-SI), where the user\nnow has side information. The user wants to retrieve any new message (not\nincluded in the side information) from its desired class without revealing the\nidentity of the desired class and its side information. A scheme for the\nPPIR-SI is given in \\cite{ref6} for the case when the users side information is\nunidentified, and this case is referred to as PPIR with Unidentifiable SI\n(PPIR-USI). In this paper, we study the problem of PPIR for the single server\ncase when the side information is partially identifiable, and we term this case\nas PPIR with Identifiable Side Information (PPIR-ISI). The user is well aware\nof the identity of the side information belonging to $\\eta$ number of classes,\nwhere $1\\leq \\eta \\leq \\Gamma$. We give a scheme for PPIR-ISI, and we prove\nthat having identifiable side information is advantageous by comparing the rate\nof the proposed scheme to the rate of the PPIR-USI scheme given in \\cite{ref6}\nfor some cases. Further, we extend the problem of PPIR-ISI for multi-user case,\nwhere users can collaboratively generate the query sets, and we give a scheme\nfor this problem.",
        "translated": ""
    },
    {
        "title": "Music Recommendation Based on Facial Emotion Recognition",
        "url": "http://arxiv.org/abs/2404.04654v1",
        "pub_date": "2024-04-06",
        "summary": "Introduction: Music provides an incredible avenue for individuals to express\ntheir thoughts and emotions, while also serving as a delightful mode of\nentertainment for enthusiasts and music lovers. Objectives: This paper presents\na comprehensive approach to enhancing the user experience through the\nintegration of emotion recognition, music recommendation, and explainable AI\nusing GRAD-CAM. Methods: The proposed methodology utilizes a ResNet50 model\ntrained on the Facial Expression Recognition (FER) dataset, consisting of real\nimages of individuals expressing various emotions. Results: The system achieves\nan accuracy of 82% in emotion classification. By leveraging GRAD-CAM, the model\nprovides explanations for its predictions, allowing users to understand the\nreasoning behind the system's recommendations. The model is trained on both FER\nand real user datasets, which include labelled facial expressions, and real\nimages of individuals expressing various emotions. The training process\ninvolves pre-processing the input images, extracting features through\nconvolutional layers, reasoning with dense layers, and generating emotion\npredictions through the output layer Conclusion: The proposed methodology,\nleveraging the Resnet50 model with ROI-based analysis and explainable AI\ntechniques, offers a robust and interpretable solution for facial emotion\ndetection paper.",
        "translated": ""
    },
    {
        "title": "Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text\n  Reranking with Large Language Models",
        "url": "http://arxiv.org/abs/2404.04522v1",
        "pub_date": "2024-04-06",
        "summary": "Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized\nin Large Language Models (LLMs) to improve the down-streaming tasks without the\ncost of fine-tuing the whole LLMs. Recent studies have shown how to effectively\nuse PEFT for fine-tuning LLMs in ranking tasks with convincing performance;\nthere are some limitations, including the learned prompt being fixed for\ndifferent documents, overfitting to specific tasks, and low adaptation ability.\nIn this paper, we introduce a query-dependent parameter efficient fine-tuning\n(Q-PEFT) approach for text reranking to leak the information of the true\nqueries to LLMs and then make the generation of true queries from input\ndocuments much easier. Specifically, we utilize the query to extract the\ntop-$k$ tokens from concatenated documents, serving as contextual clues. We\nfurther augment Q-PEFT by substituting the retrieval mechanism with a\nmulti-head attention layer to achieve end-to-end training and cover all the\ntokens in the documents, guiding the LLMs to generate more document-specific\nsynthetic queries, thereby further improving the reranking performance.\nExtensive experiments are conducted on four public datasets, demonstrating the\neffectiveness of our proposed approach.",
        "translated": ""
    },
    {
        "title": "Joint Identifiability of Cross-Domain Recommendation via Hierarchical\n  Subspace Disentanglement",
        "url": "http://arxiv.org/abs/2404.04481v1",
        "pub_date": "2024-04-06",
        "summary": "Cross-Domain Recommendation (CDR) seeks to enable effective knowledge\ntransfer across domains. Existing works rely on either representation alignment\nor transformation bridges, but they struggle on identifying domain-shared from\ndomain-specific latent factors. Specifically, while CDR describes user\nrepresentations as a joint distribution over two domains, these methods fail to\naccount for its joint identifiability as they primarily fixate on the marginal\ndistribution within a particular domain. Such a failure may overlook the\nconditionality between two domains and how it contributes to latent factor\ndisentanglement, leading to negative transfer when domains are weakly\ncorrelated. In this study, we explore what should and should not be transferred\nin cross-domain user representations from a causality perspective. We propose a\nHierarchical subspace disentanglement approach to explore the Joint\nIDentifiability of cross-domain joint distribution, termed HJID, to preserve\ndomain-specific behaviors from domain-shared factors. HJID organizes user\nrepresentations into layers: generic shallow subspaces and domain-oriented deep\nsubspaces. We first encode the generic pattern in the shallow subspace by\nminimizing the Maximum Mean Discrepancy of initial layer activation. Then, to\ndissect how domain-oriented latent factors are encoded in deeper layers\nactivation, we construct a cross-domain causality-based data generation graph,\nwhich identifies cross-domain consistent and domain-specific components,\nadhering to the Minimal Change principle. This allows HJID to maintain\nstability whilst discovering unique factors for different domains, all within a\ngenerative framework of invertible transformations that guarantee the joint\nidentifiability. With experiments on real-world datasets, we show that HJID\noutperforms SOTA methods on a range of strongly and weakly correlated CDR\ntasks.",
        "translated": ""
    },
    {
        "title": "Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and\n  Evaluation",
        "url": "http://arxiv.org/abs/2404.04445v1",
        "pub_date": "2024-04-05",
        "summary": "We introduce a meta dataset for few-shot relation extraction, which includes\ntwo datasets derived from existing supervised relation extraction datasets\nNYT29 (Takanobu et al., 2019; Nayak and Ng, 2020) and WIKIDATA (Sorokin and\nGurevych, 2017) as well as a few-shot form of the TACRED dataset (Sabo et al.,\n2021). Importantly, all these few-shot datasets were generated under realistic\nassumptions such as: the test relations are different from any relations a\nmodel might have seen before, limited training data, and a preponderance of\ncandidate relation mentions that do not correspond to any of the relations of\ninterest. Using this large resource, we conduct a comprehensive evaluation of\nsix recent few-shot relation extraction methods, and observe that no method\ncomes out as a clear winner. Further, the overall performance on this task is\nlow, indicating substantial need for future research. We release all versions\nof the data, i.e., both supervised and few-shot, for future research.",
        "translated": ""
    },
    {
        "title": "Learning State-Invariant Representations of Objects from Image\n  Collections with State, Pose, and Viewpoint Changes",
        "url": "http://arxiv.org/abs/2404.06470v1",
        "pub_date": "2024-04-09",
        "summary": "We add one more invariance - state invariance - to the more commonly used\nother invariances for learning object representations for recognition and\nretrieval. By state invariance, we mean robust with respect to changes in the\nstructural form of the object, such as when an umbrella is folded, or when an\nitem of clothing is tossed on the floor. Since humans generally have no\ndifficulty in recognizing objects despite such state changes, we are naturally\nfaced with the question of whether it is possible to devise a neural\narchitecture with similar abilities. To that end, we present a novel dataset,\nObjectsWithStateChange, that captures state and pose variations in the object\nimages recorded from arbitrary viewpoints. We believe that this dataset will\nfacilitate research in fine-grained object recognition and retrieval of objects\nthat are capable of state changes. The goal of such research would be to train\nmodels capable of generating object embeddings that remain invariant to state\nchanges while also staying invariant to transformations induced by changes in\nviewpoint, pose, illumination, etc. To demonstrate the usefulness of the\nObjectsWithStateChange dataset, we also propose a curriculum learning strategy\nthat uses the similarity relationships in the learned embedding space after\neach epoch to guide the training process. The model learns discriminative\nfeatures by comparing visually similar objects within and across different\ncategories, encouraging it to differentiate between objects that may be\nchallenging to distinguish due to changes in their state. We believe that this\nstrategy enhances the model's ability to capture discriminative features for\nfine-grained tasks that may involve objects with state changes, leading to\nperformance improvements on object-level tasks not only on our new dataset, but\nalso on two other challenging multi-view datasets such as ModelNet40 and\nObjectPI.",
        "translated": ""
    },
    {
        "title": "RAR-b: Reasoning as Retrieval Benchmark",
        "url": "http://arxiv.org/abs/2404.06347v1",
        "pub_date": "2024-04-09",
        "summary": "Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks\nhave been the two major avenues to record the progress of embedding models in\nthe past few years. Under the emerging Retrieval-augmented Generation (RAG)\nparadigm, we envision the need to evaluate next-level language understanding\nabilities of embedding models, and take a conscious look at the reasoning\nabilities stored in them. Addressing this, we pose the question: Can retrievers\nsolve reasoning problems? By transforming reasoning tasks into retrieval tasks,\nwe find that without specifically trained for reasoning-level language\nunderstanding, current state-of-the-art retriever models may still be far from\nbeing competent for playing the role of assisting LLMs, especially in\nreasoning-intensive tasks. Moreover, albeit trained to be aware of\ninstructions, instruction-aware IR models are often better off without\ninstructions in inference time for reasoning tasks, posing an overlooked\nretriever-LLM behavioral gap for the research community to align. However,\nrecent decoder-based embedding models show great promise in narrowing the gap,\nhighlighting the pathway for embedding models to achieve reasoning-level\nlanguage understanding. We also show that, although current off-the-shelf\nre-ranker models fail on these tasks, injecting reasoning abilities into them\nthrough fine-tuning still appears easier than doing so to bi-encoders, and we\nare able to achieve state-of-the-art performance across all tasks by\nfine-tuning a reranking model. We release Reasoning as Retrieval Benchmark\n(RAR-b), a holistic suite of tasks and settings to evaluate the reasoning\nabilities stored in retriever models. RAR-b is available at\nhttps://github.com/gowitheflow-1998/RAR-b.",
        "translated": ""
    },
    {
        "title": "DRE: Generating Recommendation Explanations by Aligning Large Language\n  Models at Data-level",
        "url": "http://arxiv.org/abs/2404.06311v1",
        "pub_date": "2024-04-09",
        "summary": "Recommendation systems play a crucial role in various domains, suggesting\nitems based on user behavior.However, the lack of transparency in presenting\nrecommendations can lead to user confusion. In this paper, we introduce\nData-level Recommendation Explanation (DRE), a non-intrusive explanation\nframework for black-box recommendation models.Different from existing methods,\nDRE does not require any intermediary representations of the recommendation\nmodel or latent alignment training, mitigating potential performance issues.We\npropose a data-level alignment method, leveraging large language models to\nreason relationships between user data and recommended items.Additionally, we\naddress the challenge of enriching the details of the explanation by\nintroducing target-aware user preference distillation, utilizing item reviews.\nExperimental results on benchmark datasets demonstrate the effectiveness of the\nDRE in providing accurate and user-centric explanations, enhancing user\nengagement with recommended item.",
        "translated": ""
    },
    {
        "title": "Exploring Diverse Sounds: Identifying Outliers in a Music Corpus",
        "url": "http://arxiv.org/abs/2404.06103v1",
        "pub_date": "2024-04-09",
        "summary": "Existing research on music recommendation systems primarily focuses on\nrecommending similar music, thereby often neglecting diverse and distinctive\nmusical recordings. Musical outliers can provide valuable insights due to the\ninherent diversity of music itself. In this paper, we explore music outliers,\ninvestigating their potential usefulness for music discovery and recommendation\nsystems. We argue that not all outliers should be treated as noise, as they can\noffer interesting perspectives and contribute to a richer understanding of an\nartist's work. We introduce the concept of 'Genuine' music outliers and provide\na definition for them. These genuine outliers can reveal unique aspects of an\nartist's repertoire and hold the potential to enhance music discovery by\nexposing listeners to novel and diverse musical experiences.",
        "translated": ""
    },
    {
        "title": "End-to-end training of Multimodal Model and ranking Model",
        "url": "http://arxiv.org/abs/2404.06078v1",
        "pub_date": "2024-04-09",
        "summary": "Traditional recommender systems heavily rely on ID features, which often\nencounter challenges related to cold-start and generalization. Modeling\npre-extracted content features can mitigate these issues, but is still a\nsuboptimal solution due to the discrepancies between training tasks and model\nparameters. End-to-end training presents a promising solution for these\nproblems, yet most of the existing works mainly focus on retrieval models,\nleaving the multimodal techniques under-utilized. In this paper, we propose an\nindustrial multimodal recommendation framework named EM3: End-to-end training\nof Multimodal Model and ranking Model, which sufficiently utilizes multimodal\ninformation and allows personalized ranking tasks to directly train the core\nmodules in the multimodal model to obtain more task-oriented content features,\nwithout overburdening resource consumption. First, we propose Fusion-Q-Former,\nwhich consists of transformers and a set of trainable queries, to fuse\ndifferent modalities and generate fixed-length and robust multimodal\nembeddings. Second, in our sequential modeling for user content interest, we\nutilize Low-Rank Adaptation technique to alleviate the conflict between huge\nresource consumption and long sequence length. Third, we propose a novel\nContent-ID-Contrastive learning task to complement the advantages of content\nand ID by aligning them with each other, obtaining more task-oriented content\nembeddings and more generalized ID embeddings. In experiments, we implement EM3\non different ranking models in two scenario, achieving significant improvements\nin both offline evaluation and online A/B test, verifying the generalizability\nof our method. Ablation studies and visualization are also performed.\nFurthermore, we also conduct experiments on two public datasets to show that\nour proposed method outperforms the state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free\n  Information Retrieval",
        "url": "http://arxiv.org/abs/2404.06004v1",
        "pub_date": "2024-04-09",
        "summary": "In approximate nearest neighbor search (ANNS) methods based on approximate\nproximity graphs, DiskANN achieves good recall-speed balance for large-scale\ndatasets using both of RAM and storage. Despite it claims to save memory usage\nby loading compressed vectors by product quantization (PQ), its memory usage\nincreases in proportion to the scale of datasets. In this paper, we propose\nAll-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the\ncompressed vectors to storage. Our method achieves $\\sim$10 MB memory usage in\nquery search even with billion-scale datasets with minor performance\ndegradation. AiSAQ also reduces the index load time before query search, which\nenables the index switch between muitiple billion-scale datasets and\nsignificantly enhances the flexibility of retrieval-augmented generation (RAG).\nThis method is applicable to all graph-based ANNS algorithms and can be\ncombined with higher-spec ANNS methods in the future.",
        "translated": ""
    },
    {
        "title": "Event-enhanced Retrieval in Real-time Search",
        "url": "http://arxiv.org/abs/2404.05989v1",
        "pub_date": "2024-04-09",
        "summary": "The embedding-based retrieval (EBR) approach is widely used in mainstream\nsearch engine retrieval systems and is crucial in recent retrieval-augmented\nmethods for eliminating LLM illusions. However, existing EBR models often face\nthe \"semantic drift\" problem and insufficient focus on key information, leading\nto a low adoption rate of retrieval results in subsequent steps. This issue is\nespecially noticeable in real-time search scenarios, where the various\nexpressions of popular events on the Internet make real-time retrieval heavily\nreliant on crucial event information. To tackle this problem, this paper\nproposes a novel approach called EER, which enhances real-time retrieval\nperformance by improving the dual-encoder model of traditional EBR. We\nincorporate contrastive learning to accompany pairwise learning for encoder\noptimization. Furthermore, to strengthen the focus on critical event\ninformation in events, we include a decoder module after the document encoder,\nintroduce a generative event triplet extraction scheme based on prompt-tuning,\nand correlate the events with query encoder optimization through comparative\nlearning. This decoder module can be removed during inference. Extensive\nexperiments demonstrate that EER can significantly improve the real-time search\nretrieval performance. We believe that this approach will provide new\nperspectives in the field of information retrieval. The codes and dataset are\navailable at https://github.com/open-event-hub/Event-enhanced_Retrieval .",
        "translated": ""
    },
    {
        "title": "Optimization Methods for Personalizing Large Language Models through\n  Retrieval Augmentation",
        "url": "http://arxiv.org/abs/2404.05970v1",
        "pub_date": "2024-04-09",
        "summary": "This paper studies retrieval-augmented approaches for personalizing large\nlanguage models (LLMs), which potentially have a substantial impact on various\napplications and domains. We propose the first attempt to optimize the\nretrieval models that deliver a limited number of personal documents to large\nlanguage models for the purpose of personalized generation. We develop two\noptimization algorithms that solicit feedback from the downstream personalized\ngeneration tasks for retrieval optimization--one based on reinforcement\nlearning whose reward function is defined using any arbitrary metric for\npersonalized generation and another based on knowledge distillation from the\ndownstream LLM to the retrieval model. This paper also introduces a pre- and\npost-generation retriever selection model that decides what retriever to choose\nfor each LLM input. Extensive experiments on diverse tasks from the language\nmodel personalization (LaMP) benchmark reveal statistically significant\nimprovements in six out of seven datasets.",
        "translated": ""
    },
    {
        "title": "Wasserstein Dependent Graph Attention Network for Collaborative\n  Filtering with Uncertainty",
        "url": "http://arxiv.org/abs/2404.05962v1",
        "pub_date": "2024-04-09",
        "summary": "Collaborative filtering (CF) is an essential technique in recommender systems\nthat provides personalized recommendations by only leveraging user-item\ninteractions. However, most CF methods represent users and items as fixed\npoints in the latent space, lacking the ability to capture uncertainty. In this\npaper, we propose a novel approach, called the Wasserstein dependent Graph\nATtention network (W-GAT), for collaborative filtering with uncertainty. We\nutilize graph attention network and Wasserstein distance to address the\nlimitations of LightGCN and Kullback-Leibler divergence (KL) divergence to\nlearn Gaussian embedding for each user and item. Additionally, our method\nincorporates Wasserstein-dependent mutual information further to increase the\nsimilarity between positive pairs and to tackle the challenges induced by KL\ndivergence. Experimental results on three benchmark datasets show the\nsuperiority of W-GAT compared to several representative baselines. Extensive\nexperimental analysis validates the effectiveness of W-GAT in capturing\nuncertainty by modeling the range of user preferences and categories associated\nwith items.",
        "translated": ""
    },
    {
        "title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models",
        "url": "http://arxiv.org/abs/2404.05893v1",
        "pub_date": "2024-04-08",
        "summary": "Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p&lt;0.01). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p&lt;0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base.",
        "translated": ""
    },
    {
        "title": "From Model-centered to Human-Centered: Revision Distance as a Metric for\n  Text Evaluation in LLMs-based Applications",
        "url": "http://arxiv.org/abs/2404.07108v1",
        "pub_date": "2024-04-10",
        "summary": "Evaluating large language models (LLMs) is fundamental, particularly in the\ncontext of practical applications. Conventional evaluation methods, typically\ndesigned primarily for LLM development, yield numerical scores that ignore the\nuser experience. Therefore, our study shifts the focus from model-centered to\nhuman-centered evaluation in the context of AI-powered writing assistance\napplications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs\nto suggest revision edits that mimic the human writing process. It is\ndetermined by counting the revision edits generated by LLMs. Benefiting from\nthe generated revision edit details, our metric can provide a self-explained\ntext evaluation result in a human-understandable manner beyond the\ncontext-independent score. Our results show that for the easy-writing task,\n``Revision Distance'' is consistent with established metrics (ROUGE,\nBert-score, and GPT-score), but offers more insightful, detailed feedback and\nbetter distinguishes between texts. Moreover, in the context of challenging\nacademic writing tasks, our metric still delivers reliable evaluations where\nother metrics tend to struggle. Furthermore, our metric also holds significant\npotential for scenarios lacking reference texts.",
        "translated": ""
    },
    {
        "title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs",
        "url": "http://arxiv.org/abs/2404.07103v1",
        "pub_date": "2024-04-10",
        "summary": "Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.",
        "translated": ""
    },
    {
        "title": "TransTARec: Time-Adaptive Translating Embedding Model for Next POI\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.07096v1",
        "pub_date": "2024-04-10",
        "summary": "The rapid growth of location acquisition technologies makes\nPoint-of-Interest(POI) recommendation possible due to redundant user check-in\nrecords. In this paper, we focus on next POI recommendation in which next POI\nis based on previous POI. We observe that time plays an important role in next\nPOI recommendation but is neglected in the recent proposed translating\nembedding methods. To tackle this shortage, we propose a time-adaptive\ntranslating embedding model (TransTARec) for next POI recommendation that\nnaturally incorporates temporal influence, sequential dynamics, and user\npreference within a single component. Methodologically, we treat a (previous\ntimestamp, user, next timestamp) triplet as a union translation vector and\ndevelop a neural-based fusion operation to fuse user preference and temporal\ninfluence. The superiority of TransTARec, which is confirmed by extensive\nexperiments on real-world datasets, comes from not only the introduction of\ntemporal influence but also the direct unification with user preference and\nsequential dynamics.",
        "translated": ""
    },
    {
        "title": "Quati: A Brazilian Portuguese Information Retrieval Dataset from Native\n  Speakers",
        "url": "http://arxiv.org/abs/2404.06976v1",
        "pub_date": "2024-04-10",
        "summary": "Despite Portuguese being one of the most spoken languages in the world, there\nis a lack of high-quality information retrieval datasets in that language. We\npresent Quati, a dataset specifically designed for the Brazilian Portuguese\nlanguage. It comprises a collection of queries formulated by native speakers\nand a curated set of documents sourced from a selection of high-quality\nBrazilian Portuguese websites. These websites are frequented more likely by\nreal users compared to those randomly scraped, ensuring a more representative\nand relevant corpus. To label the query-document pairs, we use a\nstate-of-the-art LLM, which shows inter-annotator agreement levels comparable\nto human performance in our assessments. We provide a detailed description of\nour annotation methodology to enable others to create similar datasets for\nother languages, providing a cost-effective way of creating high-quality IR\ndatasets with an arbitrary number of labeled documents per query. Finally, we\nevaluate a diverse range of open-source and commercial retrievers to serve as\nbaseline systems. Quati is publicly available at\nhttps://huggingface.co/datasets/unicamp-dl/quati and all scripts at\nhttps://github.com/unicamp-dl/quati .",
        "translated": ""
    },
    {
        "title": "Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise\n  Passage Re-Ranking with Cross-Encoders",
        "url": "http://arxiv.org/abs/2404.06912v1",
        "pub_date": "2024-04-10",
        "summary": "Cross-encoders are effective passage re-rankers. But when re-rank\\-ing\nmultiple passages at once, existing cross-encoders inefficiently optimize the\noutput ranking over several input permutations, as their passage interactions\nare not permutation-invariant. Moreover, their high memory footprint constrains\nthe number of passages during listwise training. To tackle these issues, we\npropose the Set-Encoder, a new cross-encoder architecture that (1) introduces\ninter-passage attention with parallel passage processing to ensure permutation\ninvariance between input passages, and that (2) uses fused-attention kernels to\nenable training with more passages at a time. In experiments on TREC Deep\nLearning and TIREx, the Set-Encoder is more effective than previous\ncross-encoders with a similar number of parameters. Compared to larger models,\nthe Set-Encoder is more efficient and either on par or even more effective.",
        "translated": ""
    },
    {
        "title": "NFARec: A Negative Feedback-Aware Recommender Model",
        "url": "http://arxiv.org/abs/2404.06900v1",
        "pub_date": "2024-04-10",
        "summary": "Graph neural network (GNN)-based models have been extensively studied for\nrecommendations, as they can extract high-order collaborative signals\naccurately which is required for high-quality recommender systems. However,\nthey neglect the valuable information gained through negative feedback in two\naspects: (1) different users might hold opposite feedback on the same item,\nwhich hampers optimal information propagation in GNNs, and (2) even when an\nitem vastly deviates from users' preferences, they might still choose it and\nprovide a negative rating. In this paper, we propose a negative feedback-aware\nrecommender model (NFARec) that maximizes the leverage of negative feedback. To\ntransfer information to multi-hop neighbors along an optimal path effectively,\nNFARec adopts a feedback-aware correlation that guides hypergraph convolutions\n(HGCs) to learn users' structural representations. Moreover, NFARec\nincorporates an auxiliary task - predicting the feedback sentiment polarity\n(i.e., positive or negative) of the next interaction - based on the Transformer\nHawkes Process. The task is beneficial for understanding users by learning the\nsentiment expressed in their previous sequential feedback patterns and\npredicting future interactions. Extensive experiments demonstrate that NFARec\noutperforms competitive baselines. Our source code and data are released at\nhttps://github.com/WangXFng/NFARec.",
        "translated": ""
    },
    {
        "title": "CaDRec: Contextualized and Debiased Recommender Model",
        "url": "http://arxiv.org/abs/2404.06895v1",
        "pub_date": "2024-04-10",
        "summary": "Recommender models aimed at mining users' behavioral patterns have raised\ngreat attention as one of the essential applications in daily life. Recent work\non graph neural networks (GNNs) or debiasing methods has attained remarkable\ngains. However, they still suffer from (1) over-smoothing node embeddings\ncaused by recursive convolutions with GNNs, and (2) the skewed distribution of\ninteractions due to popularity and user-individual biases. This paper proposes\na contextualized and debiased recommender model (CaDRec). To overcome the\nover-smoothing issue, we explore a novel hypergraph convolution operator that\ncan select effective neighbors during convolution by introducing both\nstructural context and sequential context. To tackle the skewed distribution,\nwe propose two strategies for disentangling interactions: (1) modeling\nindividual biases to learn unbiased item embeddings, and (2) incorporating item\npopularity with positional encoding. Moreover, we mathematically show that the\nimbalance of the gradients to update item embeddings exacerbates the popularity\nbias, thus adopting regularization and weighting schemes as solutions.\nExtensive experiments on four datasets demonstrate the superiority of the\nCaDRec against state-of-the-art (SOTA) methods. Our source code and data are\nreleased at https://github.com/WangXFng/CaDRec.",
        "translated": ""
    },
    {
        "title": "Milgram's experiment in the knowledge space: Individual navigation\n  strategies",
        "url": "http://arxiv.org/abs/2404.06591v1",
        "pub_date": "2024-04-09",
        "summary": "Data deluge characteristic for our times has led to information overload,\nposing a significant challenge to effectively finding our way through the\ndigital landscape. Addressing this issue requires an in-depth understanding of\nhow we navigate through the abundance of information. Previous research has\ndiscovered multiple patterns in how individuals navigate in the geographic,\nsocial, and information spaces, yet individual differences in strategies for\nnavigation in the knowledge space has remained largely unexplored. To bridge\nthe gap, we conducted an online experiment where participants played a\nnavigation game on Wikipedia and completed questionnaires about their personal\ninformation. Utilizing a graph embedding trained on the English Wikipedia, our\nstudy identified distinctive strategies that participants adopt: when the\ntarget is a famous person, participants typically use the geographical and\noccupational information of the target to navigate, reminiscent of hub-driven\nand proximity-driven approaches, respectively. We discovered that many\nparticipants playing the same game exhibit a \"wisdom of the crowd\" effect: The\nset of strategies provide a good estimate for the information landscape around\nthe target indicating that the individual differences complement each other.",
        "translated": ""
    },
    {
        "title": "Manipulating Large Language Models to Increase Product Visibility",
        "url": "http://arxiv.org/abs/2404.07981v1",
        "pub_date": "2024-04-11",
        "summary": "Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.",
        "translated": ""
    },
    {
        "title": "Auditing health-related recommendations in social media: A Case Study of\n  Abortion on YouTube",
        "url": "http://arxiv.org/abs/2404.07896v1",
        "pub_date": "2024-04-11",
        "summary": "Recommendation algorithms (RS) used by social media, like YouTube,\nsignificantly shape our information consumption across various domains,\nespecially in healthcare. Hence, algorithmic auditing becomes crucial to\nuncover their potential bias and misinformation, particularly in the context of\ncontroversial topics like abortion. We introduce a simple yet effective sock\npuppet auditing approach to investigate how YouTube recommends abortion-related\nvideos to individuals with different backgrounds. Our framework allows for\nefficient auditing of RS, regardless of the complexity of the underlying\nalgorithms",
        "translated": ""
    },
    {
        "title": "M-scan: A Multi-Scenario Causal-driven Adaptive Network for\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.07581v1",
        "pub_date": "2024-04-11",
        "summary": "We primarily focus on the field of multi-scenario recommendation, which poses\na significant challenge in effectively leveraging data from different scenarios\nto enhance predictions in scenarios with limited data. Current mainstream\nefforts mainly center around innovative model network architectures, with the\naim of enabling the network to implicitly acquire knowledge from diverse\nscenarios. However, the uncertainty of implicit learning in networks arises\nfrom the absence of explicit modeling, leading to not only difficulty in\ntraining but also incomplete user representation and suboptimal performance.\nFurthermore, through causal graph analysis, we have discovered that the\nscenario itself directly influences click behavior, yet existing approaches\ndirectly incorporate data from other scenarios during the training of the\ncurrent scenario, leading to prediction biases when they directly utilize click\nbehaviors from other scenarios to train models. To address these problems, we\npropose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model\nincorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts\nuser interests from other scenarios that align with the current scenario.\nAdditionally, it employs a Scenario Bias Eliminator module utilizing causal\ncounterfactual inference to mitigate biases introduced by data from other\nscenarios. Extensive experiments on two public datasets demonstrate the\nefficacy of our M-scan compared to the existing baseline models.",
        "translated": ""
    },
    {
        "title": "Can Large Language Models Assess Serendipity in Recommender Systems?",
        "url": "http://arxiv.org/abs/2404.07499v1",
        "pub_date": "2024-04-11",
        "summary": "Serendipity-oriented recommender systems aim to counteract\nover-specialization in user preferences. However, evaluating a user's\nserendipitous response towards a recommended item can be challenging because of\nits emotional nature. In this study, we address this issue by leveraging the\nrich knowledge of large language models (LLMs), which can perform a variety of\ntasks. First, this study explored the alignment between serendipitous\nevaluations made by LLMs and those made by humans. In this investigation, a\nbinary classification task was given to the LLMs to predict whether a user\nwould find the recommended item serendipitously. The predictive performances of\nthree LLMs on a benchmark dataset in which humans assigned the ground truth of\nserendipitous items were measured. The experimental findings reveal that\nLLM-based assessment methods did not have a very high agreement rate with human\nassessments. However, they performed as well as or better than the baseline\nmethods. Further validation results indicate that the number of user rating\nhistories provided to LLM prompts should be carefully chosen to avoid both\ninsufficient and excessive inputs and that the output of LLMs that show high\nclassification performance is difficult to interpret.",
        "translated": ""
    },
    {
        "title": "Adaptive Fair Representation Learning for Personalized Fairness in\n  Recommendations via Information Alignment",
        "url": "http://arxiv.org/abs/2404.07494v1",
        "pub_date": "2024-04-11",
        "summary": "Personalized fairness in recommendations has been attracting increasing\nattention from researchers. The existing works often treat a fairness\nrequirement, represented as a collection of sensitive attributes, as a\nhyper-parameter, and pursue extreme fairness by completely removing information\nof sensitive attributes from the learned fair embedding, which suffer from two\nchallenges: huge training cost incurred by the explosion of attribute\ncombinations, and the suboptimal trade-off between fairness and accuracy. In\nthis paper, we propose a novel Adaptive Fair Representation Learning (AFRL)\nmodel, which achieves a real personalized fairness due to its advantage of\ntraining only one model to adaptively serve different fairness requirements\nduring inference phase. Particularly, AFRL treats fairness requirements as\ninputs and can learn an attribute-specific embedding for each attribute from\nthe unfair user embedding, which endows AFRL with the adaptability during\ninference phase to determine the non-sensitive attributes under the guidance of\nthe user's unique fairness requirement. To achieve a better trade-off between\nfairness and accuracy in recommendations, AFRL conducts a novel Information\nAlignment to exactly preserve discriminative information of non-sensitive\nattributes and incorporate a debiased collaborative embedding into the fair\nembedding to capture attribute-independent collaborative signals, without loss\nof fairness. Finally, the extensive experiments conducted on real datasets\ntogether with the sound theoretical analysis demonstrate the superiority of\nAFRL.",
        "translated": ""
    },
    {
        "title": "A Conceptual Framework for Conversational Search and Recommendation:\n  Conceptualizing Agent-Human Interactions During the Conversational Search\n  Process",
        "url": "http://arxiv.org/abs/2404.08630v1",
        "pub_date": "2024-04-12",
        "summary": "The conversational search task aims to enable a user to resolve information\nneeds via natural language dialogue with an agent. In this paper, we aim to\ndevelop a conceptual framework of the actions and intents of users and agents\nexplaining how these actions enable the user to explore the search space and\nresolve their information need. We outline the different actions and intents,\nbefore discussing key decision points in the conversation where the agent needs\nto decide how to steer the conversational search process to a successful and/or\nsatisfactory conclusion. Essentially, this paper provides a conceptualization\nof the conversational search process between an agent and user, which provides\na framework and a starting point for research, development and evaluation of\nconversational search agents.",
        "translated": ""
    },
    {
        "title": "Accessibility in Information Retrieval",
        "url": "http://arxiv.org/abs/2404.08628v1",
        "pub_date": "2024-04-12",
        "summary": "This paper introduces the concept of accessibility from the field of\ntransportation planning and adopts it within the context of Information\nRetrieval (IR). An analogy is drawn between the fields, which motivates the\ndevelopment of document accessibility measures for IR systems. Considering the\naccessibility of documents within a collection given an IR System provides a\ndifferent perspective on the analysis and evaluation of such systems which\ncould be used to inform the design, tuning and management of current and future\nIR systems.",
        "translated": ""
    },
    {
        "title": "Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking",
        "url": "http://arxiv.org/abs/2404.08535v1",
        "pub_date": "2024-04-12",
        "summary": "Contrastive learning has gained widespread adoption for retrieval tasks due\nto its minimal requirement for manual annotations. However, popular contrastive\nframeworks typically learn from binary relevance, making them ineffective at\nincorporating direct fine-grained rankings. In this paper, we curate a\nlarge-scale dataset featuring detailed relevance scores for each query-document\npair to facilitate future research and evaluation. Subsequently, we propose\nGeneralized Contrastive Learning for Multi-Modal Retrieval and Ranking (GCL),\nwhich is designed to learn from fine-grained rankings beyond binary relevance\nscores. Our results show that GCL achieves a 94.5% increase in NDCG@10 for\nin-domain and 26.3 to 48.8% increases for cold-start evaluations, all relative\nto the CLIP baseline and involving ground truth rankings.",
        "translated": ""
    },
    {
        "title": "Toward FAIR Semantic Publishing of Research Dataset Metadata in the Open\n  Research Knowledge Graph",
        "url": "http://arxiv.org/abs/2404.08443v1",
        "pub_date": "2024-04-12",
        "summary": "Search engines these days can serve datasets as search results. Datasets get\npicked up by search technologies based on structured descriptions on their\nofficial web pages, informed by metadata ontologies such as the Dataset content\ntype of schema.org. Despite this promotion of the content type dataset as a\nfirst-class citizen of search results, a vast proportion of datasets,\nparticularly research datasets, still need to be made discoverable and,\ntherefore, largely remain unused. This is due to the sheer volume of datasets\nreleased every day and the inability of metadata to reflect a dataset's content\nand context accurately. This work seeks to improve this situation for a\nspecific class of datasets, namely research datasets, which are the result of\nresearch endeavors and are accompanied by a scholarly publication. We propose\nthe ORKG-Dataset content type, a specialized branch of the Open Research\nKnowledge Graoh (ORKG) platform, which provides descriptive information and a\nsemantic model for research datasets, integrating them with their accompanying\nscholarly publications. This work aims to establish a standardized framework\nfor recording and reporting research datasets within the ORKG-Dataset content\ntype. This, in turn, increases research dataset transparency on the web for\ntheir improved discoverability and applied use. In this paper, we present a\nproposal -- the minimum FAIR, comparable, semantic description of research\ndatasets in terms of salient properties of their supporting publication. We\ndesign a specific application of the ORKG-Dataset semantic model based on 40\ndiverse research datasets on scientific information extraction.",
        "translated": ""
    },
    {
        "title": "Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature\n  Extraction and Personalized Integration Framework",
        "url": "http://arxiv.org/abs/2404.08361v1",
        "pub_date": "2024-04-12",
        "summary": "Feed recommendation is currently the mainstream mode for many real-world\napplications (e.g., TikTok, Dianping), it is usually necessary to model and\npredict user interests in multiple scenarios (domains) within and even outside\nthe application. Multi-domain learning is a typical solution in this regard.\nWhile considerable efforts have been made in this regard, there are still two\nlong-standing challenges: (1) Accurately depicting the differences among\ndomains using domain features is crucial for enhancing the performance of each\ndomain. However, manually designing domain features and models for numerous\ndomains can be a laborious task. (2) Users typically have limited impressions\nin only a few domains. Extracting features automatically from other domains and\nleveraging them to improve the predictive capabilities of each domain has\nconsistently posed a challenging problem. In this paper, we propose an\nAutomatic Domain Feature Extraction and Personalized Integration (DFEI)\nframework for the large-scale multi-domain recommendation. The framework\nautomatically transforms the behavior of each individual user into an\naggregation of all user behaviors within the domain, which serves as the domain\nfeatures. Unlike offline feature engineering methods, the extracted domain\nfeatures are higher-order representations and directly related to the target\nlabel. Besides, by personalized integration of domain features from other\ndomains for each user and the innovation in the training mode, the DFEI\nframework can yield more accurate conversion identification. Experimental\nresults on both public and industrial datasets, consisting of over 20 domains,\nclearly demonstrate that the proposed framework achieves significantly better\nperformance compared with SOTA baselines. Furthermore, we have released the\nsource code of the proposed framework at https://github.com/xidongbo/DFEI.",
        "translated": ""
    },
    {
        "title": "Improving Health Question Answering with Reliable and Time-Aware\n  Evidence Retrieval",
        "url": "http://arxiv.org/abs/2404.08359v1",
        "pub_date": "2024-04-12",
        "summary": "In today's digital world, seeking answers to health questions on the Internet\nis a common practice. However, existing question answering (QA) systems often\nrely on using pre-selected and annotated evidence documents, thus making them\ninadequate for addressing novel questions. Our study focuses on the open-domain\nQA setting, where the key challenge is to first uncover relevant evidence in\nlarge knowledge bases. By utilizing the common retrieve-then-read QA pipeline\nand PubMed as a trustworthy collection of medical research documents, we answer\nhealth questions from three diverse datasets. We modify different retrieval\nsettings to observe their influence on the QA pipeline's performance, including\nthe number of retrieved documents, sentence selection process, the publication\nyear of articles, and their number of citations. Our results reveal that\ncutting down on the amount of retrieved documents and favoring more recent and\nhighly cited documents can improve the final macro F1 score up to 10%. We\ndiscuss the results, highlight interesting examples, and outline challenges for\nfuture research, like managing evidence disagreement and crafting user-friendly\nexplanations.",
        "translated": ""
    },
    {
        "title": "Collaborative-Enhanced Prediction of Spending on Newly Downloaded Mobile\n  Games under Consumption Uncertainty",
        "url": "http://arxiv.org/abs/2404.08301v1",
        "pub_date": "2024-04-12",
        "summary": "With the surge in mobile gaming, accurately predicting user spending on newly\ndownloaded games has become paramount for maximizing revenue. However, the\ninherently unpredictable nature of user behavior poses significant challenges\nin this endeavor. To address this, we propose a robust model training and\nevaluation framework aimed at standardizing spending data to mitigate label\nvariance and extremes, ensuring stability in the modeling process. Within this\nframework, we introduce a collaborative-enhanced model designed to predict user\ngame spending without relying on user IDs, thus ensuring user privacy and\nenabling seamless online training. Our model adopts a unique approach by\nseparately representing user preferences and game features before merging them\nas input to the spending prediction module. Through rigorous experimentation,\nour approach demonstrates notable improvements over production models,\nachieving a remarkable \\textbf{17.11}\\% enhancement on offline data and an\nimpressive \\textbf{50.65}\\% boost in an online A/B test. In summary, our\ncontributions underscore the importance of stable model training frameworks and\nthe efficacy of collaborative-enhanced models in predicting user spending\nbehavior in mobile gaming.",
        "translated": ""
    },
    {
        "title": "Reducing hallucination in structured outputs via Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2404.08189v1",
        "pub_date": "2024-04-12",
        "summary": "A common and fundamental limitation of Generative AI (GenAI) is its\npropensity to hallucinate. While large language models (LLM) have taken the\nworld by storm, without eliminating or at least reducing hallucinations,\nreal-world GenAI systems may face challenges in user adoption. In the process\nof deploying an enterprise application that produces workflows based on natural\nlanguage requirements, we devised a system leveraging Retrieval Augmented\nGeneration (RAG) to greatly improve the quality of the structured output that\nrepresents such workflows. Thanks to our implementation of RAG, our proposed\nsystem significantly reduces hallucinations in the output and improves the\ngeneralization of our LLM in out-of-domain settings. In addition, we show that\nusing a small, well-trained retriever encoder can reduce the size of the\naccompanying LLM, thereby making deployments of LLM-based systems less\nresource-intensive.",
        "translated": ""
    },
    {
        "title": "Generative Information Retrieval Evaluation",
        "url": "http://arxiv.org/abs/2404.08137v1",
        "pub_date": "2024-04-11",
        "summary": "In this chapter, we consider generative information retrieval evaluation from\ntwo distinct but interrelated perspectives. First, large language models (LLMs)\nthemselves are rapidly becoming tools for evaluation, with current research\nindicating that LLMs may be superior to crowdsource workers and other paid\nassessors on basic relevance judgement tasks. We review past and ongoing\nrelated research, including speculation on the future of shared task\ninitiatives, such as TREC, and a discussion on the continuing need for human\nassessments. Second, we consider the evaluation of emerging LLM-based\ngenerative information retrieval (GenIR) systems, including retrieval augmented\ngeneration (RAG) systems. We consider approaches that focus both on the\nend-to-end evaluation of GenIR systems and on the evaluation of a retrieval\ncomponent as an element in a RAG system. Going forward, we expect the\nevaluation of GenIR systems to be at least partially based on LLM-based\nassessment, creating an apparent circularity, with a system seemingly\nevaluating its own output. We resolve this apparent circularity in two ways: 1)\nby viewing LLM-based assessment as a form of \"slow search\", where a slower IR\nsystem is used for evaluation and training of a faster production IR system;\nand 2) by recognizing a continuing need to ground evaluation in human\nassessment, even if the characteristics of that human assessment must change.",
        "translated": ""
    },
    {
        "title": "Extending Translate-Train for ColBERT-X to African Language CLIR",
        "url": "http://arxiv.org/abs/2404.08134v1",
        "pub_date": "2024-04-11",
        "summary": "This paper describes the submission runs from the HLTCOE team at the CIRAL\nCLIR tasks for African languages at FIRE 2023. Our submissions use machine\ntranslation models to translate the documents and the training passages, and\nColBERT-X as the retrieval model. Additionally, we present a set of unofficial\nruns that use an alternative training procedure with a similar training\nsetting.",
        "translated": ""
    },
    {
        "title": "Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User\n  Behavior Representation to the Scenario Context",
        "url": "http://arxiv.org/abs/2404.09709v1",
        "pub_date": "2024-04-15",
        "summary": "Existing methods often adjust representations adaptively only after\naggregating user behavior sequences. This coarse-grained approach to\nre-weighting the entire user sequence hampers the model's ability to accurately\nmodel the user interest migration across different scenarios. To enhance the\nmodel's capacity to capture user interests from historical behavior sequences\nin each scenario, we develop a ranking framework named the Scenario-Adaptive\nFine-Grained Personalization Network (SFPNet), which designs a kind of\nfine-grained method for multi-scenario personalized recommendations.\nSpecifically, SFPNet comprises a series of blocks named as Scenario-Tailoring\nBlock, stacked sequentially. Each block initially deploys a parameter\npersonalization unit to integrate scenario information at a coarse-grained\nlevel by redefining fundamental features. Subsequently, we consolidate\nscenario-adaptively adjusted feature representations to serve as context\ninformation. By employing residual connection, we incorporate this context into\nthe representation of each historical behavior, allowing for context-aware\nfine-grained customization of the behavior representations at the\nscenario-level, which in turn supports scenario-aware user interest modeling.",
        "translated": ""
    },
    {
        "title": "Recall-Augmented Ranking: Enhancing Click-Through Rate Prediction\n  Accuracy with Cross-Stage Data",
        "url": "http://arxiv.org/abs/2404.09578v1",
        "pub_date": "2024-04-15",
        "summary": "Click-through rate (CTR) prediction plays an indispensable role in online\nplatforms. Numerous models have been proposed to capture users' shifting\npreferences by leveraging user behavior sequences. However, these historical\nsequences often suffer from severe homogeneity and scarcity compared to the\nextensive item pool. Relying solely on such sequences for user representations\nis inherently restrictive, as user interests extend beyond the scope of items\nthey have previously engaged with. To address this challenge, we propose a\ndata-driven approach to enrich user representations. We recognize user\nprofiling and recall items as two ideal data sources within the cross-stage\nframework, encompassing the u2u (user-to-user) and i2i (item-to-item) aspects\nrespectively. In this paper, we propose a novel architecture named\nRecall-Augmented Ranking (RAR). RAR consists of two key sub-modules, which\nsynergistically gather information from a vast pool of look-alike users and\nrecall items, resulting in enriched user representations. Notably, RAR is\northogonal to many existing CTR models, allowing for consistent performance\nimprovements in a plug-and-play manner. Extensive experiments are conducted,\nwhich verify the efficacy and compatibility of RAR against the SOTA methods.",
        "translated": ""
    },
    {
        "title": "UniSAR: Modeling User Transition Behaviors between Search and\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.09520v1",
        "pub_date": "2024-04-15",
        "summary": "Nowadays, many platforms provide users with both search and recommendation\nservices as important tools for accessing information. The phenomenon has led\nto a correlation between user search and recommendation behaviors, providing an\nopportunity to model user interests in a fine-grained way. Existing approaches\neither model user search and recommendation behaviors separately or overlook\nthe different transitions between user search and recommendation behaviors. In\nthis paper, we propose a framework named UniSAR that effectively models the\ndifferent types of fine-grained behavior transitions for providing users a\nUnified Search And Recommendation service. Specifically, UniSAR models the user\ntransition behaviors between search and recommendation through three steps:\nextraction, alignment, and fusion, which are respectively implemented by\ntransformers equipped with pre-defined masks, contrastive learning that aligns\nthe extracted fine-grained user transitions, and cross-attentions that fuse\ndifferent transitions. To provide users with a unified service, the learned\nrepresentations are fed into the downstream search and recommendation models.\nJoint learning on both search and recommendation data is employed to utilize\nthe knowledge and enhance each other. Experimental results on two public\ndatasets demonstrated the effectiveness of UniSAR in terms of enhancing both\nsearch and recommendation simultaneously. The experimental analysis further\nvalidates that UniSAR enhances the results by successfully modeling the user\ntransition behaviors between search and recommendation.",
        "translated": ""
    },
    {
        "title": "Exploring the Nexus Between Retrievability and Query Generation\n  Strategies",
        "url": "http://arxiv.org/abs/2404.09473v1",
        "pub_date": "2024-04-15",
        "summary": "Quantifying bias in retrieval functions through document retrievability\nscores is vital for assessing recall-oriented retrieval systems. However, many\nstudies investigating retrieval model bias lack validation of their query\ngeneration methods as accurate representations of retrievability for real users\nand their queries. This limitation results from the absence of established\ncriteria for query generation in retrievability assessments. Typically,\nresearchers resort to using frequent collocations from document corpora when no\nquery log is available. In this study, we address the issue of reproducibility\nand seek to validate query generation methods by comparing retrievability\nscores generated from artificially generated queries to those derived from\nquery logs. Our findings demonstrate a minimal or negligible correlation\nbetween retrievability scores from artificial queries and those from query\nlogs. This suggests that artificially generated queries may not accurately\nreflect retrievability scores as derived from query logs. We further explore\nalternative query generation techniques, uncovering a variation that exhibits\nthe highest correlation. This alternative approach holds promise for improving\nreproducibility when query logs are unavailable.",
        "translated": ""
    },
    {
        "title": "Artificial Intelligence enhanced Security Problems in Real-Time Scenario\n  using Blowfish Algorithm",
        "url": "http://arxiv.org/abs/2404.09286v1",
        "pub_date": "2024-04-14",
        "summary": "In a nutshell, \"the cloud\" refers to a collection of interconnected computing\nresources made possible by an extensive, real-time communication network like\nthe internet. Because of its potential to reduce processing costs, the emerging\nparadigm of cloud computing has recently attracted a large number of academics.\nThe exponential expansion of cloud computing has made the rapid expansion of\ncloud services very remarkable. Ensuring the security of personal information\nin today's interconnected world is no easy task. These days, security is really\ncrucial. Models of security that are relevant to cloud computing include\nconfidentiality, authenticity, accessibility, data integrity, and recovery.\nUsing the Hybrid Encryption this study, we cover all the security issues and\nleaks in cloud infrastructure.",
        "translated": ""
    },
    {
        "title": "Competitive Retrieval: Going Beyond the Single Query",
        "url": "http://arxiv.org/abs/2404.09253v1",
        "pub_date": "2024-04-14",
        "summary": "Previous work on the competitive retrieval setting focused on a single-query\nsetting: document authors manipulate their documents so as to improve their\nfuture ranking for a given query. We study a competitive setting where authors\nopt to improve their document's ranking for multiple queries. We use game\ntheoretic analysis to prove that equilibrium does not necessarily exist. We\nthen empirically show that it is more difficult for authors to improve their\ndocuments' rankings for multiple queries with a neural ranker than with a\nstate-of-the-art feature-based ranker. We also present an effective approach\nfor predicting the document most highly ranked in the next induced ranking.",
        "translated": ""
    },
    {
        "title": "Semantic In-Domain Product Identification for Search Queries",
        "url": "http://arxiv.org/abs/2404.09091v1",
        "pub_date": "2024-04-13",
        "summary": "Accurate explicit and implicit product identification in search queries is\ncritical for enhancing user experiences, especially at a company like Adobe\nwhich has over 50 products and covers queries across hundreds of tools. In this\nwork, we present a novel approach to training a product classifier from user\nbehavioral data. Our semantic model led to &gt;25% relative improvement in CTR\n(click through rate) across the deployed surfaces; a &gt;50% decrease in null\nrate; a 2x increase in the app cards surfaced, which helps drive product\nvisibility.",
        "translated": ""
    },
    {
        "title": "CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge\n  Graph Prompting",
        "url": "http://arxiv.org/abs/2404.09077v1",
        "pub_date": "2024-04-13",
        "summary": "In the field of Question Answering (QA), unifying large language models\n(LLMs) with external databases has shown great success. However, these methods\noften fall short in providing the advanced reasoning needed for complex QA\ntasks. To address these issues, we improve over a novel approach called\nKnowledge Graph Prompting (KGP), which combines knowledge graphs with a\nLLM-based agent to improve reasoning and search accuracy. Nevertheless, the\noriginal KGP framework necessitates costly fine-tuning with large datasets yet\nstill suffers from LLM hallucination. Therefore, we propose a reasoning-infused\nLLM agent to enhance this framework. This agent mimics human curiosity to ask\nfollow-up questions to more efficiently navigate the search. This simple\nmodification significantly boosts the LLM performance in QA tasks without the\nhigh costs and latency associated with the initial KGP framework. Our ultimate\ngoal is to further develop this approach, leading to more accurate, faster, and\ncost-effective solutions in the QA domain.",
        "translated": ""
    },
    {
        "title": "Introducing Super RAGs in Mistral 8x7B-v1",
        "url": "http://arxiv.org/abs/2404.08940v1",
        "pub_date": "2024-04-13",
        "summary": "The relentless pursuit of enhancing Large Language Models (LLMs) has led to\nthe advent of Super Retrieval-Augmented Generation (Super RAGs), a novel\napproach designed to elevate the performance of LLMs by integrating external\nknowledge sources with minimal structural modifications. This paper presents\nthe integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM,\nand examines the resultant improvements in accuracy, speed, and user\nsatisfaction. Our methodology uses a fine-tuned instruct model setup and a\ncache tuning fork system, ensuring efficient and relevant data retrieval. The\nevaluation, conducted over several epochs, demonstrates significant\nenhancements across all metrics. The findings suggest that Super RAGs can\neffectively augment LLMs, paving the way for more sophisticated and reliable AI\nsystems. This research contributes to the field by providing empirical evidence\nof the benefits of Super RAGs and offering insights into their potential\napplications.",
        "translated": ""
    },
    {
        "title": "Approximate Cluster-Based Sparse Document Retrieval with Segmented\n  Maximum Term Weights",
        "url": "http://arxiv.org/abs/2404.08896v1",
        "pub_date": "2024-04-13",
        "summary": "This paper revisits cluster-based retrieval that partitions the inverted\nindex into multiple groups and skips the index partially at cluster and\ndocument levels during online inference using a learned sparse representation.\nIt proposes an approximate search scheme with two parameters to control the\nrank-safeness competitiveness of pruning with segmented maximum term weights\nwithin each cluster. Cluster-level maximum weight segmentation allows an\nimprovement in the rank score bound estimation and threshold-based pruning to\nbe approximately adaptive to bound estimation tightness, resulting in better\nrelevance and efficiency. The experiments with MS MARCO passage ranking and\nBEIR datasets demonstrate the usefulness of the proposed scheme with a\ncomparison to the baselines. This paper presents the design of this approximate\nretrieval scheme with rank-safeness analysis, compares clustering and\nsegmentation options, and reports evaluation results.",
        "translated": ""
    },
    {
        "title": "Spiral of Silences: How is Large Language Model Killing Information\n  Retrieval? -- A Case Study on Open Domain Question Answering",
        "url": "http://arxiv.org/abs/2404.10496v1",
        "pub_date": "2024-04-16",
        "summary": "The practice of Retrieval-Augmented Generation (RAG), which integrates Large\nLanguage Models (LLMs) with retrieval systems, has become increasingly\nprevalent. However, the repercussions of LLM-derived content infiltrating the\nweb and influencing the retrieval-generation feedback loop are largely\nuncharted territories. In this study, we construct and iteratively run a\nsimulation pipeline to deeply investigate the short-term and long-term effects\nof LLM text on RAG systems. Taking the trending Open Domain Question Answering\n(ODQA) task as a point of entry, our findings reveal a potential digital\n\"Spiral of Silence\" effect, with LLM-generated text consistently outperforming\nhuman-authored content in search rankings, thereby diminishing the presence and\nimpact of human contributions online. This trend risks creating an imbalanced\ninformation ecosystem, where the unchecked proliferation of erroneous\nLLM-generated content may result in the marginalization of accurate\ninformation. We urge the academic community to take heed of this potential\nissue, ensuring a diverse and authentic digital information landscape.",
        "translated": ""
    },
    {
        "title": "Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large\n  Language Model for Domain Question Answering",
        "url": "http://arxiv.org/abs/2404.10384v1",
        "pub_date": "2024-04-16",
        "summary": "Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform\nsurprisingly well and outperform human experts on many tasks. However, in many\ndomain-specific evaluations, these LLMs often suffer from hallucination\nproblems due to insufficient training of relevant corpus. Furthermore,\nfine-tuning large models may face problems such as the LLMs are not open source\nor the construction of high-quality domain instruction is difficult. Therefore,\nstructured knowledge databases such as knowledge graph can better provide\ndomain back- ground knowledge for LLMs and make full use of the reasoning and\nanalysis capabilities of LLMs. In some previous works, LLM was called multiple\ntimes to determine whether the current triplet was suitable for inclusion in\nthe subgraph when retrieving subgraphs through a question. Especially for the\nquestion that require a multi-hop reasoning path, frequent calls to LLM will\nconsume a lot of computing power. Moreover, when choosing the reasoning path,\nLLM will be called once for each step, and if one of the steps is selected\nincorrectly, it will lead to the accumulation of errors in the following steps.\nIn this paper, we integrated and optimized a pipeline for selecting reasoning\npaths from KG based on LLM, which can reduce the dependency on LLM. In\naddition, we propose a simple and effective subgraph retrieval method based on\nchain of thought (CoT) and page rank which can returns the paths most likely to\ncontain the answer. We conduct experiments on three datasets: GenMedGPT-5k\n[14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using\nfewer LLM calls can achieve the same results as previous SOTAs models.",
        "translated": ""
    },
    {
        "title": "Promoting the linguistic diversity of TEI in the Maghreb and the Arab\n  region",
        "url": "http://arxiv.org/abs/2404.10371v1",
        "pub_date": "2024-04-16",
        "summary": "The project targets both oral corpus and the rich text resources written in\nthe Maghreb region. It focuses particularly on the continuity, for more than 12\ncenturies, of a classical still alive Arabic language and on the extreme\nhybridization of vernacular languages sustained by the rich Libyan, Roman,\nHebrew and Ottoman influences and by the more recent French, Spanish and\nItalian linguistic interference. In short, the Maghreb is a place of extremely\nabundant, but much unexploited, textual studies.",
        "translated": ""
    },
    {
        "title": "Exact and Efficient Unlearning for Large Language Model-based\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.10327v1",
        "pub_date": "2024-04-16",
        "summary": "The evolving paradigm of Large Language Model-based Recom- mendation (LLMRec)\ncustomizes Large Language Models (LLMs) through parameter-efficient fine-tuning\n(PEFT) using recommenda- tion data. The inclusion of user data in LLMs raises\nprivacy concerns. To protect users, the unlearning process in LLMRec,\nspecifically removing unusable data (e.g., historical behaviors) from\nestablished LLMRec models, becomes crucial. However, existing unlearning\nmethods are insufficient for the unique characteristics of LLM- Rec, mainly due\nto high computational costs or incomplete data erasure. In this study, we\nintroduce the Adapter Partition and Ag- gregation (APA) framework for exact and\nefficient unlearning while maintaining recommendation performance. APA achieves\nthis by establishing distinct adapters for partitioned training data shards and\nretraining only the adapters impacted by unusable data for un- learning. To\npreserve recommendation performance and mitigate considerable inference costs,\nAPA employs parameter-level adapter aggregation with sample-adaptive attention\nfor individual testing samples. Extensive experiments substantiate the\neffectiveness and efficiency of our proposed framework",
        "translated": ""
    },
    {
        "title": "Cluster-based Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2404.10321v1",
        "pub_date": "2024-04-16",
        "summary": "Graph Convolution Networks (GCNs) have significantly succeeded in learning\nuser and item representations for recommendation systems. The core of their\nefficacy is the ability to explicitly exploit the collaborative signals from\nboth the first- and high-order neighboring nodes. However, most existing\nGCN-based methods overlook the multiple interests of users while performing\nhigh-order graph convolution. Thus, the noisy information from unreliable\nneighbor nodes (e.g., users with dissimilar interests) negatively impacts the\nrepresentation learning of the target node. Additionally, conducting graph\nconvolution operations without differentiating high-order neighbors suffers the\nover-smoothing issue when stacking more layers, resulting in performance\ndegradation. In this paper, we aim to capture more valuable information from\nhigh-order neighboring nodes while avoiding noise for better representation\nlearning of the target node. To achieve this goal, we propose a novel GCN-based\nrecommendation model, termed Cluster-based Graph Collaborative Filtering\n(ClusterGCF). This model performs high-order graph convolution on\ncluster-specific graphs, which are constructed by capturing the multiple\ninterests of users and identifying the common interests among them.\nSpecifically, we design an unsupervised and optimizable soft node clustering\napproach to classify user and item nodes into multiple clusters. Based on the\nsoft node clustering results and the topology of the user-item interaction\ngraph, we assign the nodes with probabilities for different clusters to\nconstruct the cluster-specific graphs. To evaluate the effectiveness of\nClusterGCF, we conducted extensive experiments on four publicly available\ndatasets. Experimental results demonstrate that our model can significantly\nimprove recommendation performance.",
        "translated": ""
    },
    {
        "title": "Compressible and Searchable: AI-native Multi-Modal Retrieval System with\n  Learned Image Compression",
        "url": "http://arxiv.org/abs/2404.10234v1",
        "pub_date": "2024-04-16",
        "summary": "The burgeoning volume of digital content across diverse modalities\nnecessitates efficient storage and retrieval methods. Conventional approaches\nstruggle to cope with the escalating complexity and scale of multimedia data.\nIn this paper, we proposed framework addresses this challenge by fusing\nAI-native multi-modal search capabilities with neural image compression. First\nwe analyze the intricate relationship between compressibility and\nsearchability, recognizing the pivotal role each plays in the efficiency of\nstorage and retrieval systems. Through the usage of simple adapter is to bridge\nthe feature of Learned Image Compression(LIC) and Contrastive Language-Image\nPretraining(CLIP) while retaining semantic fidelity and retrieval of\nmulti-modal data. Experimental evaluations on Kodak datasets demonstrate the\nefficacy of our approach, showcasing significant enhancements in compression\nefficiency and search accuracy compared to existing methodologies. Our work\nmarks a significant advancement towards scalable and efficient multi-modal\nsearch systems in the era of big data.",
        "translated": ""
    },
    {
        "title": "TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table\n  Decomposition",
        "url": "http://arxiv.org/abs/2404.10150v1",
        "pub_date": "2024-04-15",
        "summary": "Table reasoning is a challenging task that requires understanding both\nnatural language questions and structured tabular data. Large language models\n(LLMs) have shown impressive capabilities in natural language understanding and\ngeneration, but they often struggle with large tables due to their limited\ninput length. In this paper, we propose TabSQLify, a novel method that\nleverages text-to-SQL generation to decompose tables into smaller and relevant\nsub-tables, containing only essential information for answering questions or\nverifying statements, before performing the reasoning task. In our\ncomprehensive evaluation on four challenging datasets, our approach\ndemonstrates comparable or superior performance compared to prevailing methods\nreliant on full tables as input. Moreover, our method can reduce the input\ncontext length significantly, making it more scalable and efficient for\nlarge-scale table reasoning applications. Our method performs remarkably well\non the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the\nTabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass\nother LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can\nreduce the table size significantly alleviating the computational load on LLMs\nwhen handling large tables without compromising performance.",
        "translated": ""
    },
    {
        "title": "LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT\n  Large Language Model",
        "url": "http://arxiv.org/abs/2404.10097v1",
        "pub_date": "2024-04-15",
        "summary": "A contract is a type of legal document commonly used in organizations.\nContract review is an integral and repetitive process to avoid business risk\nand liability. Contract analysis requires the identification and classification\nof key provisions and paragraphs within an agreement. Identification and\nvalidation of contract clauses can be a time-consuming and challenging task\ndemanding the services of trained and expensive lawyers, paralegals or other\nlegal assistants. Classification of legal provisions in contracts using\nartificial intelligence and natural language processing is complex due to the\nrequirement of domain-specialized legal language for model training and the\nscarcity of sufficient labeled data in the legal domain. Using general-purpose\nmodels is not effective in this context due to the use of specialized legal\nvocabulary in contracts which may not be recognized by a general model. To\naddress this problem, we propose the use of a pre-trained large language model\nwhich is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a\nBERT transformer architecture model that we fine- tune to efficiently handle\nclassification task for legal provisions. We conducted experiments to measure\nand compare metrics with current benchmark results. We found that LegalPro-BERT\noutperforms the previous benchmark used for comparison in this research.",
        "translated": ""
    },
    {
        "title": "Context Does Matter: Implications for Crowdsourced Evaluation Labels in\n  Task-Oriented Dialogue Systems",
        "url": "http://arxiv.org/abs/2404.09980v1",
        "pub_date": "2024-04-15",
        "summary": "Crowdsourced labels play a crucial role in evaluating task-oriented dialogue\nsystems (TDSs). Obtaining high-quality and consistent ground-truth labels from\nannotators presents challenges. When evaluating a TDS, annotators must fully\ncomprehend the dialogue before providing judgments. Previous studies suggest\nusing only a portion of the dialogue context in the annotation process.\nHowever, the impact of this limitation on label quality remains unexplored.\nThis study investigates the influence of dialogue context on annotation\nquality, considering the truncated context for relevance and usefulness\nlabeling. We further propose to use large language models (LLMs) to summarize\nthe dialogue context to provide a rich and short description of the dialogue\ncontext and study the impact of doing so on the annotator's performance.\nReducing context leads to more positive ratings. Conversely, providing the\nentire dialogue context yields higher-quality relevance ratings but introduces\nambiguity in usefulness ratings. Using the first user utterance as context\nleads to consistent ratings, akin to those obtained using the entire dialogue,\nwith significantly reduced annotation effort. Our findings show how task\ndesign, particularly the availability of dialogue context, affects the quality\nand consistency of crowdsourced evaluation labels.",
        "translated": ""
    },
    {
        "title": "Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval",
        "url": "http://arxiv.org/abs/2404.09889v1",
        "pub_date": "2024-04-15",
        "summary": "Retrieving relevant tables containing the necessary information to accurately\nanswer a given question over tables is critical to open-domain\nquestion-answering (QA) systems. Previous methods assume the answer to such a\nquestion can be found either in a single table or multiple tables identified\nthrough question decomposition or rewriting. However, neither of these\napproaches is sufficient, as many questions require retrieving multiple tables\nand joining them through a join plan that cannot be discerned from the user\nquery itself. If the join plan is not considered in the retrieval stage, the\nsubsequent steps of reasoning and answering based on those retrieved tables are\nlikely to be incorrect. To address this problem, we introduce a method that\nuncovers useful join relations for any query and database during table\nretrieval. We use a novel re-ranking method formulated as a mixed-integer\nprogram that considers not only table-query relevance but also table-table\nrelevance that requires inferring join relationships. Our method outperforms\nthe state-of-the-art approaches for table retrieval by up to 9.3% in F1 score\nand for end-to-end QA by up to 5.4% in accuracy.",
        "translated": ""
    },
    {
        "title": "Accounting for AI and Users Shaping One Another: The Role of\n  Mathematical Models",
        "url": "http://arxiv.org/abs/2404.12366v1",
        "pub_date": "2024-04-18",
        "summary": "As AI systems enter into a growing number of societal domains, these systems\nincreasingly shape and are shaped by user preferences, opinions, and behaviors.\nHowever, the design of AI systems rarely accounts for how AI and users shape\none another. In this position paper, we argue for the development of formal\ninteraction models which mathematically specify how AI and users shape one\nanother. Formal interaction models can be leveraged to (1) specify interactions\nfor implementation, (2) monitor interactions through empirical analysis, (3)\nanticipate societal impacts via counterfactual analysis, and (4) control\nsocietal impacts via interventions. The design space of formal interaction\nmodels is vast, and model design requires careful consideration of factors such\nas style, granularity, mathematical complexity, and measurability. Using\ncontent recommender systems as a case study, we critically examine the nascent\nliterature of formal interaction models with respect to these use-cases and\ndesign axes. More broadly, we call for the community to leverage formal\ninteraction models when designing, evaluating, or auditing any AI system which\ninteracts with users.",
        "translated": ""
    },
    {
        "title": "When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\n  with Many Classes",
        "url": "http://arxiv.org/abs/2404.12365v1",
        "pub_date": "2024-04-18",
        "summary": "We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.",
        "translated": ""
    },
    {
        "title": "iRAG: An Incremental Retrieval Augmented Generation System for Videos",
        "url": "http://arxiv.org/abs/2404.12309v1",
        "pub_date": "2024-04-18",
        "summary": "Retrieval augmented generation (RAG) systems combine the strengths of\nlanguage generation and information retrieval to power many real-world\napplications like chatbots. Use of RAG for combined understanding of multimodal\ndata such as text, images and videos is appealing but two critical limitations\nexist: one-time, upfront capture of all content in large multimodal data as\ntext descriptions entails high processing times, and not all information in the\nrich multimodal data is typically in the text descriptions. Since the user\nqueries are not known apriori, developing a system for multimodal to text\nconversion and interactive querying of multimodal data is challenging.\n  To address these limitations, we propose iRAG, which augments RAG with a\nnovel incremental workflow to enable interactive querying of large corpus of\nmultimodal data. Unlike traditional RAG, iRAG quickly indexes large\nrepositories of multimodal data, and in the incremental workflow, it uses the\nindex to opportunistically extract more details from select portions of the\nmultimodal data to retrieve context relevant to an interactive user query. Such\nan incremental workflow avoids long multimodal to text conversion times,\novercomes information loss issues by doing on-demand query-specific extraction\nof details in multimodal data, and ensures high quality of responses to\ninteractive user queries that are often not known apriori. To the best of our\nknowledge, iRAG is the first system to augment RAG with an incremental workflow\nto support efficient interactive querying of large, real-world multimodal data.\nExperimental results on real-world long videos demonstrate 23x to 25x faster\nvideo to text ingestion, while ensuring that quality of responses to\ninteractive user queries is comparable to responses from a traditional RAG\nwhere all video data is converted to text upfront before any querying.",
        "translated": ""
    },
    {
        "title": "De-DSI: Decentralised Differentiable Search Index",
        "url": "http://arxiv.org/abs/2404.12237v1",
        "pub_date": "2024-04-18",
        "summary": "This study introduces De-DSI, a novel framework that fuses large language\nmodels (LLMs) with genuine decentralization for information retrieval,\nparticularly employing the differentiable search index (DSI) concept in a\ndecentralized setting. Focused on efficiently connecting novel user queries\nwith document identifiers without direct document access, De-DSI operates\nsolely on query-docid pairs. To enhance scalability, an ensemble of DSI models\nis introduced, where the dataset is partitioned into smaller shards for\nindividual model training. This approach not only maintains accuracy by\nreducing the number of data each model needs to handle but also facilitates\nscalability by aggregating outcomes from multiple models. This aggregation uses\na beam search to identify top docids and applies a softmax function for score\nnormalization, selecting documents with the highest scores for retrieval. The\ndecentralized implementation demonstrates that retrieval success is comparable\nto centralized methods, with the added benefit of the possibility of\ndistributing computational complexity across the network. This setup also\nallows for the retrieval of multimedia items through magnet links, eliminating\nthe need for platforms or intermediaries.",
        "translated": ""
    },
    {
        "title": "Estimating the Hessian Matrix of Ranking Objectives for Stochastic\n  Learning to Rank with Gradient Boosted Trees",
        "url": "http://arxiv.org/abs/2404.12190v1",
        "pub_date": "2024-04-18",
        "summary": "Stochastic learning to rank (LTR) is a recent branch in the LTR field that\nconcerns the optimization of probabilistic ranking models. Their probabilistic\nbehavior enables certain ranking qualities that are impossible with\ndeterministic models. For example, they can increase the diversity of displayed\ndocuments, increase fairness of exposure over documents, and better balance\nexploitation and exploration through randomization. A core difficulty in LTR is\ngradient estimation, for this reason, existing stochastic LTR methods have been\nlimited to differentiable ranking models (e.g., neural networks). This is in\nstark contrast with the general field of LTR where Gradient Boosted Decision\nTrees (GBDTs) have long been considered the state-of-the-art.\n  In this work, we address this gap by introducing the first stochastic LTR\nmethod for GBDTs. Our main contribution is a novel estimator for the\nsecond-order derivatives, i.e., the Hessian matrix, which is a requirement for\neffective GBDTs. To efficiently compute both the first and second-order\nderivatives simultaneously, we incorporate our estimator into the existing\nPL-Rank framework, which was originally designed for first-order derivatives\nonly. Our experimental results indicate that stochastic LTR without the Hessian\nhas extremely poor performance, whilst the performance is competitive with the\ncurrent state-of-the-art with our estimated Hessian. Thus, through the\ncontribution of our novel Hessian estimation method, we have successfully\nintroduced GBDTs to stochastic LTR.",
        "translated": ""
    },
    {
        "title": "Shotit: compute-efficient image-to-video search engine for the cloud",
        "url": "http://arxiv.org/abs/2404.12169v1",
        "pub_date": "2024-04-18",
        "summary": "With the rapid growth of information technology, users are exposed to a\nmassive amount of data online, including image, music, and video. This has led\nto strong needs to provide effective corresponsive search services such as\nimage, music, and video search services. Most of them are operated based on\nkeywords, namely using keywords to find related image, music, and video.\nAdditionally, there are image-to-image search services that enable users to\nfind similar images using one input image. Given that videos are essentially\ncomposed of image frames, then similar videos can be searched by one input\nimage or screenshot. We want to target this scenario and provide an efficient\nmethod and implementation in this paper.\n  We present Shotit, a cloud-native image-to-video search engine that tailors\nthis search scenario in a compute-efficient approach. One main limitation faced\nin this scenario is the scale of its dataset. A typical image-to-image search\nengine only handles one-to-one relationships, colloquially, one image\ncorresponds to another single image. But image-to-video proliferates. Take a\n24-min length video as an example, it will generate roughly 20,000 image\nframes. As the number of videos grows, the scale of the dataset explodes\nexponentially. In this case, a compute-efficient approach ought to be\nconsidered, and the system design should cater to the cloud-native trend.\nChoosing an emerging technology - vector database as its backbone, Shotit fits\nthese two metrics performantly. Experiments for two different datasets, a 50\nthousand-scale Blender Open Movie dataset, and a 50 million-scale proprietary\nTV genre dataset at a 4 Core 32GB RAM Intel Xeon Gold 6271C cloud machine with\nobject storage reveal the effectiveness of Shotit. A demo regarding the Blender\nOpen Movie dataset is illustrated within this paper.",
        "translated": ""
    },
    {
        "title": "How Do Recommendation Models Amplify Popularity Bias? An Analysis from\n  the Spectral Perspective",
        "url": "http://arxiv.org/abs/2404.12008v1",
        "pub_date": "2024-04-18",
        "summary": "Recommendation Systems (RS) are often plagued by popularity bias.\nSpecifically,when recommendation models are trained on long-tailed datasets,\nthey not only inherit this bias but often exacerbate it. This effect undermines\nboth the precision and fairness of RS and catalyzes the so-called Matthew\nEffect. Despite the widely recognition of this issue, the fundamental causes\nremain largely elusive. In our research, we delve deeply into popularity bias\namplification. Our comprehensive theoretical and empirical investigations lead\nto two core insights: 1) Item popularity is memorized in the principal singular\nvector of the score matrix predicted by the recommendation model; 2) The\ndimension collapse phenomenon amplifies the impact of principal singular vector\non model predictions, intensifying the popularity bias. Based on these\ninsights, we propose a novel method to mitigate this bias by imposing penalties\non the magnitude of the principal singular value. Considering the heavy\ncomputational burden in directly evaluating the gradient of the principal\nsingular value, we develop an efficient algorithm that harnesses the inherent\nproperties of the singular vector. Extensive experiments across seven\nreal-world datasets and three testing scenarios have been conducted to validate\nthe superiority of our method.",
        "translated": ""
    },
    {
        "title": "Knowledge-Aware Multi-Intent Contrastive Learning for Multi-Behavior\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.11993v1",
        "pub_date": "2024-04-18",
        "summary": "Multi-behavioral recommendation optimizes user experiences by providing users\nwith more accurate choices based on their diverse behaviors, such as view, add\nto cart, and purchase. Current studies on multi-behavioral recommendation\nmainly explore the connections and differences between multi-behaviors from an\nimplicit perspective. Specifically, they directly model those relations using\nblack-box neural networks. In fact, users' interactions with items under\ndifferent behaviors are driven by distinct intents. For instance, when users\nview products, they tend to pay greater attention to information such as\nratings and brands. However, when it comes to the purchasing phase, users\nbecome more price-conscious. To tackle this challenge and data sparsity problem\nin the multi-behavioral recommendation, we propose a novel model:\nKnowledge-Aware Multi-Intent Contrastive Learning (KAMCL) model. This model\nuses relationships in the knowledge graph to construct intents, aiming to mine\nthe connections between users' multi-behaviors from the perspective of intents\nto achieve more accurate recommendations. KAMCL is equipped with two\ncontrastive learning schemes to alleviate the data scarcity problem and further\nenhance user representations. Extensive experiments on three real datasets\ndemonstrate the superiority of our model.",
        "translated": ""
    },
    {
        "title": "SIGformer: Sign-aware Graph Transformer for Recommendation",
        "url": "http://arxiv.org/abs/2404.11982v1",
        "pub_date": "2024-04-18",
        "summary": "In recommender systems, most graph-based methods focus on positive user\nfeedback, while overlooking the valuable negative feedback. Integrating both\npositive and negative feedback to form a signed graph can lead to a more\ncomprehensive understanding of user preferences. However, the existing efforts\nto incorporate both types of feedback are sparse and face two main limitations:\n1) They process positive and negative feedback separately, which fails to\nholistically leverage the collaborative information within the signed graph; 2)\nThey rely on MLPs or GNNs for information extraction from negative feedback,\nwhich may not be effective.\n  To overcome these limitations, we introduce SIGformer, a new method that\nemploys the transformer architecture to sign-aware graph-based recommendation.\nSIGformer incorporates two innovative positional encodings that capture the\nspectral properties and path patterns of the signed graph, enabling the full\nexploitation of the entire graph. Our extensive experiments across five\nreal-world datasets demonstrate the superiority of SIGformer over\nstate-of-the-art methods. The code is available at\nhttps://github.com/StupidThree/SIGformer.",
        "translated": ""
    },
    {
        "title": "Generating Diverse Criteria On-the-Fly to Improve Point-wise LLM Rankers",
        "url": "http://arxiv.org/abs/2404.11960v1",
        "pub_date": "2024-04-18",
        "summary": "The most recent pointwise Large Language Model (LLM) rankers have achieved\nremarkable ranking results. However, these rankers are hindered by two major\ndrawbacks: (1) they fail to follow a standardized comparison guidance during\nthe ranking process, and (2) they struggle with comprehensive considerations\nwhen dealing with complicated passages. To address these shortcomings, we\npropose to build a ranker that generates ranking scores based on a set of\ncriteria from various perspectives. These criteria are intended to direct each\nperspective in providing a distinct yet synergistic evaluation. Our research,\nwhich examines eight datasets from the BEIR benchmark demonstrates that\nincorporating this multi-perspective criteria ensemble approach markedly\nenhanced the performance of pointwise LLM rankers.",
        "translated": ""
    },
    {
        "title": "Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback\n  on Crowdworkers and LLMs",
        "url": "http://arxiv.org/abs/2404.12994v1",
        "pub_date": "2024-04-19",
        "summary": "In ad-hoc retrieval, evaluation relies heavily on user actions, including\nimplicit feedback. In a conversational setting such signals are usually\nunavailable due to the nature of the interactions, and, instead, the evaluation\noften relies on crowdsourced evaluation labels. The role of user feedback in\nannotators' assessment of turns in a conversational perception has been little\nstudied. We focus on how the evaluation of task-oriented dialogue systems\n(TDSs), is affected by considering user feedback, explicit or implicit, as\nprovided through the follow-up utterance of a turn being evaluated. We explore\nand compare two methodologies for assessing TDSs: one includes the user's\nfollow-up utterance and one without. We use both crowdworkers and large\nlanguage models (LLMs) as annotators to assess system responses across four\naspects: relevance, usefulness, interestingness, and explanation quality. Our\nfindings indicate that there is a distinct difference in ratings assigned by\nboth annotator groups in the two setups, indicating user feedback does\ninfluence system evaluation. Workers are more susceptible to user feedback on\nusefulness and interestingness compared to LLMs on interestingness and\nrelevance. User feedback leads to a more personalized assessment of usefulness\nby workers, aligning closely with the user's explicit feedback. Additionally,\nin cases of ambiguous or complex user requests, user feedback improves\nagreement among crowdworkers. These findings emphasize the significance of user\nfeedback in refining system evaluations and suggest the potential for automated\nfeedback integration in future research. We publicly release the annotated data\nto foster research in this area.",
        "translated": ""
    },
    {
        "title": "FineRec:Exploring Fine-grained Sequential Recommendation",
        "url": "http://arxiv.org/abs/2404.12975v1",
        "pub_date": "2024-04-19",
        "summary": "Sequential recommendation is dedicated to offering items of interest for\nusers based on their history behaviors. The attribute-opinion pairs, expressed\nby users in their reviews for items, provide the potentials to capture user\npreferences and item characteristics at a fine-grained level. To this end, we\npropose a novel framework FineRec that explores the attribute-opinion pairs of\nreviews to finely handle sequential recommendation. Specifically, we utilize a\nlarge language model to extract attribute-opinion pairs from reviews. For each\nattribute, a unique attribute-specific user-opinion-item graph is created,\nwhere corresponding opinions serve as the edges linking heterogeneous user and\nitem nodes. To tackle the diversity of opinions, we devise a diversity-aware\nconvolution operation to aggregate information within the graphs, enabling\nattribute-specific user and item representation learning. Ultimately, we\npresent an interaction-driven fusion mechanism to integrate attribute-specific\nuser/item representations across all attributes for generating recommendations.\nExtensive experiments conducted on several realworld datasets demonstrate the\nsuperiority of our FineRec over existing state-of-the-art methods. Further\nanalysis also verifies the effectiveness of our fine-grained manner in handling\nthe task.",
        "translated": ""
    },
    {
        "title": "Disentangling ID and Modality Effects for Session-based Recommendation",
        "url": "http://arxiv.org/abs/2404.12969v1",
        "pub_date": "2024-04-19",
        "summary": "Session-based recommendation aims to predict intents of anonymous users based\non their limited behaviors. Modeling user behaviors involves two distinct\nrationales: co-occurrence patterns reflected by item IDs, and fine-grained\npreferences represented by item modalities (e.g., text and images). However,\nexisting methods typically entangle these causes, leading to their failure in\nachieving accurate and explainable recommendations. To this end, we propose a\nnovel framework DIMO to disentangle the effects of ID and modality in the task.\nAt the item level, we introduce a co-occurrence representation schema to\nexplicitly incorporate cooccurrence patterns into ID representations.\nSimultaneously, DIMO aligns different modalities into a unified semantic space\nto represent them uniformly. At the session level, we present a multi-view\nself-supervised disentanglement, including proxy mechanism and counterfactual\ninference, to disentangle ID and modality effects without supervised signals.\nLeveraging these disentangled causes, DIMO provides recommendations via causal\ninference and further creates two templates for generating explanations.\nExtensive experiments on multiple real-world datasets demonstrate the\nconsistent superiority of DIMO over existing methods. Further analysis also\nconfirms DIMO's effectiveness in generating explanations.",
        "translated": ""
    },
    {
        "title": "Towards Human-centered Proactive Conversational Agents",
        "url": "http://arxiv.org/abs/2404.12670v1",
        "pub_date": "2024-04-19",
        "summary": "Recent research on proactive conversational agents (PCAs) mainly focuses on\nimproving the system's capabilities in anticipating and planning action\nsequences to accomplish tasks and achieve goals before users articulate their\nrequests. This perspectives paper highlights the importance of moving towards\nbuilding human-centered PCAs that emphasize human needs and expectations, and\nthat considers ethical and social implications of these agents, rather than\nsolely focusing on technological capabilities. The distinction between a\nproactive and a reactive system lies in the proactive system's\ninitiative-taking nature. Without thoughtful design, proactive systems risk\nbeing perceived as intrusive by human users. We address the issue by\nestablishing a new taxonomy concerning three key dimensions of human-centered\nPCAs, namely Intelligence, Adaptivity, and Civility. We discuss potential\nresearch opportunities and challenges based on this new taxonomy upon the five\nstages of PCA system construction. This perspectives paper lays a foundation\nfor the emerging area of conversational information retrieval research and\npaves the way towards advancing human-centered proactive conversational\nsystems.",
        "translated": ""
    },
    {
        "title": "Turbo-CF: Matrix Decomposition-Free Graph Filtering for Fast\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.14243v1",
        "pub_date": "2024-04-22",
        "summary": "A series of graph filtering (GF)-based collaborative filtering (CF) showcases\nstate-of-the-art performance on the recommendation accuracy by using a low-pass\nfilter (LPF) without a training process. However, conventional GF-based CF\napproaches mostly perform matrix decomposition on the item-item similarity\ngraph to realize the ideal LPF, which results in a non-trivial computational\ncost and thus makes them less practical in scenarios where rapid\nrecommendations are essential. In this paper, we propose Turbo-CF, a GF-based\nCF method that is both training-free and matrix decomposition-free. Turbo-CF\nemploys a polynomial graph filter to circumvent the issue of expensive matrix\ndecompositions, enabling us to make full use of modern computer hardware\ncomponents (i.e., GPU). Specifically, Turbo-CF first constructs an item-item\nsimilarity graph whose edge weights are effectively regulated. Then, our own\npolynomial LPFs are designed to retain only low-frequency signals without\nexplicit matrix decompositions. We demonstrate that Turbo-CF is extremely fast\nyet accurate, achieving a runtime of less than 1 second on real-world benchmark\ndatasets while achieving recommendation accuracies comparable to best\ncompetitors.",
        "translated": ""
    },
    {
        "title": "Collaborative Filtering Based on Diffusion Models: Unveiling the\n  Potential of High-Order Connectivity",
        "url": "http://arxiv.org/abs/2404.14240v1",
        "pub_date": "2024-04-22",
        "summary": "A recent study has shown that diffusion models are well-suited for modeling\nthe generative process of user-item interactions in recommender systems due to\ntheir denoising nature. However, existing diffusion model-based recommender\nsystems do not explicitly leverage high-order connectivities that contain\ncrucial collaborative signals for accurate recommendations. Addressing this\ngap, we propose CF-Diff, a new diffusion model-based collaborative filtering\n(CF) method, which is capable of making full use of collaborative signals along\nwith multi-hop neighbors. Specifically, the forward-diffusion process adds\nrandom noise to user-item interactions, while the reverse-denoising process\naccommodates our own learning model, named cross-attention-guided multi-hop\nautoencoder (CAM-AE), to gradually recover the original user-item interactions.\nCAM-AE consists of two core modules: 1) the attention-aided AE module,\nresponsible for precisely learning latent representations of user-item\ninteractions while preserving the model's complexity at manageable levels, and\n2) the multi-hop cross-attention module, which judiciously harnesses high-order\nconnectivity information to capture enhanced collaborative signals. Through\ncomprehensive experiments on three real-world datasets, we demonstrate that\nCF-Diff is (a) Superior: outperforming benchmark recommendation methods,\nachieving remarkable gains up to 7.29% compared to the best competitor, (b)\nTheoretically-validated: reducing computations while ensuring that the\nembeddings generated by our model closely approximate those from the original\ncross-attention, and (c) Scalable: proving the computational efficiency that\nscales linearly with the number of users or items.",
        "translated": ""
    },
    {
        "title": "SHE-Net: Syntax-Hierarchy-Enhanced Text-Video Retrieval",
        "url": "http://arxiv.org/abs/2404.14066v1",
        "pub_date": "2024-04-22",
        "summary": "The user base of short video apps has experienced unprecedented growth in\nrecent years, resulting in a significant demand for video content analysis. In\nparticular, text-video retrieval, which aims to find the top matching videos\ngiven text descriptions from a vast video corpus, is an essential function, the\nprimary challenge of which is to bridge the modality gap. Nevertheless, most\nexisting approaches treat texts merely as discrete tokens and neglect their\nsyntax structures. Moreover, the abundant spatial and temporal clues in videos\nare often underutilized due to the lack of interaction with text. To address\nthese issues, we argue that using texts as guidance to focus on relevant\ntemporal frames and spatial regions within videos is beneficial. In this paper,\nwe propose a novel Syntax-Hierarchy-Enhanced text-video retrieval method\n(SHE-Net) that exploits the inherent semantic and syntax hierarchy of texts to\nbridge the modality gap from two perspectives. First, to facilitate a more\nfine-grained integration of visual content, we employ the text syntax\nhierarchy, which reveals the grammatical structure of text descriptions, to\nguide the visual representations. Second, to further enhance the multi-modal\ninteraction and alignment, we also utilize the syntax hierarchy to guide the\nsimilarity calculation. We evaluated our method on four public text-video\nretrieval datasets of MSR-VTT, MSVD, DiDeMo, and ActivityNet. The experimental\nresults and ablation studies confirm the advantages of our proposed method.",
        "translated": ""
    },
    {
        "title": "SPLATE: Sparse Late Interaction Retrieval",
        "url": "http://arxiv.org/abs/2404.13950v1",
        "pub_date": "2024-04-22",
        "summary": "The late interaction paradigm introduced with ColBERT stands out in the\nneural Information Retrieval space, offering a compelling\neffectiveness-efficiency trade-off across many benchmarks. Efficient late\ninteraction retrieval is based on an optimized multi-step strategy, where an\napproximate search first identifies a set of candidate documents to re-rank\nexactly. In this work, we introduce SPLATE, a simple and lightweight adaptation\nof the ColBERTv2 model which learns an ``MLM adapter'', mapping its frozen\ntoken embeddings to a sparse vocabulary space with a partially learned SPLADE\nmodule. This allows us to perform the candidate generation step in late\ninteraction pipelines with traditional sparse retrieval techniques, making it\nparticularly appealing for running ColBERT in CPU environments. Our SPLATE\nColBERTv2 pipeline achieves the same effectiveness as the PLAID ColBERTv2\nengine by re-ranking 50 documents that can be retrieved under 10ms.",
        "translated": ""
    },
    {
        "title": "Multi-Level Sequence Denoising with Cross-Signal Contrastive Learning\n  for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2404.13878v1",
        "pub_date": "2024-04-22",
        "summary": "Sequential recommender systems (SRSs) aim to suggest next item for a user\nbased on her historical interaction sequences. Recently, many research efforts\nhave been devoted to attenuate the influence of noisy items in sequences by\neither assigning them with lower attention weights or discarding them directly.\nThe major limitation of these methods is that the former would still prone to\noverfit noisy items while the latter may overlook informative items. To the\nend, in this paper, we propose a novel model named Multi-level Sequence\nDenoising with Cross-signal Contrastive Learning (MSDCCL) for sequential\nrecommendation. To be specific, we first introduce a target-aware user interest\nextractor to simultaneously capture users' long and short term interest with\nthe guidance of target items. Then, we develop a multi-level sequence denoising\nmodule to alleviate the impact of noisy items by employing both soft and hard\nsignal denoising strategies. Additionally, we extend existing curriculum\nlearning by simulating the learning pattern of human beings. It is worth noting\nthat our proposed model can be seamlessly integrated with a majority of\nexisting recommendation models and significantly boost their effectiveness.\nExperimental studies on five public datasets are conducted and the results\ndemonstrate that the proposed MSDCCL is superior to the state-of-the-art\nbaselines. The source code is publicly available at\nhttps://github.com/lalunex/MSDCCL/tree/main.",
        "translated": ""
    },
    {
        "title": "General Item Representation Learning for Cold-start Content\n  Recommendations",
        "url": "http://arxiv.org/abs/2404.13808v1",
        "pub_date": "2024-04-22",
        "summary": "Cold-start item recommendation is a long-standing challenge in recommendation\nsystems. A common remedy is to use a content-based approach, but rich\ninformation from raw contents in various forms has not been fully utilized. In\nthis paper, we propose a domain/data-agnostic item representation learning\nframework for cold-start recommendations, naturally equipped with multimodal\nalignment among various features by adopting a Transformer-based architecture.\nOur proposed model is end-to-end trainable completely free from classification\nlabels, not just costly to collect but suboptimal for recommendation-purpose\nrepresentation learning. From extensive experiments on real-world movie and\nnews recommendation benchmarks, we verify that our approach better preserves\nfine-grained user taste than state-of-the-art baselines, universally applicable\nto multiple domains at large scale.",
        "translated": ""
    },
    {
        "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2404.13781v1",
        "pub_date": "2024-04-21",
        "summary": "Evaluating retrieval-augmented generation (RAG) presents challenges,\nparticularly for retrieval models within these systems. Traditional end-to-end\nevaluation methods are computationally expensive. Furthermore, evaluation of\nthe retrieval model's performance based on query-document relevance labels\nshows a small correlation with the RAG system's downstream performance. We\npropose a novel evaluation approach, eRAG, where each document in the retrieval\nlist is individually utilized by the large language model within the RAG\nsystem. The output generated for each document is then evaluated based on the\ndownstream task ground truth labels. In this manner, the downstream performance\nfor each document serves as its relevance label. We employ various downstream\ntask metrics to obtain document-level annotations and aggregate them using\nset-based or ranking metrics. Extensive experiments on a wide range of datasets\ndemonstrate that eRAG achieves a higher correlation with downstream RAG\nperformance compared to baseline methods, with improvements in Kendall's $\\tau$\ncorrelation ranging from 0.168 to 0.494. Additionally, eRAG offers significant\ncomputational advantages, improving runtime and consuming up to 50 times less\nGPU memory than end-to-end evaluation.",
        "translated": ""
    },
    {
        "title": "ChatRetriever: Adapting Large Language Models for Generalized and Robust\n  Conversational Dense Retrieval",
        "url": "http://arxiv.org/abs/2404.13556v1",
        "pub_date": "2024-04-21",
        "summary": "Conversational search requires accurate interpretation of user intent from\ncomplex multi-turn contexts. This paper presents ChatRetriever, which inherits\nthe strong generalization capability of large language models to robustly\nrepresent complex conversational sessions for dense retrieval. To achieve this,\nwe propose a simple and effective dual-learning approach that adapts LLM for\nretrieval via contrastive learning while enhancing the complex session\nunderstanding through masked instruction tuning on high-quality conversational\ninstruction tuning data. Extensive experiments on five conversational search\nbenchmarks demonstrate that ChatRetriever substantially outperforms existing\nconversational dense retrievers, achieving state-of-the-art performance on par\nwith LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits\nsuperior robustness in handling diverse conversational contexts. Our work\nhighlights the potential of adapting LLMs for retrieval with complex inputs\nlike conversational search sessions and proposes an effective approach to\nadvance this research direction.",
        "translated": ""
    },
    {
        "title": "Beyond Collaborative Filtering: A Relook at Task Formulation in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2404.13375v1",
        "pub_date": "2024-04-20",
        "summary": "Recommender Systems (RecSys) have become indispensable in numerous\napplications, profoundly influencing our everyday experiences. Despite their\npractical significance, academic research in RecSys often abstracts the\nformulation of research tasks from real-world contexts, aiming for a clean\nproblem formulation and more generalizable findings. However, it is observed\nthat there is a lack of collective understanding in RecSys academic research.\nThe root of this issue may lie in the simplification of research task\ndefinitions, and an overemphasis on modeling the decision outcomes rather than\nthe decision-making process. That is, we often conceptualize RecSys as the task\nof predicting missing values in a static user-item interaction matrix, rather\nthan predicting a user's decision on the next interaction within a dynamic,\nchanging, and application-specific context. There exists a mismatch between the\ninputs accessible to a model and the information available to users during\ntheir decision-making process, yet the model is tasked to predict users'\ndecisions. While collaborative filtering is effective in learning general\npreferences from historical records, it is crucial to also consider the dynamic\ncontextual factors in practical settings. Defining research tasks based on\napplication scenarios using domain-specific datasets may lead to more\ninsightful findings. Accordingly, viable solutions and effective evaluations\ncan emerge for different application scenarios.",
        "translated": ""
    },
    {
        "title": "Two-Step SPLADE: Simple, Efficient and Effective Approximation of SPLADE",
        "url": "http://arxiv.org/abs/2404.13357v1",
        "pub_date": "2024-04-20",
        "summary": "Learned sparse models such as SPLADE have successfully shown how to\nincorporate the benefits of state-of-the-art neural information retrieval\nmodels into the classical inverted index data structure. Despite their\nimprovements in effectiveness, learned sparse models are not as efficient as\nclassical sparse model such as BM25. The problem has been investigated and\naddressed by recently developed strategies, such as guided traversal query\nprocessing and static pruning, with different degrees of success on in-domain\nand out-of-domain datasets. In this work, we propose a new query processing\nstrategy for SPLADE based on a two-step cascade. The first step uses a pruned\nand reweighted version of the SPLADE sparse vectors, and the second step uses\nthe original SPLADE vectors to re-score a sample of documents retrieved in the\nfirst stage. Our extensive experiments, performed on 30 different in-domain and\nout-of-domain datasets, show that our proposed strategy is able to improve mean\nand tail response times over the original single-stage SPLADE processing by up\nto $30\\times$ and $40\\times$, respectively, for in-domain datasets, and by 12x\nto 25x, for mean response on out-of-domain datasets, while not incurring in\nstatistical significant difference in 60\\% of datasets.",
        "translated": ""
    },
    {
        "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
        "url": "http://arxiv.org/abs/2404.15269v1",
        "pub_date": "2024-04-23",
        "summary": "We study interactive learning of language agents based on user edits made to\nthe agent's output. In a typical setting such as writing assistants, the user\ninteracts with a language agent to generate a response given a context, and may\noptionally edit the agent response to personalize it based on their latent\npreference, in addition to improving the correctness. The edit feedback is\nnaturally generated, making it a suitable candidate for improving the agent's\nalignment with the user's preference, and for reducing the cost of user edits\nover time. We propose a learning framework, PRELUDE that infers a description\nof the user's latent preference based on historic edit data and using it to\ndefine a prompt policy that drives future response generation. This avoids\nfine-tuning the agent, which is costly, challenging to scale with the number of\nusers, and may even degrade its performance on other tasks. Furthermore,\nlearning descriptive preference improves interpretability, allowing the user to\nview and modify the learned preference. However, user preference can be complex\nand vary based on context, making it challenging to learn. To address this, we\npropose a simple yet effective algorithm named CIPHER that leverages a large\nlanguage model (LLM) to infer the user preference for a given context based on\nuser edits. In the future, CIPHER retrieves inferred preferences from the\nk-closest contexts in the history, and forms an aggregate preference for\nresponse generation. We introduce two interactive environments -- summarization\nand email writing, for evaluation using a GPT-4 simulated user. We compare with\nalgorithms that directly retrieve user edits but do not learn descriptive\npreference, and algorithms that learn context-agnostic preference. On both\ntasks, CIPHER achieves the lowest edit distance cost and learns preferences\nthat show significant similarity to the ground truth preferences",
        "translated": ""
    },
    {
        "title": "A Short Review for Ontology Learning from Text: Stride from Shallow\n  Learning, Deep Learning to Large Language Models Trend",
        "url": "http://arxiv.org/abs/2404.14991v1",
        "pub_date": "2024-04-23",
        "summary": "Ontologies provide formal representation of knowledge shared within Semantic\nWeb applications and Ontology learning from text involves the construction of\nontologies from a given corpus of text. In the past years, ontology learning\nhas traversed through shallow learning and deep learning methodologies, each\noffering distinct advantages and limitations in the quest for knowledge\nextraction and representation. A new trend of these approaches is relying on\nlarge language models to enhance ontology learning. This paper gives a review\nin approaches and challenges of ontology learning. It analyzes the\nmethodologies and limitations of shallow-learning-based and deep-learning-based\ntechniques for ontology learning, and provides comprehensive knowledge for the\nfrontier work of using large language models to enhance ontology learning. In\naddition, it proposes several noteworthy future directions for further\nexploration into the integration of large language models with ontology\nlearning tasks.",
        "translated": ""
    },
    {
        "title": "A Reproducibility Study of PLAID",
        "url": "http://arxiv.org/abs/2404.14989v1",
        "pub_date": "2024-04-23",
        "summary": "The PLAID (Performance-optimized Late Interaction Driver) algorithm for\nColBERTv2 uses clustered term representations to retrieve and progressively\nprune documents for final (exact) document scoring. In this paper, we reproduce\nand fill in missing gaps from the original work. By studying the parameters\nPLAID introduces, we find that its Pareto frontier is formed of a careful\nbalance among its three parameters; deviations beyond the suggested settings\ncan substantially increase latency without necessarily improving its\neffectiveness. We then compare PLAID with an important baseline missing from\nthe paper: re-ranking a lexical system. We find that applying ColBERTv2 as a\nre-ranker atop an initial pool of BM25 results provides better\nefficiency-effectiveness trade-offs in low-latency settings. However,\nre-ranking cannot reach peak effectiveness at higher latency settings due to\nlimitations in recall of lexical matching and provides a poor approximation of\nan exhaustive ColBERTv2 search. We find that recently proposed modifications to\nre-ranking that pull in the neighbors of top-scoring documents overcome this\nlimitation, providing a Pareto frontier across all operational points for\nColBERTv2 when evaluated using a well-annotated dataset. Curious about why\nre-ranking methods are highly competitive with PLAID, we analyze the token\nrepresentation clusters PLAID uses for retrieval and find that most clusters\nare predominantly aligned with a single token and vice versa. Given the\ncompetitive trade-offs that re-ranking baselines exhibit, this work highlights\nthe importance of carefully selecting pertinent baselines when evaluating the\nefficiency of retrieval engines.",
        "translated": ""
    },
    {
        "title": "Manipulating Recommender Systems: A Survey of Poisoning Attacks and\n  Countermeasures",
        "url": "http://arxiv.org/abs/2404.14942v1",
        "pub_date": "2024-04-23",
        "summary": "Recommender systems have become an integral part of online services to help\nusers locate specific information in a sea of data. However, existing studies\nshow that some recommender systems are vulnerable to poisoning attacks,\nparticularly those that involve learning schemes. A poisoning attack is where\nan adversary injects carefully crafted data into the process of training a\nmodel, with the goal of manipulating the system's final recommendations. Based\non recent advancements in artificial intelligence, such attacks have gained\nimportance recently. While numerous countermeasures to poisoning attacks have\nbeen developed, they have not yet been systematically linked to the properties\nof the attacks. Consequently, assessing the respective risks and potential\nsuccess of mitigation strategies is difficult, if not impossible. This survey\naims to fill this gap by primarily focusing on poisoning attacks and their\ncountermeasures. This is in contrast to prior surveys that mainly focus on\nattacks and their detection methods. Through an exhaustive literature review,\nwe provide a novel taxonomy for poisoning attacks, formalise its dimensions,\nand accordingly organise 30+ attacks described in the literature. Further, we\nreview 40+ countermeasures to detect and/or prevent poisoning attacks,\nevaluating their effectiveness against specific types of attacks. This\ncomprehensive survey should serve as a point of reference for protecting\nrecommender systems against poisoning attacks. The article concludes with a\ndiscussion on open issues in the field and impactful directions for future\nresearch. A rich repository of resources associated with poisoning attacks is\navailable at https://github.com/tamlhp/awesome-recsys-poisoning.",
        "translated": ""
    },
    {
        "title": "Multi-Sample Dynamic Time Warping for Few-Shot Keyword Spotting",
        "url": "http://arxiv.org/abs/2404.14903v1",
        "pub_date": "2024-04-23",
        "summary": "In multi-sample keyword spotting, each keyword class is represented by\nmultiple spoken instances, called samples. A na\\\"ive approach to detect\nkeywords in a target sequence consists of querying all samples of all classes\nusing sub-sequence dynamic time warping. However, the resulting processing time\nincreases linearly with respect to the number of samples belonging to each\nclass. Alternatively, only a single Fr\\'echet mean can be queried for each\nclass, resulting in reduced processing time but usually also in worse detection\nperformance as the variability of the query samples is not captured\nsufficiently well. In this work, multi-sample dynamic time warping is proposed\nto compute class-specific cost-tensors that include the variability of all\nquery samples. To significantly reduce the computational complexity during\ninference, these cost tensors are converted to cost matrices before applying\ndynamic time warping. In experimental evaluations for few-shot keyword\nspotting, it is shown that this method yields a very similar performance as\nusing all individual query samples as templates while having a runtime that is\nonly slightly slower than when using Fr\\'echet means.",
        "translated": ""
    },
    {
        "title": "Cross-Domain Causal Preference Learning for Out-of-Distribution\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.14856v1",
        "pub_date": "2024-04-23",
        "summary": "Recommender systems use users' historical interactions to learn their\npreferences and deliver personalized recommendations from a vast array of\ncandidate items. Current recommender systems primarily rely on the assumption\nthat the training and testing datasets have identical distributions, which may\nnot hold true in reality. In fact, the distribution shift between training and\ntesting datasets often occurs as a result of the evolution of user attributes,\nwhich degrades the performance of the conventional recommender systems because\nthey fail in Out-of-Distribution (OOD) generalization, particularly in\nsituations of data sparsity. This study delves deeply into the challenge of OOD\ngeneralization and proposes a novel model called Cross-Domain Causal Preference\nLearning for Out-of-Distribution Recommendation (CDCOR), which involves\nemploying a domain adversarial network to uncover users' domain-shared\npreferences and utilizing a causal structure learner to capture causal\ninvariance to deal with the OOD problem. Through extensive experiments on two\nreal-world datasets, we validate the remarkable performance of our model in\nhandling diverse scenarios of data sparsity and out-of-distribution\nenvironments. Furthermore, our approach surpasses the benchmark models,\nshowcasing outstanding capabilities in out-of-distribution generalization.",
        "translated": ""
    },
    {
        "title": "From Matching to Generation: A Survey on Generative Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2404.14851v1",
        "pub_date": "2024-04-23",
        "summary": "Information Retrieval (IR) systems are crucial tools for users to access\ninformation, widely applied in scenarios like search engines, question\nanswering, and recommendation systems. Traditional IR methods, based on\nsimilarity matching to return ranked lists of documents, have been reliable\nmeans of information acquisition, dominating the IR field for years. With the\nadvancement of pre-trained language models, generative information retrieval\n(GenIR) has emerged as a novel paradigm, gaining increasing attention in recent\nyears. Currently, research in GenIR can be categorized into two aspects:\ngenerative document retrieval (GR) and reliable response generation. GR\nleverages the generative model's parameters for memorizing documents, enabling\nretrieval by directly generating relevant document identifiers without explicit\nindexing. Reliable response generation, on the other hand, employs language\nmodels to directly generate the information users seek, breaking the\nlimitations of traditional IR in terms of document granularity and relevance\nmatching, offering more flexibility, efficiency, and creativity, thus better\nmeeting practical needs. This paper aims to systematically review the latest\nresearch progress in GenIR. We will summarize the advancements in GR regarding\nmodel training, document identifier, incremental learning, downstream tasks\nadaptation, multi-modal GR and generative recommendation, as well as progress\nin reliable response generation in aspects of internal knowledge memorization,\nexternal knowledge augmentation, generating response with citations and\npersonal information assistant. We also review the evaluation, challenges and\nfuture prospects in GenIR systems. This review aims to offer a comprehensive\nreference for researchers in the GenIR field, encouraging further development\nin this area.",
        "translated": ""
    },
    {
        "title": "Towards Universal Dense Blocking for Entity Resolution",
        "url": "http://arxiv.org/abs/2404.14831v1",
        "pub_date": "2024-04-23",
        "summary": "Blocking is a critical step in entity resolution, and the emergence of neural\nnetwork-based representation models has led to the development of dense\nblocking as a promising approach for exploring deep semantics in blocking.\nHowever, previous advanced self-supervised dense blocking approaches require\ndomain-specific training on the target domain, which limits the benefits and\nrapid adaptation of these methods. To address this issue, we propose UBlocker,\na dense blocker that is pre-trained on a domain-independent, easily-obtainable\ntabular corpus using self-supervised contrastive learning. By conducting\ndomain-independent pre-training, UBlocker can be adapted to various downstream\nblocking scenarios without requiring domain-specific fine-tuning. To evaluate\nthe universality of our entity blocker, we also construct a new benchmark\ncovering a wide range of blocking tasks from multiple domains and scenarios.\nOur experiments show that the proposed UBlocker, without any domain-specific\nlearning, significantly outperforms previous self- and unsupervised dense\nblocking methods and is comparable and complementary to the state-of-the-art\nsparse blocking methods.",
        "translated": ""
    },
    {
        "title": "Contrastive Quantization based Semantic Code for Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2404.14774v1",
        "pub_date": "2024-04-23",
        "summary": "With the success of large language models, generative retrieval has emerged\nas a new retrieval technique for recommendation. It can be divided into two\nstages: the first stage involves constructing discrete Codes (i.e., codes), and\nthe second stage involves decoding the code sequentially via the transformer\narchitecture. Current methods often construct item semantic codes by\nreconstructing based quantization on item textual representation, but they fail\nto capture item discrepancy that is essential in modeling item relationships in\nrecommendation sytems. In this paper, we propose to construct the code\nrepresentation of items by simultaneously considering both item relationships\nand semantic information. Specifically, we employ a pre-trained language model\nto extract item's textual description and translate it into item's embedding.\nThen we propose to enhance the encoder-decoder based RQVAE model with\ncontrastive objectives to learn item code. To be specific, we employ the\nembeddings generated by the decoder from the samples themselves as positive\ninstances and those from other samples as negative instances. Thus we\neffectively enhance the item discrepancy across all items, better preserving\nthe item neighbourhood. Finally, we train and test semantic code with with\ngenerative retrieval on a sequential recommendation model. Our experiments\ndemonstrate that our method improves NDCG@5 with 43.76% on the MIND dataset and\nRecall@10 with 80.95% on the Office dataset compared to the previous baselines.",
        "translated": ""
    },
    {
        "title": "Retrieval Augmented Generation for Domain-specific Question Answering",
        "url": "http://arxiv.org/abs/2404.14760v1",
        "pub_date": "2024-04-23",
        "summary": "Question answering (QA) has become an important application in the advanced\ndevelopment of large language models. General pre-trained large language models\nfor question-answering are not trained to properly understand the knowledge or\nterminology for a specific domain, such as finance, healthcare, education, and\ncustomer service for a product. To better cater to domain-specific\nunderstanding, we build an in-house question-answering system for Adobe\nproducts. We propose a novel framework to compile a large question-answer\ndatabase and develop the approach for retrieval-aware finetuning of a Large\nLanguage model. We showcase that fine-tuning the retriever leads to major\nimprovements in the final generation. Our overall approach reduces\nhallucinations during generation while keeping in context the latest retrieval\ninformation for contextual grounding.",
        "translated": ""
    },
    {
        "title": "ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for\n  Implicit Attribute Value Extraction",
        "url": "http://arxiv.org/abs/2404.15592v1",
        "pub_date": "2024-04-24",
        "summary": "Existing datasets for attribute value extraction (AVE) predominantly focus on\nexplicit attribute values while neglecting the implicit ones, lack product\nimages, are often not publicly available, and lack an in-depth human inspection\nacross diverse domains. To address these limitations, we present ImplicitAVE,\nthe first, publicly available multimodal dataset for implicit attribute value\nextraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curated\nand expanded to include implicit AVE and multimodality, resulting in a refined\ndataset of 68k training and 1.6k testing data across five domains. We also\nexplore the application of multimodal large language models (MLLMs) to implicit\nAVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE\ndataset. Six recent MLLMs with eleven variants are evaluated across diverse\nsettings, revealing that implicit value extraction remains a challenging task\nfor MLLMs. The contributions of this work include the development and release\nof ImplicitAVE, and the exploration and benchmarking of various MLLMs for\nimplicit AVE, providing valuable insights and potential future research\ndirections. Dataset and code are available at\nhttps://github.com/HenryPengZou/ImplicitAVE",
        "translated": ""
    },
    {
        "title": "An Annotated Glossary for Data Commons, Data Meshes, and Other Data\n  Platforms",
        "url": "http://arxiv.org/abs/2404.15475v1",
        "pub_date": "2024-04-23",
        "summary": "Cloud-based data commons, data meshes, data hubs, and other data platforms\nare important ways to manage, analyze and share data to accelerate research and\nto support reproducible research. This is an annotated glossary of some of the\nmore common terms used in articles and discussions about these platforms.",
        "translated": ""
    },
    {
        "title": "MMGRec: Multimodal Generative Recommendation with Transformer Model",
        "url": "http://arxiv.org/abs/2404.16555v1",
        "pub_date": "2024-04-25",
        "summary": "Multimodal recommendation aims to recommend user-preferred candidates based\non her/his historically interacted items and associated multimodal information.\nPrevious studies commonly employ an embed-and-retrieve paradigm: learning user\nand item representations in the same embedding space, then retrieving similar\ncandidate items for a user via embedding inner product. However, this paradigm\nsuffers from inference cost, interaction modeling, and false-negative issues.\nToward this end, we propose a new MMGRec model to introduce a generative\nparadigm into multimodal recommendation. Specifically, we first devise a\nhierarchical quantization method Graph RQ-VAE to assign Rec-ID for each item\nfrom its multimodal and CF information. Consisting of a tuple of semantically\nmeaningful tokens, Rec-ID serves as the unique identifier of each item.\nAfterward, we train a Transformer-based recommender to generate the Rec-IDs of\nuser-preferred items based on historical interaction sequences. The generative\nparadigm is qualified since this model systematically predicts the tuple of\ntokens identifying the recommended item in an autoregressive manner. Moreover,\na relation-aware self-attention mechanism is devised for the Transformer to\nhandle non-sequential interaction sequences, which explores the element\npairwise relation to replace absolute positional encoding. Extensive\nexperiments evaluate MMGRec's effectiveness compared with state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "RE-RecSys: An End-to-End system for recommending properties in\n  Real-Estate domain",
        "url": "http://arxiv.org/abs/2404.16553v1",
        "pub_date": "2024-04-25",
        "summary": "We propose an end-to-end real-estate recommendation system, RE-RecSys, which\nhas been productionized in real-world industry setting. We categorize any user\ninto 4 categories based on available historical data: i) cold-start users; ii)\nshort-term users; iii) long-term users; and iv) short-long term users. For\ncold-start users, we propose a novel rule-based engine that is based on the\npopularity of locality and user preferences. For short-term users, we propose\nto use content-filtering model which recommends properties based on recent\ninteractions of users. For long-term and short-long term users, we propose a\nnovel combination of content and collaborative filtering based approach which\ncan be easily productionized in the real-world scenario. Moreover, based on the\nconversion rate, we have designed a novel weighing scheme for different\nimpressions done by users on the platform for the training of content and\ncollaborative models. Finally, we show the efficiency of the proposed pipeline,\nRE-RecSys, on a real-world property and clickstream dataset collected from\nleading real-estate platform in India. We show that the proposed pipeline is\ndeployable in real-world scenario with an average latency of &lt;40 ms serving\n1000 rpm.",
        "translated": ""
    },
    {
        "title": "OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search",
        "url": "http://arxiv.org/abs/2404.16260v1",
        "pub_date": "2024-04-25",
        "summary": "In this paper, we present OmniSearchSage, a versatile and scalable system for\nunderstanding search queries, pins, and products for Pinterest search. We\njointly learn a unified query embedding coupled with pin and product\nembeddings, leading to an improvement of $&gt;8\\%$ relevance, $&gt;7\\%$ engagement,\nand $&gt;5\\%$ ads CTR in Pinterest's production search system. The main\ncontributors to these gains are improved content understanding, better\nmulti-task learning, and real-time serving. We enrich our entity\nrepresentations using diverse text derived from image captions from a\ngenerative LLM, historical engagement, and user-curated boards. Our multitask\nlearning setup produces a single search query embedding in the same space as\npin and product embeddings and compatible with pre-existing pin and product\nembeddings. We show the value of each feature through ablation studies, and\nshow the effectiveness of a unified model compared to standalone counterparts.\nFinally, we share how these embeddings have been deployed across the Pinterest\nsearch stack, from retrieval to ranking, scaling to serve $300k$ requests per\nsecond at low latency. Our implementation of this work is available at\nhttps://github.com/pinterest/atg-research/tree/main/omnisearchsage.",
        "translated": ""
    },
    {
        "title": "Advancing Recommender Systems by mitigating Shilling attacks",
        "url": "http://arxiv.org/abs/2404.16177v1",
        "pub_date": "2024-04-24",
        "summary": "Considering the premise that the number of products offered grow in an\nexponential fashion and the amount of data that a user can assimilate before\nmaking a decision is relatively small, recommender systems help in categorizing\ncontent according to user preferences. Collaborative filtering is a widely used\nmethod for computing recommendations due to its good performance. But, this\nmethod makes the system vulnerable to attacks which try to bias the\nrecommendations. These attacks, known as 'shilling attacks' are performed to\npush an item or nuke an item in the system. This paper proposes an algorithm to\ndetect such shilling profiles in the system accurately and also study the\neffects of such profiles on the recommendations.",
        "translated": ""
    },
    {
        "title": "From Local to Global: A Graph RAG Approach to Query-Focused\n  Summarization",
        "url": "http://arxiv.org/abs/2404.16130v1",
        "pub_date": "2024-04-24",
        "summary": "The use of retrieval-augmented generation (RAG) to retrieve relevant\ninformation from an external knowledge source enables large language models\n(LLMs) to answer questions over private and/or previously unseen document\ncollections. However, RAG fails on global questions directed at an entire text\ncorpus, such as \"What are the main themes in the dataset?\", since this is\ninherently a query-focused summarization (QFS) task, rather than an explicit\nretrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities\nof text indexed by typical RAG systems. To combine the strengths of these\ncontrasting methods, we propose a Graph RAG approach to question answering over\nprivate text corpora that scales with both the generality of user questions and\nthe quantity of source text to be indexed. Our approach uses an LLM to build a\ngraph-based text index in two stages: first to derive an entity knowledge graph\nfrom the source documents, then to pregenerate community summaries for all\ngroups of closely-related entities. Given a question, each community summary is\nused to generate a partial response, before all partial responses are again\nsummarized in a final response to the user. For a class of global sensemaking\nquestions over datasets in the 1 million token range, we show that Graph RAG\nleads to substantial improvements over a na\\\"ive RAG baseline for both the\ncomprehensiveness and diversity of generated answers. An open-source,\nPython-based implementation of both global and local Graph RAG approaches is\nforthcoming at https://aka.ms/graphrag.",
        "translated": ""
    },
    {
        "title": "Mixed Supervised Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2404.15954v1",
        "pub_date": "2024-04-24",
        "summary": "Recommender systems (RecSys) play a vital role in online platforms, offering\nusers personalized suggestions amidst vast information. Graph contrastive\nlearning aims to learn from high-order collaborative filtering signals with\nunsupervised augmentation on the user-item bipartite graph, which predominantly\nrelies on the multi-task learning framework involving both the pair-wise\nrecommendation loss and the contrastive loss. This decoupled design can cause\ninconsistent optimization direction from different losses, which leads to\nlonger convergence time and even sub-optimal performance. Besides, the\nself-supervised contrastive loss falls short in alleviating the data sparsity\nissue in RecSys as it learns to differentiate users/items from different views\nwithout providing extra supervised collaborative filtering signals during\naugmentations. In this paper, we propose Mixed Supervised Graph Contrastive\nLearning for Recommendation (MixSGCL) to address these concerns. MixSGCL\noriginally integrates the training of recommendation and unsupervised\ncontrastive losses into a supervised contrastive learning loss to align the two\ntasks within one optimization direction. To cope with the data sparsity issue,\ninstead unsupervised augmentation, we further propose node-wise and edge-wise\nmixup to mine more direct supervised collaborative filtering signals based on\nexisting user-item interactions. Extensive experiments on three real-world\ndatasets demonstrate that MixSGCL surpasses state-of-the-art methods, achieving\ntop performance on both accuracy and efficiency. It validates the effectiveness\nof MixSGCL with our coupled design on supervised graph contrastive learning.",
        "translated": ""
    },
    {
        "title": "Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language\n  Models for Telecommunications",
        "url": "http://arxiv.org/abs/2404.15939v1",
        "pub_date": "2024-04-24",
        "summary": "The application of Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG) systems in the telecommunication domain presents unique\nchallenges, primarily due to the complex nature of telecom standard documents\nand the rapid evolution of the field. The paper introduces and open-sources\nTelco-RAG, a customized RAG framework designed to handle the specific needs of\ntelecommunications standards, particularly 3rd Generation Partnership Project\n(3GPP) documents. Telco-RAG addresses the critical challenges of implementing a\nRAG pipeline on highly technical content, paving the way for applying LLMs in\ntelecommunications and offering guidelines for RAG implementation in other\ntechnical domains.",
        "translated": ""
    },
    {
        "title": "Introducing EEG Analyses to Help Personal Music Preference Prediction",
        "url": "http://arxiv.org/abs/2404.15753v1",
        "pub_date": "2024-04-24",
        "summary": "Nowadays, personalized recommender systems play an increasingly important\nrole in music scenarios in our daily life with the preference prediction\nability. However, existing methods mainly rely on users' implicit feedback\n(e.g., click, dwell time) which ignores the detailed user experience. This\npaper introduces Electroencephalography (EEG) signals to personal music\npreferences as a basis for the personalized recommender system. To realize\ncollection in daily life, we use a dry-electrodes portable device to collect\ndata. We perform a user study where participants listen to music and record\npreferences and moods. Meanwhile, EEG signals are collected with a portable\ndevice. Analysis of the collected data indicates a significant relationship\nbetween music preference, mood, and EEG signals. Furthermore, we conduct\nexperiments to predict personalized music preference with the features of EEG\nsignals. Experiments show significant improvement in rating prediction and\npreference classification with the help of EEG. Our work demonstrates the\npossibility of introducing EEG signals in personal music preference with\nportable devices. Moreover, our approach is not restricted to the music\nscenario, and the EEG signals as explicit feedback can be used in personalized\nrecommendation tasks.",
        "translated": ""
    },
    {
        "title": "Retrieval and Distill: A Temporal Data Shift-Free Paradigm for Online\n  Recommendation System",
        "url": "http://arxiv.org/abs/2404.15678v2",
        "pub_date": "2024-04-24",
        "summary": "Current recommendation systems are significantly affected by a serious issue\nof temporal data shift, which is the inconsistency between the distribution of\nhistorical data and that of online data. Most existing models focus on\nutilizing updated data, overlooking the transferable, temporal data shift-free\ninformation that can be learned from shifting data. We propose the Temporal\nInvariance of Association theorem, which suggests that given a fixed search\nspace, the relationship between the data and the data in the search space keeps\ninvariant over time. Leveraging this principle, we designed a retrieval-based\nrecommendation system framework that can train a data shift-free relevance\nnetwork using shifting data, significantly enhancing the predictive performance\nof the original model in the recommendation system. However, retrieval-based\nrecommendation models face substantial inference time costs when deployed\nonline. To address this, we further designed a distill framework that can\ndistill information from the relevance network into a parameterized module\nusing shifting data. The distilled model can be deployed online alongside the\noriginal model, with only a minimal increase in inference time. Extensive\nexperiments on multiple real datasets demonstrate that our framework\nsignificantly improves the performance of the original model by utilizing\nshifting data.",
        "translated": ""
    },
    {
        "title": "Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce\n  Search",
        "url": "http://arxiv.org/abs/2404.15675v1",
        "pub_date": "2024-04-24",
        "summary": "Leveraging generative retrieval (GR) techniques to enhance search systems is\nan emerging methodology that has shown promising results in recent years. In\nGR, a text-to-text model maps string queries directly to relevant document\nidentifiers (docIDs), so it dramatically simplifies the whole retrieval\nprocess. However, when applying most GR models in large-scale E-commerce for\npersonalized item search, we have to face two key problems in encoding and\ndecoding. (1) Existing docID generation methods ignore the encoding of\nefficiency information, which is critical in E-commerce. (2) The positional\ninformation is important in decoding docIDs, while prior studies have not\nadequately discriminated the significance of positional information or well\nexploited the inherent interrelation among these positions. To overcome these\nproblems, we introduce an efficient Hierarchical encoding-decoding Generative\nretrieval method (Hi-Gen) for large-scale personalized E-commerce search\nsystems. Specifically, we first design a representation learning model along\nwith metric learning to learn discriminative feature representations of items\nto capture both semantic relevance and efficiency information. Then, we propose\na category-guided hierarchical clustering scheme that makes full use of the\nsemantic and efficiency information of items to facilitate docID generation.\nFinally, we design a position-aware loss to discriminate the importance of\npositions and mine the inherent interrelation between different tokens at the\nsame position. This loss boosts the performance of the language model used in\nthe decoding stage. Besides, we propose two variants of Hi-Gen (i.e.,Hi-Gen-I2I\nand Hi-Gen-Cluster) to support online real-time large-scale recall in the\nonline serving process. Extensive experiments on both public and industry\ndatasets demonstrate the effectiveness and efficiency of Hi-Gen.",
        "translated": ""
    },
    {
        "title": "Towards Group-aware Search Success",
        "url": "http://arxiv.org/abs/2404.17313v1",
        "pub_date": "2024-04-26",
        "summary": "Traditional measures of search success often overlook the varying information\nneeds of different demographic groups. To address this gap, we introduce a\nnovel metric, named Group-aware Search Success (GA-SS). GA-SS redefines search\nsuccess to ensure that all demographic groups achieve satisfaction from search\noutcomes. We introduce a comprehensive mathematical framework to calculate\nGA-SS, incorporating both static and stochastic ranking policies and\nintegrating user browsing models for a more accurate assessment. In addition,\nwe have proposed Group-aware Most Popular Completion (gMPC) ranking model to\naccount for demographic variances in user intent, aligning more closely with\nthe diverse needs of all user groups. We empirically validate our metric and\napproach with two real-world datasets: one focusing on query auto-completion\nand the other on movie recommendations, where the results highlight the impact\nof stochasticity and the complex interplay among various search success\nmetrics. Our findings advocate for a more inclusive approach in measuring\nsearch success, as well as inspiring future investigations into the quality of\nservice of search.",
        "translated": ""
    },
    {
        "title": "ExcluIR: Exclusionary Neural Information Retrieval",
        "url": "http://arxiv.org/abs/2404.17288v1",
        "pub_date": "2024-04-26",
        "summary": "Exclusion is an important and universal linguistic skill that humans use to\nexpress what they do not want. However, in information retrieval community,\nthere is little research on exclusionary retrieval, where users express what\nthey do not want in their queries. In this work, we investigate the scenario of\nexclusionary retrieval in document retrieval for the first time. We present\nExcluIR, a set of resources for exclusionary retrieval, consisting of an\nevaluation benchmark and a training set for helping retrieval models to\ncomprehend exclusionary queries. The evaluation benchmark includes 3,452\nhigh-quality exclusionary queries, each of which has been manually annotated.\nThe training set contains 70,293 exclusionary queries, each paired with a\npositive document and a negative document. We conduct detailed experiments and\nanalyses, obtaining three main observations: (1) Existing retrieval models with\ndifferent architectures struggle to effectively comprehend exclusionary\nqueries; (2) Although integrating our training data can improve the performance\nof retrieval models on exclusionary retrieval, there still exists a gap\ncompared to human performance; (3) Generative retrieval models have a natural\nadvantage in handling exclusionary queries. To facilitate future research on\nexclusionary retrieval, we share the benchmark and evaluation scripts on\n\\url{https://github.com/zwh-sdu/ExcluIR}.",
        "translated": ""
    },
    {
        "title": "TruthSR: Trustworthy Sequential Recommender Systems via User-generated\n  Multimodal Content",
        "url": "http://arxiv.org/abs/2404.17238v1",
        "pub_date": "2024-04-26",
        "summary": "Sequential recommender systems explore users' preferences and behavioral\npatterns from their historically generated data. Recently, researchers aim to\nimprove sequential recommendation by utilizing massive user-generated\nmulti-modal content, such as reviews, images, etc. This content often contains\ninevitable noise. Some studies attempt to reduce noise interference by\nsuppressing cross-modal inconsistent information. However, they could\npotentially constrain the capturing of personalized user preferences. In\naddition, it is almost impossible to entirely eliminate noise in diverse\nuser-generated multi-modal content. To solve these problems, we propose a\ntrustworthy sequential recommendation method via noisy user-generated\nmulti-modal content. Specifically, we explicitly capture the consistency and\ncomplementarity of user-generated multi-modal content to mitigate noise\ninterference. We also achieve the modeling of the user's multi-modal sequential\npreferences. In addition, we design a trustworthy decision mechanism that\nintegrates subjective user perspective and objective item perspective to\ndynamically evaluate the uncertainty of prediction results. Experimental\nevaluation on four widely-used datasets demonstrates the superior performance\nof our model compared to state-of-the-art methods. The code is released at\nhttps://github.com/FairyMeng/TrustSR.",
        "translated": ""
    },
    {
        "title": "RE-RFME: Real-Estate RFME Model for customer segmentation",
        "url": "http://arxiv.org/abs/2404.17177v1",
        "pub_date": "2024-04-26",
        "summary": "Marketing is one of the high-cost activities for any online platform. With\nthe increase in the number of customers, it is crucial to understand customers\nbased on their dynamic behaviors to design effective marketing strategies.\nCustomer segmentation is a widely used approach to group customers into\ndifferent categories and design the marketing strategy targeting each group\nindividually. Therefore, in this paper, we propose an end-to-end pipeline\nRE-RFME for segmenting customers into 4 groups: high value, promising, need\nattention, and need activation. Concretely, we propose a novel RFME (Recency,\nFrequency, Monetary and Engagement) model to track behavioral features of\ncustomers and segment them into different categories. Finally, we train the\nK-means clustering algorithm to cluster the user into one of the 4 categories.\nWe show the effectiveness of the proposed approach on real-world Housing.com\ndatasets for both website and mobile application users.",
        "translated": ""
    },
    {
        "title": "Rank-Preference Consistency as the Appropriate Metric for Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2404.17097v1",
        "pub_date": "2024-04-26",
        "summary": "In this paper we argue that conventional unitary-invariant measures of\nrecommender system (RS) performance based on measuring differences between\npredicted ratings and actual user ratings fail to assess fundamental RS\nproperties. More specifically, posing the optimization problem as one of\npredicting exact user ratings provides only an indirect suboptimal\napproximation for what RS applications typically need, which is an ability to\naccurately predict user preferences. We argue that scalar measures such as RMSE\nand MAE with respect to differences between actual and predicted ratings are\nonly proxies for measuring RS ability to accurately estimate user preferences.\nWe propose what we consider to be a measure that is more fundamentally\nappropriate for assessing RS performance, rank-preference consistency, which\nsimply counts the number of prediction pairs that are inconsistent with the\nuser's expressed product preferences. For example, if an RS predicts the user\nwill prefer product A over product B, but the user's withheld ratings indicate\ns/he prefers product B over A, then rank-preference consistency has been\nviolated. Our test results conclusively demonstrate that methods tailored to\noptimize arbitrary measures such as RMSE are not generally effective at\naccurately predicting user preferences. Thus, we conclude that conventional\nmethods used for assessing RS performance are arbitrary and misleading.",
        "translated": ""
    },
    {
        "title": "A Survey of Generative Search and Recommendation in the Era of Large\n  Language Models",
        "url": "http://arxiv.org/abs/2404.16924v1",
        "pub_date": "2024-04-25",
        "summary": "With the information explosion on the Web, search and recommendation are\nfoundational infrastructures to satisfying users' information needs. As the two\nsides of the same coin, both revolve around the same core research problem,\nmatching queries with documents or users with items. In the recent few decades,\nsearch and recommendation have experienced synchronous technological paradigm\nshifts, including machine learning-based and deep learning-based paradigms.\nRecently, the superintelligent generative large language models have sparked a\nnew paradigm in search and recommendation, i.e., generative search (retrieval)\nand recommendation, which aims to address the matching problem in a generative\nmanner. In this paper, we provide a comprehensive survey of the emerging\nparadigm in information systems and summarize the developments in generative\nsearch and recommendation from a unified perspective. Rather than simply\ncategorizing existing works, we abstract a unified framework for the generative\nparadigm and break down the existing works into different stages within this\nframework to highlight the strengths and weaknesses. And then, we distinguish\ngenerative search and recommendation with their unique challenges, identify\nopen problems and future directions, and envision the next information-seeking\nparadigm.",
        "translated": ""
    },
    {
        "title": "Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse\n  Representations",
        "url": "http://arxiv.org/abs/2404.18812v1",
        "pub_date": "2024-04-29",
        "summary": "Learned sparse representations form an attractive class of contextual\nembeddings for text retrieval. That is so because they are effective models of\nrelevance and are interpretable by design. Despite their apparent compatibility\nwith inverted indexes, however, retrieval over sparse embeddings remains\nchallenging. That is due to the distributional differences between learned\nembeddings and term frequency-based lexical models of relevance such as BM25.\nRecognizing this challenge, a great deal of research has gone into, among other\nthings, designing retrieval algorithms tailored to the properties of learned\nsparse representations, including approximate retrieval systems. In fact, this\ntask featured prominently in the latest BigANN Challenge at NeurIPS 2023, where\napproximate algorithms were evaluated on a large benchmark dataset by\nthroughput and recall. In this work, we propose a novel organization of the\ninverted index that enables fast yet effective approximate retrieval over\nlearned sparse embeddings. Our approach organizes inverted lists into\ngeometrically-cohesive blocks, each equipped with a summary vector. During\nquery processing, we quickly determine if a block must be evaluated using the\nsummaries. As we show experimentally, single-threaded query processing using\nour method, Seismic, reaches sub-millisecond per-query latency on various\nsparse embeddings of the MS MARCO dataset while maintaining high recall. Our\nresults indicate that Seismic is one to two orders of magnitude faster than\nstate-of-the-art inverted index-based solutions and further outperforms the\nwinning (graph-based) submissions to the BigANN Challenge by a significant\nmargin.",
        "translated": ""
    },
    {
        "title": "Efficiency-Effectiveness Tradeoff of Probabilistic Structured Queries\n  for Cross-Language Information Retrieval",
        "url": "http://arxiv.org/abs/2404.18797v1",
        "pub_date": "2024-04-29",
        "summary": "Probabilistic Structured Queries (PSQ) is a cross-language information\nretrieval (CLIR) method that uses translation probabilities statistically\nderived from aligned corpora. PSQ is a strong baseline for efficient CLIR using\nsparse indexing. It is, therefore, useful as the first stage in a cascaded\nneural CLIR system whose second stage is more effective but too inefficient to\nbe used on its own to search a large text collection. In this reproducibility\nstudy, we revisit PSQ by introducing an efficient Python implementation.\nUnconstrained use of all translation probabilities that can be estimated from\naligned parallel text would in the limit assign a weight to every vocabulary\nterm, precluding use of an inverted index to serve queries efficiently. Thus,\nPSQ's effectiveness and efficiency both depend on how translation probabilities\nare pruned. This paper presents experiments over a range of modern CLIR test\ncollections to demonstrate that achieving Pareto optimal PSQ\neffectiveness-efficiency tradeoffs benefits from multi-criteria pruning, which\nhas not been fully explored in prior work. Our Python PSQ implementation is\navailable on GitHub(https://github.com/hltcoe/PSQ) and unpruned translation\ntables are available on Huggingface\nModels(https://huggingface.co/hltcoe/psq_translation_tables).",
        "translated": ""
    },
    {
        "title": "Going Beyond Popularity and Positivity Bias: Correcting for\n  Multifactorial Bias in Recommender Systems",
        "url": "http://arxiv.org/abs/2404.18640v1",
        "pub_date": "2024-04-29",
        "summary": "Two typical forms of bias in user interaction data with recommender systems\n(RSs) are popularity bias and positivity bias, which manifest themselves as the\nover-representation of interactions with popular items or items that users\nprefer, respectively. Debiasing methods aim to mitigate the effect of selection\nbias on the evaluation and optimization of RSs. However, existing debiasing\nmethods only consider single-factor forms of bias, e.g., only the item\n(popularity) or only the rating value (positivity). This is in stark contrast\nwith the real world where user selections are generally affected by multiple\nfactors at once. In this work, we consider multifactorial selection bias in\nRSs. Our focus is on selection bias affected by both item and rating value\nfactors, which is a generalization and combination of popularity and positivity\nbias. While the concept of multifactorial bias is intuitive, it brings a severe\npractical challenge as it requires substantially more data for accurate bias\nestimation. As a solution, we propose smoothing and alternating gradient\ndescent techniques to reduce variance and improve the robustness of its\noptimization. Our experimental results reveal that, with our proposed\ntechniques, multifactorial bias corrections are more effective and robust than\nsingle-factor counterparts on real-world and synthetic datasets.",
        "translated": ""
    },
    {
        "title": "ir_explain: a Python Library of Explainable IR Methods",
        "url": "http://arxiv.org/abs/2404.18546v1",
        "pub_date": "2024-04-29",
        "summary": "While recent advancements in Neural Ranking Models have resulted in\nsignificant improvements over traditional statistical retrieval models, it is\ngenerally acknowledged that the use of large neural architectures and the\napplication of complex language models in Information Retrieval (IR) have\nreduced the transparency of retrieval methods. Consequently, Explainability and\nInterpretability have emerged as important research topics in IR. Several\naxiomatic and post-hoc explanation methods, as well as approaches that attempt\nto be interpretable-by-design, have been proposed. This article presents\n\\irexplain, an open-source Python library that implements a variety of\nwell-known techniques for Explainable IR (ExIR) within a common, extensible\nframework. \\irexplain supports the three standard categories of post-hoc\nexplanations, namely pointwise, pairwise, and listwise explanations. The\nlibrary is designed to make it easy to reproduce state-of-the-art ExIR\nbaselines on standard test collections, as well as to explore new approaches to\nexplaining IR models and methods. To facilitate adoption, \\irexplain is\nwell-integrated with widely-used toolkits such as Pyserini and \\irdatasets.",
        "translated": ""
    },
    {
        "title": "OAEI Machine Learning Dataset for Online Model Generation",
        "url": "http://arxiv.org/abs/2404.18542v1",
        "pub_date": "2024-04-29",
        "summary": "Ontology and knowledge graph matching systems are evaluated annually by the\nOntology Alignment Evaluation Initiative (OAEI). More and more systems use\nmachine learning-based approaches, including large language models. The\ntraining and validation datasets are usually determined by the system developer\nand often a subset of the reference alignments are used. This sampling is\nagainst the OAEI rules and makes a fair comparison impossible. Furthermore,\nthose models are trained offline (a trained and optimized model is packaged\ninto the matcher) and therefore the systems are specifically trained for those\ntasks. In this paper, we introduce a dataset that contains training,\nvalidation, and test sets for most of the OAEI tracks. Thus, online model\nlearning (the systems must adapt to the given input alignment without human\nintervention) is made possible to enable a fair comparison for ML-based\nsystems. We showcase the usefulness of the dataset by fine-tuning the\nconfidence thresholds of popular systems.",
        "translated": ""
    },
    {
        "title": "M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation\n  Framework",
        "url": "http://arxiv.org/abs/2404.18465v1",
        "pub_date": "2024-04-29",
        "summary": "Multi-domain recommendation and multi-task recommendation have demonstrated\ntheir effectiveness in leveraging common information from different domains and\nobjectives for comprehensive user modeling. Nonetheless, the practical\nrecommendation usually faces multiple domains and tasks simultaneously, which\ncannot be well-addressed by current methods. To this end, we introduce M3oE, an\nadaptive multi-domain multi-task mixture-of-experts recommendation framework.\nM3oE integrates multi-domain information, maps knowledge across domains and\ntasks, and optimizes multiple objectives. We leverage three mixture-of-experts\nmodules to learn common, domain-aspect, and task-aspect user preferences\nrespectively to address the complex dependencies among multiple domains and\ntasks in a disentangled manner. Additionally, we design a two-level fusion\nmechanism for precise control over feature extraction and fusion across diverse\ndomains and tasks. The framework's adaptability is further enhanced by applying\nAutoML technique, which allows dynamic structure optimization. To the best of\nthe authors' knowledge, our M3oE is the first effort to solve multi-domain\nmulti-task recommendation self-adaptively. Extensive experiments on two\nbenchmark datasets against diverse baselines demonstrate M3oE's superior\nperformance. The implementation code is available to ensure reproducibility.",
        "translated": ""
    },
    {
        "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text\n  Retrievers",
        "url": "http://arxiv.org/abs/2404.18443v1",
        "pub_date": "2024-04-29",
        "summary": "Developing effective biomedical retrieval models is important for excelling\nat knowledge-intensive biomedical tasks but still challenging due to the\ndeficiency of sufficient publicly annotated biomedical data and computational\nresources. We present BMRetriever, a series of dense retrievers for enhancing\nbiomedical retrieval via unsupervised pre-training on large biomedical corpora,\nfollowed by instruction fine-tuning on a combination of labeled datasets and\nsynthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify\nBMRetriever's efficacy on various biomedical applications. BMRetriever also\nexhibits strong parameter efficiency, with the 410M variant outperforming\nbaselines up to 11.7 times larger, and the 2B variant matching the performance\nof models with over 5B parameters. The training data and model checkpoints are\nreleased at \\url{https://huggingface.co/BMRetriever} to ensure transparency,\nreproducibility, and application to new domains.",
        "translated": ""
    },
    {
        "title": "PromptReps: Prompting Large Language Models to Generate Dense and Sparse\n  Representations for Zero-Shot Document Retrieval",
        "url": "http://arxiv.org/abs/2404.18424v1",
        "pub_date": "2024-04-29",
        "summary": "The current use of large language models (LLMs) for zero-shot document\nranking follows one of two ways: 1) prompt-based re-ranking methods, which\nrequire no further training but are feasible for only re-ranking a handful of\ncandidate documents due to the associated computational costs; and 2)\nunsupervised contrastive trained dense retrieval methods, which can retrieve\nrelevant documents from the entire corpus but require a large amount of paired\ntext data for contrastive training. In this paper, we propose PromptReps, which\ncombines the advantages of both categories: no need for training and the\nability to retrieve from the whole corpus. Our method only requires prompts to\nguide an LLM to generate query and document representations for effective\ndocument retrieval. Specifically, we prompt the LLMs to represent a given text\nusing a single word, and then use the last token's hidden states and the\ncorresponding logits associated to the prediction of the next token to\nconstruct a hybrid document retrieval system. The retrieval system harnesses\nboth dense text embedding and sparse bag-of-words representations given by the\nLLM. Our experimental evaluation on the BEIR zero-shot document retrieval\ndatasets illustrates that this simple prompt-based LLM retrieval method can\nachieve a similar or higher retrieval effectiveness than state-of-the-art LLM\nembedding methods that are trained with large amounts of unsupervised data,\nespecially when using a larger LLM.",
        "translated": ""
    },
    {
        "title": "User Welfare Optimization in Recommender Systems with Competing Content\n  Creators",
        "url": "http://arxiv.org/abs/2404.18319v1",
        "pub_date": "2024-04-28",
        "summary": "Driven by the new economic opportunities created by the creator economy, an\nincreasing number of content creators rely on and compete for revenue generated\nfrom online content recommendation platforms. This burgeoning competition\nreshapes the dynamics of content distribution and profoundly impacts long-term\nuser welfare on the platform. However, the absence of a comprehensive picture\nof global user preference distribution often traps the competition, especially\nthe creators, in states that yield sub-optimal user welfare. To encourage\ncreators to best serve a broad user population with relevant content, it\nbecomes the platform's responsibility to leverage its information advantage\nregarding user preference distribution to accurately signal creators.\n  In this study, we perform system-side user welfare optimization under a\ncompetitive game setting among content creators. We propose an algorithmic\nsolution for the platform, which dynamically computes a sequence of weights for\neach user based on their satisfaction of the recommended content. These weights\nare then utilized to design mechanisms that adjust the recommendation policy or\nthe post-recommendation rewards, thereby influencing creators' content\nproduction strategies. To validate the effectiveness of our proposed method, we\nreport our findings from a series of experiments, including: 1. a\nproof-of-concept negative example illustrating how creators' strategies\nconverge towards sub-optimal states without platform intervention; 2. offline\nexperiments employing our proposed intervention mechanisms on diverse datasets;\nand 3. results from a three-week online experiment conducted on a leading\nshort-video recommendation platform.",
        "translated": ""
    },
    {
        "title": "Retrieval-Oriented Knowledge for Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2404.18304v1",
        "pub_date": "2024-04-28",
        "summary": "Click-through rate (CTR) prediction plays an important role in personalized\nrecommendations. Recently, sample-level retrieval-based models (e.g., RIM) have\nachieved remarkable performance by retrieving and aggregating relevant samples.\nHowever, their inefficiency at the inference stage makes them impractical for\nindustrial applications. To overcome this issue, this paper proposes a\nuniversal plug-and-play Retrieval-Oriented Knowledge (ROK) framework.\nSpecifically, a knowledge base, consisting of a retrieval-oriented embedding\nlayer and a knowledge encoder, is designed to preserve and imitate the\nretrieved &amp; aggregated representations in a decomposition-reconstruction\nparadigm. Knowledge distillation and contrastive learning methods are utilized\nto optimize the knowledge base, and the learned retrieval-enhanced\nrepresentations can be integrated with arbitrary CTR models in both\ninstance-wise and feature-wise manners. Extensive experiments on three\nlarge-scale datasets show that ROK achieves competitive performance with the\nretrieval-based CTR models while reserving superior inference efficiency and\nmodel compatibility.",
        "translated": ""
    },
    {
        "title": "When to Retrieve: Teaching LLMs to Utilize Information Retrieval\n  Effectively",
        "url": "http://arxiv.org/abs/2404.19705v1",
        "pub_date": "2024-04-30",
        "summary": "In this paper, we demonstrate how Large Language Models (LLMs) can\neffectively learn to use an off-the-shelf information retrieval (IR) system\nspecifically when additional context is required to answer a given question.\nGiven the performance of IR systems, the optimal strategy for question\nanswering does not always entail external information retrieval; rather, it\noften involves leveraging the parametric memory of the LLM itself. Prior\nresearch has identified this phenomenon in the PopQA dataset, wherein the most\npopular questions are effectively addressed using the LLM's parametric memory,\nwhile less popular ones require IR system usage. Following this, we propose a\ntailored training approach for LLMs, leveraging existing open-domain question\nanswering datasets. Here, LLMs are trained to generate a special token, &lt;RET&gt;,\nwhen they do not know the answer to a question. Our evaluation of the Adaptive\nRetrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over the\nsame LLM under three configurations: (i) retrieving information for all the\nquestions, (ii) using always the parametric memory of the LLM, and (iii) using\na popularity threshold to decide when to use a retriever. Through our analysis,\nwe demonstrate that Adapt-LLM is able to generate the &lt;RET&gt; token when it\ndetermines that it does not know how to answer a question, indicating the need\nfor IR, while it achieves notably high accuracy levels when it chooses to rely\nonly on its parametric memory.",
        "translated": ""
    },
    {
        "title": "Be Aware of the Neighborhood Effect: Modeling Selection Bias under\n  Interference",
        "url": "http://arxiv.org/abs/2404.19620v1",
        "pub_date": "2024-04-30",
        "summary": "Selection bias in recommender system arises from the recommendation process\nof system filtering and the interactive process of user selection. Many\nprevious studies have focused on addressing selection bias to achieve unbiased\nlearning of the prediction model, but ignore the fact that potential outcomes\nfor a given user-item pair may vary with the treatments assigned to other\nuser-item pairs, named neighborhood effect. To fill the gap, this paper\nformally formulates the neighborhood effect as an interference problem from the\nperspective of causal inference and introduces a treatment representation to\ncapture the neighborhood effect. On this basis, we propose a novel ideal loss\nthat can be used to deal with selection bias in the presence of neighborhood\neffect. We further develop two new estimators for estimating the proposed ideal\nloss. We theoretically establish the connection between the proposed and\nprevious debiasing methods ignoring the neighborhood effect, showing that the\nproposed methods can achieve unbiased learning when both selection bias and\nneighborhood effect are present, while the existing methods are biased.\nExtensive semi-synthetic and real-world experiments are conducted to\ndemonstrate the effectiveness of the proposed methods.",
        "translated": ""
    },
    {
        "title": "Debiased Collaborative Filtering with Kernel-Based Causal Balancing",
        "url": "http://arxiv.org/abs/2404.19596v1",
        "pub_date": "2024-04-30",
        "summary": "Debiased collaborative filtering aims to learn an unbiased prediction model\nby removing different biases in observational datasets. To solve this problem,\none of the simple and effective methods is based on the propensity score, which\nadjusts the observational sample distribution to the target one by reweighting\nobserved instances. Ideally, propensity scores should be learned with causal\nbalancing constraints. However, existing methods usually ignore such\nconstraints or implement them with unreasonable approximations, which may\naffect the accuracy of the learned propensity scores. To bridge this gap, in\nthis paper, we first analyze the gaps between the causal balancing requirements\nand existing methods such as learning the propensity with cross-entropy loss or\nmanually selecting functions to balance. Inspired by these gaps, we propose to\napproximate the balancing functions in reproducing kernel Hilbert space and\ndemonstrate that, based on the universal property and representer theorem of\nkernel functions, the causal balancing constraints can be better satisfied.\nMeanwhile, we propose an algorithm that adaptively balances the kernel function\nand theoretically analyze the generalization error bound of our methods. We\nconduct extensive experiments to demonstrate the effectiveness of our methods,\nand to promote this research direction, we have released our project at\nhttps://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.",
        "translated": ""
    },
    {
        "title": "Large Language Model Informed Patent Image Retrieval",
        "url": "http://arxiv.org/abs/2404.19360v1",
        "pub_date": "2024-04-30",
        "summary": "In patent prosecution, image-based retrieval systems for identifying\nsimilarities between current patent images and prior art are pivotal to ensure\nthe novelty and non-obviousness of patent applications. Despite their growing\npopularity in recent years, existing attempts, while effective at recognizing\nimages within the same patent, fail to deliver practical value due to their\nlimited generalizability in retrieving relevant prior art. Moreover, this task\ninherently involves the challenges posed by the abstract visual features of\npatent images, the skewed distribution of image classifications, and the\nsemantic information of image descriptions. Therefore, we propose a\nlanguage-informed, distribution-aware multimodal approach to patent image\nfeature learning, which enriches the semantic understanding of patent image by\nintegrating Large Language Models and improves the performance of\nunderrepresented classes with our proposed distribution-aware contrastive\nlosses. Extensive experiments on DeepPatent2 dataset show that our proposed\nmethod achieves state-of-the-art or comparable performance in image-based\npatent retrieval with mAP +53.3%, Recall@10 +41.8%, and MRR@10 +51.9%.\nFurthermore, through an in-depth user analysis, we explore our model in aiding\npatent professionals in their image retrieval efforts, highlighting the model's\nreal-world applicability and effectiveness.",
        "translated": ""
    },
    {
        "title": "Interest Clock: Time Perception in Real-Time Streaming Recommendation\n  System",
        "url": "http://arxiv.org/abs/2404.19357v1",
        "pub_date": "2024-04-30",
        "summary": "User preferences follow a dynamic pattern over a day, e.g., at 8 am, a user\nmight prefer to read news, while at 8 pm, they might prefer to watch movies.\nTime modeling aims to enable recommendation systems to perceive time changes to\ncapture users' dynamic preferences over time, which is an important and\nchallenging problem in recommendation systems. Especially, streaming\nrecommendation systems in the industry, with only available samples of the\ncurrent moment, present greater challenges for time modeling. There is still a\nlack of effective time modeling methods for streaming recommendation systems.\nIn this paper, we propose an effective and universal method Interest Clock to\nperceive time information in recommendation systems. Interest Clock first\nencodes users' time-aware preferences into a clock (hour-level personalized\nfeatures) and then uses Gaussian distribution to smooth and aggregate them into\nthe final interest clock embedding according to the current time for the final\nprediction. By arming base models with Interest Clock, we conduct online A/B\ntests, obtaining +0.509% and +0.758% improvements on user active days and app\nduration respectively. Besides, the extended offline experiments show\nimprovements as well. Interest Clock has been deployed on Douyin Music App.",
        "translated": ""
    },
    {
        "title": "Align-Free Multi-Plane Phase Retrieval",
        "url": "http://arxiv.org/abs/2404.18946v1",
        "pub_date": "2024-04-30",
        "summary": "The multi-plane phase retrieval method provides a budget-friendly and\neffective way to perform phase imaging, yet it often encounters alignment\nchallenges due to shifts along the optical axis in experiments. Traditional\nmethods, such as employing beamsplitters instead of mechanical stage movements\nor adjusting focus using tunable light sources, add complexity to the setup\nrequired for multi-plane phase retrieval. Attempts to address these issues\ncomputationally face difficulties due to the variable impact of diffraction,\nwhich renders conventional homography techniques inadequate. In our research,\nwe introduce a novel Adaptive Cascade Calibrated (ACC) strategy for multi-plane\nphase retrieval that overcomes misalignment issues. This technique detects\nfeature points within the refocused sample space and calculates the\ntransformation matrix for neighboring planes on-the-fly to digitally adjust\nmeasurements, facilitating alignment-free multi-plane phase retrieval. This\napproach not only avoids the need for complex and expensive optical hardware\nbut also simplifies the imaging setup, reducing overall costs. The\neffectiveness of our method is validated through simulations and real-world\noptical experiments.",
        "translated": ""
    },
    {
        "title": "Automated Construction of Theme-specific Knowledge Graphs",
        "url": "http://arxiv.org/abs/2404.19146v1",
        "pub_date": "2024-04-29",
        "summary": "Despite widespread applications of knowledge graphs (KGs) in various tasks\nsuch as question answering and intelligent conversational systems, existing KGs\nface two major challenges: information granularity and deficiency in\ntimeliness. These hinder considerably the retrieval and analysis of in-context,\nfine-grained, and up-to-date knowledge from KGs, particularly in highly\nspecialized themes (e.g., specialized scientific research) and rapidly evolving\ncontexts (e.g., breaking news or disaster tracking). To tackle such challenges,\nwe propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed\nfrom a theme-specific corpus, and design an unsupervised framework for ThemeKG\nconstruction (named TKGCon). The framework takes raw theme-specific corpus and\ngenerates a high-quality KG that includes salient entities and relations under\nthe theme. Specifically, we start with an entity ontology of the theme from\nWikipedia, based on which we then generate candidate relations by Large\nLanguage Models (LLMs) to construct a relation ontology. To parse the documents\nfrom the theme corpus, we first map the extracted entity pairs to the ontology\nand retrieve the candidate relations. Finally, we incorporate the context and\nontology to consolidate the relations for entity pairs. We observe that\ndirectly prompting GPT-4 for theme-specific KG leads to inaccurate entities\n(such as \"two main types\" as one entity in the query result) and unclear (such\nas \"is\", \"has\") or wrong relations (such as \"have due to\", \"to start\"). In\ncontrast, by constructing the theme-specific KG step by step, our model\noutperforms GPT-4 and could consistently identify accurate entities and\nrelations. Experimental results also show that our framework excels in\nevaluations compared with various KG construction baselines.",
        "translated": ""
    },
    {
        "title": "SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set\n  Retrieval",
        "url": "http://arxiv.org/abs/2404.19130v1",
        "pub_date": "2024-04-29",
        "summary": "Knowledge graphs (KGs), which store an extensive number of relational facts\n(head, relation, tail), serve various applications. While many downstream tasks\nhighly rely on the expressive modeling and predictive embedding of KGs, most of\nthe current KG representation learning methods, where each entity is embedded\nas a vector in the Euclidean space and each relation is embedded as a\ntransformation, follow an entity ranking protocol. On one hand, such an\nembedding design cannot capture many-to-many relations. On the other hand, in\nmany retrieval cases, the users wish to get an exact set of answers without any\nranking, especially when the results are expected to be precise, e.g., which\ngenes cause an illness. Such scenarios are commonly referred to as \"set\nretrieval\". This work presents a pioneering study on the KG set retrieval\nproblem. We show that the set retrieval highly depends on expressive modeling\nof many-to-many relations, and propose a new KG embedding model SpherE to\naddress this problem. SpherE is based on rotational embedding methods, but each\nentity is embedded as a sphere instead of a vector. While inheriting the high\ninterpretability of rotational-based models, our SpherE can more expressively\nmodel one-to-many, many-to-one, and many-to-many relations. Through extensive\nexperiments, we show that our SpherE can well address the set retrieval problem\nwhile still having a good predictive ability to infer missing facts. The code\nis available at https://github.com/Violet24K/SpherE.",
        "translated": ""
    },
    {
        "title": "Catalyzing Social Interactions in Mixed Reality using ML Recommendation\n  Systems",
        "url": "http://arxiv.org/abs/2404.19095v1",
        "pub_date": "2024-04-29",
        "summary": "We create an innovative mixed reality-first social recommendation model,\nutilizing features uniquely collected through mixed reality (MR) systems to\npromote social interaction, such as gaze recognition, proximity, noise level,\ncongestion level, and conversational intensity. We further extend these models\nto include right-time features to deliver timely notifications. We measure\nperformance metrics across various models by creating a new intersection of\nuser features, MR features, and right-time features. We create four model types\ntrained on different combinations of the feature classes, where we compare the\nbaseline model trained on the class of user features against the models trained\non MR features, right-time features, and a combination of all of the feature\nclasses. Due to limitations in data collection and cost, we observe performance\ndegradation in the right-time, mixed reality, and combination models. Despite\nthese challenges, we introduce optimizations to improve accuracy across all\nmodels by over 14 percentage points, where the best performing model achieved\n24% greater accuracy.",
        "translated": ""
    },
    {
        "title": "Large Language Models as Conversational Movie Recommenders: A User Study",
        "url": "http://arxiv.org/abs/2404.19093v1",
        "pub_date": "2024-04-29",
        "summary": "This paper explores the effectiveness of using large language models (LLMs)\nfor personalized movie recommendations from users' perspectives in an online\nfield experiment. Our study involves a combination of between-subject prompt\nand historic consumption assessments, along with within-subject recommendation\nscenario evaluations. By examining conversation and survey response data from\n160 active users, we find that LLMs offer strong recommendation explainability\nbut lack overall personalization, diversity, and user trust. Our results also\nindicate that different personalized prompting techniques do not significantly\naffect user-perceived recommendation quality, but the number of movies a user\nhas watched plays a more significant role. Furthermore, LLMs show a greater\nability to recommend lesser-known or niche movies. Through qualitative\nanalysis, we identify key conversational patterns linked to positive and\nnegative user interaction experiences and conclude that providing personal\ncontext and examples is crucial for obtaining high-quality recommendations from\nLLMs.",
        "translated": ""
    },
    {
        "title": "A First Look at Selection Bias in Preference Elicitation for\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.00554v1",
        "pub_date": "2024-05-01",
        "summary": "Preference elicitation explicitly asks users what kind of recommendations\nthey would like to receive. It is a popular technique for conversational\nrecommender systems to deal with cold-starts. Previous work has studied\nselection bias in implicit feedback, e.g., clicks, and in some forms of\nexplicit feedback, i.e., ratings on items. Despite the fact that the extreme\nsparsity of preference elicitation interactions make them severely more prone\nto selection bias than natural interactions, the effect of selection bias in\npreference elicitation on the resulting recommendations has not been studied\nyet. To address this gap, we take a first look at the effects of selection bias\nin preference elicitation and how they may be further investigated in the\nfuture. We find that a big hurdle is the current lack of any publicly available\ndataset that has preference elicitation interactions. As a solution, we propose\na simulation of a topic-based preference elicitation process. The results from\nour simulation-based experiments indicate (i) that ignoring the effect of\nselection bias early in preference elicitation can lead to an exacerbation of\noverrepresentation in subsequent item recommendations, and (ii) that debiasing\nmethods can alleviate this effect, which leads to significant improvements in\nsubsequent item recommendation performance. Our aim is for the proposed\nsimulator and initial results to provide a starting point and motivation for\nfuture research into this important but overlooked problem setting.",
        "translated": ""
    },
    {
        "title": "KVP10k : A Comprehensive Dataset for Key-Value Pair Extraction in\n  Business Documents",
        "url": "http://arxiv.org/abs/2405.00505v1",
        "pub_date": "2024-05-01",
        "summary": "In recent years, the challenge of extracting information from business\ndocuments has emerged as a critical task, finding applications across numerous\ndomains. This effort has attracted substantial interest from both industry and\nacademy, highlighting its significance in the current technological landscape.\nMost datasets in this area are primarily focused on Key Information Extraction\n(KIE), where the extraction process revolves around extracting information\nusing a specific, predefined set of keys. Unlike most existing datasets and\nbenchmarks, our focus is on discovering key-value pairs (KVPs) without relying\non predefined keys, navigating through an array of diverse templates and\ncomplex layouts. This task presents unique challenges, primarily due to the\nabsence of comprehensive datasets and benchmarks tailored for non-predetermined\nKVP extraction. To address this gap, we introduce KVP10k , a new dataset and\nbenchmark specifically designed for KVP extraction. The dataset contains 10707\nrichly annotated images. In our benchmark, we also introduce a new challenging\ntask that combines elements of KIE as well as KVP in a single task. KVP10k sets\nitself apart with its extensive diversity in data and richly detailed\nannotations, paving the way for advancements in the field of information\nextraction from complex business documents.",
        "translated": ""
    },
    {
        "title": "Exploiting Positional Bias for Query-Agnostic Generative Content in\n  Search",
        "url": "http://arxiv.org/abs/2405.00469v1",
        "pub_date": "2024-05-01",
        "summary": "In recent years, neural ranking models (NRMs) have been shown to\nsubstantially outperform their lexical counterparts in text retrieval. In\ntraditional search pipelines, a combination of features leads to well-defined\nbehaviour. However, as neural approaches become increasingly prevalent as the\nfinal scoring component of engines or as standalone systems, their robustness\nto malicious text and, more generally, semantic perturbation needs to be better\nunderstood. We posit that the transformer attention mechanism can induce\nexploitable defects through positional bias in search models, leading to an\nattack that could generalise beyond a single query or topic. We demonstrate\nsuch defects by showing that non-relevant text--such as promotional\ncontent--can be easily injected into a document without adversely affecting its\nposition in search results. Unlike previous gradient-based attacks, we\ndemonstrate these biases in a query-agnostic fashion. In doing so, without the\nknowledge of topicality, we can still reduce the negative effects of\nnon-relevant content injection by controlling injection position. Our\nexperiments are conducted with simulated on-topic promotional text\nautomatically generated by prompting LLMs with topical context from target\ndocuments. We find that contextualisation of a non-relevant text further\nreduces negative effects whilst likely circumventing existing content filtering\nmechanisms. In contrast, lexical models are found to be more resilient to such\ncontent injection attacks. We then investigate a simple yet effective\ncompensation for the weaknesses of the NRMs in search, validating our\nhypotheses regarding transformer bias.",
        "translated": ""
    },
    {
        "title": "RAG-based Explainable Prediction of Road Users Behaviors for Automated\n  Driving using Knowledge Graphs and Large Language Models",
        "url": "http://arxiv.org/abs/2405.00449v1",
        "pub_date": "2024-05-01",
        "summary": "Prediction of road users' behaviors in the context of autonomous driving has\ngained considerable attention by the scientific community in the last years.\nMost works focus on predicting behaviors based on kinematic information alone,\na simplification of the reality since road users are humans, and as such they\nare highly influenced by their surrounding context. In addition, a large\nplethora of research works rely on powerful Deep Learning techniques, which\nexhibit high performance metrics in prediction tasks but may lack the ability\nto fully understand and exploit the contextual semantic information contained\nin the road scene, not to mention their inability to provide explainable\npredictions that can be understood by humans. In this work, we propose an\nexplainable road users' behavior prediction system that integrates the\nreasoning abilities of Knowledge Graphs (KG) and the expressiveness\ncapabilities of Large Language Models (LLM) by using Retrieval Augmented\nGeneration (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)\nand Bayesian inference are combined to allow the deployment of a fully\ninductive reasoning system that enables the issuing of predictions that rely on\nlegacy information contained in the graph as well as on current evidence\ngathered in real time by onboard sensors. Two use cases have been implemented\nfollowing the proposed approach: 1) Prediction of pedestrians' crossing\nactions; 2) Prediction of lane change maneuvers. In both cases, the performance\nattained surpasses the current state of the art in terms of anticipation and\nF1-score, showing a promising avenue for future research in this field.",
        "translated": ""
    },
    {
        "title": "Distance Sampling-based Paraphraser Leveraging ChatGPT for Text Data\n  Manipulation",
        "url": "http://arxiv.org/abs/2405.00367v1",
        "pub_date": "2024-05-01",
        "summary": "There has been growing interest in audio-language retrieval research, where\nthe objective is to establish the correlation between audio and text\nmodalities. However, most audio-text paired datasets often lack rich expression\nof the text data compared to the audio samples. One of the significant\nchallenges facing audio-text datasets is the presence of similar or identical\ncaptions despite different audio samples. Therefore, under many-to-one mapping\nconditions, audio-text datasets lead to poor performance of retrieval tasks. In\nthis paper, we propose a novel approach to tackle the data imbalance problem in\naudio-language retrieval task. To overcome the limitation, we introduce a\nmethod that employs a distance sampling-based paraphraser leveraging ChatGPT,\nutilizing distance function to generate a controllable distribution of\nmanipulated text data. For a set of sentences with the same context, the\ndistance is used to calculate a degree of manipulation for any two sentences,\nand ChatGPT's few-shot prompting is performed using a text cluster with a\nsimilar distance defined by the Jaccard similarity. Therefore, ChatGPT, when\napplied to few-shot prompting with text clusters, can adjust the diversity of\nthe manipulated text based on the distance. The proposed approach is shown to\nsignificantly enhance performance in audio-text retrieval, outperforming\nconventional text augmentation techniques.",
        "translated": ""
    },
    {
        "title": "Distillation Matters: Empowering Sequential Recommenders to Match the\n  Performance of Large Language Model",
        "url": "http://arxiv.org/abs/2405.00338v1",
        "pub_date": "2024-05-01",
        "summary": "Owing to their powerful semantic reasoning capabilities, Large Language\nModels (LLMs) have been effectively utilized as recommenders, achieving\nimpressive performance. However, the high inference latency of LLMs\nsignificantly restricts their practical deployment. To address this issue, this\nwork investigates knowledge distillation from cumbersome LLM-based\nrecommendation models to lightweight conventional sequential models. It\nencounters three challenges: 1) the teacher's knowledge may not always be\nreliable; 2) the capacity gap between the teacher and student makes it\ndifficult for the student to assimilate the teacher's knowledge; 3) divergence\nin semantic space poses a challenge to distill the knowledge from embeddings.\nTo tackle these challenges, this work proposes a novel distillation strategy,\nDLLM2Rec, specifically tailored for knowledge distillation from LLM-based\nrecommendation models to conventional sequential models. DLLM2Rec comprises: 1)\nImportance-aware ranking distillation, which filters reliable and\nstudent-friendly knowledge by weighting instances according to teacher\nconfidence and student-teacher consistency; 2) Collaborative embedding\ndistillation integrates knowledge from teacher embeddings with collaborative\nsignals mined from the data. Extensive experiments demonstrate the\neffectiveness of the proposed DLLM2Rec, boosting three typical sequential\nmodels with an average improvement of 47.97%, even enabling them to surpass\nLLM-based recommenders in some cases.",
        "translated": ""
    },
    {
        "title": "Characterizing Information Seeking Processes with Multiple Physiological\n  Signals",
        "url": "http://arxiv.org/abs/2405.00322v1",
        "pub_date": "2024-05-01",
        "summary": "Information access systems are getting complex, and our understanding of user\nbehavior during information seeking processes is mainly drawn from qualitative\nmethods, such as observational studies or surveys. Leveraging the advances in\nsensing technologies, our study aims to characterize user behaviors with\nphysiological signals, particularly in relation to cognitive load, affective\narousal, and valence. We conduct a controlled lab study with 26 participants,\nand collect data including Electrodermal Activities, Photoplethysmogram,\nElectroencephalogram, and Pupillary Responses. This study examines\ninformational search with four stages: the realization of Information Need\n(IN), Query Formulation (QF), Query Submission (QS), and Relevance Judgment\n(RJ). We also include different interaction modalities to represent modern\nsystems, e.g., QS by text-typing or verbalizing, and RJ with text or audio\ninformation. We analyze the physiological signals across these stages and\nreport outcomes of pairwise non-parametric repeated-measure statistical tests.\nThe results show that participants experience significantly higher cognitive\nloads at IN with a subtle increase in alertness, while QF requires higher\nattention. QS involves demanding cognitive loads than QF. Affective responses\nare more pronounced at RJ than QS or IN, suggesting greater interest and\nengagement as knowledge gaps are resolved. To the best of our knowledge, this\nis the first study that explores user behaviors in a search process employing a\nmore nuanced quantitative analysis of physiological signals. Our findings offer\nvaluable insights into user behavior and emotional responses in information\nseeking processes. We believe our proposed methodology can inform the\ncharacterization of more complex processes, such as conversational information\nseeking.",
        "translated": ""
    },
    {
        "title": "Stochastic Sampling for Contrastive Views and Hard Negative Samples in\n  Graph-based Collaborative Filtering",
        "url": "http://arxiv.org/abs/2405.00287v1",
        "pub_date": "2024-05-01",
        "summary": "Graph-based collaborative filtering (CF) has emerged as a promising approach\nin recommendation systems. Despite its achievements, graph-based CF models face\nchallenges due to data sparsity and negative sampling. In this paper, we\npropose a novel Stochastic sampling for i) COntrastive views and ii) hard\nNEgative samples (SCONE) to overcome these issues. By considering that they are\nboth sampling tasks, we generate dynamic augmented views and diverse hard\nnegative samples via our unified stochastic sampling framework based on\nscore-based generative models. In our comprehensive evaluations with 6\nbenchmark datasets, our proposed SCONE significantly improves recommendation\naccuracy and robustness, and demonstrates the superiority of our approach over\nexisting CF models. Furthermore, we prove the efficacy of user-item specific\nstochastic sampling for addressing the user sparsity and item popularity\nissues. The integration of the stochastic sampling and graph-based CF obtains\nthe state-of-the-art in personalized recommendation systems, making significant\nstrides in information-rich environments.",
        "translated": ""
    },
    {
        "title": "Global News Synchrony and Diversity During the Start of the COVID-19\n  Pandemic",
        "url": "http://arxiv.org/abs/2405.00280v1",
        "pub_date": "2024-05-01",
        "summary": "News coverage profoundly affects how countries and individuals behave in\ninternational relations. Yet, we have little empirical evidence of how news\ncoverage varies across countries. To enable studies of global news coverage, we\ndevelop an efficient computational methodology that comprises three components:\n(i) a transformer model to estimate multilingual news similarity; (ii) a global\nevent identification system that clusters news based on a similarity network of\nnews articles; and (iii) measures of news synchrony across countries and news\ndiversity within a country, based on country-specific distributions of news\ncoverage of the global events. Each component achieves state-of-the art\nperformance, scaling seamlessly to massive datasets of millions of news\narticles. We apply the methodology to 60 million news articles published\nglobally between January 1 and June 30, 2020, across 124 countries and 10\nlanguages, detecting 4357 news events. We identify the factors explaining\ndiversity and synchrony of news coverage across countries. Our study reveals\nthat news media tend to cover a more diverse set of events in countries with\nlarger Internet penetration, more official languages, larger religious\ndiversity, higher economic inequality, and larger populations. Coverage of news\nevents is more synchronized between countries that not only actively\nparticipate in commercial and political relations -- such as, pairs of\ncountries with high bilateral trade volume, and countries that belong to the\nNATO military alliance or BRICS group of major emerging economies -- but also\ncountries that share certain traits: an official language, high GDP, and high\ndemocracy indices.",
        "translated": ""
    },
    {
        "title": "Grounding Realizable Entities",
        "url": "http://arxiv.org/abs/2405.00197v1",
        "pub_date": "2024-04-30",
        "summary": "Ontological representations of qualities, dispositions, and roles have been\nrefined over the past decade, clarifying subtle distinctions in life science\nresearch. After articulating a widely-used characterization of these entities\nwithin the context of Basic Formal Ontology (BFO), we identify gaps in this\ntreatment and motivate the need for supplementing the BFO characterization. By\nway of supplement, we propose definitions for grounding relations holding\nbetween qualities and dispositions, and dispositions and roles, illustrating\nour proposal by representing subtle aspects of host-pathogen interactions.",
        "translated": ""
    },
    {
        "title": "UQA: Corpus for Urdu Question Answering",
        "url": "http://arxiv.org/abs/2405.01458v1",
        "pub_date": "2024-05-02",
        "summary": "This paper introduces UQA, a novel dataset for question answering and text\ncomprehension in Urdu, a low-resource language with over 70 million native\nspeakers. UQA is generated by translating the Stanford Question Answering\nDataset (SQuAD2.0), a large-scale English QA dataset, using a technique called\nEATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in\nthe translated context paragraphs. The paper describes the process of selecting\nand evaluating the best translation model among two candidates: Google\nTranslator and Seamless M4T. The paper also benchmarks several state-of-the-art\nmultilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and\nreports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and\n74.56 EM. UQA is a valuable resource for developing and testing multilingual\nNLP systems for Urdu and for enhancing the cross-lingual transferability of\nexisting models. Further, the paper demonstrates the effectiveness of EATS for\ncreating high-quality datasets for other languages and domains. The UQA dataset\nand the code are publicly available at www.github.com/sameearif/UQA.",
        "translated": ""
    },
    {
        "title": "Modeling Activity-Driven Music Listening with PACE",
        "url": "http://arxiv.org/abs/2405.01417v1",
        "pub_date": "2024-05-02",
        "summary": "While the topic of listening context is widely studied in the literature of\nmusic recommender systems, the integration of regular user behavior is often\nomitted. In this paper, we propose PACE (PAttern-based user Consumption\nEmbedding), a framework for building user embeddings that takes advantage of\nperiodic listening behaviors. PACE leverages users' multichannel time-series\nconsumption patterns to build understandable user vectors. We believe the\nembeddings learned with PACE unveil much about the repetitive nature of user\nlistening dynamics. By applying this framework on long-term user histories, we\nevaluate the embeddings through a predictive task of activities performed while\nlistening to music. The validation task's interest is two-fold, while it shows\nthe relevance of our approach, it also offers an insightful way of\nunderstanding users' musical consumption habits.",
        "translated": ""
    },
    {
        "title": "Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf\n  Disease Remediation",
        "url": "http://arxiv.org/abs/2405.01310v1",
        "pub_date": "2024-05-02",
        "summary": "This research introduces an innovative AI-driven precision agriculture\nsystem, leveraging YOLOv8 for disease identification and Retrieval Augmented\nGeneration (RAG) for context-aware diagnosis. Focused on addressing the\nchallenges of diseases affecting the coffee production sector in Karnataka, The\nsystem integrates sophisticated object detection techniques with language\nmodels to address the inherent constraints associated with Large Language\nModels (LLMs). Our methodology not only tackles the issue of hallucinations in\nLLMs, but also introduces dynamic disease identification and remediation\nstrategies. Real-time monitoring, collaborative dataset expansion, and\norganizational involvement ensure the system's adaptability in diverse\nagricultural settings. The effect of the suggested system extends beyond\nautomation, aiming to secure food supplies, protect livelihoods, and promote\neco-friendly farming practices. By facilitating precise disease identification,\nthe system contributes to sustainable and environmentally conscious\nagriculture, reducing reliance on pesticides. Looking to the future, the\nproject envisions continuous development in RAG-integrated object detection\nsystems, emphasizing scalability, reliability, and usability. This research\nstrives to be a beacon for positive change in agriculture, aligning with global\nefforts toward sustainable and technologically enhanced food production.",
        "translated": ""
    },
    {
        "title": "Are We Really Achieving Better Beyond-Accuracy Performance in Next\n  Basket Recommendation?",
        "url": "http://arxiv.org/abs/2405.01143v1",
        "pub_date": "2024-05-02",
        "summary": "Next basket recommendation (NBR) is a special type of sequential\nrecommendation that is increasingly receiving attention. So far, most NBR\nstudies have focused on optimizing the accuracy of the recommendation, whereas\noptimizing for beyond-accuracy metrics, e.g., item fairness and diversity\nremains largely unexplored. Recent studies into NBR have found a substantial\nperformance difference between recommending repeat items and explore items.\nRepeat items contribute most of the users' perceived accuracy compared with\nexplore items. Informed by these findings, we identify a potential \"short-cut\"\nto optimize for beyond-accuracy metrics while maintaining high accuracy. To\nleverage and verify the existence of such short-cuts, we propose a\nplug-and-play two-step repetition-exploration (TREx) framework that treats\nrepeat items and explores items separately, where we design a simple yet highly\neffective repetition module to ensure high accuracy, while two exploration\nmodules target optimizing only beyond-accuracy metrics. Experiments are\nperformed on two widely-used datasets w.r.t. a range of beyond-accuracy\nmetrics, viz. five fairness metrics and three diversity metrics. Our\nexperimental results verify the effectiveness of TREx. Prima facie, this\nappears to be good news: we can achieve high accuracy and improved\nbeyond-accuracy metrics at the same time. However, we argue that the real-world\nvalue of our algorithmic solution, TREx, is likely to be limited and reflect on\nthe reasonableness of the evaluation setup. We end up challenging existing\nevaluation paradigms, particularly in the context of beyond-accuracy metrics,\nand provide insights for researchers to navigate potential pitfalls and\ndetermine reasonable metrics to consider when optimizing for accuracy and\nbeyond-accuracy metrics.",
        "translated": ""
    },
    {
        "title": "Generative Relevance Feedback and Convergence of Adaptive Re-Ranking:\n  University of Glasgow Terrier Team at TREC DL 2023",
        "url": "http://arxiv.org/abs/2405.01122v1",
        "pub_date": "2024-05-02",
        "summary": "This paper describes our participation in the TREC 2023 Deep Learning Track.\nWe submitted runs that apply generative relevance feedback from a large\nlanguage model in both a zero-shot and pseudo-relevance feedback setting over\ntwo sparse retrieval approaches, namely BM25 and SPLADE. We couple this first\nstage with adaptive re-ranking over a BM25 corpus graph scored using a\nmonoELECTRA cross-encoder. We investigate the efficacy of these generative\napproaches for different query types in first-stage retrieval. In re-ranking,\nwe investigate operating points of adaptive re-ranking with different first\nstages to find the point in graph traversal where the first stage no longer has\nan effect on the performance of the overall retrieval pipeline. We find some\nperformance gains from the application of generative query reformulation.\nHowever, our strongest run in terms of P@10 and nDCG@10 applied both adaptive\nre-ranking and generative pseudo-relevance feedback, namely uogtr_b_grf_e_gb.",
        "translated": ""
    },
    {
        "title": "Faster Learned Sparse Retrieval with Block-Max Pruning",
        "url": "http://arxiv.org/abs/2405.01117v1",
        "pub_date": "2024-05-02",
        "summary": "Learned sparse retrieval systems aim to combine the effectiveness of\ncontextualized language models with the scalability of conventional data\nstructures such as inverted indexes. Nevertheless, the indexes generated by\nthese systems exhibit significant deviations from the ones that use traditional\nretrieval models, leading to a discrepancy in the performance of existing query\noptimizations that were specifically developed for traditional structures.\nThese disparities arise from structural variations in query and document\nstatistics, including sub-word tokenization, leading to longer queries, smaller\nvocabularies, and different score distributions within posting lists. This\npaper introduces Block-Max Pruning (BMP), an innovative dynamic pruning\nstrategy tailored for indexes arising in learned sparse retrieval environments.\nBMP employs a block filtering mechanism to divide the document space into\nsmall, consecutive document ranges, which are then aggregated and sorted on the\nfly, and fully processed only as necessary, guided by a defined safe early\ntermination criterion or based on approximate retrieval requirements. Through\nrigorous experimentation, we show that BMP substantially outperforms existing\ndynamic pruning strategies, offering unparalleled efficiency in safe retrieval\ncontexts and improved tradeoffs between precision and efficiency in approximate\nretrieval tasks.",
        "translated": ""
    },
    {
        "title": "\"In-Context Learning\" or: How I learned to stop worrying and love\n  \"Applied Information Retrieval\"",
        "url": "http://arxiv.org/abs/2405.01116v1",
        "pub_date": "2024-05-02",
        "summary": "With the increasing ability of large language models (LLMs), in-context\nlearning (ICL) has evolved as a new paradigm for natural language processing\n(NLP), where instead of fine-tuning the parameters of an LLM specific to a\ndownstream task with labeled examples, a small number of such examples is\nappended to a prompt instruction for controlling the decoder's generation\nprocess. ICL, thus, is conceptually similar to a non-parametric approach, such\nas $k$-NN, where the prediction for each instance essentially depends on the\nlocal topology, i.e., on a localised set of similar instances and their labels\n(called few-shot examples). This suggests that a test instance in ICL is\nanalogous to a query in IR, and similar examples in ICL retrieved from a\ntraining set relate to a set of documents retrieved from a collection in IR.\nWhile standard unsupervised ranking models can be used to retrieve these\nfew-shot examples from a training set, the effectiveness of the examples can\npotentially be improved by re-defining the notion of relevance specific to its\nutility for the downstream task, i.e., considering an example to be relevant if\nincluding it in the prompt instruction leads to a correct prediction. With this\ntask-specific notion of relevance, it is possible to train a supervised ranking\nmodel (e.g., a bi-encoder or cross-encoder), which potentially learns to\noptimally select the few-shot examples. We believe that the recent advances in\nneural rankers can potentially find a use case for this task of optimally\nchoosing examples for more effective downstream ICL predictions.",
        "translated": ""
    },
    {
        "title": "Silencing the Risk, Not the Whistle: A Semi-automated Text Sanitization\n  Tool for Mitigating the Risk of Whistleblower Re-Identification",
        "url": "http://arxiv.org/abs/2405.01097v1",
        "pub_date": "2024-05-02",
        "summary": "Whistleblowing is essential for ensuring transparency and accountability in\nboth public and private sectors. However, (potential) whistleblowers often fear\nor face retaliation, even when reporting anonymously. The specific content of\ntheir disclosures and their distinct writing style may re-identify them as the\nsource. Legal measures, such as the EU WBD, are limited in their scope and\neffectiveness. Therefore, computational methods to prevent re-identification\nare important complementary tools for encouraging whistleblowers to come\nforward. However, current text sanitization tools follow a one-size-fits-all\napproach and take an overly limited view of anonymity. They aim to mitigate\nidentification risk by replacing typical high-risk words (such as person names\nand other NE labels) and combinations thereof with placeholders. Such an\napproach, however, is inadequate for the whistleblowing scenario since it\nneglects further re-identification potential in textual features, including\nwriting style. Therefore, we propose, implement, and evaluate a novel\nclassification and mitigation strategy for rewriting texts that involves the\nwhistleblower in the assessment of the risk and utility. Our prototypical tool\nsemi-automatically evaluates risk at the word/term level and applies\nrisk-adapted anonymization techniques to produce a grammatically disjointed yet\nappropriately sanitized text. We then use a LLM that we fine-tuned for\nparaphrasing to render this text coherent and style-neutral. We evaluate our\ntool's effectiveness using court cases from the ECHR and excerpts from a\nreal-world whistleblower testimony and measure the protection against\nauthorship attribution (AA) attacks and utility loss statistically using the\npopular IMDb62 movie reviews dataset. Our method can significantly reduce AA\naccuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original\ncontent's semantics.",
        "translated": ""
    },
    {
        "title": "Fair Recommendations with Limited Sensitive Attributes: A\n  Distributionally Robust Optimization Approach",
        "url": "http://arxiv.org/abs/2405.01063v1",
        "pub_date": "2024-05-02",
        "summary": "As recommender systems are indispensable in various domains such as job\nsearching and e-commerce, providing equitable recommendations to users with\ndifferent sensitive attributes becomes an imperative requirement. Prior\napproaches for enhancing fairness in recommender systems presume the\navailability of all sensitive attributes, which can be difficult to obtain due\nto privacy concerns or inadequate means of capturing these attributes. In\npractice, the efficacy of these approaches is limited, pushing us to\ninvestigate ways of promoting fairness with limited sensitive attribute\ninformation.\n  Toward this goal, it is important to reconstruct missing sensitive\nattributes. Nevertheless, reconstruction errors are inevitable due to the\ncomplexity of real-world sensitive attribute reconstruction problems and legal\nregulations. Thus, we pursue fair learning methods that are robust to\nreconstruction errors. To this end, we propose Distributionally Robust Fair\nOptimization (DRFO), which minimizes the worst-case unfairness over all\npotential probability distributions of missing sensitive attributes instead of\nthe reconstructed one to account for the impact of the reconstruction errors.\nWe provide theoretical and empirical evidence to demonstrate that our method\ncan effectively ensure fairness in recommender systems when only limited\nsensitive attributes are accessible.",
        "translated": ""
    },
    {
        "title": "Multi-intent-aware Session-based Recommendation",
        "url": "http://arxiv.org/abs/2405.00986v1",
        "pub_date": "2024-05-02",
        "summary": "Session-based recommendation (SBR) aims to predict the following item a user\nwill interact with during an ongoing session. Most existing SBR models focus on\ndesigning sophisticated neural-based encoders to learn a session\nrepresentation, capturing the relationship among session items. However, they\ntend to focus on the last item, neglecting diverse user intents that may exist\nwithin a session. This limitation leads to significant performance drops,\nespecially for longer sessions. To address this issue, we propose a novel SBR\nmodel, called Multi-intent-aware Session-based Recommendation Model (MiaSRec).\nIt adopts frequency embedding vectors indicating the item frequency in session\nto enhance the information about repeated items. MiaSRec represents various\nuser intents by deriving multiple session representations centered on each item\nand dynamically selecting the important ones. Extensive experimental results\nshow that MiaSRec outperforms existing state-of-the-art SBR models on six\ndatasets, particularly those with longer average session length, achieving up\nto 6.27% and 24.56% gains for MRR@20 and Recall@20. Our code is available at\nhttps://github.com/jin530/MiaSRec.",
        "translated": ""
    },
    {
        "title": "Comparing Personalized Relevance Algorithms for Directed Graphs",
        "url": "http://arxiv.org/abs/2405.02261v1",
        "pub_date": "2024-05-03",
        "summary": "We present an interactive Web platform that, given a directed graph, allows\nidentifying the most relevant nodes related to a given query node. Besides\nwell-established algorithms such as PageRank and Personalized PageRank, the\ndemo includes Cyclerank, a novel algorithm that addresses some of their\nlimitations by leveraging cyclic paths to compute personalized relevance\nscores. Our demo design enables two use cases: (a) algorithm comparison,\ncomparing the results obtained with different algorithms, and (b) dataset\ncomparison, for exploring and gaining insights into a dataset and comparing it\nwith others. We provide 50 pre-loaded datasets from Wikipedia, Twitter, and\nAmazon and seven algorithms. Users can upload new datasets, and new algorithms\ncan be easily added. By showcasing efficient algorithms to compute relevance\nscores in directed graphs, our tool helps to uncover hidden relationships\nwithin the data, which makes of it a valuable addition to the repertoire of\ngraph analysis algorithms.",
        "translated": ""
    },
    {
        "title": "REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\n  Sentences using Public and Proprietary LLMs",
        "url": "http://arxiv.org/abs/2405.02228v1",
        "pub_date": "2024-05-03",
        "summary": "Automatic citation generation for sentences in a document or report is\nparamount for intelligence analysts, cybersecurity, news agencies, and\neducation personnel. In this research, we investigate whether large language\nmodels (LLMs) are capable of generating references based on two forms of\nsentence queries: (a) Direct Queries, LLMs are asked to provide author names of\nthe given research article, and (b) Indirect Queries, LLMs are asked to provide\nthe title of a mentioned article when given a sentence from a different\narticle. To demonstrate where LLM stands in this task, we introduce a large\ndataset called REASONS comprising abstracts of the 12 most popular domains of\nscientific research on arXiv. From around 20K research articles, we make the\nfollowing deductions on public and proprietary LLMs: (a) State-of-the-art,\noften called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass\npercentage (PP) to minimize the hallucination rate (HR). When tested with\nPerplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant\nmetadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented\ngeneration (RAG) using Mistral demonstrates consistent and robust citation\nsupport on indirect queries and matched performance to GPT-3.5 and GPT-4. The\nHR across all domains and models decreased by an average of 41.93% and the PP\nwas reduced to 0% in most cases. In terms of generation quality, the average F1\nScore and BLEU were 68.09% and 57.51%, respectively; (d) Testing with\nadversarial samples showed that LLMs, including the Advance RAG Mistral,\nstruggle to understand context, but the extent of this issue was small in\nMistral and GPT-4-Preview. Our study con tributes valuable insights into the\nreliability of RAG for automated citation generation tasks.",
        "translated": ""
    },
    {
        "title": "FairEvalLLM. A Comprehensive Framework for Benchmarking Fairness in\n  Large Language Model Recommender Systems",
        "url": "http://arxiv.org/abs/2405.02219v1",
        "pub_date": "2024-05-03",
        "summary": "This paper presents a framework for evaluating fairness in recommender\nsystems powered by Large Language Models (RecLLMs), addressing the need for a\nunified approach that spans various fairness dimensions including sensitivity\nto user attributes, intrinsic fairness, and discussions of fairness based on\nunderlying benefits. In addition, our framework introduces counterfactual\nevaluations and integrates diverse user group considerations to enhance the\ndiscourse on fairness evaluation for RecLLMs.\n  Our key contributions include the development of a robust framework for\nfairness evaluation in LLM-based recommendations and a structured method to\ncreate \\textit{informative user profiles} from demographic data, historical\nuser preferences, and recent interactions. We argue that the latter is\nessential for enhancing personalization in such systems, especially in\ntemporal-driven scenarios. We demonstrate the utility of our framework through\npractical applications on two datasets, LastFM-1K and ML-1M. We conduct\nexperiments on a subsample of 80 users from each dataset, testing and assessing\nthe effectiveness of various prompt construction scenarios and in-context\nlearning, comprising more than 50 scenarios. This results in more than 4000\nrecommendations (80 * 50 = 4000). Our study reveals that while there are no\nsignificant unfairness issues in scenarios involving sensitive attributes, some\nconcerns remain. However, in terms of intrinsic fairness, which does not\ninvolve direct sensitivity, unfairness across demographic groups remains\nsignificant. The code and data used for this paper are available at:\n\\url{https://shorturl.at/awBFM}.",
        "translated": ""
    },
    {
        "title": "How to Diversify any Personalized Recommender? A User-centric\n  Pre-processing approach",
        "url": "http://arxiv.org/abs/2405.02156v1",
        "pub_date": "2024-05-03",
        "summary": "In this paper, we introduce a novel approach to improve the diversity of\nTop-N recommendations while maintaining recommendation performance. Our\napproach employs a user-centric pre-processing strategy aimed at exposing users\nto a wide array of content categories and topics. We personalize this strategy\nby selectively adding and removing a percentage of interactions from user\nprofiles. This personalization ensures we remain closely aligned with user\npreferences while gradually introducing distribution shifts. Our pre-processing\ntechnique offers flexibility and can seamlessly integrate into any recommender\narchitecture. To evaluate our approach, we run extensive experiments on two\npublicly available data sets for news and book recommendations. We test various\nstandard and neural network-based recommender system algorithms. Our results\nshow that our approach generates diverse recommendations, ensuring users are\nexposed to a wider range of items. Furthermore, leveraging pre-processed data\nfor training leads to recommender systems achieving performance levels\ncomparable to, and in some cases, better than those trained on original,\nunmodified data. Additionally, our approach promotes provider fairness by\nfacilitating exposure to minority or niche categories.",
        "translated": ""
    },
    {
        "title": "Multi-Objective Recommendation via Multivariate Policy Learning",
        "url": "http://arxiv.org/abs/2405.02141v1",
        "pub_date": "2024-05-03",
        "summary": "Real-world recommender systems often need to balance multiple objectives when\ndeciding which recommendations to present to users. These include behavioural\nsignals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g.\ndiversity, fairness). Scalarisation methods are commonly used to handle this\nbalancing task, where a weighted average of per-objective reward signals\ndetermines the final score used for ranking. Naturally, how these weights are\ncomputed exactly, is key to success for any online platform. We frame this as a\ndecision-making task, where the scalarisation weights are actions taken to\nmaximise an overall North Star reward (e.g. long-term user retention or\ngrowth). We extend existing policy learning methods to the continuous\nmultivariate action domain, proposing to maximise a pessimistic lower bound on\nthe North Star reward that the learnt policy will yield. Typical lower bounds\nbased on normal approximations suffer from insufficient coverage, and we\npropose an efficient and effective policy-dependent correction for this. We\nprovide guidance to design stochastic data collection policies, as well as\nhighly sensitive reward signals. Empirical observations from simulations,\noffline and online experiments highlight the efficacy of our deployed approach.",
        "translated": ""
    },
    {
        "title": "Ah, that's the great puzzle: On the Quest of a Holistic Understanding of\n  the Harms of Recommender Systems on Children",
        "url": "http://arxiv.org/abs/2405.02050v1",
        "pub_date": "2024-05-03",
        "summary": "Children come across various media items online, many of which are selected\nby recommender systems (RS) primarily designed for adults. The specific nature\nof the content selected by RS to display on online platforms used by children -\nalthough not necessarily targeting them as a user base - remains largely\nunknown. This raises questions about whether such content is appropriate given\nchildren's vulnerable stages of development and the potential risks to their\nwell-being.\n  In this position paper, we reflect on the relationship between RS and\nchildren, emphasizing the possible adverse effects of the content this user\ngroup might be exposed to online. As a step towards fostering safer\ninteractions for children in online environments, we advocate for researchers,\npractitioners, and policymakers to undertake a more comprehensive examination\nof the impact of RS on children - one focused on harms. This would result in a\nmore holistic understanding that could inform the design and deployment of\nstrategies that would better suit children's needs and preferences while\nactively mitigating the potential harm posed by RS; acknowledging that\nidentifying and addressing these harms is complex and multifaceted.",
        "translated": ""
    },
    {
        "title": "Comparative Analysis of Retrieval Systems in the Real World",
        "url": "http://arxiv.org/abs/2405.02048v1",
        "pub_date": "2024-05-03",
        "summary": "This research paper presents a comprehensive analysis of integrating advanced\nlanguage models with search and retrieval systems in the fields of information\nretrieval and natural language processing. The objective is to evaluate and\ncompare various state-of-the-art methods based on their performance in terms of\naccuracy and efficiency. The analysis explores different combinations of\ntechnologies, including Azure Cognitive Search Retriever with GPT-4, Pinecone's\nCanopy framework, Langchain with Pinecone and different language models\n(OpenAI, Cohere), LlamaIndex with Weaviate Vector Store's hybrid search,\nGoogle's RAG implementation on Cloud VertexAI-Search, Amazon SageMaker's RAG,\nand a novel approach called KG-FID Retrieval. The motivation for this analysis\narises from the increasing demand for robust and responsive question-answering\nsystems in various domains. The RobustQA metric is used to evaluate the\nperformance of these systems under diverse paraphrasing of questions. The\nreport aims to provide insights into the strengths and weaknesses of each\nmethod, facilitating informed decisions in the deployment and development of\nAI-driven search and retrieval systems.",
        "translated": ""
    },
    {
        "title": "Diversity of What? On the Different Conceptualizations of Diversity in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2405.02026v1",
        "pub_date": "2024-05-03",
        "summary": "Diversity is a commonly known principle in the design of recommender systems,\nbut also ambiguous in its conceptualization. Through semi-structured interviews\nwe explore how practitioners at three different public service media\norganizations in the Netherlands conceptualize diversity within the scope of\ntheir recommender systems. We provide an overview of the goals that they have\nwith diversity in their systems, which aspects are relevant, and how\nrecommendations should be diversified. We show that even within this limited\ndomain, conceptualization of diversity greatly varies, and argue that it is\nunlikely that a standardized conceptualization will be achieved. Instead, we\nshould focus on effective communication of what diversity in this particular\nsystem means, thus allowing for operationalizations of diversity that are\ncapable of expressing the nuances and requirements of that particular domain.",
        "translated": ""
    },
    {
        "title": "A quantitative and typological study of Early Slavic participle clauses\n  and their competition",
        "url": "http://arxiv.org/abs/2405.01972v1",
        "pub_date": "2024-05-03",
        "summary": "This thesis is a corpus-based, quantitative, and typological analysis of the\nfunctions of Early Slavic participle constructions and their finite competitors\n($jegda$-'when'-clauses). The first part leverages detailed linguistic\nannotation on Early Slavic corpora at the morphosyntactic, dependency,\ninformation-structural, and lexical levels to obtain indirect evidence for\ndifferent potential functions of participle clauses and their main finite\ncompetitor and understand the roles of compositionality and default discourse\nreasoning as explanations for the distribution of participle constructions and\n$jegda$-clauses in the corpus. The second part uses massively parallel data to\nanalyze typological variation in how languages express the semantic space of\nEnglish $when$, whose scope encompasses that of Early Slavic participle\nconstructions and $jegda$-clauses. Probabilistic semantic maps are generated\nand statistical methods (including Kriging, Gaussian Mixture Modelling,\nprecision and recall analysis) are used to induce cross-linguistically salient\ndimensions from the parallel corpus and to study conceptual variation within\nthe semantic space of the hypothetical concept WHEN.",
        "translated": ""
    },
    {
        "title": "Semi-Parametric Retrieval via Binary Token Index",
        "url": "http://arxiv.org/abs/2405.01924v1",
        "pub_date": "2024-05-03",
        "summary": "The landscape of information retrieval has broadened from search services to\na critical component in various advanced applications, where indexing\nefficiency, cost-effectiveness, and freshness are increasingly important yet\nremain less explored. To address these demands, we introduce Semi-parametric\nVocabulary Disentangled Retrieval (SVDR). SVDR is a novel semi-parametric\nretrieval framework that supports two types of indexes: an embedding-based\nindex for high effectiveness, akin to existing neural retrieval methods; and a\nbinary token index that allows for quick and cost-effective setup, resembling\ntraditional term-based retrieval. In our evaluation on three open-domain\nquestion answering benchmarks with the entire Wikipedia as the retrieval\ncorpus, SVDR consistently demonstrates superiority. It achieves a 3% higher\ntop-1 retrieval accuracy compared to the dense retriever DPR when using an\nembedding-based index and an 9% higher top-1 accuracy compared to BM25 when\nusing a binary token index. Specifically, the adoption of a binary token index\nreduces index preparation time from 30 GPU hours to just 2 CPU hours and\nstorage size from 31 GB to 2 GB, achieving a 90% reduction compared to an\nembedding-based index.",
        "translated": ""
    },
    {
        "title": "Adaptive Retrieval and Scalable Indexing for k-NN Search with\n  Cross-Encoders",
        "url": "http://arxiv.org/abs/2405.03651v1",
        "pub_date": "2024-05-06",
        "summary": "Cross-encoder (CE) models which compute similarity by jointly encoding a\nquery-item pair perform better than embedding-based models (dual-encoders) at\nestimating query-item relevance. Existing approaches perform k-NN search with\nCE by approximating the CE similarity with a vector embedding space fit either\nwith dual-encoders (DE) or CUR matrix factorization. DE-based\nretrieve-and-rerank approaches suffer from poor recall on new domains and the\nretrieval with DE is decoupled from the CE. While CUR-based approaches can be\nmore accurate than the DE-based approach, they require a prohibitively large\nnumber of CE calls to compute item embeddings, thus making it impractical for\ndeployment at scale. In this paper, we address these shortcomings with our\nproposed sparse-matrix factorization based method that efficiently computes\nlatent query and item embeddings to approximate CE scores and performs k-NN\nsearch with the approximate CE similarity. We compute item embeddings offline\nby factorizing a sparse matrix containing query-item CE scores for a set of\ntrain queries. Our method produces a high-quality approximation while requiring\nonly a fraction of CE calls as compared to CUR-based methods, and allows for\nleveraging DE to initialize the embedding space while avoiding compute- and\nresource-intensive finetuning of DE via distillation. At test time, the item\nembeddings remain fixed and retrieval occurs over rounds, alternating between\na) estimating the test query embedding by minimizing error in approximating CE\nscores of items retrieved thus far, and b) using the updated test query\nembedding for retrieving more items. Our k-NN search method improves recall by\nup to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our\nindexing approach achieves a speedup of up to 100x over CUR-based and 5x over\nDE distillation methods, while matching or improving k-NN search recall over\nbaselines.",
        "translated": ""
    },
    {
        "title": "ID-centric Pre-training for Recommendation",
        "url": "http://arxiv.org/abs/2405.03562v1",
        "pub_date": "2024-05-06",
        "summary": "Classical sequential recommendation models generally adopt ID embeddings to\nstore knowledge learned from user historical behaviors and represent items.\nHowever, these unique IDs are challenging to be transferred to new domains.\nWith the thriving of pre-trained language model (PLM), some pioneer works adopt\nPLM for pre-trained recommendation, where modality information (e.g., text) is\nconsidered universal across domains via PLM. Unfortunately, the behavioral\ninformation in ID embeddings is still verified to be dominating in PLM-based\nrecommendation models compared to modality information and thus limits these\nmodels' performance. In this work, we propose a novel ID-centric recommendation\npre-training paradigm (IDP), which directly transfers informative ID embeddings\nlearned in pre-training domains to item representations in new domains.\nSpecifically, in pre-training stage, besides the ID-based sequential model for\nrecommendation, we also build a Cross-domain ID-matcher (CDIM) learned by both\nbehavioral and modality information. In the tuning stage, modality information\nof new domain items is regarded as a cross-domain bridge built by CDIM. We\nfirst leverage the textual information of downstream domain items to retrieve\nbehaviorally and semantically similar items from pre-training domains using\nCDIM. Next, these retrieved pre-trained ID embeddings, rather than certain\ntextual embeddings, are directly adopted to generate downstream new items'\nembeddings. Through extensive experiments on real-world datasets, both in cold\nand warm settings, we demonstrate that our proposed model significantly\noutperforms all baselines. Codes will be released upon acceptance.",
        "translated": ""
    },
    {
        "title": "Doing Personal LAPS: LLM-Augmented Dialogue Construction for\n  Personalized Multi-Session Conversational Search",
        "url": "http://arxiv.org/abs/2405.03480v1",
        "pub_date": "2024-05-06",
        "summary": "The future of conversational agents will provide users with personalized\ninformation responses. However, a significant challenge in developing models is\nthe lack of large-scale dialogue datasets that span multiple sessions and\nreflect real-world user preferences. Previous approaches rely on experts in a\nwizard-of-oz setup that is difficult to scale, particularly for personalized\ntasks. Our method, LAPS, addresses this by using large language models (LLMs)\nto guide a single human worker in generating personalized dialogues. This\nmethod has proven to speed up the creation process and improve quality. LAPS\ncan collect large-scale, human-written, multi-session, and multi-domain\nconversations, including extracting user preferences. When compared to existing\ndatasets, LAPS-produced conversations are as natural and diverse as\nexpert-created ones, which stays in contrast with fully synthetic methods. The\ncollected dataset is suited to train preference extraction and personalized\nresponse generation. Our results show that responses generated explicitly using\nextracted preferences better match user's actual preferences, highlighting the\nvalue of using extracted preferences over simple dialogue history. Overall,\nLAPS introduces a new method to leverage LLMs to create realistic personalized\nconversational data more efficiently and effectively than previous methods.",
        "translated": ""
    },
    {
        "title": "Improving (Re-)Usability of Musical Datasets: An Overview of the DOREMUS\n  Project",
        "url": "http://arxiv.org/abs/2405.03382v1",
        "pub_date": "2024-05-06",
        "summary": "DOREMUS works on a better description of music by building new tools to link\nand explore the data of three French institutions. This paper gives an overview\nof the data model based on FRBRoo, explains the conversion and linking\nprocesses using linked data technologies and presents the prototypes created to\nconsume the data according to the web users' needs.",
        "translated": ""
    },
    {
        "title": "MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language\n  Models in the Context of the Pediatric Hypertension Guideline",
        "url": "http://arxiv.org/abs/2405.03359v1",
        "pub_date": "2024-05-06",
        "summary": "This research focuses on evaluating the non-commercial open-source large\nlanguage models (LLMs) Meditron, MedAlpaca, Mistral, and Llama-2 for their\nefficacy in interpreting medical guidelines saved in PDF format. As a specific\ntest scenario, we applied these models to the guidelines for hypertension in\nchildren and adolescents provided by the European Society of Cardiology (ESC).\nLeveraging Streamlit, a Python library, we developed a user-friendly medical\ndocument chatbot tool (MedDoc-Bot). This tool enables authorized users to\nupload PDF files and pose questions, generating interpretive responses from\nfour locally stored LLMs. A pediatric expert provides a benchmark for\nevaluation by formulating questions and responses extracted from the ESC\nguidelines. The expert rates the model-generated responses based on their\nfidelity and relevance. Additionally, we evaluated the METEOR and chrF metric\nscores to assess the similarity of model responses to reference answers. Our\nstudy found that Llama-2 and Mistral performed well in metrics evaluation.\nHowever, Llama-2 was slower when dealing with text and tabular data. In our\nhuman evaluation, we observed that responses created by Mistral, Meditron, and\nLlama-2 exhibited reasonable fidelity and relevance. This study provides\nvaluable insights into the strengths and limitations of LLMs for future\ndevelopments in medical document interpretation. Open-Source Code:\nhttps://github.com/yaseen28/MedDoc-Bot",
        "translated": ""
    },
    {
        "title": "Explainability for Transparent Conversational Information-Seeking",
        "url": "http://arxiv.org/abs/2405.03303v1",
        "pub_date": "2024-05-06",
        "summary": "The increasing reliance on digital information necessitates advancements in\nconversational search systems, particularly in terms of information\ntransparency. While prior research in conversational information-seeking has\nconcentrated on improving retrieval techniques, the challenge remains in\ngenerating responses useful from a user perspective. This study explores\ndifferent methods of explaining the responses, hypothesizing that transparency\nabout the source of the information, system confidence, and limitations can\nenhance users' ability to objectively assess the response. By exploring\ntransparency across explanation type, quality, and presentation mode, this\nresearch aims to bridge the gap between system-generated responses and\nresponses verifiable by the user. We design a user study to answer questions\nconcerning the impact of (1) the quality of explanations enhancing the response\non its usefulness and (2) ways of presenting explanations to users. The\nanalysis of the collected data reveals lower user ratings for noisy\nexplanations, although these scores seem insensitive to the quality of the\nresponse. Inconclusive results on the explanations presentation format suggest\nthat it may not be a critical factor in this setting.",
        "translated": ""
    },
    {
        "title": "Characterizing the Dilemma of Performance and Index Size in\n  Billion-Scale Vector Search and Breaking It with Second-Tier Memory",
        "url": "http://arxiv.org/abs/2405.03267v1",
        "pub_date": "2024-05-06",
        "summary": "Vector searches on large-scale datasets are critical to modern online\nservices like web search and RAG, which necessity storing the datasets and\ntheir index on the secondary storage like SSD. In this paper, we are the first\nto characterize the trade-off of performance and index size in existing\nSSD-based graph and cluster indexes: to improve throughput by {5.7\\,$\\times$}\nand {1.7\\,$\\times$}, these indexes have to pay a {5.8\\,$\\times$} storage\namplification and {7.7\\,$\\times$} with respect to the dataset size,\nrespectively. The root cause is that the coarse-grained access of SSD\nmismatches the fine-grained random read required by vector indexes with small\namplification.\n  This paper argues that second-tier memory, such as remote DRAM/NVM connected\nvia RDMA or CXL, is a powerful storage for addressing the problem from a\nsystem's perspective, thanks to its fine-grained access granularity. However,\nputting existing indexes -- primarily designed for SSD -- directly on\nsecond-tier memory cannot fully utilize its power. Meanwhile, second-tier\nmemory still behaves more like storage, so using it as DRAM is also\ninefficient. To this end, we build a graph and cluster index that centers\naround the performance features of second-tier memory. With careful execution\nengine and index layout designs, we show that vector indexes can achieve\noptimal performance with orders of magnitude smaller index amplification, on a\nvariety of second-tier memory devices.\n  Based on our improved graph and vector indexes on second-tier memory, we\nfurther conduct a systematic study between them to facilitate developers\nchoosing the right index for their workloads. Interestingly, the findings on\nthe second-tier memory contradict the ones on SSDs.",
        "translated": ""
    },
    {
        "title": "TF4CTR: Twin Focus Framework for CTR Prediction via Adaptive Sample\n  Differentiation",
        "url": "http://arxiv.org/abs/2405.03167v1",
        "pub_date": "2024-05-06",
        "summary": "Effective feature interaction modeling is critical for enhancing the accuracy\nof click-through rate (CTR) prediction in industrial recommender systems. Most\nof the current deep CTR models resort to building complex network architectures\nto better capture intricate feature interactions or user behaviors. However, we\nidentify two limitations in these models: (1) the samples given to the model\nare undifferentiated, which may lead the model to learn a larger number of easy\nsamples in a single-minded manner while ignoring a smaller number of hard\nsamples, thus reducing the model's generalization ability; (2) differentiated\nfeature interaction encoders are designed to capture different interactions\ninformation but receive consistent supervision signals, thereby limiting the\neffectiveness of the encoder. To bridge the identified gaps, this paper\nintroduces a novel CTR prediction framework by integrating the plug-and-play\nTwin Focus (TF) Loss, Sample Selection Embedding Module (SSEM), and Dynamic\nFusion Module (DFM), named the Twin Focus Framework for CTR (TF4CTR).\nSpecifically, the framework employs the SSEM at the bottom of the model to\ndifferentiate between samples, thereby assigning a more suitable encoder for\neach sample. Meanwhile, the TF Loss provides tailored supervision signals to\nboth simple and complex encoders. Moreover, the DFM dynamically fuses the\nfeature interaction information captured by the encoders, resulting in more\naccurate predictions. Experiments on five real-world datasets confirm the\neffectiveness and compatibility of the framework, demonstrating its capacity to\nenhance various representative baselines in a model-agnostic manner. To\nfacilitate reproducible research, our open-sourced code and detailed running\nlogs will be made available at: https://github.com/salmon1802/TF4CTR.",
        "translated": ""
    },
    {
        "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
        "url": "http://arxiv.org/abs/2405.03110v1",
        "pub_date": "2024-05-06",
        "summary": "Vector quantization, renowned for its unparalleled feature compression\ncapabilities, has been a prominent topic in signal processing and machine\nlearning research for several decades and remains widely utilized today. With\nthe emergence of large models and generative AI, vector quantization has gained\npopularity in recommender systems, establishing itself as a preferred solution.\nThis paper starts with a comprehensive review of vector quantization\ntechniques. It then explores systematic taxonomies of vector quantization\nmethods for recommender systems (VQ4Rec), examining their applications from\nmultiple perspectives. Further, it provides a thorough introduction to research\nefforts in diverse recommendation scenarios, including efficiency-oriented\napproaches and quality-oriented approaches. Finally, the survey analyzes the\nremaining challenges and anticipates future trends in VQ4Rec, including the\nchallenges associated with the training of vector quantization, the\nopportunities presented by large language models, and emerging trends in\nmultimodal recommender systems. We hope this survey can pave the way for future\nresearchers in the recommendation community and accelerate their exploration in\nthis promising field.",
        "translated": ""
    },
    {
        "title": "iSEARLE: Improving Textual Inversion for Zero-Shot Composed Image\n  Retrieval",
        "url": "http://arxiv.org/abs/2405.02951v1",
        "pub_date": "2024-05-05",
        "summary": "Given a query consisting of a reference image and a relative caption,\nComposed Image Retrieval (CIR) aims to retrieve target images visually similar\nto the reference one while incorporating the changes specified in the relative\ncaption. The reliance of supervised methods on labor-intensive manually labeled\ndatasets hinders their broad applicability. In this work, we introduce a new\ntask, Zero-Shot CIR (ZS-CIR), that addresses CIR without the need for a labeled\ntraining dataset. We propose an approach named iSEARLE (improved zero-Shot\ncomposEd imAge Retrieval with textuaL invErsion) that involves mapping the\nvisual information of the reference image into a pseudo-word token in CLIP\ntoken embedding space and combining it with the relative caption. To foster\nresearch on ZS-CIR, we present an open-domain benchmarking dataset named CIRCO\n(Composed Image Retrieval on Common Objects in context), the first CIR dataset\nwhere each query is labeled with multiple ground truths and a semantic\ncategorization. The experimental results illustrate that iSEARLE obtains\nstate-of-the-art performance on three different CIR datasets -- FashionIQ,\nCIRR, and the proposed CIRCO -- and two additional evaluation settings, namely\ndomain conversion and object composition. The dataset, the code, and the model\nare publicly available at https://github.com/miccunifi/SEARLE.",
        "translated": ""
    },
    {
        "title": "BBK: a simpler, faster algorithm for enumerating maximal bicliques in\n  large sparse bipartite graphs",
        "url": "http://arxiv.org/abs/2405.04428v1",
        "pub_date": "2024-05-07",
        "summary": "Bipartite graphs are a prevalent modeling tool for real-world networks,\ncapturing interactions between vertices of two different types. Within this\nframework, bicliques emerge as crucial structures when studying dense\nsubgraphs: they are sets of vertices such that all vertices of the first type\ninteract with all vertices of the second type. Therefore, they allow\nidentifying groups of closely related vertices of the network, such as\nindividuals with similar interests or webpages with similar contents. This\narticle introduces a new algorithm designed for the exhaustive enumeration of\nmaximal bicliques within a bipartite graph. This algorithm, called BBK for\nBipartite Bron-Kerbosch, is a new extension to the bipartite case of the\nBron-Kerbosch algorithm, which enumerates the maximal cliques in standard\n(non-bipartite) graphs. It is faster than the state-of-the-art algorithms and\nallows the enumeration on massive bipartite graphs that are not manageable with\nexisting implementations. We analyze it theoretically to establish two\ncomplexity formulas: one as a function of the input and one as a function of\nthe output characteristics of the algorithm. We also provide an open-access\nimplementation of BBK in C++, which we use to experiment and validate its\nefficiency on massive real-world datasets and show that its execution time is\nshorter in practice than state-of-the art algorithms. These experiments also\nshow that the order in which the vertices are processed, as well as the choice\nof one of the two types of vertices on which to initiate the enumeration have\nan impact on the computation time.",
        "translated": ""
    },
    {
        "title": "Dataset and Models for Item Recommendation Using Multi-Modal User\n  Interactions",
        "url": "http://arxiv.org/abs/2405.04246v1",
        "pub_date": "2024-05-07",
        "summary": "While recommender systems with multi-modal item representations (image,\naudio, and text), have been widely explored, learning recommendations from\nmulti-modal user interactions (e.g., clicks and speech) remains an open\nproblem. We study the case of multi-modal user interactions in a setting where\nusers engage with a service provider through multiple channels (website and\ncall center). In such cases, incomplete modalities naturally occur, since not\nall users interact through all the available channels. To address these\nchallenges, we publish a real-world dataset that allows progress in this\nunder-researched area. We further present and benchmark various methods for\nleveraging multi-modal user interactions for item recommendations, and propose\na novel approach that specifically deals with missing modalities by mapping\nuser interactions to a common feature space. Our analysis reveals important\ninteractions between the different modalities and that a frequently occurring\nmodality can enhance learning from a less frequent one.",
        "translated": ""
    },
    {
        "title": "Masked Graph Transformer for Large-Scale Recommendation",
        "url": "http://arxiv.org/abs/2405.04028v1",
        "pub_date": "2024-05-07",
        "summary": "Graph Transformers have garnered significant attention for learning\ngraph-structured data, thanks to their superb ability to capture long-range\ndependencies among nodes. However, the quadratic space and time complexity\nhinders the scalability of Graph Transformers, particularly for large-scale\nrecommendation. Here we propose an efficient Masked Graph Transformer, named\nMGFormer, capable of capturing all-pair interactions among nodes with a linear\ncomplexity. To achieve this, we treat all user/item nodes as independent\ntokens, enhance them with positional embeddings, and feed them into a\nkernelized attention module. Additionally, we incorporate learnable relative\ndegree information to appropriately reweigh the attentions. Experimental\nresults show the superior performance of our MGFormer, even with a single\nattention layer.",
        "translated": ""
    },
    {
        "title": "Knowledge Adaptation from Large Language Model to Recommendation for\n  Practical Industrial Application",
        "url": "http://arxiv.org/abs/2405.03988v1",
        "pub_date": "2024-05-07",
        "summary": "Contemporary recommender systems predominantly rely on collaborative\nfiltering techniques, employing ID-embedding to capture latent associations\namong users and items. However, this approach overlooks the wealth of semantic\ninformation embedded within textual descriptions of items, leading to\nsuboptimal performance in cold-start scenarios and long-tail user\nrecommendations. Leveraging the capabilities of Large Language Models (LLMs)\npretrained on massive text corpus presents a promising avenue for enhancing\nrecommender systems by integrating open-world domain knowledge. In this paper,\nwe propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework\nthat synergizes open-world knowledge with collaborative knowledge. We address\ncomputational complexity concerns by utilizing pretrained LLMs as item encoders\nand freezing LLM parameters to avoid catastrophic forgetting and preserve\nopen-world knowledge. To bridge the gap between the open-world and\ncollaborative domains, we design a twin-tower structure supervised by the\nrecommendation task and tailored for practical industrial application. Through\noffline experiments on the large-scale industrial dataset and online\nexperiments on A/B tests, we demonstrate the efficacy of our approach.",
        "translated": ""
    },
    {
        "title": "Contextualization with SPLADE for High Recall Retrieval",
        "url": "http://arxiv.org/abs/2405.03972v1",
        "pub_date": "2024-05-07",
        "summary": "High Recall Retrieval (HRR), such as eDiscovery and medical systematic\nreview, is a search problem that optimizes the cost of retrieving most relevant\ndocuments in a given collection. Iterative approaches, such as iterative\nrelevance feedback and uncertainty sampling, are shown to be effective under\nvarious operational scenarios. Despite neural models demonstrating success in\nother text-related tasks, linear models such as logistic regression, in\ngeneral, are still more effective and efficient in HRR since the model is\ntrained and retrieves documents from the same fixed collection. In this work,\nwe leverage SPLADE, an efficient retrieval model that transforms documents into\ncontextualized sparse vectors, for HRR. Our approach combines the best of both\nworlds, leveraging both the contextualization from pretrained language models\nand the efficiency of linear models. It reduces 10% and 18% of the review cost\nin two HRR evaluation collections under a one-phase review workflow with a\ntarget recall of 80%. The experiment is implemented with TARexp and is\navailable at https://github.com/eugene-yang/LSR-for-TAR.",
        "translated": ""
    },
    {
        "title": "The Fault in Our Recommendations: On the Perils of Optimizing the\n  Measurable",
        "url": "http://arxiv.org/abs/2405.03948v1",
        "pub_date": "2024-05-07",
        "summary": "Recommendation systems are widespread, and through customized\nrecommendations, promise to match users with options they will like. To that\nend, data on engagement is collected and used. Most recommendation systems are\nranking-based, where they rank and recommend items based on their predicted\nengagement. However, the engagement signals are often only a crude proxy for\nutility, as data on the latter is rarely collected or available. This paper\nexplores the following question: By optimizing for measurable proxies, are\nrecommendation systems at risk of significantly under-delivering on utility? If\nso, how can one improve utility which is seldom measured? To study these\nquestions, we introduce a model of repeated user consumption in which, at each\ninteraction, users select between an outside option and the best option from a\nrecommendation set. Our model accounts for user heterogeneity, with the\nmajority preferring ``popular'' content, and a minority favoring ``niche''\ncontent. The system initially lacks knowledge of individual user preferences\nbut can learn them through observations of users' choices over time. Our\ntheoretical and numerical analysis demonstrate that optimizing for engagement\ncan lead to significant utility losses. Instead, we propose a utility-aware\npolicy that initially recommends a mix of popular and niche content. As the\nplatform becomes more forward-looking, our utility-aware policy achieves the\nbest of both worlds: near-optimal utility and near-optimal engagement\nsimultaneously. Our study elucidates an important feature of recommendation\nsystems; given the ability to suggest multiple items, one can perform\nsignificant exploration without incurring significant reductions in engagement.\nBy recommending high-risk, high-reward items alongside popular items, systems\ncan enhance discovery of high utility items without significantly affecting\nengagement.",
        "translated": ""
    },
    {
        "title": "GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation",
        "url": "http://arxiv.org/abs/2405.03764v1",
        "pub_date": "2024-05-06",
        "summary": "Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. For practical\ndeployment, it is critical to carry out knowledge distillation to preserve high\nperformance under computational constraints. In this paper, we address a key\nquestion: given the importance of unsupervised distillation for student\nperformance, how does one effectively ensemble knowledge from multiple teachers\nat this stage without the guidance of ground-truth labels? We propose a novel\nalgorithm, GOVERN, to tackle this issue. GOVERN has demonstrated significant\nimprovements in both offline and online experiments. The proposed algorithm has\nbeen successfully deployed in a real-world commercial question-answering\nsystem.",
        "translated": ""
    },
    {
        "title": "myAURA: Personalized health library for epilepsy management via\n  knowledge graph sparsification and visualization",
        "url": "http://arxiv.org/abs/2405.05229v1",
        "pub_date": "2024-05-08",
        "summary": "Objective: We report the development of the patient-centered myAURA\napplication and suite of methods designed to aid epilepsy patients, caregivers,\nand researchers in making decisions about care and self-management.\n  Materials and Methods: myAURA rests on the federation of an unprecedented\ncollection of heterogeneous data resources relevant to epilepsy, such as\nbiomedical databases, social media, and electronic health records. A\ngeneralizable, open-source methodology was developed to compute a multi-layer\nknowledge graph linking all this heterogeneous data via the terms of a\nhuman-centered biomedical dictionary.\n  Results: The power of the approach is first exemplified in the study of the\ndrug-drug interaction phenomenon. Furthermore, we employ a novel network\nsparsification methodology using the metric backbone of weighted graphs, which\nreveals the most important edges for inference, recommendation, and\nvisualization, such as pharmacology factors patients discuss on social media.\nThe network sparsification approach also allows us to extract focused digital\ncohorts from social media whose discourse is more relevant to epilepsy or other\nbiomedical problems. Finally, we present our patient-centered design and\npilot-testing of myAURA, including its user interface, based on focus groups\nand other stakeholder input.\n  Discussion: The ability to search and explore myAURA's heterogeneous data\nsources via a sparsified multi-layer knowledge graph, as well as the\ncombination of those layers in a single map, are useful features for\nintegrating relevant information for epilepsy.\n  Conclusion: Our stakeholder-driven, scalable approach to integrate\ntraditional and non-traditional data sources, enables biomedical discovery and\ndata-powered patient self-management in epilepsy, and is generalizable to other\nchronic conditions.",
        "translated": ""
    },
    {
        "title": "Graded Relevance Scoring of Written Essays with Dense Retrieval",
        "url": "http://arxiv.org/abs/2405.05200v1",
        "pub_date": "2024-05-08",
        "summary": "Automated Essay Scoring automates the grading process of essays, providing a\ngreat advantage for improving the writing proficiency of students. While\nholistic essay scoring research is prevalent, a noticeable gap exists in\nscoring essays for specific quality traits. In this work, we focus on the\nrelevance trait, which measures the ability of the student to stay on-topic\nthroughout the entire essay. We propose a novel approach for graded relevance\nscoring of written essays that employs dense retrieval encoders. Dense\nrepresentations of essays at different relevance levels then form clusters in\nthe embeddings space, such that their centroids are potentially separate enough\nto effectively represent their relevance levels. We hence use the simple\n1-Nearest-Neighbor classification over those centroids to determine the\nrelevance level of an unseen essay. As an effective unsupervised dense encoder,\nwe leverage Contriever, which is pre-trained with contrastive learning and\ndemonstrated comparable performance to supervised dense retrieval models. We\ntested our approach on both task-specific (i.e., training and testing on same\ntask) and cross-task (i.e., testing on unseen task) scenarios using the widely\nused ASAP++ dataset. Our method establishes a new state-of-the-art performance\nin the task-specific scenario, while its extension for the cross-task scenario\nexhibited a performance that is on par with the state-of-the-art model for that\nscenario. We also analyzed the performance of our approach in a more practical\nfew-shot scenario, showing that it can significantly reduce the labeling cost\nwhile sacrificing only 10% of its effectiveness.",
        "translated": ""
    },
    {
        "title": "Impact of Tone-Aware Explanations in Recommender Systems",
        "url": "http://arxiv.org/abs/2405.05061v1",
        "pub_date": "2024-05-08",
        "summary": "In recommender systems, the presentation of explanations plays a crucial role\nin supporting users' decision-making processes. Although numerous existing\nstudies have focused on the effects (transparency or persuasiveness) of\nexplanation content, explanation expression is largely overlooked. Tone, such\nas formal and humorous, is directly linked to expressiveness and is an\nimportant element in human communication. However, studies on the impact of\ntone on explanations within the context of recommender systems are\ninsufficient. Therefore, this study investigates the effect of explanation\ntones through an online user study from three aspects: perceived effects,\ndomain differences, and user attributes. We create a dataset using a large\nlanguage model to generate fictional items and explanations with various tones\nin the domain of movies, hotels, and home products. Collected data analysis\nreveals different perceived effects of tones depending on the domains.\nMoreover, user attributes such as age and personality traits are found to\ninfluence the impact of tone. This research underscores the critical role of\ntones in explanations within recommender systems, suggesting that attention to\ntone can enhance user experience.",
        "translated": ""
    },
    {
        "title": "Dual-domain Collaborative Denoising for Social Recommendation",
        "url": "http://arxiv.org/abs/2405.04942v1",
        "pub_date": "2024-05-08",
        "summary": "Social recommendation leverages social network to complement user-item\ninteraction data for recommendation task, aiming to mitigate the data sparsity\nissue in recommender systems. However, existing social recommendation methods\nencounter the following challenge: both social network and interaction data\ncontain substaintial noise, and the propagation of such noise through Graph\nNeural Networks (GNNs) not only fails to enhance recommendation performance but\nmay also interfere with the model's normal training. Despite the importance of\ndenoising for social network and interaction data, only a limited number of\nstudies have considered the denoising for social network and all of them\noverlook that for interaction data, hindering the denoising effect and\nrecommendation performance. Based on this, we propose a novel model called\nDual-domain Collaborative Denoising for Social Recommendation\n($\\textbf{DCDSR}$). DCDSR comprises two primary modules: the structure-level\ncollaborative denoising module and the embedding-space collaborative denoising\nmodule. In the structure-level collaborative denoising module, information from\ninteraction domain is first employed to guide social network denoising.\nSubsequently, the denoised social network is used to supervise the denoising\nfor interaction data. The embedding-space collaborative denoising module\ndevotes to resisting the noise cross-domain diffusion problem through\ncontrastive learning with dual-domain embedding collaborative perturbation.\nAdditionally, a novel contrastive learning strategy, named Anchor-InfoNCE, is\nintroduced to better harness the denoising capability of contrastive learning.\nEvaluating our model on three real-world datasets verifies that DCDSR has a\nconsiderable denoising effect, thus outperforms the state-of-the-art social\nrecommendation methods.",
        "translated": ""
    },
    {
        "title": "Enabling Roll-up and Drill-down Operations in News Exploration with\n  Knowledge Graphs for Due Diligence and Risk Management",
        "url": "http://arxiv.org/abs/2405.04929v1",
        "pub_date": "2024-05-08",
        "summary": "Efficient news exploration is crucial in real-world applications,\nparticularly within the financial sector, where numerous control and risk\nassessment tasks rely on the analysis of public news reports. The current\nprocesses in this domain predominantly rely on manual efforts, often involving\nkeywordbased searches and the compilation of extensive keyword lists. In this\npaper, we introduce NCEXPLORER, a framework designed with OLAP-like operations\nto enhance the news exploration experience. NCEXPLORER empowers users to use\nroll-up operations for a broader content overview and drill-down operations for\ndetailed insights. These operations are achieved through integration with\nexternal knowledge graphs (KGs), encompassing both fact-based and\nontology-based structures. This integration significantly augments exploration\ncapabilities, offering a more comprehensive and efficient approach to unveiling\nthe underlying structures and nuances embedded in news content. Extensive\nempirical studies through master-qualified evaluators on Amazon Mechanical Turk\ndemonstrate NCEXPLORER's superiority over existing state-of-the-art news search\nmethodologies across an array of topic domains, using real-world news datasets.",
        "translated": ""
    },
    {
        "title": "Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems",
        "url": "http://arxiv.org/abs/2405.04844v1",
        "pub_date": "2024-05-08",
        "summary": "The Probability Ranking Principle (PRP) has been considered as the\nfoundational standard in the design of information retrieval (IR) systems. The\nprinciple requires an IR module's returned list of results to be ranked with\nrespect to the underlying user interests, so as to maximize the results'\nutility.\n  Nevertheless, we point out that it is inappropriate to indiscriminately apply\nPRP through every stage of a contemporary IR system. Such systems contain\nmultiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages,\nas examined in this paper). The \\emph{selection bias} inherent in the model of\neach stage significantly influences the results that are ultimately presented\nto users.\n  To address this issue, we propose an improved ranking principle for\nmulti-stage systems, namely the Generalized Probability Ranking Principle\n(GPRP), to emphasize both the selection bias in each stage of the system\npipeline as well as the underlying interest of users.\n  We realize GPRP via a unified algorithmic framework named Full Stage Learning\nto Rank. Our core idea is to first estimate the selection bias in the\nsubsequent stages and then learn a ranking model that best complies with the\ndownstream modules' selection bias so as to deliver its top ranked results to\nthe final ranked list in the system's output.\n  We performed extensive experiment evaluations of our developed Full Stage\nLearning to Rank solution, using both simulations and online A/B tests in one\nof the leading short-video recommendation platforms. The algorithm is proved to\nbe effective in both retrieval and ranking stages. Since deployed, the\nalgorithm has brought consistent and significant performance gain to the\nplatform.",
        "translated": ""
    },
    {
        "title": "Federated Adaptation for Foundation Model-based Recommendations",
        "url": "http://arxiv.org/abs/2405.04840v1",
        "pub_date": "2024-05-08",
        "summary": "With the recent success of large language models, particularly foundation\nmodels with generalization abilities, applying foundation models for\nrecommendations becomes a new paradigm to improve existing recommendation\nsystems. It becomes a new open challenge to enable the foundation model to\ncapture user preference changes in a timely manner with reasonable\ncommunication and computation costs while preserving privacy. This paper\nproposes a novel federated adaptation mechanism to enhance the foundation\nmodel-based recommendation system in a privacy-preserving manner. Specifically,\neach client will learn a lightweight personalized adapter using its private\ndata. The adapter then collaborates with pre-trained foundation models to\nprovide recommendation service efficiently with fine-grained manners.\nImportantly, users' private behavioral data remains secure as it is not shared\nwith the server. This data localization-based privacy preservation is embodied\nvia the federated learning framework. The model can ensure that shared\nknowledge is incorporated into all adapters while simultaneously preserving\neach user's personal preferences. Experimental results on four benchmark\ndatasets demonstrate our method's superior performance. Implementation code is\navailable to ease reproducibility.",
        "translated": ""
    },
    {
        "title": "Hypergraph-enhanced Dual Semi-supervised Graph Classification",
        "url": "http://arxiv.org/abs/2405.04773v1",
        "pub_date": "2024-05-08",
        "summary": "In this paper, we study semi-supervised graph classification, which aims at\naccurately predicting the categories of graphs in scenarios with limited\nlabeled graphs and abundant unlabeled graphs. Despite the promising capability\nof graph neural networks (GNNs), they typically require a large number of\ncostly labeled graphs, while a wealth of unlabeled graphs fail to be\neffectively utilized. Moreover, GNNs are inherently limited to encoding local\nneighborhood information using message-passing mechanisms, thus lacking the\nability to model higher-order dependencies among nodes. To tackle these\nchallenges, we propose a Hypergraph-Enhanced DuAL framework named HEAL for\nsemi-supervised graph classification, which captures graph semantics from the\nperspective of the hypergraph and the line graph, respectively. Specifically,\nto better explore the higher-order relationships among nodes, we design a\nhypergraph structure learning to adaptively learn complex node dependencies\nbeyond pairwise relations. Meanwhile, based on the learned hypergraph, we\nintroduce a line graph to capture the interaction between hyperedges, thereby\nbetter mining the underlying semantic structures. Finally, we develop a\nrelational consistency learning to facilitate knowledge transfer between the\ntwo branches and provide better mutual guidance. Extensive experiments on\nreal-world graph datasets verify the effectiveness of the proposed method\nagainst existing state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "SVD-AE: Simple Autoencoders for Collaborative Filtering",
        "url": "http://arxiv.org/abs/2405.04746v1",
        "pub_date": "2024-05-08",
        "summary": "Collaborative filtering (CF) methods for recommendation systems have been\nextensively researched, ranging from matrix factorization and autoencoder-based\nto graph filtering-based methods. Recently, lightweight methods that require\nalmost no training have been recently proposed to reduce overall computation.\nHowever, existing methods still have room to improve the trade-offs among\naccuracy, efficiency, and robustness. In particular, there are no well-designed\nclosed-form studies for \\emph{balanced} CF in terms of the aforementioned\ntrade-offs. In this paper, we design SVD-AE, a simple yet effective singular\nvector decomposition (SVD)-based linear autoencoder, whose closed-form solution\ncan be defined based on SVD for CF. SVD-AE does not require iterative training\nprocesses as its closed-form solution can be calculated at once. Furthermore,\ngiven the noisy nature of the rating matrix, we explore the robustness against\nsuch noisy interactions of existing CF methods and our SVD-AE. As a result, we\ndemonstrate that our simple design choice based on truncated SVD can be used to\nstrengthen the noise robustness of the recommendation while improving\nefficiency. Code is available at https://github.com/seoyoungh/svd-ae.",
        "translated": ""
    },
    {
        "title": "Cryptanalysis of the SIMON Cypher Using Neo4j",
        "url": "http://arxiv.org/abs/2405.04735v1",
        "pub_date": "2024-05-08",
        "summary": "The exponential growth in the number of Internet of Things (IoT) devices has\nseen the introduction of several Lightweight Encryption Algorithms (LEA). While\nLEAs are designed to enhance the integrity, privacy and security of data\ncollected and transmitted by IoT devices, it is hazardous to assume that all\nLEAs are secure and exhibit similar levels of protection. To improve encryption\nstrength, cryptanalysts and algorithm designers routinely probe LEAs using\nvarious cryptanalysis techniques to identify vulnerabilities and limitations of\nLEAs. Despite recent improvements in the efficiency of cryptanalysis utilising\nheuristic methods and a Partial Difference Distribution Table (PDDT), the\nprocess remains inefficient, with the random nature of the heuristic inhibiting\nreproducible results. However, the use of a PDDT presents opportunities to\nidentify relationships between differentials utilising knowledge graphs,\nleading to the identification of efficient paths throughout the PDDT. This\npaper introduces the novel use of knowledge graphs to identify intricate\nrelationships between differentials in the SIMON LEA, allowing for the\nidentification of optimal paths throughout the differentials, and increasing\nthe effectiveness of the differential security analyses of SIMON.",
        "translated": ""
    },
    {
        "title": "Optimal Baseline Corrections for Off-Policy Contextual Bandits",
        "url": "http://arxiv.org/abs/2405.05736v1",
        "pub_date": "2024-05-09",
        "summary": "The off-policy learning paradigm allows for recommender systems and general\nranking applications to be framed as decision-making problems, where we aim to\nlearn decision policies that optimize an unbiased offline estimate of an online\nreward metric. With unbiasedness comes potentially high variance, and prevalent\nmethods exist to reduce estimation variance. These methods typically make use\nof control variates, either additive (i.e., baseline corrections or doubly\nrobust methods) or multiplicative (i.e., self-normalisation). Our work unifies\nthese approaches by proposing a single framework built on their equivalence in\nlearning scenarios. The foundation of our framework is the derivation of an\nequivalent baseline correction for all of the existing control variates.\nConsequently, our framework enables us to characterize the variance-optimal\nunbiased estimator and provide a closed-form solution for it. This optimal\nestimator brings significantly improved performance in both evaluation and\nlearning, and minimizes data requirements. Empirical observations corroborate\nour theoretical findings.",
        "translated": ""
    },
    {
        "title": "Computational lexical analysis of Flamenco genres",
        "url": "http://arxiv.org/abs/2405.05723v1",
        "pub_date": "2024-05-09",
        "summary": "Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of\nHumanity, is a profound expression of cultural identity rooted in Andalusia,\nSpain. However, there is a lack of quantitative studies that help identify\ncharacteristic patterns in this long-lived music tradition. In this work, we\npresent a computational analysis of Flamenco lyrics, employing natural language\nprocessing and machine learning to categorize over 2000 lyrics into their\nrespective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial\nNaive Bayes classifier, we find that lexical variation across styles enables to\naccurately identify distinct $\\textit{palos}$. More importantly, from an\nautomatic method of word usage, we obtain the semantic fields that characterize\neach style. Further, applying a metric that quantifies the inter-genre distance\nwe perform a network analysis that sheds light on the relationship between\nFlamenco styles. Remarkably, our results suggest historical connections and\n$\\textit{palo}$ evolutions. Overall, our work illuminates the intricate\nrelationships and cultural significance embedded within Flamenco lyrics,\ncomplementing previous qualitative discussions with quantitative analyses and\nsparking new discussions on the origin and development of traditional music\ngenres.",
        "translated": ""
    },
    {
        "title": "Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent\n  Pre-Ranking Model",
        "url": "http://arxiv.org/abs/2405.05606v1",
        "pub_date": "2024-05-09",
        "summary": "In large e-commerce platforms, search systems are typically composed of a\nseries of modules, including recall, pre-ranking, and ranking phases. The\npre-ranking phase, serving as a lightweight module, is crucial for filtering\nout the bulk of products in advance for the downstream ranking module.\nIndustrial efforts on optimizing the pre-ranking model have predominantly\nfocused on enhancing ranking consistency, model structure, and generalization\ntowards long-tail items. Beyond these optimizations, meeting the system\nperformance requirements presents a significant challenge. Contrasting with\nexisting industry works, we propose a novel method: a Generalizable and\nRAnk-ConsistEnt Pre-Ranking Model (GRACE), which achieves: 1) Ranking\nconsistency by introducing multiple binary classification tasks that predict\nwhether a product is within the top-k results as estimated by the ranking\nmodel, which facilitates the addition of learning objectives on common\npoint-wise ranking models; 2) Generalizability through contrastive learning of\nrepresentation for all products by pre-training on a subset of ranking product\nembeddings; 3) Ease of implementation in feature construction and online\ndeployment. Our extensive experiments demonstrate significant improvements in\nboth offline metrics and online A/B test: a 0.75% increase in AUC and a 1.28%\nincrease in CVR.",
        "translated": ""
    },
    {
        "title": "Can We Use Large Language Models to Fill Relevance Judgment Holes?",
        "url": "http://arxiv.org/abs/2405.05600v1",
        "pub_date": "2024-05-09",
        "summary": "Incomplete relevance judgments limit the re-usability of test collections.\nWhen new systems are compared against previous systems used to build the pool\nof judged documents, they often do so at a disadvantage due to the ``holes'' in\ntest collection (i.e., pockets of un-assessed documents returned by the new\nsystem). In this paper, we take initial steps towards extending existing test\ncollections by employing Large Language Models (LLM) to fill the holes by\nleveraging and grounding the method using existing human judgments. We explore\nthis problem in the context of Conversational Search using TREC iKAT, where\ninformation needs are highly dynamic and the responses (and, the results\nretrieved) are much more varied (leaving bigger holes). While previous work has\nshown that automatic judgments from LLMs result in highly correlated rankings,\nwe find substantially lower correlates when human plus automatic judgments are\nused (regardless of LLM, one/two/few shot, or fine-tuned). We further find\nthat, depending on the LLM employed, new runs will be highly favored (or\npenalized), and this effect is magnified proportionally to the size of the\nholes. Instead, one should generate the LLM annotations on the whole document\npool to achieve more consistent rankings with human-generated labels. Future\nwork is required to prompt engineering and fine-tuning LLMs to reflect and\nrepresent the human annotations, in order to ground and align the models, such\nthat they are more fit for purpose.",
        "translated": ""
    },
    {
        "title": "LayerPlexRank: Exploring Node Centrality and Layer Influence through\n  Algebraic Connectivity in Multiplex Networks",
        "url": "http://arxiv.org/abs/2405.05576v1",
        "pub_date": "2024-05-09",
        "summary": "As the calculation of centrality in complex networks becomes increasingly\nvital across technological, biological, and social systems, precise and\nscalable ranking methods are essential for understanding these networks. This\npaper introduces LayerPlexRank, an algorithm that simultaneously assesses node\ncentrality and layer influence in multiplex networks using algebraic\nconnectivity metrics. This method enhances the robustness of the ranking\nalgorithm by effectively assessing structural changes across layers using\nrandom walk, considering the overall connectivity of the graph. We substantiate\nthe utility of LayerPlexRank with theoretical analyses and empirical\nvalidations on varied real-world datasets, contrasting it with established\ncentrality measures.",
        "translated": ""
    },
    {
        "title": "Review-based Recommender Systems: A Survey of Approaches, Challenges and\n  Future Perspectives",
        "url": "http://arxiv.org/abs/2405.05562v1",
        "pub_date": "2024-05-09",
        "summary": "Recommender systems play a pivotal role in helping users navigate an\noverwhelming selection of products and services. On online platforms, users\nhave the opportunity to share feedback in various modes, including numerical\nratings, textual reviews, and likes/dislikes. Traditional recommendation\nsystems rely on users explicit ratings or implicit interactions (e.g. likes,\nclicks, shares, saves) to learn user preferences and item characteristics.\nBeyond these numerical ratings, textual reviews provide insights into users\nfine-grained preferences and item features. Analyzing these reviews is crucial\nfor enhancing the performance and interpretability of personalized\nrecommendation results. In recent years, review-based recommender systems have\nemerged as a significant sub-field in this domain. In this paper, we provide a\ncomprehensive overview of the developments in review-based recommender systems\nover recent years, highlighting the importance of reviews in recommender\nsystems, as well as the challenges associated with extracting features from\nreviews and integrating them into ratings. Specifically, we present a\ncategorization of these systems and summarize the state-of-the-art methods,\nanalyzing their unique features, effectiveness, and limitations. Finally, we\npropose potential directions for future research, including the integration of\nmulti-modal data, multi-criteria rating information, and ethical\nconsiderations.",
        "translated": ""
    },
    {
        "title": "Redefining Information Retrieval of Structured Database via Large\n  Language Models",
        "url": "http://arxiv.org/abs/2405.05508v1",
        "pub_date": "2024-05-09",
        "summary": "Retrieval augmentation is critical when Language Models (LMs) exploit\nnon-parametric knowledge related to the query through external knowledge bases\nbefore reasoning. The retrieved information is incorporated into LMs as context\nalongside the query, enhancing the reliability of responses towards factual\nquestions. Prior researches in retrieval augmentation typically follow a\nretriever-generator paradigm. In this context, traditional retrievers encounter\nchallenges in precisely and seamlessly extracting query-relevant information\nfrom knowledge bases. To address this issue, this paper introduces a novel\nretrieval augmentation framework called ChatLR that primarily employs the\npowerful semantic understanding ability of Large Language Models (LLMs) as\nretrievers to achieve precise and concise information retrieval. Additionally,\nwe construct an LLM-based search and question answering system tailored for the\nfinancial domain by fine-tuning LLM on two tasks including Text2API and API-ID\nrecognition. Experimental results demonstrate the effectiveness of ChatLR in\naddressing user queries, achieving an overall information retrieval accuracy\nexceeding 98.8\\%.",
        "translated": ""
    },
    {
        "title": "Information Extraction from Historical Well Records Using A Large\n  Language Model",
        "url": "http://arxiv.org/abs/2405.05438v1",
        "pub_date": "2024-05-08",
        "summary": "To reduce environmental risks and impacts from orphaned wells (abandoned oil\nand gas wells), it is essential to first locate and then plug these wells.\nAlthough some historical documents are available, they are often unstructured,\nnot cleaned, and outdated. Additionally, they vary widely by state and type.\nManual reading and digitizing this information from historical documents are\nnot feasible, given the high number of wells. Here, we propose a new\ncomputational approach for rapidly and cost-effectively locating these wells.\nSpecifically, we leverage the advanced capabilities of large language models\n(LLMs) to extract vital information including well location and depth from\nhistorical records of orphaned wells. In this paper, we present an information\nextraction workflow based on open-source Llama 2 models and test them on a\ndataset of 160 well documents. Our results show that the developed workflow\nachieves excellent accuracy in extracting location and depth from clean,\nPDF-based reports, with a 100% accuracy rate. However, it struggles with\nunstructured image-based well records, where accuracy drops to 70%. The\nworkflow provides significant benefits over manual human digitization,\nincluding reduced labor and increased automation. In general, more detailed\nprompting leads to improved information extraction, and those LLMs with more\nparameters typically perform better. We provided a detailed discussion of the\ncurrent challenges and the corresponding opportunities/approaches to address\nthem. Additionally, a vast amount of geoscientific information is locked up in\nold documents, and this work demonstrates that recent breakthroughs in LLMs\nenable us to unlock this information more broadly.",
        "translated": ""
    },
    {
        "title": "Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models",
        "url": "http://arxiv.org/abs/2405.05374v1",
        "pub_date": "2024-05-08",
        "summary": "This report describes the training dataset creation and recipe behind the\nfamily of \\texttt{arctic-embed} text embedding models (a set of five models\nranging from 22 to 334 million parameters with weights open-sourced under an\nApache-2 license). At the time of their release, each model achieved\nstate-of-the-art retrieval accuracy for models of their size on the MTEB\nRetrieval leaderboard, with the largest model, arctic-embed-l outperforming\nclosed source embedding models such as Cohere's embed-v3 and Open AI's\ntext-embed-3-large. In addition to the details of our training recipe, we have\nprovided several informative ablation studies, which we believe are the cause\nof our model performance.",
        "translated": ""
    },
    {
        "title": "ProCIS: A Benchmark for Proactive Retrieval in Conversations",
        "url": "http://arxiv.org/abs/2405.06460v1",
        "pub_date": "2024-05-10",
        "summary": "The field of conversational information seeking, which is rapidly gaining\ninterest in both academia and industry, is changing how we interact with search\nengines through natural language interactions. Existing datasets and methods\nare mostly evaluating reactive conversational information seeking systems that\nsolely provide response to every query from the user. We identify a gap in\nbuilding and evaluating proactive conversational information seeking systems\nthat can monitor a multi-party human conversation and proactively engage in the\nconversation at an opportune moment by retrieving useful resources and\nsuggestions. In this paper, we introduce a large-scale dataset for proactive\ndocument retrieval that consists of over 2.8 million conversations. We conduct\ncrowdsourcing experiments to obtain high-quality and relatively complete\nrelevance judgments through depth-k pooling. We also collect annotations\nrelated to the parts of the conversation that are related to each document,\nenabling us to evaluate proactive retrieval systems. We introduce normalized\nproactive discounted cumulative gain (npDCG) for evaluating these systems, and\nfurther provide benchmark results for a wide range of models, including a novel\nmodel we developed for this task. We believe that the developed dataset, called\nProCIS, paves the path towards developing proactive conversational information\nseeking systems.",
        "translated": ""
    },
    {
        "title": "Impedance vs. Power Side-channel Vulnerabilities: A Comparative Study",
        "url": "http://arxiv.org/abs/2405.06242v1",
        "pub_date": "2024-05-10",
        "summary": "In recent times, impedance side-channel analysis has emerged as a potent\nstrategy for adversaries seeking to extract sensitive information from\ncomputing systems. It leverages variations in the intrinsic impedance of a\nchip's internal structure across different logic states. In this study, we\nconduct a comparative analysis between the newly explored impedance side\nchannel and the well-established power side channel. Through experimental\nevaluation, we investigate the efficacy of these two side channels in\nextracting the cryptographic key from the Advanced Encryption Standard (AES)\nand analyze their performance. Our results indicate that impedance analysis\ndemonstrates a higher potential for cryptographic key extraction compared to\npower side-channel analysis. Moreover, we identify scenarios where power\nside-channel analysis does not yield satisfactory results, whereas impedance\nanalysis proves to be more robust and effective. This work not only underscores\nthe significance of impedance side-channel analysis in enhancing cryptographic\nsecurity but also emphasizes the necessity for a deeper understanding of its\nmechanisms and implications.",
        "translated": ""
    },
    {
        "title": "A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language\n  Models",
        "url": "http://arxiv.org/abs/2405.06211v1",
        "pub_date": "2024-05-10",
        "summary": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation\n(RAG) techniques can offer reliable and up-to-date external knowledge,\nproviding huge convenience for numerous tasks. Particularly in the era of\nAI-generated content (AIGC), the powerful capacity of retrieval in RAG in\nproviding additional knowledge enables retrieval-augmented generation to assist\nexisting generative AI in producing high-quality outputs. Recently, large\nLanguage Models (LLMs) have demonstrated revolutionary abilities in language\nunderstanding and generation, while still facing inherent limitations, such as\nhallucinations and out-of-date internal knowledge. Given the powerful abilities\nof RAG in providing the latest and helpful auxiliary information,\nretrieval-augmented large language models have emerged to harness external and\nauthoritative knowledge bases, rather than solely relying on the model's\ninternal knowledge, to augment the generation quality of LLMs. In this survey,\nwe comprehensively review existing research studies in retrieval-augmented\nlarge language models (RA-LLMs), covering three primary technical perspectives:\narchitectures, training strategies, and applications. As the preliminary\nknowledge, we briefly introduce the foundations and recent advances of LLMs.\nThen, to illustrate the practical significance of RAG for LLMs, we categorize\nmainstream relevant work by application areas, detailing specifically the\nchallenges of each and the corresponding capabilities of RA-LLMs. Finally, to\ndeliver deeper insights, we discuss current limitations and several promising\ndirections for future research.",
        "translated": ""
    },
    {
        "title": "Seasonality Patterns in 311-Reported Foodborne Illness Cases and Machine\n  Learning-Identified Indications of Foodborne Illnesses from Yelp Reviews, New\n  York City, 2022-2023",
        "url": "http://arxiv.org/abs/2405.06138v1",
        "pub_date": "2024-05-09",
        "summary": "Restaurants are critical venues at which to investigate foodborne illness\noutbreaks due to shared sourcing, preparation, and distribution of foods.\nFormal channels to report illness after food consumption, such as 311, New York\nCity's non-emergency municipal service platform, are underutilized. Given this,\nonline social media platforms serve as abundant sources of user-generated\ncontent that provide critical insights into the needs of individuals and\npopulations. We extracted restaurant reviews and metadata from Yelp to identify\npotential outbreaks of foodborne illness in connection with consuming food from\nrestaurants. Because the prevalence of foodborne illnesses may increase in\nwarmer months as higher temperatures breed more favorable conditions for\nbacterial growth, we aimed to identify seasonal patterns in foodborne illness\nreports from 311 and identify seasonal patterns of foodborne illness from Yelp\nreviews for New York City restaurants using a Hierarchical Sigmoid Attention\nNetwork (HSAN). We found no evidence of significant bivariate associations\nbetween any variables of interest. Given the inherent limitations of relying\nsolely on user-generated data for public health insights, it is imperative to\ncomplement these sources with other data streams and insights from subject\nmatter experts. Future investigations should involve conducting these analyses\nat more granular spatial and temporal scales to explore the presence of such\ndifferences or associations.",
        "translated": ""
    },
    {
        "title": "Creating Geospatial Trajectories from Human Trafficking Text Corpora",
        "url": "http://arxiv.org/abs/2405.06130v1",
        "pub_date": "2024-05-09",
        "summary": "Human trafficking is a crime that affects the lives of millions of people\nacross the globe. Traffickers exploit the victims through forced labor,\ninvoluntary sex, or organ harvesting. Migrant smuggling could also be seen as a\nform of human trafficking when the migrant fails to pay the smuggler and is\nforced into coerced activities. Several news agencies and anti-trafficking\norganizations have reported trafficking survivor stories that include the names\nof locations visited along the trafficking route. Identifying such routes can\nprovide knowledge that is essential to preventing such heinous crimes. In this\npaper we propose a Narrative to Trajectory (N2T) information extraction system\nthat analyzes reported narratives, extracts relevant information through the\nuse of Natural Language Processing (NLP) techniques, and applies geospatial\naugmentation in order to automatically plot trajectories of human trafficking\nroutes. We evaluate N2T on human trafficking text corpora and demonstrate that\nour approach of utilizing data preprocessing and augmenting database techniques\nwith NLP libraries outperforms existing geolocation detection methods.",
        "translated": ""
    },
    {
        "title": "Narrative to Trajectory (N2T+): Extracting Routes of Life or Death from\n  Human Trafficking Text Corpora",
        "url": "http://arxiv.org/abs/2405.06129v1",
        "pub_date": "2024-05-09",
        "summary": "Climate change and political unrest in certain regions of the world are\nimposing extreme hardship on many communities and are forcing millions of\nvulnerable populations to abandon their homelands and seek refuge in safer\nlands. As international laws are not fully set to deal with the migration\ncrisis, people are relying on networks of exploiting smugglers to escape the\ndevastation in order to live in stability. During the smuggling journey,\nmigrants can become victims of human trafficking if they fail to pay the\nsmuggler and may be forced into coerced labor. Government agencies and\nanti-trafficking organizations try to identify the trafficking routes based on\nstories of survivors in order to gain knowledge and help prevent such crimes.\nIn this paper, we propose a system called Narrative to Trajectory (N2T+), which\nextracts trajectories of trafficking routes. N2T+ uses Data Science and Natural\nLanguage Processing techniques to analyze trafficking narratives, automatically\nextract relevant location names, disambiguate possible name ambiguities, and\nplot the trafficking route on a map. In a comparative evaluation we show that\nthe proposed multi-dimensional approach offers significantly higher geolocation\ndetection than other state of the art techniques.",
        "translated": ""
    },
    {
        "title": "A Systematic Investigation of Distilling Large Language Models into\n  Cross-Encoders for Passage Re-ranking",
        "url": "http://arxiv.org/abs/2405.07920v1",
        "pub_date": "2024-05-13",
        "summary": "Cross-encoders distilled from large language models are more effective\nre-rankers than cross-encoders fine-tuned using manually labeled data. However,\nthe distilled models do not reach the language model's effectiveness. We\nconstruct and release a new distillation dataset, named Rank-DistiLLM, to\ninvestigate whether insights from fine-tuning cross-encoders on manually\nlabeled data -- hard-negative sampling, deep sampling, and listwise loss\nfunctions -- are transferable to large language model ranker distillation. Our\ndataset can be used to train cross-encoders that reach the effectiveness of\nlarge language models while being orders of magnitude more efficient. Code and\ndata is available at: https://github.com/webis-de/msmarco-llm-distillation",
        "translated": ""
    },
    {
        "title": "A Decentralized and Self-Adaptive Approach for Monitoring Volatile Edge\n  Environments",
        "url": "http://arxiv.org/abs/2405.07806v1",
        "pub_date": "2024-05-13",
        "summary": "Edge computing provides resources for IoT workloads at the network edge.\nMonitoring systems are vital for efficiently managing resources and application\nworkloads by collecting, storing, and providing relevant information about the\nstate of the resources. However, traditional monitoring systems have a\ncentralized architecture for both data plane and control plane, which increases\nlatency, creates a failure bottleneck, and faces challenges in providing quick\nand trustworthy data in volatile edge environments, especially where\ninfrastructures are often built upon failure-prone, unsophisticated computing\nand network resources. Thus, we propose DEMon, a decentralized, self-adaptive\nmonitoring system for edge. DEMon leverages the stochastic gossip communication\nprotocol at its core. It develops efficient protocols for information\ndissemination, communication, and retrieval, avoiding a single point of failure\nand ensuring fast and trustworthy data access. Its decentralized control\nenables self-adaptive management of monitoring parameters, addressing the\ntrade-offs between the quality of service of monitoring and resource\nconsumption. We implement the proposed system as a lightweight and portable\ncontainer-based system and evaluate it through experiments. We also present a\nuse case demonstrating its feasibility. The results show that DEMon efficiently\ndisseminates and retrieves the monitoring information, addressing the\nchallenges of edge monitoring.",
        "translated": ""
    },
    {
        "title": "Decoding Geometric Properties in Non-Random Data from First\n  Information-Theoretic Principles",
        "url": "http://arxiv.org/abs/2405.07803v1",
        "pub_date": "2024-05-13",
        "summary": "Based on the principles of information theory, measure theory, and\ntheoretical computer science, we introduce a univariate signal deconvolution\nmethod with a wide range of applications to coding theory, particularly in\nzero-knowledge one-way communication channels, such as in deciphering messages\nfrom unknown generating sources about which no prior knowledge is available and\nto which no return message can be sent. Our multidimensional space\nreconstruction method from an arbitrary received signal is proven to be\nagnostic vis-a-vis the encoding-decoding scheme, computation model, programming\nlanguage, formal theory, the computable (or semi-computable) method of\napproximation to algorithmic complexity, and any arbitrarily chosen\n(computable) probability measure of the events. The method derives from the\nprinciples of an approach to Artificial General Intelligence capable of\nbuilding a general-purpose model of models independent of any arbitrarily\nassumed prior probability distribution. We argue that this optimal and\nuniversal method of decoding non-random data has applications to signal\nprocessing, causal deconvolution, topological and geometric properties\nencoding, cryptography, and bio- and technosignature detection.",
        "translated": ""
    },
    {
        "title": "Is Interpretable Machine Learning Effective at Feature Selection for\n  Neural Learning-to-Rank?",
        "url": "http://arxiv.org/abs/2405.07782v1",
        "pub_date": "2024-05-13",
        "summary": "Neural ranking models have become increasingly popular for real-world search\nand recommendation systems in recent years. Unlike their tree-based\ncounterparts, neural models are much less interpretable. That is, it is very\ndifficult to understand their inner workings and answer questions like how do\nthey make their ranking decisions? or what document features do they find\nimportant? This is particularly disadvantageous since interpretability is\nhighly important for real-world systems. In this work, we explore feature\nselection for neural learning-to-rank (LTR). In particular, we investigate six\nwidely-used methods from the field of interpretable machine learning (ML) and\nintroduce our own modification, to select the input features that are most\nimportant to the ranking behavior. To understand whether these methods are\nuseful for practitioners, we further study whether they contribute to\nefficiency enhancement. Our experimental results reveal a large feature\nredundancy in several LTR benchmarks: the local selection method TabNet can\nachieve optimal ranking performance with less than 10 features; the global\nmethods, particularly our G-L2X, require slightly more selected features, but\nexhibit higher potential in improving efficiency. We hope that our analysis of\nthese feature selection methods will bring the fields of interpretable ML and\nLTR closer together.",
        "translated": ""
    },
    {
        "title": "Synthetic Test Collections for Retrieval Evaluation",
        "url": "http://arxiv.org/abs/2405.07767v1",
        "pub_date": "2024-05-13",
        "summary": "Test collections play a vital role in evaluation of information retrieval\n(IR) systems. Obtaining a diverse set of user queries for test collection\nconstruction can be challenging, and acquiring relevance judgments, which\nindicate the appropriateness of retrieved documents to a query, is often costly\nand resource-intensive. Generating synthetic datasets using Large Language\nModels (LLMs) has recently gained significant attention in various\napplications. In IR, while previous work exploited the capabilities of LLMs to\ngenerate synthetic queries or documents to augment training data and improve\nthe performance of ranking models, using LLMs for constructing synthetic test\ncollections is relatively unexplored. Previous studies demonstrate that LLMs\nhave the potential to generate synthetic relevance judgments for use in the\nevaluation of IR systems. In this paper, we comprehensively investigate whether\nit is possible to use LLMs to construct fully synthetic test collections by\ngenerating not only synthetic judgments but also synthetic queries. In\nparticular, we analyse whether it is possible to construct reliable synthetic\ntest collections and the potential risks of bias such test collections may\nexhibit towards LLM-based models. Our experiments indicate that using LLMs it\nis possible to construct synthetic test collections that can reliably be used\nfor retrieval evaluation.",
        "translated": ""
    },
    {
        "title": "DynLLM: When Large Language Models Meet Dynamic Graph Recommendation",
        "url": "http://arxiv.org/abs/2405.07580v1",
        "pub_date": "2024-05-13",
        "summary": "Last year has witnessed the considerable interest of Large Language Models\n(LLMs) for their potential applications in recommender systems, which may\nmitigate the persistent issue of data sparsity. Though large efforts have been\nmade for user-item graph augmentation with better graph-based recommendation\nperformance, they may fail to deal with the dynamic graph recommendation task,\nwhich involves both structural and temporal graph dynamics with inherent\ncomplexity in processing time-evolving data. To bridge this gap, in this paper,\nwe propose a novel framework, called DynLLM, to deal with the dynamic graph\nrecommendation task with LLMs. Specifically, DynLLM harnesses the power of LLMs\nto generate multi-faceted user profiles based on the rich textual features of\nhistorical purchase records, including crowd segments, personal interests,\npreferred categories, and favored brands, which in turn supplement and enrich\nthe underlying relationships between users and items. Along this line, to fuse\nthe multi-faceted profiles with temporal graph embedding, we engage LLMs to\nderive corresponding profile embeddings, and further employ a distilled\nattention mechanism to refine the LLM-generated profile embeddings for\nalleviating noisy signals, while also assessing and adjusting the relevance of\neach distilled facet embedding for seamless integration with temporal graph\nembedding from continuous time dynamic graphs (CTDGs). Extensive experiments on\ntwo real e-commerce datasets have validated the superior improvements of DynLLM\nover a wide range of state-of-the-art baseline methods.",
        "translated": ""
    },
    {
        "title": "MS MARCO Web Search: a Large-scale Information-rich Web Dataset with\n  Millions of Real Click Labels",
        "url": "http://arxiv.org/abs/2405.07526v1",
        "pub_date": "2024-05-13",
        "summary": "Recent breakthroughs in large models have highlighted the critical\nsignificance of data scale, labels and modals. In this paper, we introduce MS\nMARCO Web Search, the first large-scale information-rich web dataset, featuring\nmillions of real clicked query-document labels. This dataset closely mimics\nreal-world web document and query distribution, provides rich information for\nvarious kinds of downstream tasks and encourages research in various areas,\nsuch as generic end-to-end neural indexer models, generic embedding models, and\nnext generation information access system with large language models. MS MARCO\nWeb Search offers a retrieval benchmark with three web retrieval challenge\ntasks that demand innovations in both machine learning and information\nretrieval system research domains. As the first dataset that meets large, real\nand rich data requirements, MS MARCO Web Search paves the way for future\nadvancements in AI and system research. MS MARCO Web Search dataset is\navailable at: https://github.com/microsoft/MS-MARCO-Web-Search.",
        "translated": ""
    },
    {
        "title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical\n  Concept Linking",
        "url": "http://arxiv.org/abs/2405.07500v1",
        "pub_date": "2024-05-13",
        "summary": "Linking (aligning) biomedical concepts across diverse data sources enables\nvarious integrative analyses, but it is challenging due to the discrepancies in\nconcept naming conventions. Various strategies have been developed to overcome\nthis challenge, such as those based on string-matching rules, manually crafted\nthesauri, and machine learning models. However, these methods are constrained\nby limited prior biomedical knowledge and can hardly generalize beyond the\nlimited amounts of rules, thesauri, or training samples. Recently, large\nlanguage models (LLMs) have exhibited impressive results in diverse biomedical\nNLP tasks due to their unprecedentedly rich prior knowledge and strong\nzero-shot prediction abilities. However, LLMs suffer from issues including high\ncosts, limited context length, and unreliable predictions. In this research, we\npropose PromptLink, a novel biomedical concept linking framework that leverages\nLLMs. It first employs a biomedical-specialized pre-trained language model to\ngenerate candidate concepts that can fit in the LLM context windows. Then it\nutilizes an LLM to link concepts through two-stage prompts, where the\nfirst-stage prompt aims to elicit the biomedical prior knowledge from the LLM\nfor the concept linking task and the second-stage prompt enforces the LLM to\nreflect on its own predictions to further enhance their reliability. Empirical\nresults on the concept linking task between two EHR datasets and an external\nbiomedical KG demonstrate the effectiveness of PromptLink. Furthermore,\nPromptLink is a generic framework without reliance on additional prior\nknowledge, context, or training data, making it well-suited for concept linking\nacross various types of data sources. The source code is available at\nhttps://github.com/constantjxyz/PromptLink.",
        "translated": ""
    },
    {
        "title": "SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset",
        "url": "http://arxiv.org/abs/2405.07354v1",
        "pub_date": "2024-05-12",
        "summary": "The application of Automatic Speech Recognition (ASR) technology in soccer\noffers numerous opportunities for sports analytics. Specifically, extracting\naudio commentaries with ASR provides valuable insights into the events of the\ngame, and opens the door to several downstream applications such as automatic\nhighlight generation. This paper presents SoccerNet-Echoes, an augmentation of\nthe SoccerNet dataset with automatically generated transcriptions of audio\ncommentaries from soccer game broadcasts, enhancing video content with rich\nlayers of textual information derived from the game audio using ASR. These\ntextual commentaries, generated using the Whisper model and translated with\nGoogle Translate, extend the usefulness of the SoccerNet dataset in diverse\napplications such as enhanced action spotting, automatic caption generation,\nand game summarization. By incorporating textual data alongside visual and\nauditory content, SoccerNet-Echoes aims to serve as a comprehensive resource\nfor the development of algorithms specialized in capturing the dynamics of\nsoccer games. We detail the methods involved in the curation of this dataset\nand the integration of ASR. We also highlight the implications of a multimodal\napproach in sports analytics, and how the enriched dataset can support diverse\napplications, thus broadening the scope of research and development in the\nfield of sports analytics.",
        "translated": ""
    },
    {
        "title": "Learnable Tokenizer for LLM-based Generative Recommendation",
        "url": "http://arxiv.org/abs/2405.07314v1",
        "pub_date": "2024-05-12",
        "summary": "Harnessing Large Language Models (LLMs) for generative recommendation has\ngarnered significant attention due to LLMs' powerful capacities such as rich\nworld knowledge and reasoning. However, a critical challenge lies in\ntransforming recommendation data into the language space of LLMs through\neffective item tokenization. Existing approaches, such as ID identifiers,\ntextual identifiers, and codebook-based identifiers, exhibit limitations in\nencoding semantic information, incorporating collaborative signals, or handling\ncode assignment bias. To address these shortcomings, we propose LETTER (a\nLEarnable Tokenizer for generaTivE Recommendation), designed to meet the key\ncriteria of identifiers by integrating hierarchical semantics, collaborative\nsignals, and code assignment diversity. LETTER integrates Residual Quantized\nVAE for semantic regularization, a contrastive alignment loss for collaborative\nregularization, and a diversity loss to mitigate code assignment bias. We\ninstantiate LETTER within two generative recommender models and introduce a\nranking-guided generation loss to enhance their ranking ability. Extensive\nexperiments across three datasets demonstrate the superiority of LETTER in item\ntokenization, thereby advancing the state-of-the-art in the field of generative\nrecommendation.",
        "translated": ""
    },
    {
        "title": "From Text to Context: An Entailment Approach for News Stakeholder\n  Classification",
        "url": "http://arxiv.org/abs/2405.08751v1",
        "pub_date": "2024-05-14",
        "summary": "Navigating the complex landscape of news articles involves understanding the\nvarious actors or entities involved, referred to as news stakeholders. These\nstakeholders, ranging from policymakers to opposition figures, citizens, and\nmore, play pivotal roles in shaping news narratives. Recognizing their\nstakeholder types, reflecting their roles, political alignments, social\nstanding, and more, is paramount for a nuanced comprehension of news content.\nDespite existing works focusing on salient entity extraction, coverage\nvariations, and political affiliations through social media data, the automated\ndetection of stakeholder roles within news content remains an underexplored\ndomain. In this paper, we bridge this gap by introducing an effective approach\nto classify stakeholder types in news articles. Our method involves\ntransforming the stakeholder classification problem into a natural language\ninference task, utilizing contextual information from news articles and\nexternal knowledge to enhance the accuracy of stakeholder type detection.\nMoreover, our proposed model showcases efficacy in zero-shot settings, further\nextending its applicability to diverse news contexts.",
        "translated": ""
    },
    {
        "title": "Treatment Effect Estimation for User Interest Exploration on Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2405.08582v1",
        "pub_date": "2024-05-14",
        "summary": "Recommender systems learn personalized user preferences from user feedback\nlike clicks. However, user feedback is usually biased towards partially\nobserved interests, leaving many users' hidden interests unexplored. Existing\napproaches typically mitigate the bias, increase recommendation diversity, or\nuse bandit algorithms to balance exploration-exploitation trade-offs.\nNevertheless, they fail to consider the potential rewards of recommending\ndifferent categories of items and lack the global scheduling of allocating\ntop-N recommendations to categories, leading to suboptimal exploration. In this\nwork, we propose an Uplift model-based Recommender (UpliftRec) framework, which\nregards top-N recommendation as a treatment optimization problem. UpliftRec\nestimates the treatment effects, i.e., the click-through rate (CTR) under\ndifferent category exposure ratios, by using observational user feedback.\nUpliftRec calculates group-level treatment effects to discover users' hidden\ninterests with high CTR rewards and leverages inverse propensity weighting to\nalleviate confounder bias. Thereafter, UpliftRec adopts a dynamic programming\nmethod to calculate the optimal treatment for overall CTR maximization. We\nimplement UpliftRec on different backend models and conduct extensive\nexperiments on three datasets. The empirical results validate the effectiveness\nof UpliftRec in discovering users' hidden interests while achieving superior\nrecommendation accuracy.",
        "translated": ""
    },
    {
        "title": "How to Surprisingly Consider Recommendations? A Knowledge-Graph-based\n  Approach Relying on Complex Network Metrics",
        "url": "http://arxiv.org/abs/2405.08465v1",
        "pub_date": "2024-05-14",
        "summary": "Traditional recommendation proposals, including content-based and\ncollaborative filtering, usually focus on similarity between items or users.\nExisting approaches lack ways of introducing unexpectedness into\nrecommendations, prioritizing globally popular items over exposing users to\nunforeseen items. This investigation aims to design and evaluate a novel layer\non top of recommender systems suited to incorporate relational information and\nsuggest items with a user-defined degree of surprise. We propose a Knowledge\nGraph (KG) based recommender system by encoding user interactions on item\ncatalogs. Our study explores whether network-level metrics on KGs can influence\nthe degree of surprise in recommendations. We hypothesize that surprisingness\ncorrelates with certain network metrics, treating user profiles as subgraphs\nwithin a larger catalog KG. The achieved solution reranks recommendations based\non their impact on structural graph metrics. Our research contributes to\noptimizing recommendations to reflect the metrics. We experimentally evaluate\nour approach on two datasets of LastFM listening histories and synthetic\nNetflix viewing profiles. We find that reranking items based on complex network\nmetrics leads to a more unexpected and surprising composition of recommendation\nlists.",
        "translated": ""
    },
    {
        "title": "Diffusion-based Contrastive Learning for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2405.09369v1",
        "pub_date": "2024-05-15",
        "summary": "Contrastive learning has been effectively applied to alleviate the data\nsparsity issue and enhance recommendation performance.The majority of existing\nmethods employ random augmentation to generate augmented views of original\nsequences. The learning objective then aims to minimize the distance between\nrepresentations of different views for the same user. However, these random\naugmentation strategies (e.g., mask or substitution) neglect the semantic\nconsistency of different augmented views for the same user, leading to\nsemantically inconsistent sequences with similar representations. Furthermore,\nmost augmentation methods fail to utilize context information, which is\ncritical for understanding sequence semantics. To address these limitations, we\nintroduce a diffusion-based contrastive learning approach for sequential\nrecommendation. Specifically, given a user sequence, we first select some\npositions and then leverage context information to guide the generation of\nalternative items via a guided diffusion model. By repeating this approach, we\ncan get semantically consistent augmented views for the same user, which are\nused to improve the effectiveness of contrastive learning. To maintain cohesion\nbetween the representation spaces of both the diffusion model and the\nrecommendation model, we train the entire framework in an end-to-end fashion\nwith shared item embeddings. Extensive experiments on five benchmark datasets\ndemonstrate the superiority of our proposed method.",
        "translated": ""
    },
    {
        "title": "Content-Based Image Retrieval for Multi-Class Volumetric Radiology\n  Images: A Benchmark Study",
        "url": "http://arxiv.org/abs/2405.09334v1",
        "pub_date": "2024-05-15",
        "summary": "While content-based image retrieval (CBIR) has been extensively studied in\nnatural image retrieval, its application to medical images presents ongoing\nchallenges, primarily due to the 3D nature of medical images. Recent studies\nhave shown the potential use of pre-trained vision embeddings for CBIR in the\ncontext of radiology image retrieval. However, a benchmark for the retrieval of\n3D volumetric medical images is still lacking, hindering the ability to\nobjectively evaluate and compare the efficiency of proposed CBIR approaches in\nmedical imaging. In this study, we extend previous work and establish a\nbenchmark for region-based and multi-organ retrieval using the TotalSegmentator\ndataset (TS) with detailed multi-organ annotations. We benchmark embeddings\nderived from pre-trained supervised models on medical images against embeddings\nderived from pre-trained unsupervised models on non-medical images for 29\ncoarse and 104 detailed anatomical structures in volume and region levels. We\nadopt a late interaction re-ranking method inspired by text matching for image\nretrieval, comparing it against the original method proposed for volume and\nregion retrieval achieving retrieval recall of 1.0 for diverse anatomical\nregions with a wide size range. The findings and methodologies presented in\nthis paper provide essential insights and benchmarks for the development and\nevaluation of CBIR approaches in the context of medical imaging.",
        "translated": ""
    },
    {
        "title": "Words Blending Boxes. Obfuscating Queries in Information Retrieval using\n  Differential Privacy",
        "url": "http://arxiv.org/abs/2405.09306v1",
        "pub_date": "2024-05-15",
        "summary": "Ensuring the effectiveness of search queries while protecting user privacy\nremains an open issue. When an Information Retrieval System (IRS) does not\nprotect the privacy of its users, sensitive information may be disclosed\nthrough the queries sent to the system. Recent improvements, especially in NLP,\nhave shown the potential of using Differential Privacy to obfuscate texts while\nmaintaining satisfactory effectiveness. However, such approaches may protect\nthe user's privacy only from a theoretical perspective while, in practice, the\nreal user's information need can still be inferred if perturbed terms are too\nsemantically similar to the original ones. We overcome such limitations by\nproposing Word Blending Boxes, a novel differentially private mechanism for\nquery obfuscation, which protects the words in the user queries by employing\nsafe boxes. To measure the overall effectiveness of the proposed WBB mechanism,\nwe measure the privacy obtained by the obfuscation process, i.e., the lexical\nand semantic similarity between original and obfuscated queries. Moreover, we\nassess the effectiveness of the privatized queries in retrieving relevant\ndocuments from the IRS. Our findings indicate that WBB can be integrated\neffectively into existing IRSs, offering a key to the challenge of protecting\nuser privacy from both a theoretical and a practical point of view.",
        "translated": ""
    },
    {
        "title": "Exploring the Individuality and Collectivity of Intents behind\n  Interactions for Graph Collaborative Filtering",
        "url": "http://arxiv.org/abs/2405.09042v1",
        "pub_date": "2024-05-15",
        "summary": "Intent modeling has attracted widespread attention in recommender systems. As\nthe core motivation behind user selection of items, intent is crucial for\nelucidating recommendation results. The current mainstream modeling method is\nto abstract the intent into unknowable but learnable shared or non-shared\nparameters. Despite considerable progress, we argue that it still confronts the\nfollowing challenges: firstly, these methods only capture the coarse-grained\naspects of intent, ignoring the fact that user-item interactions will be\naffected by collective and individual factors (e.g., a user may choose a movie\nbecause of its high box office or because of his own unique preferences);\nsecondly, modeling believable intent is severely hampered by implicit feedback,\nwhich is incredibly sparse and devoid of true semantics. To address these\nchallenges, we propose a novel recommendation framework designated as Bilateral\nIntent-guided Graph Collaborative Filtering (BIGCF). Specifically, we take a\ncloser look at user-item interactions from a causal perspective and put forth\nthe concepts of individual intent-which signifies private preferences-and\ncollective intent-which denotes overall awareness. To counter the sparsity of\nimplicit feedback, the feature distributions of users and items are encoded via\na Gaussian-based graph generation strategy, and we implement the recommendation\nprocess through bilateral intent-guided graph reconstruction re-sampling.\nFinally, we propose graph contrastive regularization for both interaction and\nintent spaces to uniformize users, items, intents, and interactions in a\nself-supervised and non-augmented paradigm. Experimental results on three\nreal-world datasets demonstrate the effectiveness of BIGCF compared with\nexisting solutions.",
        "translated": ""
    },
    {
        "title": "A Click-Through Rate Prediction Method Based on Cross-Importance of\n  Multi-Order Features",
        "url": "http://arxiv.org/abs/2405.08852v1",
        "pub_date": "2024-05-14",
        "summary": "Most current click-through rate prediction(CTR)models create explicit or\nimplicit high-order feature crosses through Hadamard product or inner product,\nwith little attention to the importance of feature crossing; only few models\nare either limited to the second-order explicit feature crossing, implicitly to\nhigh-order feature crossing, or can learn the importance of high-order explicit\nfeature crossing but fail to provide good interpretability for the model. This\npaper proposes a new model, FiiNet (Multiple Order Feature Interaction\nImportance Neural Networks). The model first uses the selective kernel network\n(SKNet) to explicitly construct multi-order feature crosses. It dynamically\nlearns the importance of feature interaction combinations in a fine grained\nmanner, increasing the attention weight of important feature cross combinations\nand reducing the weight of featureless crosses. To verify that the FiiNet model\ncan dynamically learn the importance of feature interaction combinations in a\nfine-grained manner and improve the model's recommendation performance and\ninterpretability, this paper compares it with many click-through rate\nprediction models on two real datasets, proving that the FiiNet model\nincorporating the selective kernel network can effectively improve the\nrecommendation effect and provide better interpretability. FiiNet model\nimplementations are available in PyTorch.",
        "translated": ""
    },
    {
        "title": "Evaluating Supply Chain Resilience During Pandemic Using Agent-based\n  Simulation",
        "url": "http://arxiv.org/abs/2405.08830v1",
        "pub_date": "2024-05-13",
        "summary": "Recent pandemics have highlighted vulnerabilities in our global economic\nsystems, especially supply chains. Possible future pandemic raises a dilemma\nfor businesses owners between short-term profitability and long-term supply\nchain resilience planning. In this study, we propose a novel agent-based\nsimulation model integrating extended Susceptible-Infected-Recovered (SIR)\nepidemiological model and supply and demand economic model to evaluate supply\nchain resilience strategies during pandemics. Using this model, we explore a\nrange of supply chain resilience strategies under pandemic scenarios using in\nsilico experiments. We find that a balanced approach to supply chain resilience\nperforms better in both pandemic and non-pandemic times compared to extreme\nstrategies, highlighting the importance of preparedness in the form of a better\nsupply chain resilience. However, our analysis shows that the exact supply\nchain resilience strategy is hard to obtain for each firm and is relatively\nsensitive to the exact profile of the pandemic and economic state at the\nbeginning of the pandemic. As such, we used a machine learning model that uses\nthe agent-based simulation to estimate a near-optimal supply chain resilience\nstrategy for a firm. The proposed model offers insights for policymakers and\nbusinesses to enhance supply chain resilience in the face of future pandemics,\ncontributing to understanding the trade-offs between short-term gains and\nlong-term sustainability in supply chain management before and during\npandemics.",
        "translated": ""
    },
    {
        "title": "UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language\n  Models",
        "url": "http://arxiv.org/abs/2405.10311v1",
        "pub_date": "2024-05-16",
        "summary": "Recently, Multi-Modal(MM) Large Language Models(LLMs) have unlocked many\ncomplex use-cases that require MM understanding (e.g., image captioning or\nvisual question answering) and MM generation (e.g., text-guided image\ngeneration or editing) capabilities. To further improve the output fidelity of\nMM-LLMs we introduce the model-agnostic UniRAG technique that adds relevant\nretrieved information to prompts as few-shot examples during inference. Unlike\nthe common belief that Retrieval Augmentation (RA) mainly improves generation\nor understanding of uncommon entities, our evaluation results on the MSCOCO\ndataset with common entities show that both proprietary models like GPT4 and\nGemini-Pro and smaller open-source models like Llava, LaVIT, and Emu2\nsignificantly enhance their generation quality when their input prompts are\naugmented with relevant information retrieved by MM retrievers like UniIR\nmodels.",
        "translated": ""
    },
    {
        "title": "Co-Matching: Towards Human-Machine Collaborative Legal Case Matching",
        "url": "http://arxiv.org/abs/2405.10248v1",
        "pub_date": "2024-05-16",
        "summary": "Recent efforts have aimed to improve AI machines in legal case matching by\nintegrating legal domain knowledge. However, successful legal case matching\nrequires the tacit knowledge of legal practitioners, which is difficult to\nverbalize and encode into machines. This emphasizes the crucial role of\ninvolving legal practitioners in high-stakes legal case matching. To address\nthis, we propose a collaborative matching framework called Co-Matching, which\nencourages both the machine and the legal practitioner to participate in the\nmatching process, integrating tacit knowledge. Unlike existing methods that\nrely solely on the machine, Co-Matching allows both the legal practitioner and\nthe machine to determine key sentences and then combine them probabilistically.\nCo-Matching introduces a method called ProtoEM to estimate human decision\nuncertainty, facilitating the probabilistic combination. Experimental results\ndemonstrate that Co-Matching consistently outperforms existing legal case\nmatching methods, delivering significant performance improvements over human-\nand machine-based matching in isolation (on average, +5.51% and +8.71%,\nrespectively). Further analysis shows that Co-Matching also ensures better\nhuman-machine collaboration effectiveness. Our study represents a pioneering\neffort in human-machine collaboration for the matching task, marking a\nmilestone for future collaborative matching studies.",
        "translated": ""
    },
    {
        "title": "iDRAMA-Scored-2024: A Dataset of the Scored Social Media Platform from\n  2020 to 2023",
        "url": "http://arxiv.org/abs/2405.10233v1",
        "pub_date": "2024-05-16",
        "summary": "Online web communities often face bans for violating platform policies,\nencouraging their migration to alternative platforms. This migration, however,\ncan result in increased toxicity and unforeseen consequences on the new\nplatform. In recent years, researchers have collected data from many\nalternative platforms, indicating coordinated efforts leading to offline\nevents, conspiracy movements, hate speech propagation, and harassment. Thus, it\nbecomes crucial to characterize and understand these alternative platforms. To\nadvance research in this direction, we collect and release a large-scale\ndataset from Scored -- an alternative Reddit platform that sheltered banned\nfringe communities, for example, c/TheDonald (a prominent right-wing community)\nand c/GreatAwakening (a conspiratorial community). Over four years, we\ncollected approximately 57M posts from Scored, with at least 58 communities\nidentified as migrating from Reddit and over 950 communities created since the\nplatform's inception. Furthermore, we provide sentence embeddings of all posts\nin our dataset, generated through a state-of-the-art model, to further advance\nthe field in characterizing the discussions within these communities. We aim to\nprovide these resources to facilitate their investigations without the need for\nextensive data collection and processing efforts.",
        "translated": ""
    },
    {
        "title": "Beyond Static Calibration: The Impact of User Preference Dynamics on\n  Calibrated Recommendation",
        "url": "http://arxiv.org/abs/2405.10232v1",
        "pub_date": "2024-05-16",
        "summary": "Calibration in recommender systems is an important performance criterion that\nensures consistency between the distribution of user preference categories and\nthat of recommendations generated by the system. Standard methods for\nmitigating miscalibration typically assume that user preference profiles are\nstatic, and they measure calibration relative to the full history of user's\ninteractions, including possibly outdated and stale preference categories. We\nconjecture that this approach can lead to recommendations that, while appearing\ncalibrated, in fact, distort users' true preferences. In this paper, we conduct\na preliminary investigation of recommendation calibration at a more granular\nlevel, taking into account evolving user preferences. By analyzing differently\nsized training time windows from the most recent interactions to the oldest, we\nidentify the most relevant segment of user's preferences that optimizes the\ncalibration metric. We perform an exploratory analysis with datasets from\ndifferent domains with distinctive user-interaction characteristics. We\ndemonstrate how the evolving nature of user preferences affects recommendation\ncalibration, and how this effect is manifested differently depending on the\ncharacteristics of the data in a given domain. Datasets, codes, and more\ndetailed experimental results are available at:\nhttps://github.com/nicolelin13/DynamicCalibrationUMAP.",
        "translated": ""
    },
    {
        "title": "$Δ\\text{-}{\\rm OPE}$: Off-Policy Estimation with Pairs of Policies",
        "url": "http://arxiv.org/abs/2405.10024v1",
        "pub_date": "2024-05-16",
        "summary": "The off-policy paradigm casts recommendation as a counterfactual\ndecision-making task, allowing practitioners to unbiasedly estimate online\nmetrics using offline data. This leads to effective evaluation metrics, as well\nas learning procedures that directly optimise online success. Nevertheless, the\nhigh variance that comes with unbiasedness is typically the crux that\ncomplicates practical applications. An important insight is that the difference\nbetween policy values can often be estimated with significantly reduced\nvariance, if said policies have positive covariance. This allows us to\nformulate a pairwise off-policy estimation task: $\\Delta\\text{-}{\\rm OPE}$.\n  $\\Delta\\text{-}{\\rm OPE}$ subsumes the common use-case of estimating\nimprovements of a learnt policy over a production policy, using data collected\nby a stochastic logging policy. We introduce $\\Delta\\text{-}{\\rm OPE}$ methods\nbased on the widely used Inverse Propensity Scoring estimator and its\nextensions. Moreover, we characterise a variance-optimal additive control\nvariate that further enhances efficiency. Simulated, offline, and online\nexperiments show that our methods significantly improve performance for both\nevaluation and learning tasks.",
        "translated": ""
    },
    {
        "title": "GenToC: Leveraging Partially-Labeled Data for Product Attribute-Value\n  Identification",
        "url": "http://arxiv.org/abs/2405.10918v1",
        "pub_date": "2024-05-17",
        "summary": "In the e-commerce domain, the accurate extraction of attribute-value pairs\nfrom product listings (e.g., Brand: Apple) is crucial for enhancing search and\nrecommendation systems. The automation of this extraction process is\nchallenging due to the vast diversity of product categories and their\nrespective attributes, compounded by the lack of extensive, accurately\nannotated training datasets and the demand for low latency to meet the\nreal-time needs of e-commerce platforms. To address these challenges, we\nintroduce GenToC, a novel two-stage model for extracting attribute-value pairs\nfrom product titles. GenToC is designed to train with partially-labeled data,\nleveraging incomplete attribute-value pairs and obviating the need for a fully\nannotated dataset. Moreover, we introduce a bootstrapping method that enables\nGenToC to progressively refine and expand its training dataset. This\nenhancement substantially improves the quality of data available for training\nother neural network models that are typically faster but are inherently less\ncapable than GenToC in terms of their capacity to handle partially-labeled\ndata. By supplying an enriched dataset for training, GenToC significantly\nadvances the performance of these alternative models, making them more suitable\nfor real-time deployment. Our results highlight the unique capability of GenToC\nto learn from a limited set of labeled data and to contribute to the training\nof more efficient models, marking a significant leap forward in the automated\nextraction of attribute-value pairs from product titles. GenToC has been\nsuccessfully integrated into India's largest B2B e-commerce platform,\nIndiaMART.com, achieving a significant increase of 21.1% in recall over the\nexisting deployed system while maintaining a high precision of 89.5% in this\nchallenging task.",
        "translated": ""
    },
    {
        "title": "A Unified Search and Recommendation Framework Based on Multi-Scenario\n  Learning for Ranking in E-commerce",
        "url": "http://arxiv.org/abs/2405.10835v1",
        "pub_date": "2024-05-17",
        "summary": "Search and recommendation (S&amp;R) are the two most important scenarios in\ne-commerce. The majority of users typically interact with products in S&amp;R\nscenarios, indicating the need and potential for joint modeling. Traditional\nmulti-scenario models use shared parameters to learn the similarity of multiple\ntasks, and task-specific parameters to learn the divergence of individual\ntasks. This coarse-grained modeling approach does not effectively capture the\ndifferences between S&amp;R scenarios. Furthermore, this approach does not\nsufficiently exploit the information across the global label space. These\nissues can result in the suboptimal performance of multi-scenario models in\nhandling both S&amp;R scenarios. To address these issues, we propose an effective\nand universal framework for Unified Search and Recommendation (USR), designed\nwith S&amp;R Views User Interest Extractor Layer (IE) and S&amp;R Views Feature\nGenerator Layer (FG) to separately generate user interests and\nscenario-agnostic feature representations for S&amp;R. Next, we introduce a Global\nLabel Space Multi-Task Layer (GLMT) that uses global labels as supervised\nsignals of auxiliary tasks and jointly models the main task and auxiliary tasks\nusing conditional probability. Extensive experimental evaluations on real-world\nindustrial datasets show that USR can be applied to various multi-scenario\nmodels and significantly improve their performance. Online A/B testing also\nindicates substantial performance gains across multiple metrics. Currently, USR\nhas been successfully deployed in the 7Fresh App.",
        "translated": ""
    },
    {
        "title": "INDUS: Effective and Efficient Language Models for Scientific\n  Applications",
        "url": "http://arxiv.org/abs/2405.10725v1",
        "pub_date": "2024-05-17",
        "summary": "Large language models (LLMs) trained on general domain corpora showed\nremarkable results on natural language processing (NLP) tasks. However,\nprevious research demonstrated LLMs trained using domain-focused corpora\nperform better on specialized tasks. Inspired by this pivotal insight, we\ndeveloped INDUS, a comprehensive suite of LLMs tailored for the Earth science,\nbiology, physics, heliophysics, planetary sciences and astrophysics domains and\ntrained using curated scientific corpora drawn from diverse data sources. The\nsuite of models include: (1) an encoder model trained using domain-specific\nvocabulary and corpora to address natural language understanding tasks, (2) a\ncontrastive-learning-based general text embedding model trained using a diverse\nset of datasets drawn from multiple sources to address information retrieval\ntasks and (3) smaller versions of these models created using knowledge\ndistillation techniques to address applications which have latency or resource\nconstraints. We also created three new scientific benchmark datasets namely,\nCLIMATE-CHANGE-NER (entity-recognition), NASA-QA (extractive QA) and NASA-IR\n(IR) to accelerate research in these multi-disciplinary fields. Finally, we\nshow that our models outperform both general-purpose encoders (RoBERTa) and\nexisting domain-specific encoders (SciBERT) on these new tasks as well as\nexisting benchmark tasks in the domains of interest.",
        "translated": ""
    },
    {
        "title": "SynDy: Synthetic Dynamic Dataset Generation Framework for Misinformation\n  Tasks",
        "url": "http://arxiv.org/abs/2405.10700v1",
        "pub_date": "2024-05-17",
        "summary": "Diaspora communities are disproportionately impacted by off-the-radar\nmisinformation and often neglected by mainstream fact-checking efforts,\ncreating a critical need to scale-up efforts of nascent fact-checking\ninitiatives. In this paper we present SynDy, a framework for Synthetic Dynamic\nDataset Generation to leverage the capabilities of the largest frontier Large\nLanguage Models (LLMs) to train local, specialized language models. To the best\nof our knowledge, SynDy is the first paper utilizing LLMs to create\nfine-grained synthetic labels for tasks of direct relevance to misinformation\nmitigation, namely Claim Matching, Topical Clustering, and Claim Relationship\nClassification. SynDy utilizes LLMs and social media queries to automatically\ngenerate distantly-supervised, topically-focused datasets with synthetic labels\non these three tasks, providing essential tools to scale up human-led\nfact-checking at a fraction of the cost of human-annotated data. Training on\nSynDy's generated labels shows improvement over a standard baseline and is not\nsignificantly worse compared to training on human labels (which may be\ninfeasible to acquire). SynDy is being integrated into Meedan's chatbot\ntiplines that are used by over 50 organizations, serve over 230K users\nannually, and automatically distribute human-written fact-checks via messaging\napps such as WhatsApp. SynDy will also be integrated into our deployed\nCo-Insights toolkit, enabling low-resource organizations to launch tiplines for\ntheir communities. Finally, we envision SynDy enabling additional fact-checking\ntools such as matching new misinformation claims to high-quality explainers on\ncommon misinformation topics.",
        "translated": ""
    },
    {
        "title": "Know in AdVance: Linear-Complexity Forecasting of Ad Campaign\n  Performance with Evolving User Interest",
        "url": "http://arxiv.org/abs/2405.10681v1",
        "pub_date": "2024-05-17",
        "summary": "Real-time Bidding (RTB) advertisers wish to \\textit{know in advance} the\nexpected cost and yield of ad campaigns to avoid trial-and-error expenses.\nHowever, Campaign Performance Forecasting (CPF), a sequence modeling task\ninvolving tens of thousands of ad auctions, poses challenges of evolving user\ninterest, auction representation, and long context, making coarse-grained and\nstatic-modeling methods sub-optimal. We propose \\textit{AdVance}, a time-aware\nframework that integrates local auction-level and global campaign-level\nmodeling. User preference and fatigue are disentangled using a time-positioned\nsequence of clicked items and a concise vector of all displayed items.\nCross-attention, conditioned on the fatigue vector, captures the dynamics of\nuser interest toward each candidate ad. Bidders compete with each other,\npresenting a complete graph similar to the self-attention mechanism. Hence, we\nemploy a Transformer Encoder to compress each auction into embedding by solving\nauxiliary tasks. These sequential embeddings are then summarized by a\nconditional state space model (SSM) to comprehend long-range dependencies while\nmaintaining global linear complexity. Considering the irregular time intervals\nbetween auctions, we make SSM's parameters dependent on the current auction\nembedding and the time interval. We further condition SSM's global predictions\non the accumulation of local results. Extensive evaluations and ablation\nstudies demonstrate its superiority over state-of-the-art methods. AdVance has\nbeen deployed on the Tencent Advertising platform, and A/B tests show a\nremarkable 4.5\\% uplift in Average Revenue per User (ARPU).",
        "translated": ""
    },
    {
        "title": "CELA: Cost-Efficient Language Model Alignment for CTR Prediction",
        "url": "http://arxiv.org/abs/2405.10596v1",
        "pub_date": "2024-05-17",
        "summary": "Click-Through Rate (CTR) prediction holds a paramount position in recommender\nsystems. The prevailing ID-based paradigm underperforms in cold-start scenarios\ndue to the skewed distribution of feature frequency. Additionally, the\nutilization of a single modality fails to exploit the knowledge contained\nwithin textual features. Recent efforts have sought to mitigate these\nchallenges by integrating Pre-trained Language Models (PLMs). They design hard\nprompts to structure raw features into text for each interaction and then apply\nPLMs for text processing. With external knowledge and reasoning capabilities,\nPLMs extract valuable information even in cases of sparse interactions.\nNevertheless, compared to ID-based models, pure text modeling degrades the\nefficacy of collaborative filtering, as well as feature scalability and\nefficiency during both training and inference. To address these issues, we\npropose \\textbf{C}ost-\\textbf{E}fficient \\textbf{L}anguage Model\n\\textbf{A}lignment (\\textbf{CELA}) for CTR prediction. CELA incorporates\ntextual features and language models while preserving the collaborative\nfiltering capabilities of ID-based models. This model-agnostic framework can be\nequipped with plug-and-play textual features, with item-level alignment\nenhancing the utilization of external information while maintaining training\nand inference efficiency. Through extensive offline experiments, CELA\ndemonstrates superior performance compared to state-of-the-art methods.\nFurthermore, an online A/B test conducted on an industrial App recommender\nsystem showcases its practical effectiveness, solidifying the potential for\nreal-world applications of CELA.",
        "translated": ""
    },
    {
        "title": "RDRec: Rationale Distillation for LLM-based Recommendation",
        "url": "http://arxiv.org/abs/2405.10587v1",
        "pub_date": "2024-05-17",
        "summary": "Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec.",
        "translated": ""
    },
    {
        "title": "In-context Contrastive Learning for Event Causality Identification",
        "url": "http://arxiv.org/abs/2405.10512v1",
        "pub_date": "2024-05-17",
        "summary": "Event Causality Identification (ECI) aims at determining the existence of a\ncausal relation between two events. Although recent prompt learning-based\napproaches have shown promising improvements on the ECI task, their performance\nare often subject to the delicate design of multiple prompts and the positive\ncorrelations between the main task and derivate tasks. The in-context learning\nparadigm provides explicit guidance for label prediction in the prompt learning\nparadigm, alleviating its reliance on complex prompts and derivative tasks.\nHowever, it does not distinguish between positive and negative demonstrations\nfor analogy learning. Motivated from such considerations, this paper proposes\nan In-Context Contrastive Learning (ICCL) model that utilizes contrastive\nlearning to enhance the effectiveness of both positive and negative\ndemonstrations. Additionally, we apply contrastive learning to event pairs to\nbetter facilitate event causality identification. Our ICCL is evaluated on the\nwidely used corpora, including the EventStoryLine and Causal-TimeBank, and\nresults show significant performance improvements over the state-of-the-art\nalgorithms.",
        "translated": ""
    },
    {
        "title": "Neural Optimization with Adaptive Heuristics for Intelligent Marketing\n  System",
        "url": "http://arxiv.org/abs/2405.10490v1",
        "pub_date": "2024-05-17",
        "summary": "Computational marketing has become increasingly important in today's digital\nworld, facing challenges such as massive heterogeneous data, multi-channel\ncustomer journeys, and limited marketing budgets. In this paper, we propose a\ngeneral framework for marketing AI systems, the Neural Optimization with\nAdaptive Heuristics (NOAH) framework. NOAH is the first general framework for\nmarketing optimization that considers both to-business (2B) and to-consumer\n(2C) products, as well as both owned and paid channels. We describe key modules\nof the NOAH framework, including prediction, optimization, and adaptive\nheuristics, providing examples for bidding and content optimization. We then\ndetail the successful application of NOAH to LinkedIn's email marketing system,\nshowcasing significant wins over the legacy ranking system. Additionally, we\nshare details and insights that are broadly useful, particularly on: (i)\naddressing delayed feedback with lifetime value, (ii) performing large-scale\nlinear programming with randomization, (iii) improving retrieval with audience\nexpansion, (iv) reducing signal dilution in targeting tests, and (v) handling\nzero-inflated heavy-tail metrics in statistical testing.",
        "translated": ""
    },
    {
        "title": "Positional encoding is not the same as context: A study on positional\n  encoding for Sequential recommendation",
        "url": "http://arxiv.org/abs/2405.10436v1",
        "pub_date": "2024-05-16",
        "summary": "The expansion of streaming media and e-commerce has led to a boom in\nrecommendation systems, including Sequential recommendation systems, which\nconsider the user's previous interactions with items. In recent years, research\nhas focused on architectural improvements such as transformer blocks and\nfeature extraction that can augment model information. Among these features are\ncontext and attributes. Of particular importance is the temporal footprint,\nwhich is often considered part of the context and seen in previous publications\nas interchangeable with positional information. Other publications use\npositional encodings with little attention to them. In this paper, we analyse\npositional encodings, showing that they provide relative information between\nitems that are not inferable from the temporal footprint. Furthermore, we\nevaluate different encodings and how they affect metrics and stability using\nAmazon datasets. We added some new encodings to help with these problems along\nthe way. We found that we can reach new state-of-the-art results by finding the\ncorrect positional encoding, but more importantly, certain encodings stabilise\nthe training.",
        "translated": ""
    },
    {
        "title": "Optimistic Query Routing in Clustering-based Approximate Maximum Inner\n  Product Search",
        "url": "http://arxiv.org/abs/2405.12207v1",
        "pub_date": "2024-05-20",
        "summary": "Clustering-based nearest neighbor search is a simple yet effective method in\nwhich data points are partitioned into geometric shards to form an index, and\nonly a few shards are searched during query processing to find an approximate\nset of top-$k$ vectors. Even though the search efficacy is heavily influenced\nby the algorithm that identifies the set of shards to probe, it has received\nlittle attention in the literature. This work attempts to bridge that gap by\nstudying the problem of routing in clustering-based maximum inner product\nsearch (MIPS). We begin by unpacking existing routing protocols and notice the\nsurprising contribution of optimism. We then take a page from the sequential\ndecision making literature and formalize that insight following the principle\nof ``optimism in the face of uncertainty.'' In particular, we present a new\nframework that incorporates the moments of the distribution of inner products\nwithin each shard to optimistically estimate the maximum inner product. We then\npresent a simple instance of our algorithm that uses only the first two moments\nto reach the same accuracy as state-of-the-art routers such as \\scann by\nprobing up to $50%$ fewer points on a suite of benchmark MIPS datasets. Our\nalgorithm is also space-efficient: we design a sketch of the second moment\nwhose size is independent of the number of points and in practice requires\nstoring only $O(1)$ additional vectors per shard.",
        "translated": ""
    },
    {
        "title": "Reindex-Then-Adapt: Improving Large Language Models for Conversational\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.12119v1",
        "pub_date": "2024-05-20",
        "summary": "Large language models (LLMs) are revolutionizing conversational recommender\nsystems by adeptly indexing item content, understanding complex conversational\ncontexts, and generating relevant item titles. However, controlling the\ndistribution of recommended items remains a challenge. This leads to suboptimal\nperformance due to the failure to capture rapidly changing data distributions,\nsuch as item popularity, on targeted conversational recommendation platforms.\nIn conversational recommendation, LLMs recommend items by generating the titles\n(as multiple tokens) autoregressively, making it difficult to obtain and\ncontrol the recommendations over all items. Thus, we propose a\nReindex-Then-Adapt (RTA) framework, which converts multi-token item titles into\nsingle tokens within LLMs, and then adjusts the probability distributions over\nthese single-token item titles accordingly. The RTA framework marries the\nbenefits of both LLMs and traditional recommender systems (RecSys):\nunderstanding complex queries as LLMs do; while efficiently controlling the\nrecommended item distributions in conversational recommendations as traditional\nRecSys do. Our framework demonstrates improved accuracy metrics across three\ndifferent conversational recommendation datasets and two adaptation settings",
        "translated": ""
    },
    {
        "title": "Adaptive Extraction Network for Multivariate Long Sequence Time-Series\n  Forecasting",
        "url": "http://arxiv.org/abs/2405.12038v1",
        "pub_date": "2024-05-20",
        "summary": "Models employing CNN architecture have made significant progress in\nmultivariate long sequence time-series forecasting (MLSTF), particularly in\nmodeling local time series characteristics. However, during the MLSTF process,\nextracting the global time series patterns and understanding the correlations\namong different variables are highly significant. To address this challenge, we\nintroduce multi-resolution convolution and deformable convolution operations.\nBy enlarging the receptive field using convolution kernels with different\ndilation factors to capture temporal correlation information across different\nresolutions, and adaptively adjusting the sampling positions through additional\noffset vectors, we enhance the network's ability to capture correlated features\nbetween variables. Building upon this, we propose ATVCNet, an adaptive\ntemporal-variable convolutional network designed to effectively model the\nlocal/global temporal dependencies and inter-variable dependencies of\nmultivariate time series. Specifically, extracting and fusing time series\nfeatures at different resolutions, captures both local contextual information\nand global patterns in the time series. The designed inter-variable feature\nadaptive extraction module captures the correlation among different variables\nin the time series. We evaluated the performance of ATVCNet across eight\nreal-world datasets. The results indicate that ATVCNet achieved a performance\nimprovement of approximately 63.4% over state-of-the-art MLSTF models.",
        "translated": ""
    },
    {
        "title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity",
        "url": "http://arxiv.org/abs/2405.12035v1",
        "pub_date": "2024-05-20",
        "summary": "Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.",
        "translated": ""
    },
    {
        "title": "Recommender Algorithm for Supporting Self-Management of CVD Risk Factors\n  in an Adult Population at Home",
        "url": "http://arxiv.org/abs/2405.11967v1",
        "pub_date": "2024-05-20",
        "summary": "One of the new trends in the development of recommendation algorithms is the\ndissemination of their capabilities to support the population in managing their\nhealth. This article focuses on the problem of improving the effectiveness of\ncardiovascular diseases (CVD) prevention, since CVD is the leading cause of\ndeath worldwide. To address this issue, a knowledge-based recommendation\nalgorithm was proposed to support self-management of CVD risk factors in adults\nat home. The proposed algorithm is based on the original multidimensional\nrecommendation model and on a new user profile model, which includes predictive\nassessments of CVD health in addition to its current ones as outlined in\nofficial guidelines. The main feature of the proposed algorithm is the\ncombination of rule-based logic with the capabilities of a large language model\nin generating human-like text for explanatory component of multidimensional\nrecommendation. The verification and evaluation of the proposed algorithm\nshowed the usefulness of the proposed recommendation algorithm for supporting\nadults in self-management of their CVD risk factors at home. As follows from\nthe comparison with similar knowledge-based recommendation algorithms, the\nproposed algorithm evaluates a larger number of CVD risk factors and has a\ngreater information and semantic capacity of the generated recommendations.",
        "translated": ""
    },
    {
        "title": "Towards Graph Contrastive Learning: A Survey and Beyond",
        "url": "http://arxiv.org/abs/2405.11868v1",
        "pub_date": "2024-05-20",
        "summary": "In recent years, deep learning on graphs has achieved remarkable success in\nvarious domains. However, the reliance on annotated graph data remains a\nsignificant bottleneck due to its prohibitive cost and time-intensive nature.\nTo address this challenge, self-supervised learning (SSL) on graphs has gained\nincreasing attention and has made significant progress. SSL enables machine\nlearning models to produce informative representations from unlabeled graph\ndata, reducing the reliance on expensive labeled data. While SSL on graphs has\nwitnessed widespread adoption, one critical component, Graph Contrastive\nLearning (GCL), has not been thoroughly investigated in the existing\nliterature. Thus, this survey aims to fill this gap by offering a dedicated\nsurvey on GCL. We provide a comprehensive overview of the fundamental\nprinciples of GCL, including data augmentation strategies, contrastive modes,\nand contrastive optimization objectives. Furthermore, we explore the extensions\nof GCL to other aspects of data-efficient graph learning, such as weakly\nsupervised learning, transfer learning, and related scenarios. We also discuss\npractical applications spanning domains such as drug discovery, genomics\nanalysis, recommender systems, and finally outline the challenges and potential\nfuture directions in this field.",
        "translated": ""
    },
    {
        "title": "CaseGNN++: Graph Contrastive Learning for Legal Case Retrieval with\n  Graph Augmentation",
        "url": "http://arxiv.org/abs/2405.11791v1",
        "pub_date": "2024-05-20",
        "summary": "Legal case retrieval (LCR) is a specialised information retrieval task that\naims to find relevant cases to a given query case. LCR holds pivotal\nsignificance in facilitating legal practitioners in finding precedents. Most of\nexisting LCR methods are based on traditional lexical models and language\nmodels, which have gained promising performance in retrieval. However, the\ndomain-specific structural information inherent in legal documents is yet to be\nexploited to further improve the performance. Our previous work CaseGNN\nsuccessfully harnesses text-attributed graphs and graph neural networks to\naddress the problem of legal structural information neglect. Nonetheless, there\nremain two aspects for further investigation: (1) The underutilization of rich\nedge information within text-attributed case graphs limits CaseGNN to generate\ninformative case representation. (2) The inadequacy of labelled data in legal\ndatasets hinders the training of CaseGNN model. In this paper, CaseGNN++, which\nis extended from CaseGNN, is proposed to simultaneously leverage the edge\ninformation and additional label data to discover the latent potential of LCR\nmodels. Specifically, an edge feature-based graph attention layer (EUGAT) is\nproposed to comprehensively update node and edge features during graph\nmodelling, resulting in a full utilisation of structural information of legal\ncases. Moreover, a novel graph contrastive learning objective with graph\naugmentation is developed in CaseGNN++ to provide additional training signals,\nthereby enhancing the legal comprehension capabilities of CaseGNN++ model.\nExtensive experiments on two benchmark datasets from COLIEE 2022 and COLIEE\n2023 demonstrate that CaseGNN++ not only significantly improves CaseGNN but\nalso achieves supreme performance compared to state-of-the-art LCR methods.\nCode has been released on https://github.com/yanran-tang/CaseGNN.",
        "translated": ""
    },
    {
        "title": "Modeling User Fatigue for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2405.11764v1",
        "pub_date": "2024-05-20",
        "summary": "Recommender systems filter out information that meets user interests.\nHowever, users may be tired of the recommendations that are too similar to the\ncontent they have been exposed to in a short historical period, which is the\nso-called user fatigue. Despite the significance for a better user experience,\nuser fatigue is seldom explored by existing recommenders. In fact, there are\nthree main challenges to be addressed for modeling user fatigue, including what\nfeatures support it, how it influences user interests, and how its explicit\nsignals are obtained. In this paper, we propose to model user Fatigue in\ninterest learning for sequential Recommendations (FRec). To address the first\nchallenge, based on a multi-interest framework, we connect the target item with\nhistorical items and construct an interest-aware similarity matrix as features\nto support fatigue modeling. Regarding the second challenge, built upon feature\ncross, we propose a fatigue-enhanced multi-interest fusion to capture long-term\ninterest. In addition, we develop a fatigue-gated recurrent unit for short-term\ninterest learning, with temporal fatigue representations as important inputs\nfor constructing update and reset gates. For the last challenge, we propose a\nnovel sequence augmentation to obtain explicit fatigue signals for contrastive\nlearning. We conduct extensive experiments on real-world datasets, including\ntwo public datasets and one large-scale industrial dataset. Experimental\nresults show that FRec can improve AUC and GAUC up to 0.026 and 0.019 compared\nwith state-of-the-art models, respectively. Moreover, large-scale online\nexperiments demonstrate the effectiveness of FRec for fatigue reduction. Our\ncodes are released at https://github.com/tsinghua-fib-lab/SIGIR24-FRec.",
        "translated": ""
    },
    {
        "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
        "url": "http://arxiv.org/abs/2405.11724v1",
        "pub_date": "2024-05-20",
        "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
        "translated": ""
    },
    {
        "title": "Increasing the LLM Accuracy for Question Answering: Ontologies to the\n  Rescue!",
        "url": "http://arxiv.org/abs/2405.11706v1",
        "pub_date": "2024-05-20",
        "summary": "There is increasing evidence that question-answering (QA) systems with Large\nLanguage Models (LLMs), which employ a knowledge graph/semantic representation\nof an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy\ncompared to systems that answer questions directly on SQL databases (i.e.\nText-to-SQL). Our previous benchmark research showed that by using a knowledge\ngraph, the accuracy improved from 16% to 54%. The question remains: how can we\nfurther improve the accuracy and reduce the error rate? Building on the\nobservations of our previous research where the inaccurate LLM-generated SPARQL\nqueries followed incorrect paths, we present an approach that consists of 1)\nOntology-based Query Check (OBQC): detects errors by leveraging the ontology of\nthe knowledge graph to check if the LLM-generated SPARQL query matches the\nsemantic of ontology and 2) LLM Repair: use the error explanations with an LLM\nto repair the SPARQL query. Using the chat with the data benchmark, our primary\nfinding is that our approach increases the overall accuracy to 72% including an\nadditional 8% of \"I don't know\" unknown results. Thus, the overall error rate\nis 20%. These results provide further evidence that investing knowledge graphs,\nnamely the ontology, provides higher accuracy for LLM powered question\nanswering systems.",
        "translated": ""
    },
    {
        "title": "Address-Specific Sustainable Accommodation Choice Through Real-World\n  Data Integration",
        "url": "http://arxiv.org/abs/2405.12934v1",
        "pub_date": "2024-05-21",
        "summary": "Consumers wish to choose sustainable accommodation for their travels, and in\nthe case of corporations, may be required to do so. Yet accommodation\nmarketplaces provide no meaningful capability for sustainable choice: typically\nCO2 estimates are provided that are identical for all accommodation of the same\ntype across an entire country. We propose a decision support system that\nenables real choice of sustainable accommodation. We develop a data-driven\naddress- specific metric called EcoGrade, which integrates government approved\ndatasets and uses interpolation where data is sparse. We validate the metric on\n10,000 UK addresses in 10 cities, showing the match of our interpolations to\nreality is statistically significant. We show how the metric has been embedded\ninto a decision support system for a global accommodation marketplace and\ntested by real users over several months with positive user feedback. In the\nEU, forty percent of final energy consumption is from buildings. We need to\nencourage all building owners to make their accommodation more efficient. The\nrental sector is one area where change can occur rapidly, as rented\naccommodation is renovated frequently. We anticipate our decision support\nsystem using EcoGrade will encourage this positive change.",
        "translated": ""
    },
    {
        "title": "Panmodal Information Interaction",
        "url": "http://arxiv.org/abs/2405.12923v1",
        "pub_date": "2024-05-21",
        "summary": "The emergence of generative artificial intelligence (GenAI) is transforming\ninformation interaction. For decades, search engines such as Google and Bing\nhave been the primary means of locating relevant information for the general\npopulation. They have provided search results in the same standard format (the\nso-called \"10 blue links\"). The recent ability to chat via natural language\nwith AI-based agents and have GenAI automatically synthesize answers in\nreal-time (grounded in top-ranked results) is changing how people interact with\nand consume information at massive scale. These two information interaction\nmodalities (traditional search and AI-powered chat) coexist in current search\nengines, either loosely coupled (e.g., as separate options/tabs) or tightly\ncoupled (e.g., integrated as a chat answer embedded directly within a\ntraditional search result page). We believe that the existence of these two\ndifferent modalities, and potentially many others, is creating an opportunity\nto re-imagine the search experience, capitalize on the strengths of many\nmodalities, and develop systems and strategies to support seamless flow between\nthem. We refer to these as panmodal experiences. Unlike monomodal experiences,\nwhere only one modality is available and/or used for the task at hand, panmodal\nexperiences make multiple modalities available to users (multimodal), directly\nsupport transitions between modalities (crossmodal), and seamlessly combine\nmodalities to tailor task assistance (transmodal). While our focus is search\nand chat, with learnings from insights from a survey of over 100 individuals\nwho have recently performed common tasks on these two modalities, we also\npresent a more general vision for the future of information interaction using\nmultiple modalities and the emergent capabilities of GenAI.",
        "translated": ""
    },
    {
        "title": "Retrievable Domain-Sensitive Feature Memory for Multi-Domain\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.12892v1",
        "pub_date": "2024-05-21",
        "summary": "With the increase in the business scale and number of domains in online\nadvertising, multi-domain ad recommendation has become a mainstream solution in\nthe industry. The core of multi-domain recommendation is effectively modeling\nthe commonalities and distinctions among domains. Existing works are dedicated\nto designing model architectures for implicit multi-domain modeling while\noverlooking an in-depth investigation from a more fundamental perspective of\nfeature distributions. This paper focuses on features with significant\ndifferences across various domains in both distributions and effects on model\npredictions. We refer to these features as domain-sensitive features, which\nserve as carriers of domain distinctions and are crucial for multi-domain\nmodeling. Experiments demonstrate that existing multi-domain modeling methods\nmay neglect domain-sensitive features, indicating insufficient learning of\ndomain distinctions. To avoid this neglect, we propose a domain-sensitive\nfeature attribution method to identify features that best reflect domain\ndistinctions from the feature set. Further, we design a memory architecture\nthat extracts domain-specific information from domain-sensitive features for\nthe model to retrieve and integrate, thereby enhancing the awareness of domain\ndistinctions. Extensive offline and online experiments demonstrate the\nsuperiority of our method in capturing domain distinctions and improving\nmulti-domain recommendation performance.",
        "translated": ""
    },
    {
        "title": "Robust portfolio optimization model for electronic coupon allocation",
        "url": "http://arxiv.org/abs/2405.12865v1",
        "pub_date": "2024-05-21",
        "summary": "Currently, many e-commerce websites issue online/electronic coupons as an\neffective tool for promoting sales of various products and services. We focus\non the problem of optimally allocating coupons to customers subject to a budget\nconstraint on an e-commerce website. We apply a robust portfolio optimization\nmodel based on customer segmentation to the coupon allocation problem. We also\nvalidate the efficacy of our method through numerical experiments using actual\ndata from randomly distributed coupons. Main contributions of our research are\ntwofold. First, we handle six types of coupons, thereby making it extremely\ndifficult to accurately estimate the difference in the effects of various\ncoupons. Second, we demonstrate from detailed numerical results that the robust\noptimization model achieved larger uplifts of sales than did the commonly-used\nmultiple-choice knapsack model and the conventional mean-variance optimization\nmodel. Our results open up great potential for robust portfolio optimization as\nan effective tool for practical coupon allocation.",
        "translated": ""
    },
    {
        "title": "A Dataset and Baselines for Measuring and Predicting the Music Piece\n  Memorability",
        "url": "http://arxiv.org/abs/2405.12847v1",
        "pub_date": "2024-05-21",
        "summary": "Nowadays, humans are constantly exposed to music, whether through voluntary\nstreaming services or incidental encounters during commercial breaks. Despite\nthe abundance of music, certain pieces remain more memorable and often gain\ngreater popularity. Inspired by this phenomenon, we focus on measuring and\npredicting music memorability. To achieve this, we collect a new music piece\ndataset with reliable memorability labels using a novel interactive\nexperimental procedure. We then train baselines to predict and analyze music\nmemorability, leveraging both interpretable features and audio mel-spectrograms\nas inputs. To the best of our knowledge, we are the first to explore music\nmemorability using data-driven deep learning-based methods. Through a series of\nexperiments and ablation studies, we demonstrate that while there is room for\nimprovement, predicting music memorability with limited data is possible.\nCertain intrinsic elements, such as higher valence, arousal, and faster tempo,\ncontribute to memorable music. As prediction techniques continue to evolve,\nreal-life applications like music recommendation systems and music style\ntransfer will undoubtedly benefit from this new area of research.",
        "translated": ""
    },
    {
        "title": "GotFunding: A grant recommendation system based on scientific articles",
        "url": "http://arxiv.org/abs/2405.12840v1",
        "pub_date": "2024-05-21",
        "summary": "Obtaining funding is an important part of becoming a successful scientist.\nJunior faculty spend a great deal of time finding the right agencies and\nprograms that best match their research profile. But what are the factors that\ninfluence the best publication--grant matching? Some universities might employ\npre-award personnel to understand these factors, but not all institutions can\nafford to hire them. Historical records of publications funded by grants can\nhelp us understand the matching process and also help us develop recommendation\nsystems to automate it. In this work, we present \\textsc{GotFunding} (Grant\nrecOmmendaTion based on past FUNDING), a recommendation system trained on\nNational Institutes of Health's (NIH) grant--publication records. Our system\nachieves a high performance (NDCG@1 = 0.945) by casting the problem as learning\nto rank. By analyzing the features that make predictions effective, our results\nshow that the ranking considers most important 1) the year difference between\npublication and grant grant, 2) the amount of information provided in the\npublication, and 3) the relevance of the publication to the grant. We discuss\nfuture improvements of the system and an online tool for scientists to try.",
        "translated": ""
    },
    {
        "title": "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple\n  Candidates for Efficient and Effective Retrieval",
        "url": "http://arxiv.org/abs/2405.12801v1",
        "pub_date": "2024-05-21",
        "summary": "A common retrieve-and-rerank paradigm involves retrieving a broad set of\nrelevant candidates using a scalable bi-encoder, followed by expensive but more\naccurate cross-encoders to a limited candidate set. However, this small subset\noften leads to error propagation from the bi-encoders, thereby restricting the\nperformance of the overall pipeline. To address these issues, we propose the\nComparing Multiple Candidates (CMC) framework, which compares a query and\nmultiple candidate embeddings jointly through shallow self-attention layers.\nWhile providing contextualized representations, CMC is scalable enough to\nhandle multiple comparisons simultaneously, where comparing 2K candidates takes\nonly twice as long as comparing 100. Practitioners can use CMC as a lightweight\nand effective reranker to improve top-1 accuracy. Moreover, when integrated\nwith another retriever, CMC reranking can function as a virtually enhanced\nretriever. This configuration adds only negligible latency compared to using a\nsingle retriever (virtual), while significantly improving recall at K\n(enhanced).} Through experiments, we demonstrate that CMC, as a virtually\nenhanced retriever, significantly improves Recall@k (+6.7, +3.5%-p for R@16,\nR@64) compared to the initial retrieval stage on the ZeSHEL dataset. Meanwhile,\nwe conduct experiments for direct reranking on entity, passage, and dialogue\nranking. The results indicate that CMC is not only faster (11x) than\ncross-encoders but also often more effective, with improved prediction\nperformance in Wikipedia entity linking (+0.7%-p) and DSTC7 dialogue ranking\n(+3.3%-p). The code and link to datasets are available at\nhttps://github.com/yc-song/cmc",
        "translated": ""
    },
    {
        "title": "RecGPT: Generative Pre-training for Text-based Recommendation",
        "url": "http://arxiv.org/abs/2405.12715v1",
        "pub_date": "2024-05-21",
        "summary": "We present the first domain-adapted and fully-trained large language model,\nRecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for\ntext-based recommendation. Experimental results on rating prediction and\nsequential recommendation tasks show that our model, RecGPT-7B-Instruct,\noutperforms previous strong baselines. We are releasing our RecGPT models as\nwell as their pre-training and fine-tuning datasets to facilitate future\nresearch and downstream applications in text-based recommendation. Public\n\"huggingface\" links to our RecGPT models and datasets are available at:\nhttps://github.com/VinAIResearch/RecGPT",
        "translated": ""
    },
    {
        "title": "Disentangled Representation with Cross Experts Covariance Loss for\n  Multi-Domain Recommendation",
        "url": "http://arxiv.org/abs/2405.12706v1",
        "pub_date": "2024-05-21",
        "summary": "Multi-domain learning (MDL) has emerged as a prominent research area aimed at\nenhancing the quality of personalized services. The key challenge in MDL lies\nin striking a balance between learning commonalities across domains while\npreserving the distinct characteristics of each domain. However, this gives\nrise to a challenging dilemma. On one hand, a model needs to leverage\ndomain-specific modules, such as experts or embeddings, to preserve the\nuniqueness of each domain. On the other hand, due to the long-tailed\ndistributions observed in real-world domains, some tail domains may lack\nsufficient samples to fully learn their corresponding modules. Unfortunately,\nexisting approaches have not adequately addressed this dilemma. To address this\nissue, we propose a novel model called Crocodile, which stands for\nCross-experts Covariance Loss for Disentangled Learning. Crocodile adopts a\nmulti-embedding paradigm to facilitate model learning and employs a Covariance\nLoss on these embeddings to disentangle them. This disentanglement enables the\nmodel to capture diverse user interests across domains effectively.\nAdditionally, we introduce a novel gating mechanism to further enhance the\ncapabilities of Crocodile. Through empirical analysis, we demonstrate that our\nproposed method successfully resolves these two challenges and outperforms all\nstate-of-the-art methods on publicly available datasets. We firmly believe that\nthe analytical perspectives and design concept of disentanglement presented in\nour work can pave the way for future research in the field of MDL.",
        "translated": ""
    },
    {
        "title": "RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error\n  Bound for Approximate Nearest Neighbor Search",
        "url": "http://arxiv.org/abs/2405.12497v1",
        "pub_date": "2024-05-21",
        "summary": "Searching for approximate nearest neighbors (ANN) in the high-dimensional\nEuclidean space is a pivotal problem. Recently, with the help of fast\nSIMD-based implementations, Product Quantization (PQ) and its variants can\noften efficiently and accurately estimate the distances between the vectors and\nhave achieved great success in the in-memory ANN search. Despite their\nempirical success, we note that these methods do not have a theoretical error\nbound and are observed to fail disastrously on some real-world datasets.\nMotivated by this, we propose a new randomized quantization method named\nRaBitQ, which quantizes $D$-dimensional vectors into $D$-bit strings. RaBitQ\nguarantees a sharp theoretical error bound and provides good empirical accuracy\nat the same time. In addition, we introduce efficient implementations of\nRaBitQ, supporting to estimate the distances with bitwise operations or\nSIMD-based operations. Extensive experiments on real-world datasets confirm\nthat (1) our method outperforms PQ and its variants in terms of\naccuracy-efficiency trade-off by a clear margin and (2) its empirical\nperformance is well-aligned with our theoretical analysis.",
        "translated": ""
    },
    {
        "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models",
        "url": "http://arxiv.org/abs/2405.14768v1",
        "pub_date": "2024-05-23",
        "summary": "Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code will be\nreleased at https://github.com/zjunlp/EasyEdit.",
        "translated": ""
    },
    {
        "title": "Push and Pull: A Framework for Measuring Attentional Agency",
        "url": "http://arxiv.org/abs/2405.14614v1",
        "pub_date": "2024-05-23",
        "summary": "We propose a framework for measuring attentional agency - the ability to\nallocate one's attention according to personal desires, goals, and intentions -\non digital platforms. Platforms extend people's limited powers of attention by\nextrapolating their preferences to large collections of previously unconsidered\ninformational objects. However, platforms typically also allow people to\ninfluence one another's attention. We introduce a formal framework for\nmeasuring how much a given platform empowers people to both pull information\ninto their own attentional field and push information into the attentional\nfields of others. We also use these definitions to shed light on the\nimplications of generative foundation models, which enable users to bypass the\nimplicit \"attentional bargain\" that underlies embedded advertising and other\nmethods for capturing economic value from informational goods. We conclude with\na set of policy strategies that can be used to understand and reshape the\ndistribution of attentional agency online.",
        "translated": ""
    },
    {
        "title": "Data Augmentation Techniques for Process Extraction from Scientific\n  Publications",
        "url": "http://arxiv.org/abs/2405.14594v1",
        "pub_date": "2024-05-23",
        "summary": "We present data augmentation techniques for process extraction tasks in\nscientific publications. We cast the process extraction task as a sequence\nlabeling task where we identify all the entities in a sentence and label them\naccording to their process-specific roles. The proposed method attempts to\ncreate meaningful augmented sentences by utilizing (1) process-specific\ninformation from the original sentence, (2) role label similarity, and (3)\nsentence similarity. We demonstrate that the proposed methods substantially\nimprove the performance of the process extraction model trained on chemistry\ndomain datasets, up to 12.3 points improvement in performance accuracy\n(F-score). The proposed methods could potentially reduce overfitting as well,\nespecially when training on small datasets or in a low-resource setting such as\nin chemistry and other scientific domains.",
        "translated": ""
    },
    {
        "title": "Top-Down Partitioning for Efficient List-Wise Ranking",
        "url": "http://arxiv.org/abs/2405.14589v1",
        "pub_date": "2024-05-23",
        "summary": "Large Language Models (LLMs) have significantly impacted many facets of\nnatural language processing and information retrieval. Unlike previous\nencoder-based approaches, the enlarged context window of these generative\nmodels allows for ranking multiple documents at once, commonly called list-wise\nranking. However, there are still limits to the number of documents that can be\nranked in a single inference of the model, leading to the broad adoption of a\nsliding window approach to identify the k most relevant items in a ranked list.\nWe argue that the sliding window approach is not well-suited for list-wise\nre-ranking because it (1) cannot be parallelized in its current form, (2) leads\nto redundant computational steps repeatedly re-scoring the best set of\ndocuments as it works its way up the initial ranking, and (3) prioritizes the\nlowest-ranked documents for scoring rather than the highest-ranked documents by\ntaking a bottom-up approach. Motivated by these shortcomings and an initial\nstudy that shows list-wise rankers are biased towards relevant documents at the\nstart of their context window, we propose a novel algorithm that partitions a\nranking to depth k and processes documents top-down. Unlike sliding window\napproaches, our algorithm is inherently parallelizable due to the use of a\npivot element, which can be compared to documents down to an arbitrary depth\nconcurrently. In doing so, we reduce the number of expected inference calls by\naround 33% when ranking at depth 100 while matching the performance of prior\napproaches across multiple strong re-rankers.",
        "translated": ""
    },
    {
        "title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG",
        "url": "http://arxiv.org/abs/2405.14431v1",
        "pub_date": "2024-05-23",
        "summary": "As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG)\ntechniques have evolved, query rewriting has been widely incorporated into the\nRAG system for downstream tasks like open-domain QA. Many works have attempted\nto utilize small models with reinforcement learning rather than costly LLMs to\nimprove query rewriting. However, current methods require annotations (e.g.,\nlabeled relevant documents or downstream answers) or predesigned rewards for\nfeedback, which lack generalization, and fail to utilize signals tailored for\nquery rewriting. In this paper, we propose ours, a framework for training query\nrewriting models free of annotations. By leveraging a publicly available\nreranker, ours~provides feedback aligned well with the rewriting objectives.\nExperimental results demonstrate that ours~can obtain better performance than\nbaselines.",
        "translated": ""
    },
    {
        "title": "Look into the Future: Deep Contextualized Sequential Recommendation",
        "url": "http://arxiv.org/abs/2405.14359v1",
        "pub_date": "2024-05-23",
        "summary": "Sequential recommendation focuses on mining useful patterns from the user\nbehavior history to better estimate his preference on the candidate items.\nPrevious solutions adopt recurrent networks or retrieval methods to obtain the\nuser's profile representation so as to perform the preference estimation. In\nthis paper, we propose a novel framework of sequential recommendation called\nLook into the Future (LIFT), which builds and leverages the contexts of\nsequential recommendation. The context in LIFT refers to a user's current\nprofile that can be represented based on both past and future behaviors. As\nsuch, the learned context will be more effective in predicting the user's\nbehaviors in sequential recommendation. Apparently, it is impossible to use\nreal future information to predict the current behavior, we thus propose a\nnovel retrieval-based framework to use the most similar interaction's future\ninformation as the future context of the target interaction without data\nleakage. Furthermore, in order to exploit the intrinsic information embedded\nwithin the context itself, we introduce an innovative pretraining methodology\nincorporating behavior masking. This approach is designed to facilitate the\nefficient acquisition of context representations. We demonstrate that finding\nrelevant contexts from the global user pool via retrieval methods will greatly\nimprove preference estimation performance. In our extensive experiments over\nreal-world datasets, LIFT demonstrates significant performance improvement on\nclick-through rate prediction tasks in sequential recommendation over strong\nbaselines.",
        "translated": ""
    },
    {
        "title": "ASI++: Towards Distributionally Balanced End-to-End Generative Retrieval",
        "url": "http://arxiv.org/abs/2405.14280v1",
        "pub_date": "2024-05-23",
        "summary": "Generative retrieval, a promising new paradigm in information retrieval,\nemploys a seq2seq model to encode document features into parameters and decode\nrelevant document identifiers (IDs) based on search queries. Existing\ngenerative retrieval solutions typically rely on a preprocessing stage to\npre-define document IDs, which can suffer from a semantic gap between these IDs\nand the retrieval task. However, end-to-end training for both ID assignments\nand retrieval tasks is challenging due to the long-tailed distribution\ncharacteristics of real-world data, resulting in inefficient and unbalanced ID\nspace utilization. To address these issues, we propose ASI++, a novel fully\nend-to-end generative retrieval method that aims to simultaneously learn\nbalanced ID assignments and improve retrieval performance. ASI++ builds on the\nfully end-to-end training framework of vanilla ASI and introduces several key\ninnovations. First, a distributionally balanced criterion addresses the\nimbalance in ID assignments, promoting more efficient utilization of the ID\nspace. Next, a representation bottleneck criterion enhances dense\nrepresentations to alleviate bottlenecks in learning ID assignments. Finally,\nan information consistency criterion integrates these processes into a joint\noptimization framework grounded in information theory. We further explore\nvarious module structures for learning ID assignments, including neural\nquantization, differentiable product quantization, and residual quantization.\nExtensive experiments on both public and industrial datasets demonstrate the\neffectiveness of ASI++ in improving retrieval performance and achieving\nbalanced ID assignments.",
        "translated": ""
    },
    {
        "title": "Identifying Breakdowns in Conversational Recommender Systems using User\n  Simulation",
        "url": "http://arxiv.org/abs/2405.14249v1",
        "pub_date": "2024-05-23",
        "summary": "We present a methodology to systematically test conversational recommender\nsystems with regards to conversational breakdowns. It involves examining\nconversations generated between the system and simulated users for a set of\npre-defined breakdown types, extracting responsible conversational paths, and\ncharacterizing them in terms of the underlying dialogue intents. User\nsimulation offers the advantages of simplicity, cost-effectiveness, and time\nefficiency for obtaining conversations where potential breakdowns can be\nidentified. The proposed methodology can be used as diagnostic tool as well as\na development tool to improve conversational recommendation systems. We apply\nour methodology in a case study with an existing conversational recommender\nsystem and user simulator, demonstrating that with just a few iterations, we\ncan make the system more robust to conversational breakdowns.",
        "translated": ""
    },
    {
        "title": "Generative AI Search Engines as Arbiters of Public Knowledge: An Audit\n  of Bias and Authority",
        "url": "http://arxiv.org/abs/2405.14034v1",
        "pub_date": "2024-05-22",
        "summary": "This paper reports on an audit study of generative AI systems (ChatGPT, Bing\nChat, and Perplexity) which investigates how these new search engines construct\nresponses and establish authority for topics of public importance. We collected\nsystem responses using a set of 48 authentic queries for 4 topics over a 7-day\nperiod and analyzed the data using sentiment analysis, inductive coding and\nsource classification. Results provide an overview of the nature of system\nresponses across these systems and provide evidence of sentiment bias based on\nthe queries and topics, and commercial and geographic bias in sources. The\nquality of sources used to support claims is uneven, relying heavily on News\nand Media, Business and Digital Media websites. Implications for system users\nemphasize the need to critically examine Generative AI system outputs when\nmaking decisions related to public interest and personal well-being.",
        "translated": ""
    },
    {
        "title": "AutoLCZ: Towards Automatized Local Climate Zone Mapping from Rule-Based\n  Remote Sensing",
        "url": "http://arxiv.org/abs/2405.13993v1",
        "pub_date": "2024-05-22",
        "summary": "Local climate zones (LCZs) established a standard classification system to\ncategorize the landscape universe for improved urban climate studies. Existing\nLCZ mapping is guided by human interaction with geographic information systems\n(GIS) or modelled from remote sensing (RS) data. GIS-based methods do not scale\nto large areas. However, RS-based methods leverage machine learning techniques\nto automatize LCZ classification from RS. Yet, RS-based methods require huge\namounts of manual labels for training.\n  We propose a novel LCZ mapping framework, termed AutoLCZ, to extract the LCZ\nclassification features from high-resolution RS modalities. We study the\ndefinition of numerical rules designed to mimic the LCZ definitions. Those\nrules model geometric and surface cover properties from LiDAR data.\nCorrespondingly, we enable LCZ classification from RS data in a GIS-based\nscheme. The proposed AutoLCZ method has potential to reduce the human labor to\nacquire accurate metadata. At the same time, AutoLCZ sheds light on the\nphysical interpretability of RS-based methods. In a proof-of-concept for New\nYork City (NYC) we leverage airborne LiDAR surveys to model 4 LCZ features to\ndistinguish 10 LCZ types. The results indicate the potential of AutoLCZ as\npromising avenue for large-scale LCZ mapping from RS data.",
        "translated": ""
    },
    {
        "title": "A Preference-oriented Diversity Model Based on Mutual-information in\n  Re-ranking for E-commerce Search",
        "url": "http://arxiv.org/abs/2405.15521v1",
        "pub_date": "2024-05-24",
        "summary": "Re-ranking is a process of rearranging ranking list to more effectively meet\nuser demands by accounting for the interrelationships between items. Existing\nmethods predominantly enhance the precision of search results, often at the\nexpense of diversity, leading to outcomes that may not fulfill the varied needs\nof users. Conversely, methods designed to promote diversity might compromise\nthe precision of the results, failing to satisfy the users' requirements for\naccuracy. To alleviate the above problems, this paper proposes a\nPreference-oriented Diversity Model Based on Mutual-information (PODM-MI),\nwhich consider both accuracy and diversity in the re-ranking process.\nSpecifically, PODM-MI adopts Multidimensional Gaussian distributions based on\nvariational inference to capture users' diversity preferences with uncertainty.\nThen we maximize the mutual information between the diversity preferences of\nthe users and the candidate items using the maximum variational inference lower\nbound to enhance their correlations. Subsequently, we derive a utility matrix\nbased on the correlations, enabling the adaptive ranking of items in line with\nuser preferences and establishing a balance between the aforementioned\nobjectives. Experimental results on real-world online e-commerce systems\ndemonstrate the significant improvements of PODM-MI, and we have successfully\ndeployed PODM-MI on an e-commerce search platform.",
        "translated": ""
    },
    {
        "title": "From Data Complexity to User Simplicity: A Framework for Linked Open\n  Data Reconciliation and Serendipitous Discovery",
        "url": "http://arxiv.org/abs/2405.15520v1",
        "pub_date": "2024-05-24",
        "summary": "This article introduces a novel software solution to create a Web portal to\nalign Linked Open Data sources and provide user-friendly interfaces for\nserendipitous discovery. We present the Polifonia Web portal as a motivating\nscenario and case study to address research problems such as data\nreconciliation and serving generous interfaces in the music heritage domain.",
        "translated": ""
    },
    {
        "title": "Self-distilled Dynamic Fusion Network for Language-based Fashion\n  Retrieval",
        "url": "http://arxiv.org/abs/2405.15451v1",
        "pub_date": "2024-05-24",
        "summary": "In the domain of language-based fashion image retrieval, pinpointing the\ndesired fashion item using both a reference image and its accompanying textual\ndescription is an intriguing challenge. Existing approaches lean heavily on\nstatic fusion techniques, intertwining image and text. Despite their\ncommendable advancements, these approaches are still limited by a deficiency in\nflexibility. In response, we propose a Self-distilled Dynamic Fusion Network to\ncompose the multi-granularity features dynamically by considering the\nconsistency of routing path and modality-specific information simultaneously.\nTwo new modules are included in our proposed method: (1) Dynamic Fusion Network\nwith Modality Specific Routers. The dynamic network enables a flexible\ndetermination of the routing for each reference image and modification text,\ntaking into account their distinct semantics and distributions. (2) Self Path\nDistillation Loss. A stable path decision for queries benefits the optimization\nof feature extraction as well as routing, and we approach this by progressively\nrefine the path decision with previous path information. Extensive experiments\ndemonstrate the effectiveness of our proposed model compared to existing\nmethods.",
        "translated": ""
    },
    {
        "title": "Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented\n  Knowledge Graphs and Vector Database for Accreditation Reporting Assistance",
        "url": "http://arxiv.org/abs/2405.15436v1",
        "pub_date": "2024-05-24",
        "summary": "In higher education, accreditation is a quality assurance process, where an\ninstitution demonstrates a commitment to delivering high quality programs and\nservices to their students. For business schools nationally and internationally\nthe Association to Advance Collegiate Schools of Business (AACSB) accreditation\nis the gold standard. For a business school to receive and subsequently\nmaintain accreditation, the school must undertake a rigorous, time consuming\nreporting and peer review process, to demonstrate alignment with the AACSB\nStandards. For this project we create a hybrid context retrieval augmented\ngeneration pipeline that can assist in the documentation alignment and\nreporting process necessary for accreditation. We implement both a vector\ndatabase and knowledge graph, as knowledge stores containing both institutional\ndata and AACSB Standard data. The output of the pipeline can be used by\ninstitution stakeholders to build their accreditation report, dually grounded\nby the context from the knowledge stores. To develop our knowledge graphs we\nutilized both a manual construction process as well as an LLM Augmented\nKnowledge Graph approach. We evaluated the pipeline using the RAGAs framework\nand observed optimal performance on answer relevancy and answer correctness\nmetrics.",
        "translated": ""
    },
    {
        "title": "Leveraging Large Language Models for Semantic Query Processing in a\n  Scholarly Knowledge Graph",
        "url": "http://arxiv.org/abs/2405.15374v1",
        "pub_date": "2024-05-24",
        "summary": "The proposed research aims to develop an innovative semantic query processing\nsystem that enables users to obtain comprehensive information about research\nworks produced by Computer Science (CS) researchers at the Australian National\nUniversity (ANU). The system integrates Large Language Models (LLMs) with the\nANU Scholarly Knowledge Graph (ASKG), a structured repository of all\nresearch-related artifacts produced at ANU in the CS field. Each artifact and\nits parts are represented as textual nodes stored in a Knowledge Graph (KG).\n  To address the limitations of traditional scholarly KG construction and\nutilization methods, which often fail to capture fine-grained details, we\npropose a novel framework that integrates the Deep Document Model (DDM) for\ncomprehensive document representation and the KG-enhanced Query Processing\n(KGQP) for optimized complex query handling. DDM enables a fine-grained\nrepresentation of the hierarchical structure and semantic relationships within\nacademic papers, while KGQP leverages the KG structure to improve query\naccuracy and efficiency with LLMs.\n  By combining the ASKG with LLMs, our approach enhances knowledge utilization\nand natural language understanding capabilities. The proposed system employs an\nautomatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from\nthe ASKG. Initial experiments demonstrate that our framework is superior to\nbaseline methods in terms of accuracy retrieval and query efficiency.\n  We showcase the practical application of our framework in academic research\nscenarios, highlighting its potential to revolutionize scholarly knowledge\nmanagement and discovery. This work empowers researchers to acquire and utilize\nknowledge from documents more effectively and provides a foundation for\ndeveloping precise and reliable interactions with LLMs.",
        "translated": ""
    },
    {
        "title": "Multi-Modal Recommendation Unlearning",
        "url": "http://arxiv.org/abs/2405.15328v1",
        "pub_date": "2024-05-24",
        "summary": "Unlearning methods for recommender systems (RS) have emerged to address\nprivacy issues and concerns about legal compliance. However, evolving user\npreferences and content licensing issues still remain unaddressed. This is\nparticularly true in case of multi-modal recommender systems (MMRS), which aim\nto accommodate the growing influence of multi-modal information on user\npreferences. Previous unlearning methods for RS are inapplicable to MMRS due to\nincompatibility of multi-modal user-item behavior data graph with the matrix\nbased representation of RS. Partitioning based methods degrade recommendation\nperformance and incur significant overhead costs during aggregation. This paper\nintroduces MMRecUN, a new framework for multi-modal recommendation unlearning,\nwhich, to the best of our knowledge, is the first attempt in this direction.\nGiven the trained recommendation model and marked forget data, we devise\nReverse Bayesian Personalized Ranking (BPR) objective to force the model to\nforget it. MMRecUN employs both reverse and forward BPR loss mechanisms to\nselectively attenuate the impact of interactions within the forget set while\nconcurrently reinforcing the significance of interactions within the retain\nset. Our experiments demonstrate that MMRecUN outperforms baseline methods\nacross various unlearning requests when evaluated on benchmark multi-modal\nrecommender datasets. MMRecUN achieves recall performance improvements of up to\n$\\mathbf{49.85%}$ compared to the baseline methods. It is up to\n$\\mathbf{1.3}\\times$ faster than the \\textsc{Gold} model, which is trained on\nretain data from scratch. MMRecUN offers advantages such as superior\nperformance in removing target elements, preservation of performance for\nretained elements, and zero overhead costs in comparison to previous methods.",
        "translated": ""
    },
    {
        "title": "DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback",
        "url": "http://arxiv.org/abs/2405.15280v1",
        "pub_date": "2024-05-24",
        "summary": "The graph-based recommendation has achieved great success in recent years.\nHowever, most existing graph-based recommendations focus on capturing user\npreference based on positive edges/feedback, while ignoring negative\nedges/feedback (e.g., dislike, low rating) that widely exist in real-world\nrecommender systems. How to utilize negative feedback in graph-based\nrecommendations still remains underexplored. In this study, we first conducted\na comprehensive experimental analysis and found that (1) existing graph neural\nnetworks are not well-suited for modeling negative feedback, which acts as a\nhigh-frequency signal in a user-item graph. (2) The graph-based recommendation\nsuffers from the representation degeneration problem. Based on the two\nobservations, we propose a novel model that models positive and negative\nfeedback from a frequency filter perspective called Dual-frequency Graph Neural\nNetwork for Sign-aware Recommendation (DFGNN). Specifically, in DFGNN, the\ndesigned dual-frequency graph filter (DGF) captures both low-frequency and\nhigh-frequency signals that contain positive and negative feedback.\nFurthermore, the proposed signed graph regularization is applied to maintain\nthe user/item embedding uniform in the embedding space to alleviate the\nrepresentation degeneration problem. Additionally, we conduct extensive\nexperiments on real-world datasets and demonstrate the effectiveness of the\nproposed model. Codes of our model will be released upon acceptance.",
        "translated": ""
    },
    {
        "title": "Shopping Queries Image Dataset (SQID): An Image-Enriched ESCI Dataset\n  for Exploring Multimodal Learning in Product Search",
        "url": "http://arxiv.org/abs/2405.15190v1",
        "pub_date": "2024-05-24",
        "summary": "Recent advances in the fields of Information Retrieval and Machine Learning\nhave focused on improving the performance of search engines to enhance the user\nexperience, especially in the world of online shopping. The focus has thus been\non leveraging cutting-edge learning techniques and relying on large enriched\ndatasets. This paper introduces the Shopping Queries Image Dataset (SQID), an\nextension of the Amazon Shopping Queries Dataset enriched with image\ninformation associated with 190,000 products. By integrating visual\ninformation, SQID facilitates research around multimodal learning techniques\nthat can take into account both textual and visual information for improving\nproduct search and ranking. We also provide experimental results leveraging\nSQID and pretrained models, showing the value of using multimodal data for\nsearch and ranking. SQID is available at:\nhttps://github.com/Crossing-Minds/shopping-queries-image-dataset.",
        "translated": ""
    },
    {
        "title": "Let Me Do It For You: Towards LLM Empowered Recommendation via Tool\n  Learning",
        "url": "http://arxiv.org/abs/2405.15114v1",
        "pub_date": "2024-05-24",
        "summary": "Conventional recommender systems (RSs) face challenges in precisely capturing\nusers' fine-grained preferences. Large language models (LLMs) have shown\ncapabilities in commonsense reasoning and leveraging external tools that may\nhelp address these challenges. However, existing LLM-based RSs suffer from\nhallucinations, misalignment between the semantic space of items and the\nbehavior space of users, or overly simplistic control strategies (e.g., whether\nto rank or directly present existing results). To bridge these gap, we\nintroduce ToolRec, a framework for LLM-empowered recommendations via tool\nlearning that uses LLMs as surrogate users, thereby guiding the recommendation\nprocess and invoking external tools to generate a recommendation list that\naligns closely with users' nuanced preferences.\n  We formulate the recommendation process as a process aimed at exploring user\ninterests in attribute granularity. The process factors in the nuances of the\ncontext and user preferences. The LLM then invokes external tools based on a\nuser's attribute instructions and probes different segments of the item pool.\nWe consider two types of attribute-oriented tools: rank tools and retrieval\ntools. Through the integration of LLMs, ToolRec enables conventional\nrecommender systems to become external tools with a natural language interface.\nExtensive experiments verify the effectiveness of ToolRec, particularly in\nscenarios that are rich in semantic content.",
        "translated": ""
    },
    {
        "title": "AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings",
        "url": "http://arxiv.org/abs/2405.15028v1",
        "pub_date": "2024-05-23",
        "summary": "Ranking is a fundamental and popular problem in search. However, existing\nranking algorithms usually restrict the granularity of ranking to full passages\nor require a specific dense index for each desired level of granularity. Such\nlack of flexibility in granularity negatively affects many applications that\ncan benefit from more granular ranking, such as sentence-level ranking for\nopen-domain question-answering, or proposition-level ranking for attribution.\nIn this work, we introduce the idea of any-granularity ranking, which leverages\nmulti-vector embeddings to rank at varying levels of granularity while\nmaintaining encoding at a single (coarser) level of granularity. We propose a\nmulti-granular contrastive loss for training multi-vector approaches, and\nvalidate its utility with both sentences and propositions as ranking units.\nFinally, we demonstrate the application of proposition-level ranking to\npost-hoc citation addition in retrieval-augmented generation, surpassing the\nperformance of prompt-driven citation generation.",
        "translated": ""
    },
    {
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding\n  Models",
        "url": "http://arxiv.org/abs/2405.17428v1",
        "pub_date": "2024-05-27",
        "summary": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last &lt;EOS&gt; token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.",
        "translated": ""
    },
    {
        "title": "KSW: Khmer Stop Word based Dictionary for Keyword Extraction",
        "url": "http://arxiv.org/abs/2405.17390v1",
        "pub_date": "2024-05-27",
        "summary": "This paper introduces KSW, a Khmer-specific approach to keyword extraction\nthat leverages a specialized stop word dictionary. Due to the limited\navailability of natural language processing resources for the Khmer language,\neffective keyword extraction has been a significant challenge. KSW addresses\nthis by developing a tailored stop word dictionary and implementing a\npreprocessing methodology to remove stop words, thereby enhancing the\nextraction of meaningful keywords. Our experiments demonstrate that KSW\nachieves substantial improvements in accuracy and relevance compared to\nprevious methods, highlighting its potential to advance Khmer text processing\nand information retrieval. The KSW resources, including the stop word\ndictionary, are available at the following GitHub repository:\n(https://github.com/back-kh/KSWv2-Khmer-Stop-Word-based-Dictionary-for-Keyword-Extraction.git).",
        "translated": ""
    },
    {
        "title": "DeeperImpact: Optimizing Sparse Learned Index Structures",
        "url": "http://arxiv.org/abs/2405.17093v1",
        "pub_date": "2024-05-27",
        "summary": "A lot of recent work has focused on sparse learned indexes that use deep\nneural architectures to significantly improve retrieval quality while keeping\nthe efficiency benefits of the inverted index. While such sparse learned\nstructures achieve effectiveness far beyond those of traditional inverted\nindex-based rankers, there is still a gap in effectiveness to the best dense\nretrievers, or even to sparse methods that leverage more expensive\noptimizations such as query expansion and query term weighting. We focus on\nnarrowing this gap by revisiting and optimizing DeepImpact, a sparse retrieval\napproach that uses DocT5Query for document expansion followed by a BERT\nlanguage model to learn impact scores for document terms. We first\nreinvestigate the expansion process and find that the recently proposed\nDoc2Query query filtration does not enhance retrieval quality when used with\nDeepImpact. Instead, substituting T5 with a fine-tuned Llama 2 model for query\nprediction results in a considerable improvement. Subsequently, we study\ntraining strategies that have proven effective for other models, in particular\nthe use of hard negatives, distillation, and pre-trained CoCondenser model\ninitialization. Our results significantly narrow the effectiveness gap with the\nmost effective versions of SPLADE.",
        "translated": ""
    },
    {
        "title": "Leveraging small language models for Text2SPARQL tasks to improve the\n  resilience of AI assistance",
        "url": "http://arxiv.org/abs/2405.17076v1",
        "pub_date": "2024-05-27",
        "summary": "In this work we will show that language models with less than one billion\nparameters can be used to translate natural language to SPARQL queries after\nfine-tuning. Using three different datasets ranging from academic to real\nworld, we identify prerequisites that the training data must fulfill in order\nfor the training to be successful. The goal is to empower users of semantic web\ntechnology to use AI assistance with affordable commodity hardware, making them\nmore resilient against external factors.",
        "translated": ""
    },
    {
        "title": "Robust kernel-free quadratic surface twin support vector machine with\n  capped $L_1$-norm distance metric",
        "url": "http://arxiv.org/abs/2405.16982v1",
        "pub_date": "2024-05-27",
        "summary": "Twin support vector machine (TSVM) is a very classical and practical\nclassifier for pattern classification. However, the traditional TSVM has two\nlimitations. Firstly, it uses the L_2-norm distance metric that leads to its\nsensitivity to outliers. Second, it needs to select the appropriate kernel\nfunction and the kernel parameters for nonlinear classification. To effectively\navoid these two problems, this paper proposes a robust capped L_1-norm\nkernel-free quadratic surface twin support vector machine (CL_1QTSVM). The\nstrengths of our model are briefly summarized as follows. 1) The robustness of\nour model is further improved by employing the capped L_1 norm distance metric.\n2) Our model is a kernel-free method that avoids the time-consuming process of\nselecting appropriate kernel functions and kernel parameters. 3) The\nintroduction of L_2-norm regularization term to improve the generalization\nability of the model. 4) To efficiently solve the proposed model, an iterative\nalgorithm is developed. 5) The convergence, time complexity and existence of\nlocally optimal solutions of the developed algorithms are further discussed.\nNumerical experiments on numerous types of datasets validate the classification\nperformance and robustness of the proposed model.",
        "translated": ""
    },
    {
        "title": "Empowering Large Language Models to Set up a Knowledge Retrieval Indexer\n  via Self-Learning",
        "url": "http://arxiv.org/abs/2405.16933v1",
        "pub_date": "2024-05-27",
        "summary": "Retrieval-Augmented Generation (RAG) offers a cost-effective approach to\ninjecting real-time knowledge into large language models (LLMs). Nevertheless,\nconstructing and validating high-quality knowledge repositories require\nconsiderable effort. We propose a pre-retrieval framework named Pseudo-Graph\nRetrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students\nby providing them with abundant raw reading materials and encouraging them to\nengage in autonomous reading to record factual information in their own words.\nThe resulting concise, well-organized mental indices are interconnected through\ncommon topics or complementary facts to form a pseudo-graph database. During\nthe retrieval phase, PG-RAG mimics the human behavior in flipping through\nnotes, identifying fact paths and subsequently exploring the related contexts.\nAdhering to the principle of the path taken by many is the best, it integrates\nhighly corroborated fact paths to provide a structured and refined sub-graph\nassisting LLMs. We validated PG-RAG on three specialized question-answering\ndatasets. In single-document tasks, PG-RAG significantly outperformed the\ncurrent best baseline, KGP-LLaMA, across all key evaluation metrics, with an\naverage overall performance improvement of 11.6%. Specifically, its BLEU score\nincreased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In\nmulti-document scenarios, the average metrics of PG-RAG were at least 2.35%\nhigher than the best baseline. Notably, the BLEU score and QE-F1 metric showed\nstable improvements of around 7.55% and 12.75%, respectively. Our code:\nhttps://github.com/IAAR-Shanghai/PGRAG.",
        "translated": ""
    },
    {
        "title": "Multi-Behavior Generative Recommendation",
        "url": "http://arxiv.org/abs/2405.16871v1",
        "pub_date": "2024-05-27",
        "summary": "Multi-behavior sequential recommendation (MBSR) aims to incorporate behavior\ntypes of interactions for better recommendations. Existing approaches focus on\nthe next-item prediction objective, neglecting the value of integrating the\ntarget behavior type into the learning objective. In this paper, we propose\nMBGen, a novel Multi-Behavior sequential Generative recommendation framework.\nWe formulate the MBSR task into a consecutive two-step process: (1) given item\nsequences, MBGen first predicts the next behavior type to frame the user\nintention, (2) given item sequences and a target behavior type, MBGen then\npredicts the next items. To model such a two-step process, we tokenize both\nbehaviors and items into tokens and construct one single token sequence with\nboth behaviors and items placed interleaved. Furthermore, MBGen learns to\nautoregressively generate the next behavior and item tokens in a unified\ngenerative recommendation paradigm, naturally enabling a multi-task capability.\nAdditionally, we exploit the heterogeneous nature of token sequences in the\ngenerative recommendation and propose a position-routed sparse architecture to\nefficiently and effectively scale up models. Extensive experiments on public\ndatasets demonstrate that MBGen significantly outperforms existing MBSR models\nacross multiple tasks.",
        "translated": ""
    },
    {
        "title": "NoteLLM-2: Multimodal Large Representation Models for Recommendation",
        "url": "http://arxiv.org/abs/2405.16789v1",
        "pub_date": "2024-05-27",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional text\nunderstanding. Existing works explore their application in text embedding\ntasks. However, there are few works utilizing LLMs to assist multimodal\nrepresentation tasks. In this work, we investigate the potential of LLMs to\nenhance multimodal representation in multimodal item-to-item (I2I)\nrecommendations. One feasible method is the transfer of Multimodal Large\nLanguage Models (MLLMs) for representation tasks. However, pre-training MLLMs\nusually requires collecting high-quality, web-scale multimodal data, resulting\nin complex training procedures and high costs. This leads the community to rely\nheavily on open-source MLLMs, hindering customized training for representation\nscenarios. Therefore, we aim to design an end-to-end training method that\ncustomizes the integration of any existing LLMs and vision encoders to\nconstruct efficient multimodal representation models. Preliminary experiments\nshow that fine-tuned LLMs in this end-to-end method tend to overlook image\ncontent. To overcome this challenge, we propose a novel training framework,\nNoteLLM-2, specifically designed for multimodal representation. We propose two\nways to enhance the focus on visual information. The first method is based on\nthe prompt viewpoint, which separates multimodal content into visual content\nand textual content. NoteLLM-2 adopts the multimodal In-Content Learning method\nto teach LLMs to focus on both modalities and aggregate key information. The\nsecond method is from the model architecture, utilizing a late fusion mechanism\nto directly fuse visual information into textual information. Extensive\nexperiments have been conducted to validate the effectiveness of our method.",
        "translated": ""
    },
    {
        "title": "ReCODE: Modeling Repeat Consumption with Neural ODE",
        "url": "http://arxiv.org/abs/2405.16550v1",
        "pub_date": "2024-05-26",
        "summary": "In real-world recommender systems, such as in the music domain, repeat\nconsumption is a common phenomenon where users frequently listen to a small set\nof preferred songs or artists repeatedly. The key point of modeling repeat\nconsumption is capturing the temporal patterns between a user's repeated\nconsumption of the items. Existing studies often rely on heuristic assumptions,\nsuch as assuming an exponential distribution for the temporal gaps. However,\ndue to the high complexity of real-world recommender systems, these pre-defined\ndistributions may fail to capture the intricate dynamic user consumption\npatterns, leading to sub-optimal performance. Drawing inspiration from the\nflexibility of neural ordinary differential equations (ODE) in capturing the\ndynamics of complex systems, we propose ReCODE, a novel model-agnostic\nframework that utilizes neural ODE to model repeat consumption. ReCODE\ncomprises two essential components: a user's static preference prediction\nmodule and the modeling of user dynamic repeat intention. By considering both\nimmediate choices and historical consumption patterns, ReCODE offers\ncomprehensive modeling of user preferences in the target context. Moreover,\nReCODE seamlessly integrates with various existing recommendation models,\nincluding collaborative-based and sequential-based models, making it easily\napplicable in different scenarios. Experimental results on two real-world\ndatasets consistently demonstrate that ReCODE significantly improves the\nperformance of base models and outperforms other baseline methods.",
        "translated": ""
    },
    {
        "title": "Cocktail: A Comprehensive Information Retrieval Benchmark with\n  LLM-Generated Documents Integration",
        "url": "http://arxiv.org/abs/2405.16546v1",
        "pub_date": "2024-05-26",
        "summary": "The proliferation of Large Language Models (LLMs) has led to an influx of\nAI-generated content (AIGC) on the internet, transforming the corpus of\nInformation Retrieval (IR) systems from solely human-written to a coexistence\nwith LLM-generated content. The impact of this surge in AIGC on IR systems\nremains an open question, with the primary challenge being the lack of a\ndedicated benchmark for researchers. In this paper, we introduce Cocktail, a\ncomprehensive benchmark tailored for evaluating IR models in this mixed-sourced\ndata landscape of the LLM era. Cocktail consists of 16 diverse datasets with\nmixed human-written and LLM-generated corpora across various text retrieval\ntasks and domains. Additionally, to avoid the potential bias from previously\nincluded dataset information in LLMs, we also introduce an up-to-date dataset,\nnamed NQ-UTD, with queries derived from recent events. Through conducting over\n1,000 experiments to assess state-of-the-art retrieval models against the\nbenchmarked datasets in Cocktail, we uncover a clear trade-off between ranking\nperformance and source bias in neural retrieval models, highlighting the\nnecessity for a balanced approach in designing future IR systems. We hope\nCocktail can serve as a foundational resource for IR research in the LLM era,\nwith all data and code publicly available at\n\\url{https://github.com/KID-22/Cocktail}.",
        "translated": ""
    },
    {
        "title": "Can We Trust Recommender System Fairness Evaluation? The Role of\n  Fairness and Relevance",
        "url": "http://arxiv.org/abs/2405.18276v1",
        "pub_date": "2024-05-28",
        "summary": "Relevance and fairness are two major objectives of recommender systems (RSs).\nRecent work proposes measures of RS fairness that are either independent from\nrelevance (fairness-only) or conditioned on relevance (joint measures). While\nfairness-only measures have been studied extensively, we look into whether\njoint measures can be trusted. We collect all joint evaluation measures of RS\nrelevance and fairness, and ask: How much do they agree with each other? To\nwhat extent do they agree with relevance/fairness measures? How sensitive are\nthey to changes in rank position, or to increasingly fair and relevant\nrecommendations? We empirically study for the first time the behaviour of these\nmeasures across 4 real-world datasets and 4 recommenders. We find that most of\nthese measures: i) correlate weakly with one another and even contradict each\nother at times; ii) are less sensitive to rank position changes than relevance-\nand fairness-only measures, meaning that they are less granular than\ntraditional RS measures; and iii) tend to compress scores at the low end of\ntheir range, meaning that they are not very expressive. We counter the above\nlimitations with a set of guidelines on the appropriate usage of such measures,\ni.e., they should be used with caution due to their tendency to contradict each\nother and of having a very small empirical range.",
        "translated": ""
    },
    {
        "title": "A Vlogger-augmented Graph Neural Network Model for Micro-video\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.18260v1",
        "pub_date": "2024-05-28",
        "summary": "Existing micro-video recommendation models exploit the interactions between\nusers and micro-videos and/or multi-modal information of micro-videos to\npredict the next micro-video a user will watch, ignoring the information\nrelated to vloggers, i.e., the producers of micro-videos. However, in\nmicro-video scenarios, vloggers play a significant role in user-video\ninteractions, since vloggers generally focus on specific topics and users tend\nto follow the vloggers they are interested in. Therefore, in the paper, we\npropose a vlogger-augmented graph neural network model VA-GNN, which takes the\neffect of vloggers into consideration. Specifically, we construct a tripartite\ngraph with users, micro-videos, and vloggers as nodes, capturing user\npreferences from different views, i.e., the video-view and the vlogger-view.\nMoreover, we conduct cross-view contrastive learning to keep the consistency\nbetween node embeddings from the two different views. Besides, when predicting\nthe next user-video interaction, we adaptively combine the user preferences for\na video itself and its vlogger. We conduct extensive experiments on two\nreal-world datasets. The experimental results show that VA-GNN outperforms\nmultiple existing GNN-based recommendation models.",
        "translated": ""
    },
    {
        "title": "Ranking with Ties based on Noisy Performance Data",
        "url": "http://arxiv.org/abs/2405.18259v1",
        "pub_date": "2024-05-28",
        "summary": "We consider the problem of ranking a set of objects based on their\nperformance when the measurement of said performance is subject to noise. In\nthis scenario, the performance is measured repeatedly, resulting in a range of\nmeasurements for each object. If the ranges of two objects do not overlap, then\nwe consider one object as 'better' than the other, and we expect it to receive\na higher rank; if, however, the ranges overlap, then the objects are\nincomparable, and we wish them to be assigned the same rank. Unfortunately, the\nincomparability relation of ranges is in general not transitive; as a\nconsequence, in general the two requirements cannot be satisfied\nsimultaneously, i.e., it is not possible to guarantee both distinct ranks for\nobjects with separated ranges, and same rank for objects with overlapping\nranges. This conflict leads to more than one reasonable way to rank a set of\nobjects. In this paper, we explore the ambiguities that arise when ranking with\nties, and define a set of reasonable rankings, which we call partial rankings.\nWe develop and analyse three different methodologies to compute a partial\nranking. Finally, we show how performance differences among objects can be\ninvestigated with the help of partial ranking.",
        "translated": ""
    },
    {
        "title": "Unified Low-rank Compression Framework for Click-through Rate Prediction",
        "url": "http://arxiv.org/abs/2405.18146v1",
        "pub_date": "2024-05-28",
        "summary": "Deep Click-Through Rate (CTR) prediction models play an important role in\nmodern industrial recommendation scenarios. However, high memory overhead and\ncomputational costs limit their deployment in resource-constrained\nenvironments. Low-rank approximation is an effective method for computer vision\nand natural language processing models, but its application in compressing CTR\nprediction models has been less explored. Due to the limited memory and\ncomputing resources, compression of CTR prediction models often confronts three\nfundamental challenges, i.e., (1). How to reduce the model sizes to adapt to\nedge devices? (2). How to speed up CTR prediction model inference? (3). How to\nretain the capabilities of original models after compression? Previous low-rank\ncompression research mostly uses tensor decomposition, which can achieve a high\nparameter compression ratio, but brings in AUC degradation and additional\ncomputing overhead. To address these challenges, we propose a unified low-rank\ndecomposition framework for compressing CTR prediction models. We find that\neven with the most classic matrix decomposition SVD method, our framework can\nachieve better performance than the original model. To further improve the\neffectiveness of our framework, we locally compress the output features instead\nof compressing the model weights. Our unified low-rank compression framework\ncan be applied to embedding tables and MLP layers in various CTR prediction\nmodels. Extensive experiments on two academic datasets and one real industrial\nbenchmark demonstrate that, with 3-5x model size reduction, our compressed\nmodels can achieve both faster inference and higher AUC than the uncompressed\noriginal models. Our code is at\nhttps://github.com/yuhao318/Atomic_Feature_Mimicking.",
        "translated": ""
    },
    {
        "title": "A Survey of Latent Factor Models in Recommender Systems",
        "url": "http://arxiv.org/abs/2405.18068v1",
        "pub_date": "2024-05-28",
        "summary": "Recommender systems are essential tools in the digital era, providing\npersonalized content to users in areas like e-commerce, entertainment, and\nsocial media. Among the many approaches developed to create these systems,\nlatent factor models have proven particularly effective. This survey\nsystematically reviews latent factor models in recommender systems, focusing on\ntheir core principles, methodologies, and recent advancements. The literature\nis examined through a structured framework covering learning data, model\narchitecture, learning strategies, and optimization techniques. The analysis\nincludes a taxonomy of contributions and detailed discussions on the types of\nlearning data used, such as implicit feedback, trust, and content data, various\nmodels such as probabilistic, nonlinear, and neural models, and an exploration\nof diverse learning strategies like online learning, transfer learning, and\nactive learning. Furthermore, the survey addresses the optimization strategies\nused to train latent factor models, improving their performance and\nscalability. By identifying trends, gaps, and potential research directions,\nthis survey aims to provide valuable insights for researchers and practitioners\nlooking to advance the field of recommender systems.",
        "translated": ""
    },
    {
        "title": "ReChorus2.0: A Modular and Task-Flexible Recommendation Library",
        "url": "http://arxiv.org/abs/2405.18058v1",
        "pub_date": "2024-05-28",
        "summary": "With the applications of recommendation systems rapidly expanding, an\nincreasing number of studies have focused on every aspect of recommender\nsystems with different data inputs, models, and task settings. Therefore, a\nflexible library is needed to help researchers implement the experimental\nstrategies they require. Existing open libraries for recommendation scenarios\nhave enabled reproducing various recommendation methods and provided standard\nimplementations. However, these libraries often impose certain restrictions on\ndata and seldom support the same model to perform different tasks and input\nformats, limiting users from customized explorations. To fill the gap, we\npropose ReChorus2.0, a modular and task-flexible library for recommendation\nresearchers. Based on ReChorus, we upgrade the supported input formats, models,\nand training&amp;evaluation strategies to help realize more recommendation tasks\nwith more data types. The main contributions of ReChorus2.0 include: (1)\nRealization of complex and practical tasks, including reranking and CTR\nprediction tasks; (2) Inclusion of various context-aware and rerank\nrecommenders; (3) Extension of existing and new models to support different\ntasks with the same models; (4) Support of highly-customized input with\nimpression logs, negative items, or click labels, as well as user, item, and\nsituation contexts. To summarize, ReChorus2.0 serves as a comprehensive and\nflexible library better aligning with the practical problems in the\nrecommendation scenario and catering to more diverse research needs. The\nimplementation and detailed tutorials of ReChorus2.0 can be found at\nhttps://github.com/THUwangcy/ReChorus.",
        "translated": ""
    },
    {
        "title": "Rethinking Recommender Systems: Cluster-based Algorithm Selection",
        "url": "http://arxiv.org/abs/2405.18011v1",
        "pub_date": "2024-05-28",
        "summary": "Cluster-based algorithm selection deals with selecting recommendation\nalgorithms on clusters of users to obtain performance gains. No studies have\nbeen attempted for many combinations of clustering approaches and\nrecommendation algorithms. We want to show that clustering users prior to\nalgorithm selection increases the performance of recommendation algorithms. Our\nstudy covers eight datasets, four clustering approaches, and eight\nrecommendation algorithms. We select the best performing recommendation\nalgorithm for each cluster. Our work shows that cluster-based algorithm\nselection is an effective technique for optimizing recommendation algorithm\nperformance. For five out of eight datasets, we report an increase in nDCG@10\nbetween 19.28% (0.032) and 360.38% (0.191) compared to algorithm selection\nwithout prior clustering.",
        "translated": ""
    },
    {
        "title": "Source Echo Chamber: Exploring the Escalation of Source Bias in User,\n  Data, and Recommender System Feedback Loop",
        "url": "http://arxiv.org/abs/2405.17998v1",
        "pub_date": "2024-05-28",
        "summary": "Recently, researchers have uncovered that neural retrieval models prefer\nAI-generated content (AIGC), called source bias. Compared to active search\nbehavior, recommendation represents another important means of information\nacquisition, where users are more prone to source bias. Furthermore, delving\ninto the recommendation scenario, as AIGC becomes integrated within the\nfeedback loop involving users, data, and the recommender system, it\nprogressively contaminates the candidate items, the user interaction history,\nand ultimately, the data used to train the recommendation models. How and to\nwhat extent the source bias affects the neural recommendation models within\nfeedback loop remains unknown. In this study, we extend the investigation of\nsource bias into the realm of recommender systems, specifically examining its\nimpact across different phases of the feedback loop. We conceptualize the\nprogression of AIGC integration into the recommendation content ecosystem in\nthree distinct phases-HGC dominate, HGC-AIGC coexist, and AIGC dominance-each\nrepresenting past, present, and future states, respectively. Through extensive\nexperiments across three datasets from diverse domains, we demonstrate the\nprevalence of source bias and reveal a potential digital echo chamber with\nsource bias amplification throughout the feedback loop. This trend risks\ncreating a recommender ecosystem with limited information source, such as AIGC,\nbeing disproportionately recommended. To counteract this bias and prevent its\nescalation in the feedback loop, we introduce a black-box debiasing method that\nmaintains model impartiality towards both HGC and AIGC. Our experimental\nresults validate the effectiveness of the proposed debiasing method, confirming\nits potential to disrupt the feedback loop.",
        "translated": ""
    },
    {
        "title": "Knowledge Circuits in Pretrained Transformers",
        "url": "http://arxiv.org/abs/2405.17969v1",
        "pub_date": "2024-05-28",
        "summary": "The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, has allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuit holds\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.",
        "translated": ""
    },
    {
        "title": "Attention-based sequential recommendation system using multimodal data",
        "url": "http://arxiv.org/abs/2405.17959v1",
        "pub_date": "2024-05-28",
        "summary": "Sequential recommendation systems that model dynamic preferences based on a\nuse's past behavior are crucial to e-commerce. Recent studies on these systems\nhave considered various types of information such as images and texts. However,\nmultimodal data have not yet been utilized directly to recommend products to\nusers. In this study, we propose an attention-based sequential recommendation\nmethod that employs multimodal data of items such as images, texts, and\ncategories. First, we extract image and text features from pre-trained VGG and\nBERT and convert categories into multi-labeled forms. Subsequently, attention\noperations are performed independent of the item sequence and multimodal\nrepresentations. Finally, the individual attention information is integrated\nthrough an attention fusion function. In addition, we apply multitask learning\nloss for each modality to improve the generalization performance. The\nexperimental results obtained from the Amazon datasets show that the proposed\nmethod outperforms those of conventional sequential recommendation systems.",
        "translated": ""
    },
    {
        "title": "A Multi-Source Retrieval Question Answering Framework Based on RAG",
        "url": "http://arxiv.org/abs/2405.19207v1",
        "pub_date": "2024-05-29",
        "summary": "With the rapid development of large-scale language models,\nRetrieval-Augmented Generation (RAG) has been widely adopted. However, existing\nRAG paradigms are inevitably influenced by erroneous retrieval information,\nthereby reducing the reliability and correctness of generated results.\nTherefore, to improve the relevance of retrieval information, this study\nproposes a method that replaces traditional retrievers with GPT-3.5, leveraging\nits vast corpus knowledge to generate retrieval information. We also propose a\nweb retrieval based method to implement fine-grained knowledge retrieval,\nUtilizing the powerful reasoning capability of GPT-3.5 to realize semantic\npartitioning of problem.In order to mitigate the illusion of GPT retrieval and\nreduce noise in Web retrieval,we proposes a multi-source retrieval framework,\nnamed MSRAG, which combines GPT retrieval with web retrieval. Experiments on\nmultiple knowledge-intensive QA datasets demonstrate that the proposed\nframework in this study performs better than existing RAG framework in\nenhancing the overall efficiency and accuracy of QA systems.",
        "translated": ""
    },
    {
        "title": "Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in\n  eDiscovery",
        "url": "http://arxiv.org/abs/2405.19164v1",
        "pub_date": "2024-05-29",
        "summary": "Electronic Discovery (eDiscovery) involves identifying relevant documents\nfrom a vast collection based on legal production requests. The integration of\nartificial intelligence (AI) and natural language processing (NLP) has\ntransformed this process, helping document review and enhance efficiency and\ncost-effectiveness. Although traditional approaches like BM25 or fine-tuned\npre-trained models are common in eDiscovery, they face performance,\ncomputational, and interpretability challenges. In contrast, Large Language\nModel (LLM)-based methods prioritize interpretability but sacrifice performance\nand throughput. This paper introduces DISCOvery Graph (DISCOG), a hybrid\napproach that combines the strengths of two worlds: a heterogeneous graph-based\nmethod for accurate document relevance prediction and subsequent LLM-driven\napproach for reasoning. Graph representational learning generates embeddings\nand predicts links, ranking the corpus for a given request, and the LLMs\nprovide reasoning for document relevance. Our approach handles datasets with\nbalanced and imbalanced distributions, outperforming baselines in F1-score,\nprecision, and recall by an average of 12%, 3%, and 16%, respectively. In an\nenterprise context, our approach drastically reduces document review costs by\n99.9% compared to manual processes and by 95% compared to LLM-based\nclassification methods",
        "translated": ""
    },
    {
        "title": "CaLa: Complementary Association Learning for Augmenting Composed Image\n  Retrieval",
        "url": "http://arxiv.org/abs/2405.19149v1",
        "pub_date": "2024-05-29",
        "summary": "Composed Image Retrieval (CIR) involves searching for target images based on\nan image-text pair query. While current methods treat this as a query-target\nmatching problem, we argue that CIR triplets contain additional associations\nbeyond this primary relation. In our paper, we identify two new relations\nwithin triplets, treating each triplet as a graph node. Firstly, we introduce\nthe concept of text-bridged image alignment, where the query text serves as a\nbridge between the query image and the target image. We propose a hinge-based\ncross-attention mechanism to incorporate this relation into network learning.\nSecondly, we explore complementary text reasoning, considering CIR as a form of\ncross-modal retrieval where two images compose to reason about complementary\ntext. To integrate these perspectives effectively, we design a twin\nattention-based compositor. By combining these complementary associations with\nthe explicit query pair-target image relation, we establish a comprehensive set\nof constraints for CIR. Our framework, CaLa (Complementary Association Learning\nfor Augmenting Composed Image Retrieval), leverages these insights. We evaluate\nCaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating\nits superiority in composed image retrieval.",
        "translated": ""
    },
    {
        "title": "Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.19093v1",
        "pub_date": "2024-05-29",
        "summary": "The International Classification of Diseases (ICD) serves as a definitive\nmedical classification system encompassing a wide range of diseases and\nconditions. The primary objective of ICD indexing is to allocate a subset of\nICD codes to a medical record, which facilitates standardized documentation and\nmanagement of various health conditions. Most existing approaches have suffered\nfrom selecting the proper label subsets from an extremely large ICD collection\nwith a heavy long-tailed label distribution. In this paper, we leverage a\nmulti-stage ``retrieve and re-rank'' framework as a novel solution to ICD\nindexing, via a hybrid discrete retrieval method, and re-rank retrieved\ncandidates with contrastive learning that allows the model to make more\naccurate predictions from a simplified label space. The retrieval model is a\nhybrid of auxiliary knowledge of the electronic health records (EHR) and a\ndiscrete retrieval method (BM25), which efficiently collects high-quality\ncandidates. In the last stage, we propose a label co-occurrence guided\ncontrastive re-ranking model, which re-ranks the candidate labels by pulling\ntogether the clinical notes with positive ICD codes. Experimental results show\nthe proposed method achieves state-of-the-art performance on a number of\nmeasures on the MIMIC-III benchmark.",
        "translated": ""
    },
    {
        "title": "An engine not a camera: Measuring performative power of online search",
        "url": "http://arxiv.org/abs/2405.19073v1",
        "pub_date": "2024-05-29",
        "summary": "The power of digital platforms is at the center of major ongoing policy and\nregulatory efforts. To advance existing debates, we designed and executed an\nexperiment to measure the power of online search providers, building on the\nrecent definition of performative power. Instantiated in our setting,\nperformative power quantifies the ability of a search engine to steer web\ntraffic by rearranging results. To operationalize this definition we developed\na browser extension that performs unassuming randomized experiments in the\nbackground. These randomized experiments emulate updates to the search\nalgorithm and identify the causal effect of different content arrangements on\nclicks. We formally relate these causal effects to performative power.\nAnalyzing tens of thousands of clicks, we discuss what our robust quantitative\nfindings say about the power of online search engines. More broadly, we\nenvision our work to serve as a blueprint for how performative power and online\nexperiments can be integrated with future investigations into the economic\npower of digital platforms.",
        "translated": ""
    },
    {
        "title": "Continual Collaborative Distillation for Recommender System",
        "url": "http://arxiv.org/abs/2405.19046v1",
        "pub_date": "2024-05-29",
        "summary": "Knowledge distillation (KD) has emerged as a promising technique for\naddressing the computational challenges associated with deploying large-scale\nrecommender systems. KD transfers the knowledge of a massive teacher system to\na compact student model, to reduce the huge computational burdens for inference\nwhile retaining high accuracy. The existing KD studies primarily focus on\none-time distillation in static environments, leaving a substantial gap in\ntheir applicability to real-world scenarios dealing with continuously incoming\nusers, items, and their interactions. In this work, we delve into a systematic\napproach to operating the teacher-student KD in a non-stationary data stream.\nOur goal is to enable efficient deployment through a compact student, which\npreserves the high performance of the massive teacher, while effectively\nadapting to continuously incoming data. We propose Continual Collaborative\nDistillation (CCD) framework, where both the teacher and the student\ncontinually and collaboratively evolve along the data stream. CCD facilitates\nthe student in effectively adapting to new data, while also enabling the\nteacher to fully leverage accumulated knowledge. We validate the effectiveness\nof CCD through extensive quantitative, ablative, and exploratory experiments on\ntwo real-world datasets. We expect this research direction to contribute to\nnarrowing the gap between existing KD studies and practical applications,\nthereby enhancing the applicability of KD in real-world systems.",
        "translated": ""
    },
    {
        "title": "SynerGraph: An Integrated Graph Convolution Network for Multimodal\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.19031v1",
        "pub_date": "2024-05-29",
        "summary": "This article presents a novel approach to multimodal recommendation systems,\nfocusing on integrating and purifying multimodal data. Our methodology starts\nby developing a filter to remove noise from various types of data, making the\nrecommendations more reliable. We studied the impact of top-K sparsification on\ndifferent datasets, finding optimal values that strike a balance between\nunderfitting and overfitting concerns. The study emphasizes the significant\nrole of textual information compared to visual data in providing a deep\nunderstanding of items. We conducted sensitivity analyses to understand how\ndifferent modalities and the use of purifier circle loss affect the efficiency\nof the model. The findings indicate that systems that incorporate multiple\nmodalities perform better than those relying on just one modality. Our approach\nhighlights the importance of modality purifiers in filtering out irrelevant\ndata, ensuring that user preferences remain relevant. Models without modality\npurifiers showed reduced performance, emphasizing the need for effective\nintegration of pre-extracted features. The proposed model, which includes an\nnovel self supervised auxiliary task, shows promise in accurately capturing\nuser preferences. The main goal of the fusion technique is to enhance the\nmodeling of user preferences by combining knowledge with item information,\nutilizing sophisticated language models. Extensive experiments show that our\nmodel produces better results than the existing state-of-the-art multimodal\nrecommendation systems.",
        "translated": ""
    },
    {
        "title": "Evaluating the External and Parametric Knowledge Fusion of Large\n  Language Models",
        "url": "http://arxiv.org/abs/2405.19010v1",
        "pub_date": "2024-05-29",
        "summary": "Integrating external knowledge into large language models (LLMs) presents a\npromising solution to overcome the limitations imposed by their antiquated and\nstatic parametric memory. Prior studies, however, have tended to over-reliance\non external knowledge, underestimating the valuable contributions of an LLMs'\nintrinsic parametric knowledge. The efficacy of LLMs in blending external and\nparametric knowledge remains largely unexplored, especially in cases where\nexternal knowledge is incomplete and necessitates supplementation by their\nparametric knowledge. We propose to deconstruct knowledge fusion into four\ndistinct scenarios, offering the first thorough investigation of LLM behavior\nacross each. We develop a systematic pipeline for data construction and\nknowledge infusion to simulate these fusion scenarios, facilitating a series of\ncontrolled experiments. Our investigation reveals that enhancing parametric\nknowledge within LLMs can significantly bolster their capability for knowledge\nintegration. Nonetheless, we identify persistent challenges in memorizing and\neliciting parametric knowledge, and determining parametric knowledge\nboundaries. Our findings aim to steer future explorations on harmonizing\nexternal and parametric knowledge within LLMs.",
        "translated": ""
    },
    {
        "title": "Mitigate Position Bias with Coupled Ranking Bias on CTR Prediction",
        "url": "http://arxiv.org/abs/2405.18971v1",
        "pub_date": "2024-05-29",
        "summary": "Position bias, i.e., users' preference of an item is affected by its placing\nposition, is well studied in the recommender system literature. However, most\nexisting methods ignore the widely coupled ranking bias, which is also related\nto the placing position of the item. Using both synthetic and industrial\ndatasets, we first show how this widely coexisted ranking bias deteriorates the\nperformance of the existing position bias estimation methods. To mitigate the\nposition bias with the presence of the ranking bias, we propose a novel\nposition bias estimation method, namely gradient interpolation, which fuses two\nestimation methods using a fusing weight. We further propose an adaptive method\nto automatically determine the optimal fusing weight. Extensive experiments on\nboth synthetic and industrial datasets demonstrate the superior performance of\nthe proposed methods.",
        "translated": ""
    },
    {
        "title": "Content-Agnostic Moderation for Stance-Neutral Recommendation",
        "url": "http://arxiv.org/abs/2405.18941v1",
        "pub_date": "2024-05-29",
        "summary": "Personalized recommendation systems often drive users towards more extreme\ncontent, exacerbating opinion polarization. While (content-aware) moderation\nhas been proposed to mitigate these effects, such approaches risk curtailing\nthe freedom of speech and of information. To address this concern, we propose\nand explore the feasibility of \\emph{content-agnostic} moderation as an\nalternative approach for reducing polarization. Content-agnostic moderation\ndoes not rely on the actual content being moderated, arguably making it less\nprone to forms of censorship. We establish theoretically that content-agnostic\nmoderation cannot be guaranteed to work in a fully generic setting. However, we\nshow that it can often be effectively achieved in practice with plausible\nassumptions. We introduce two novel content-agnostic moderation methods that\nmodify the recommendations from the content recommender to disperse user-item\nco-clusters without relying on content features.\n  To evaluate the potential of content-agnostic moderation in controlled\nexperiments, we built a simulation environment to analyze the closed-loop\nbehavior of a system with a given set of users, recommendation system, and\nmoderation approach. Through comprehensive experiments in this environment, we\nshow that our proposed moderation methods significantly enhance stance\nneutrality and maintain high recommendation quality across various data\nscenarios. Our results indicate that achieving stance neutrality without direct\ncontent information is not only feasible but can also help in developing more\nbalanced and informative recommendation systems without substantially degrading\nuser engagement.",
        "translated": ""
    },
    {
        "title": "Jina CLIP: Your CLIP Model Is Also Your Text Retriever",
        "url": "http://arxiv.org/abs/2405.20204v1",
        "pub_date": "2024-05-30",
        "summary": "Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.",
        "translated": ""
    },
    {
        "title": "Generating Query Recommendations via LLMs",
        "url": "http://arxiv.org/abs/2405.19749v1",
        "pub_date": "2024-05-30",
        "summary": "Query recommendation systems are ubiquitous in modern search engines,\nassisting users in producing effective queries to meet their information needs.\nHowever, these systems require a large amount of data to produce good\nrecommendations, such as a large collection of documents to index and query\nlogs. In particular, query logs and user data are not available in cold start\nscenarios. Query logs are expensive to collect and maintain and require complex\nand time-consuming cascading pipelines for creating, combining, and ranking\nrecommendations. To address these issues, we frame the query recommendation\nproblem as a generative task, proposing a novel approach called Generative\nQuery Recommendation (GQR). GQR uses an LLM as its foundation and does not\nrequire to be trained or fine-tuned to tackle the query recommendation problem.\nWe design a prompt that enables the LLM to understand the specific\nrecommendation task, even using a single example. We then improved our system\nby proposing a version that exploits query logs called Retriever-Augmented GQR\n(RA-GQR). RA-GQr dynamically composes its prompt by retrieving similar queries\nfrom query logs. GQR approaches reuses a pre-existing neural architecture\nresulting in a simpler and more ready-to-market approach, even in a cold start\nscenario. Our proposed GQR obtains state-of-the-art performance in terms of\nNDCG@10 and clarity score against two commercial search engines and the\nprevious state-of-the-art approach on the Robust04 and ClueWeb09B collections,\nimproving on average the NDCG@10 performance up to ~4% on Robust04 and\nClueWeb09B w.r.t the previous best competitor. RA-GQR further improve the\nNDCG@10 obtaining an increase of ~11%, ~6\\% on Robust04 and ClueWeb09B w.r.t\nthe best competitor. Furthermore, our system obtained ~59% of user preferences\nin a blind user study, proving that our method produces the most engaging\nqueries.",
        "translated": ""
    },
    {
        "title": "Uncertainty-aware sign language video retrieval with probability\n  distribution modeling",
        "url": "http://arxiv.org/abs/2405.19689v1",
        "pub_date": "2024-05-30",
        "summary": "Sign language video retrieval plays a key role in facilitating information\naccess for the deaf community. Despite significant advances in video-text\nretrieval, the complexity and inherent uncertainty of sign language preclude\nthe direct application of these techniques. Previous methods achieve the\nmapping between sign language video and text through fine-grained modal\nalignment. However, due to the scarcity of fine-grained annotation, the\nuncertainty inherent in sign language video is underestimated, limiting the\nfurther development of sign language retrieval tasks. To address this\nchallenge, we propose a novel Uncertainty-aware Probability Distribution\nRetrieval (UPRet), that conceptualizes the mapping process of sign language\nvideo and text in terms of probability distributions, explores their potential\ninterrelationships, and enables flexible mappings. Experiments on three\nbenchmarks demonstrate the effectiveness of our method, which achieves\nstate-of-the-art results on How2Sign (59.1%), PHOENIX-2014T (72.0%), and\nCSL-Daily (78.4%).",
        "translated": ""
    },
    {
        "title": "Keyword-driven Retrieval-Augmented Large Language Models for Cold-start\n  User Recommendations",
        "url": "http://arxiv.org/abs/2405.19612v1",
        "pub_date": "2024-05-30",
        "summary": "Recent advancements in Large Language Models (LLMs) have shown significant\npotential in enhancing recommender systems. However, addressing the cold-start\nrecommendation problem, where users lack historical data, remains a\nconsiderable challenge. In this paper, we introduce KALM4Rec (Keyword-driven\nRetrieval-Augmented Large Language Models for Cold-start User Recommendations),\na novel framework specifically designed to tackle this problem by requiring\nonly a few input keywords from users in a practical scenario of cold-start user\nrestaurant recommendations. KALM4Rec operates in two main stages: candidates\nretrieval and LLM-based candidates re-ranking. In the first stage,\nkeyword-driven retrieval models are used to identify potential candidates,\naddressing LLMs' limitations in processing extensive tokens and reducing the\nrisk of generating misleading information. In the second stage, we employ LLMs\nwith various prompting strategies, including zero-shot and few-shot techniques,\nto re-rank these candidates by integrating multiple examples directly into the\nLLM prompts. Our evaluation, using a Yelp restaurant dataset with user reviews\nfrom three English-speaking cities, shows that our proposed framework\nsignificantly improves recommendation quality. Specifically, the integration of\nin-context instructions with LLMs for re-ranking markedly enhances the\nperformance of the cold-start user recommender system.",
        "translated": ""
    },
    {
        "title": "MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings",
        "url": "http://arxiv.org/abs/2405.19504v1",
        "pub_date": "2024-05-29",
        "summary": "Neural embedding models have become a fundamental component of modern\ninformation retrieval (IR) pipelines. These models produce a single embedding\n$x \\in \\mathbb{R}^d$ per data-point, allowing for fast retrieval via highly\noptimized maximum inner product search (MIPS) algorithms. Recently, beginning\nwith the landmark ColBERT paper, multi-vector models, which produce a set of\nembedding per data point, have achieved markedly superior performance for IR\ntasks. Unfortunately, using these models for IR is computationally expensive\ndue to the increased complexity of multi-vector retrieval and scoring.\n  In this paper, we introduce MUVERA (MUlti-VEctor Retrieval Algorithm), a\nretrieval mechanism which reduces multi-vector similarity search to\nsingle-vector similarity search. This enables the usage of off-the-shelf MIPS\nsolvers for multi-vector retrieval. MUVERA asymmetrically generates Fixed\nDimensional Encodings (FDEs) of queries and documents, which are vectors whose\ninner product approximates multi-vector similarity. We prove that FDEs give\nhigh-quality $\\epsilon$-approximations, thus providing the first single-vector\nproxy for multi-vector similarity with theoretical guarantees. Empirically, we\nfind that FDEs achieve the same recall as prior state-of-the-art heuristics\nwhile retrieving 2-5$\\times$ fewer candidates. Compared to prior state of the\nart implementations, MUVERA achieves consistently good end-to-end recall and\nlatency across a diverse set of the BEIR retrieval datasets, achieving an\naverage of 10$\\%$ improved recall with $90\\%$ lower latency.",
        "translated": ""
    },
    {
        "title": "Retrieval Augmented Structured Generation: Business Document Information\n  Extraction As Tool Use",
        "url": "http://arxiv.org/abs/2405.20245v1",
        "pub_date": "2024-05-30",
        "summary": "Business Document Information Extraction (BDIE) is the problem of\ntransforming a blob of unstructured information (raw text, scanned documents,\netc.) into a structured format that downstream systems can parse and use. It\nhas two main tasks: Key-Information Extraction (KIE) and Line Items Recognition\n(LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem,\nwhere the tools are these downstream systems. We then present Retrieval\nAugmented Structured Generation (RASG), a novel general framework for BDIE that\nachieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE\nbenchmarks.\n  The contributions of this paper are threefold: (1) We show, with ablation\nbenchmarks, that Large Language Models (LLMs) with RASG are already competitive\nwith or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on\nBDIE benchmarks. (2) We propose a new metric class for Line Items Recognition,\nGeneral Line Items Recognition Metric (GLIRM), that is more aligned with\npractical BDIE use cases compared to existing metrics, such as ANLS*, DocILE,\nand GriTS. (3) We provide a heuristic algorithm for backcalculating bounding\nboxes of predicted line items and tables without the need for vision encoders.\nFinally, we claim that, while LMMs might sometimes offer marginal performance\nbenefits, LLMs + RASG is oftentimes superior given real-world applications and\nconstraints of BDIE.",
        "translated": ""
    },
    {
        "title": "CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to\n  Web Relevance Ranking",
        "url": "http://arxiv.org/abs/2405.20994v1",
        "pub_date": "2024-05-31",
        "summary": "We present CWRCzech, Click Web Ranking dataset for Czech, a 100M\nquery-document Czech click dataset for relevance ranking with user behavior\ndata collected from search engine logs of Seznam.cz. To the best of our\nknowledge, CWRCzech is the largest click dataset with raw text published so\nfar. It provides document positions in the search results as well as\ninformation about user behavior: 27.6M clicked documents and 10.8M dwell times.\nIn addition, we also publish a manually annotated Czech test for the relevance\ntask, containing nearly 50k query-document pairs, each annotated by at least 2\nannotators. Finally, we analyze how the user behavior data improve relevance\nranking and show that models trained on data automatically harnessed at\nsufficient scale can surpass the performance of models trained on human\nannotated data. CWRCzech is published under an academic non-commercial license\nand is available to the research community at\nhttps://github.com/seznam/CWRCzech.",
        "translated": ""
    },
    {
        "title": "SelfGNN: Self-Supervised Graph Neural Networks for Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.20878v1",
        "pub_date": "2024-05-31",
        "summary": "Sequential recommendation effectively addresses information overload by\nmodeling users' temporal and sequential interaction patterns. To overcome the\nlimitations of supervision signals, recent approaches have adopted\nself-supervised learning techniques in recommender systems. However, there are\nstill two critical challenges that remain unsolved. Firstly, existing\nsequential models primarily focus on long-term modeling of individual\ninteraction sequences, overlooking the valuable short-term collaborative\nrelationships among the behaviors of different users. Secondly, real-world data\noften contain noise, particularly in users' short-term behaviors, which can\narise from temporary intents or misclicks. Such noise negatively impacts the\naccuracy of both graph and sequence models, further complicating the modeling\nprocess. To address these challenges, we propose a novel framework called\nSelf-Supervised Graph Neural Network (SelfGNN) for sequential recommendation.\nThe SelfGNN framework encodes short-term graphs based on time intervals and\nutilizes Graph Neural Networks (GNNs) to learn short-term collaborative\nrelationships. It captures long-term user and item representations at multiple\ngranularity levels through interval fusion and dynamic behavior modeling.\nImportantly, our personalized self-augmented learning structure enhances model\nrobustness by mitigating noise in short-term graphs based on long-term user\ninterests and personal stability. Extensive experiments conducted on four\nreal-world datasets demonstrate that SelfGNN outperforms various\nstate-of-the-art baselines. Our model implementation codes are available at\nhttps://github.com/HKUDS/SelfGNN.",
        "translated": ""
    },
    {
        "title": "Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias",
        "url": "http://arxiv.org/abs/2405.20718v1",
        "pub_date": "2024-05-31",
        "summary": "Collaborative Filtering (CF) typically suffers from the significant challenge\nof popularity bias due to the uneven distribution of items in real-world\ndatasets. This bias leads to a significant accuracy gap between popular and\nunpopular items. It not only hinders accurate user preference understanding but\nalso exacerbates the Matthew effect in recommendation systems. To alleviate\npopularity bias, existing efforts focus on emphasizing unpopular items or\nseparating the correlation between item representations and their popularity.\nDespite the effectiveness, existing works still face two persistent challenges:\n(1) how to extract common supervision signals from popular items to improve the\nunpopular item representations, and (2) how to alleviate the representation\nseparation caused by popularity bias. In this work, we conduct an empirical\nanalysis of popularity bias and propose Popularity-Aware Alignment and Contrast\n(PAAC) to address two challenges. Specifically, we use the common supervisory\nsignals modeled in popular item representations and propose a novel\npopularity-aware supervised alignment module to learn unpopular item\nrepresentations. Additionally, we suggest re-weighting the contrastive learning\nloss to mitigate the representation separation from a popularity-centric\nperspective. Finally, we validate the effectiveness and rationale of PAAC in\nmitigating popularity bias through extensive experiments on three real-world\ndatasets. Our code is available at\nhttps://github.com/miaomiao-cai2/KDD2024-PAAC.",
        "translated": ""
    },
    {
        "title": "Information Maximization via Variational Autoencoders for Cross-Domain\n  Recommendation",
        "url": "http://arxiv.org/abs/2405.20710v1",
        "pub_date": "2024-05-31",
        "summary": "Cross-Domain Sequential Recommendation (CDSR) methods aim to address the data\nsparsity and cold-start problems present in Single-Domain Sequential\nRecommendation (SDSR). Existing CDSR methods typically rely on overlapping\nusers, designing complex cross-domain modules to capture users' latent\ninterests that can propagate across different domains. However, their\npropagated informative information is limited to the overlapping users and the\nusers who have rich historical behavior records. As a result, these methods\noften underperform in real-world scenarios, where most users are\nnon-overlapping (cold-start) and long-tailed. In this research, we introduce a\nnew CDSR framework named Information Maximization Variational Autoencoder\n(\\textbf{\\texttt{IM-VAE}}). Here, we suggest using a Pseudo-Sequence Generator\nto enhance the user's interaction history input for downstream fine-grained\nCDSR models to alleviate the cold-start issues. We also propose a Generative\nRecommendation Framework combined with three regularizers inspired by the\nmutual information maximization (MIM) theory \\cite{mcgill1954multivariate} to\ncapture the semantic differences between a user's interests shared across\ndomains and those specific to certain domains, as well as address the\ninformational gap between a user's actual interaction sequences and the\npseudo-sequences generated. To the best of our knowledge, this paper is the\nfirst CDSR work that considers the information disentanglement and denoising of\npseudo-sequences in the open-world recommendation scenario. Empirical\nexperiments illustrate that \\texttt{IM-VAE} outperforms the state-of-the-art\napproaches on two real-world cross-domain datasets on all sorts of users,\nincluding cold-start and tailed users, demonstrating the effectiveness of\n\\texttt{IM-VAE} in open-world recommendation.",
        "translated": ""
    },
    {
        "title": "Passage-specific Prompt Tuning for Passage Reranking in Question\n  Answering with Large Language Models",
        "url": "http://arxiv.org/abs/2405.20654v1",
        "pub_date": "2024-05-31",
        "summary": "Effective passage retrieval and reranking methods have been widely utilized\nto identify suitable candidates in open-domain question answering tasks, recent\nstudies have resorted to LLMs for reranking the retrieved passages by the\nlog-likelihood of the question conditioned on each passage. Although these\nmethods have demonstrated promising results, the performance is notably\nsensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs\ncan be computationally intensive and time-consuming. Furthermore, this approach\nlimits the leverage of question-passage relevance pairs and passage-specific\nknowledge to enhance the ranking capabilities of LLMs. In this paper, we\npropose passage-specific prompt tuning for reranking in open-domain question\nanswering (PSPT): a parameter-efficient method that fine-tunes learnable\npassage-specific soft prompts, incorporating passage-specific knowledge from a\nlimited set of question-passage relevance pairs. The method involves ranking\nretrieved passages based on the log-likelihood of the model generating the\nquestion conditioned on each passage and the learned soft prompt. We conducted\nextensive experiments utilizing the Llama-2-chat-7B model across three publicly\navailable open-domain question answering datasets and the results demonstrate\nthe effectiveness of the proposed approach.",
        "translated": ""
    },
    {
        "title": "Large Language Models Enhanced Sequential Recommendation for Long-tail\n  User and Item",
        "url": "http://arxiv.org/abs/2405.20646v1",
        "pub_date": "2024-05-31",
        "summary": "Sequential recommendation systems (SRS) serve the purpose of predicting\nusers' subsequent preferences based on their past interactions and have been\napplied across various domains such as e-commerce and social networking\nplatforms. However, practical SRS encounters challenges due to the fact that\nmost users engage with only a limited number of items, while the majority of\nitems are seldom consumed. These challenges, termed as the long-tail user and\nlong-tail item dilemmas, often create obstacles for traditional SRS methods.\nMitigating these challenges is crucial as they can significantly impact user\nsatisfaction and business profitability. While some research endeavors have\nalleviated these issues, they still grapple with issues such as seesaw or noise\nstemming from the scarcity of interactions. The emergence of large language\nmodels (LLMs) presents a promising avenue to address these challenges from a\nsemantic standpoint. In this study, we introduce the Large Language Models\nEnhancement framework for Sequential Recommendation (LLM-ESR), which leverages\nsemantic embeddings from LLMs to enhance SRS performance without increasing\ncomputational overhead. To combat the long-tail item challenge, we propose a\ndual-view modeling approach that fuses semantic information from LLMs with\ncollaborative signals from traditional SRS. To address the long-tail user\nchallenge, we introduce a retrieval augmented self-distillation technique to\nrefine user preference representations by incorporating richer interaction data\nfrom similar users. Through comprehensive experiments conducted on three\nauthentic datasets using three widely used SRS models, our proposed enhancement\nframework demonstrates superior performance compared to existing methodologies.",
        "translated": ""
    },
    {
        "title": "Causal Distillation for Alleviating Performance Heterogeneity in\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2405.20626v1",
        "pub_date": "2024-05-31",
        "summary": "Recommendation performance usually exhibits a long-tail distribution over\nusers -- a small portion of head users enjoy much more accurate recommendation\nservices than the others. We reveal two sources of this performance\nheterogeneity problem: the uneven distribution of historical interactions (a\nnatural source); and the biased training of recommender models (a model\nsource). As addressing this problem cannot sacrifice the overall performance, a\nwise choice is to eliminate the model bias while maintaining the natural\nheterogeneity. The key to debiased training lies in eliminating the effect of\nconfounders that influence both the user's historical behaviors and the next\nbehavior. The emerging causal recommendation methods achieve this by modeling\nthe causal effect between user behaviors, however potentially neglect\nunobserved confounders (\\eg, friend suggestions) that are hard to measure in\npractice. To address unobserved confounders, we resort to the front-door\nadjustment (FDA) in causal theory and propose a causal multi-teacher\ndistillation framework (CausalD). FDA requires proper mediators in order to\nestimate the causal effects of historical behaviors on the next behavior. To\nachieve this, we equip CausalD with multiple heterogeneous recommendation\nmodels to model the mediator distribution. Then, the causal effect estimated by\nFDA is the expectation of recommendation prediction over the mediator\ndistribution and the prior distribution of historical behaviors, which is\ntechnically achieved by multi-teacher ensemble. To pursue efficient inference,\nCausalD further distills multiple teachers into one student model to directly\ninfer the causal effect for making recommendations.",
        "translated": ""
    },
    {
        "title": "Knowledge Enhanced Multi-intent Transformer Network for Recommendation",
        "url": "http://arxiv.org/abs/2405.20565v1",
        "pub_date": "2024-05-31",
        "summary": "Incorporating Knowledge Graphs into Recommendation has attracted growing\nattention in industry, due to the great potential of KG in providing abundant\nsupplementary information and interpretability for the underlying models.\nHowever, simply integrating KG into recommendation usually brings in negative\nfeedback in industry, due to the ignorance of the following two factors: i)\nusers' multiple intents, which involve diverse nodes in KG. For example, in\ne-commerce scenarios, users may exhibit preferences for specific styles,\nbrands, or colors. ii) knowledge noise, which is a prevalent issue in Knowledge\nEnhanced Recommendation (KGR) and even more severe in industry scenarios. The\nirrelevant knowledge properties of items may result in inferior model\nperformance compared to approaches that do not incorporate knowledge. To tackle\nthese challenges, we propose a novel approach named Knowledge Enhanced\nMulti-intent Transformer Network for Recommendation (KGTN), comprising two\nprimary modules: Global Intents Modeling with Graph Transformer, and Knowledge\nContrastive Denoising under Intents. Specifically, Global Intents with Graph\nTransformer focuses on capturing learnable user intents, by incorporating\nglobal signals from user-item-relation-entity interactions with a graph\ntransformer, meanwhile learning intent-aware user/item representations.\nKnowledge Contrastive Denoising under Intents is dedicated to learning precise\nand robust representations. It leverages intent-aware representations to sample\nrelevant knowledge, and proposes a local-global contrastive mechanism to\nenhance noise-irrelevant representation learning. Extensive experiments\nconducted on benchmark datasets show the superior performance of our proposed\nmethod over the state-of-the-arts. And online A/B testing results on Alibaba\nlarge-scale industrial recommendation platform also indicate the real-scenario\neffectiveness of KGTN.",
        "translated": ""
    },
    {
        "title": "Extending the Massive Text Embedding Benchmark to French",
        "url": "http://arxiv.org/abs/2405.20468v1",
        "pub_date": "2024-05-30",
        "summary": "In recent years, numerous embedding models have been made available and\nwidely used for various NLP tasks. Choosing a model that performs well for\nseveral tasks in English has been largely simplified by the Massive Text\nEmbedding Benchmark (MTEB), but extensions to other languages remain\nchallenging. This is why we expand MTEB to propose the first massive benchmark\nof sentence embeddings for French. Not only we gather 22 existing datasets in\nan easy-to-use interface, but we also create three new French datasets for a\nglobal evaluation over 8 different tasks. We perform a large scale comparison\nwith 46 carefully selected embedding models, conduct comprehensive statistical\ntests, and analyze the correlation between model performance and many of their\ncharacteristics. We find out that even if no model is the best on all tasks,\nlarge multilingual models pre-trained on sentence similarity perform\nparticularly well. Our work comes with open-source code, new datasets and a\npublic leaderboard.",
        "translated": ""
    },
    {
        "title": "Designing an Evaluation Framework for Large Language Models in Astronomy\n  Research",
        "url": "http://arxiv.org/abs/2405.20389v1",
        "pub_date": "2024-05-30",
        "summary": "Large Language Models (LLMs) are shifting how scientific research is done. It\nis imperative to understand how researchers interact with these models and how\nscientific sub-communities like astronomy might benefit from them. However,\nthere is currently no standard for evaluating the use of LLMs in astronomy.\nTherefore, we present the experimental design for an evaluation study on how\nastronomy researchers interact with LLMs. We deploy a Slack chatbot that can\nanswer queries from users via Retrieval-Augmented Generation (RAG); these\nresponses are grounded in astronomy papers from arXiv. We record and anonymize\nuser questions and chatbot answers, user upvotes and downvotes to LLM\nresponses, user feedback to the LLM, and retrieved documents and similarity\nscores with the query. Our data collection method will enable future dynamic\nevaluations of LLM tools for astronomy.",
        "translated": ""
    },
    {
        "title": "XRec: Large Language Models for Explainable Recommendation",
        "url": "http://arxiv.org/abs/2406.02377v1",
        "pub_date": "2024-06-04",
        "summary": "Recommender systems help users navigate information overload by providing\npersonalized recommendations aligned with their preferences. Collaborative\nFiltering (CF) is a widely adopted approach, but while advanced techniques like\ngraph neural networks (GNNs) and self-supervised learning (SSL) have enhanced\nCF models for better user representations, they often lack the ability to\nprovide explanations for the recommended items. Explainable recommendations aim\nto address this gap by offering transparency and insights into the\nrecommendation decision-making process, enhancing users' understanding. This\nwork leverages the language capabilities of Large Language Models (LLMs) to\npush the boundaries of explainable recommender systems. We introduce a\nmodel-agnostic framework called XRec, which enables LLMs to provide\ncomprehensive explanations for user behaviors in recommender systems. By\nintegrating collaborative signals and designing a lightweight collaborative\nadaptor, the framework empowers LLMs to understand complex patterns in\nuser-item interactions and gain a deeper understanding of user preferences. Our\nextensive experiments demonstrate the effectiveness of XRec, showcasing its\nability to generate comprehensive and meaningful explanations that outperform\nbaseline approaches in explainable recommender systems. We open-source our\nmodel implementation at https://github.com/HKUDS/XRec.",
        "translated": ""
    },
    {
        "title": "Large Language Models Make Sample-Efficient Recommender Systems",
        "url": "http://arxiv.org/abs/2406.02368v1",
        "pub_date": "2024-06-04",
        "summary": "Large language models (LLMs) have achieved remarkable progress in the field\nof natural language processing (NLP), demonstrating remarkable abilities in\nproducing text that resembles human language for various tasks. This opens up\nnew opportunities for employing them in recommender systems (RSs). In this\npaper, we specifically examine the sample efficiency of LLM-enhanced\nrecommender systems, which pertains to the model's capacity to attain superior\nperformance with a limited quantity of training data. Conventional\nrecommendation models (CRMs) often need a large amount of training data because\nof the sparsity of features and interactions. Hence, we propose and verify our\ncore viewpoint: Large Language Models Make Sample-Efficient Recommender\nSystems. We propose a simple yet effective framework (i.e., Laser) to validate\nthe viewpoint from two aspects: (1) LLMs themselves are sample-efficient\nrecommenders; and (2) LLMs, as feature generators and encoders, make CRMs more\nsample-efficient. Extensive experiments on two public datasets show that Laser\nrequires only a small fraction of training samples to match or even surpass\nCRMs that are trained on the entire training set, demonstrating superior sample\nefficiency.",
        "translated": ""
    },
    {
        "title": "Description Boosting for Zero-Shot Entity and Relation Classification",
        "url": "http://arxiv.org/abs/2406.02245v1",
        "pub_date": "2024-06-04",
        "summary": "Zero-shot entity and relation classification models leverage available\nexternal information of unseen classes -- e.g., textual descriptions -- to\nannotate input text data. Thanks to the minimum data requirement, Zero-Shot\nLearning (ZSL) methods have high value in practice, especially in applications\nwhere labeled data is scarce. Even though recent research in ZSL has\ndemonstrated significant results, our analysis reveals that those methods are\nsensitive to provided textual descriptions of entities (or relations). Even a\nminor modification of descriptions can lead to a change in the decision\nboundary between entity (or relation) classes. In this paper, we formally\ndefine the problem of identifying effective descriptions for zero shot\ninference. We propose a strategy for generating variations of an initial\ndescription, a heuristic for ranking them and an ensemble method capable of\nboosting the predictions of zero-shot models through description enhancement.\nEmpirical results on four different entity and relation classification datasets\nshow that our proposed method outperform existing approaches and achieve new\nSOTA results on these datasets under the ZSL settings. The source code of the\nproposed solutions and the evaluation framework are open-sourced.",
        "translated": ""
    },
    {
        "title": "Pairwise Ranking Loss for Multi-Task Learning in Recommender Systems",
        "url": "http://arxiv.org/abs/2406.02163v1",
        "pub_date": "2024-06-04",
        "summary": "Multi-Task Learning (MTL) plays a crucial role in real-world advertising\napplications such as recommender systems, aiming to achieve robust\nrepresentations while minimizing resource consumption. MTL endeavors to\nsimultaneously optimize multiple tasks to construct a unified model serving\ndiverse objectives. In online advertising systems, tasks like Click-Through\nRate (CTR) and Conversion Rate (CVR) are often treated as MTL problems\nconcurrently. However, it has been overlooked that a conversion ($y_{cvr}=1$)\nnecessitates a preceding click ($y_{ctr}=1$). In other words, while certain CTR\ntasks are associated with corresponding conversions, others lack such\nassociations. Moreover, the likelihood of noise is significantly higher in CTR\ntasks where conversions do not occur compared to those where they do, and\nexisting methods lack the ability to differentiate between these two scenarios.\nIn this study, exposure labels corresponding to conversions are regarded as\ndefinitive indicators, and a novel task-specific loss is introduced by\ncalculating a \\textbf{p}air\\textbf{wise} \\textbf{r}anking (PWiseR) loss between\nmodel predictions, manifesting as pairwise ranking loss, to encourage the model\nto rely more on them. To demonstrate the effect of the proposed loss function,\nexperiments were conducted on different MTL and Single-Task Learning (STL)\nmodels using four distinct public MTL datasets, namely Alibaba FR, NL, US, and\nCCP, along with a proprietary industrial dataset. The results indicate that our\nproposed loss function outperforms the BCE loss function in most cases in terms\nof the AUC metric.",
        "translated": ""
    },
    {
        "title": "Robust Interaction-based Relevance Modeling for Online E-Commerce and\n  LLM-based Retrieval",
        "url": "http://arxiv.org/abs/2406.02135v1",
        "pub_date": "2024-06-04",
        "summary": "Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world.",
        "translated": ""
    },
    {
        "title": "Auto-Encoding or Auto-Regression? A Reality Check on Causality of\n  Self-Attention-Based Sequential Recommenders",
        "url": "http://arxiv.org/abs/2406.02048v1",
        "pub_date": "2024-06-04",
        "summary": "The comparison between Auto-Encoding (AE) and Auto-Regression (AR) has become\nan increasingly important topic with recent advances in sequential\nrecommendation. At the heart of this discussion lies the comparison of BERT4Rec\nand SASRec, which serve as representative AE and AR models for self-attentive\nsequential recommenders. Yet the conclusion of this debate remains uncertain\ndue to: (1) the lack of fair and controlled environments for experiments and\nevaluations; and (2) the presence of numerous confounding factors w.r.t.\nfeature selection, modeling choices and optimization algorithms. In this work,\nwe aim to answer this question by conducting a series of controlled\nexperiments. We start by tracing the AE/AR debate back to its origin through a\nsystematic re-evaluation of SASRec and BERT4Rec, discovering that AR models\ngenerally surpass AE models in sequential recommendation. In addition, we find\nthat AR models further outperforms AE models when using a customized design\nspace that includes additional features, modeling approaches and optimization\ntechniques. Furthermore, the performance advantage of AR models persists in the\nbroader HuggingFace transformer ecosystems. Lastly, we provide potential\nexplanations and insights into AE/AR performance from two key perspectives:\nlow-rank approximation and inductive bias. We make our code and data available\nat https://github.com/yueqirex/ModSAR",
        "translated": ""
    },
    {
        "title": "ProGEO: Generating Prompts through Image-Text Contrastive Learning for\n  Visual Geo-localization",
        "url": "http://arxiv.org/abs/2406.01906v1",
        "pub_date": "2024-06-04",
        "summary": "Visual Geo-localization (VG) refers to the process to identify the location\ndescribed in query images, which is widely applied in robotics field and\ncomputer vision tasks, such as autonomous driving, metaverse, augmented\nreality, and SLAM. In fine-grained images lacking specific text descriptions,\ndirectly applying pure visual methods to represent neighborhood features often\nleads to the model focusing on overly fine-grained features, unable to fully\nmine the semantic information in the images. Therefore, we propose a two-stage\ntraining method to enhance visual performance and use contrastive learning to\nmine challenging samples. We first leverage the multi-modal description\ncapability of CLIP (Contrastive Language-Image Pretraining) to create a set of\nlearnable text prompts for each geographic image feature to form vague\ndescriptions. Then, by utilizing dynamic text prompts to assist the training of\nthe image encoder, we enable the image encoder to learn better and more\ngeneralizable visual features. This strategy of applying text to purely visual\ntasks addresses the challenge of using multi-modal models for geographic\nimages, which often suffer from a lack of precise descriptions, making them\ndifficult to utilize widely. We validate the effectiveness of the proposed\nstrategy on several large-scale visual geo-localization datasets, and our\nmethod achieves competitive results on multiple visual geo-localization\ndatasets. Our code and model are available at\nhttps://github.com/Chain-Mao/ProGEO.",
        "translated": ""
    },
    {
        "title": "GRAM: Generative Retrieval Augmented Matching of Data Schemas in the\n  Context of Data Security",
        "url": "http://arxiv.org/abs/2406.01876v1",
        "pub_date": "2024-06-04",
        "summary": "Schema matching constitutes a pivotal phase in the data ingestion process for\ncontemporary database systems. Its objective is to discern pairwise\nsimilarities between two sets of attributes, each associated with a distinct\ndata table. This challenge emerges at the initial stages of data analytics,\nsuch as when incorporating a third-party table into existing databases to\ninform business insights. Given its significance in the realm of database\nsystems, schema matching has been under investigation since the 2000s. This\nstudy revisits this foundational problem within the context of large language\nmodels. Adhering to increasingly stringent data security policies, our focus\nlies on the zero-shot and few-shot scenarios: the model should analyze only a\nminimal amount of customer data to execute the matching task, contrasting with\nthe conventional approach of scrutinizing the entire data table. We emphasize\nthat the zero-shot or few-shot assumption is imperative to safeguard the\nidentity and privacy of customer data, even at the potential cost of accuracy.\nThe capability to accurately match attributes under such stringent requirements\ndistinguishes our work from previous literature in this domain.",
        "translated": ""
    },
    {
        "title": "Session Context Embedding for Intent Understanding in Product Search",
        "url": "http://arxiv.org/abs/2406.01702v1",
        "pub_date": "2024-06-03",
        "summary": "It is often noted that single query-item pair relevance training in search\ndoes not capture the customer intent. User intent can be better deduced from a\nseries of engagements (Clicks, ATCs, Orders) in a given search session. We\npropose a novel method for vectorizing session context for capturing and\nutilizing context in retrieval and rerank. In the runtime, session embedding is\nan alternative to query embedding, saved and updated after each request in the\nsession, it can be used for retrieval and ranking. We outline session\nembedding's solution to session-based intent understanding and its\narchitecture, the background to this line of thought in search and\nrecommendation, detail the methodologies implemented, and finally present the\nresults of an implementation of session embedding for query product type\nclassification. We demonstrate improvements over strategies ignoring session\ncontext in the runtime for user intent understanding.",
        "translated": ""
    },
    {
        "title": "Privacy in LLM-based Recommendation: Recent Advances and Future\n  Directions",
        "url": "http://arxiv.org/abs/2406.01363v1",
        "pub_date": "2024-06-03",
        "summary": "Nowadays, large language models (LLMs) have been integrated with conventional\nrecommendation models to improve recommendation performance. However, while\nmost of the existing works have focused on improving the model performance, the\nprivacy issue has only received comparatively less attention. In this paper, we\nreview recent advancements in privacy within LLM-based recommendation,\ncategorizing them into privacy attacks and protection mechanisms. Additionally,\nwe highlight several challenges and propose future directions for the community\nto address these critical problems.",
        "translated": ""
    },
    {
        "title": "Large Language Models as Evaluators for Recommendation Explanations",
        "url": "http://arxiv.org/abs/2406.03248v1",
        "pub_date": "2024-06-05",
        "summary": "The explainability of recommender systems has attracted significant attention\nin academia and industry. Many efforts have been made for explainable\nrecommendations, yet evaluating the quality of the explanations remains a\nchallenging and unresolved issue. In recent years, leveraging LLMs as\nevaluators presents a promising avenue in Natural Language Processing tasks\n(e.g., sentiment classification, information extraction), as they perform\nstrong capabilities in instruction following and common-sense reasoning.\nHowever, evaluating recommendation explanatory texts is different from these\nNLG tasks, as its criteria are related to human perceptions and are usually\nsubjective. In this paper, we investigate whether LLMs can serve as evaluators\nof recommendation explanations. To answer the question, we utilize real user\nfeedback on explanations given from previous work and additionally collect\nthird-party annotations and LLM evaluations. We design and apply a 3-level meta\nevaluation strategy to measure the correlation between evaluator labels and the\nground truth provided by users. Our experiments reveal that LLMs, such as GPT4,\ncan provide comparable evaluations with appropriate prompts and settings. We\nalso provide further insights into combining human labels with the LLM\nevaluation process and utilizing ensembles of multiple heterogeneous LLM\nevaluators to enhance the accuracy and stability of evaluations. Our study\nverifies that utilizing LLMs as evaluators can be an accurate, reproducible and\ncost-effective solution for evaluating recommendation explanation texts. Our\ncode is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator.",
        "translated": ""
    },
    {
        "title": "Linking Named Entities in Diderot's \\textit{Encyclopédie} to Wikidata",
        "url": "http://arxiv.org/abs/2406.03221v1",
        "pub_date": "2024-06-05",
        "summary": "Diderot's \\textit{Encyclop\\'edie} is a reference work from XVIIIth century in\nEurope that aimed at collecting the knowledge of its era. \\textit{Wikipedia}\nhas the same ambition with a much greater scope. However, the lack of digital\nconnection between the two encyclopedias may hinder their comparison and the\nstudy of how knowledge has evolved. A key element of \\textit{Wikipedia} is\nWikidata that backs the articles with a graph of structured data. In this\npaper, we describe the annotation of more than 10,300 of the\n\\textit{Encyclop\\'edie} entries with Wikidata identifiers enabling us to\nconnect these entries to the graph. We considered geographic and human\nentities. The \\textit{Encyclop\\'edie} does not contain biographic entries as\nthey mostly appear as subentries of locations. We extracted all the geographic\nentries and we completely annotated all the entries containing a description of\nhuman entities. This represents more than 2,600 links referring to locations or\nhuman entities. In addition, we annotated more than 9,500 entries having a\ngeographic content only. We describe the annotation process as well as\napplication examples. This resource is available at\nhttps://github.com/pnugues/encyclopedie_1751",
        "translated": ""
    },
    {
        "title": "Text-like Encoding of Collaborative Information in Large Language Models\n  for Recommendation",
        "url": "http://arxiv.org/abs/2406.03210v1",
        "pub_date": "2024-06-05",
        "summary": "When adapting Large Language Models for Recommendation (LLMRec), it is\ncrucial to integrate collaborative information. Existing methods achieve this\nby learning collaborative embeddings in LLMs' latent space from scratch or by\nmapping from external models. However, they fail to represent the information\nin a text-like format, which may not align optimally with LLMs. To bridge this\ngap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates\ncollaborative information through text-like encoding. BinLLM converts\ncollaborative embeddings from external models into binary sequences -- a\nspecific text format that LLMs can understand and operate on directly,\nfacilitating the direct usage of collaborative information in text-like format\nby LLMs. Additionally, BinLLM provides options to compress the binary sequence\nusing dot-decimal notation to avoid excessively long lengths. Extensive\nexperiments validate that BinLLM introduces collaborative information in a\nmanner better aligned with LLMs, resulting in enhanced performance. We release\nour code at https://github.com/zyang1580/BinLLM.",
        "translated": ""
    },
    {
        "title": "CAPRI-FAIR: Integration of Multi-sided Fairness in Contextual POI\n  Recommendation Framework",
        "url": "http://arxiv.org/abs/2406.03109v1",
        "pub_date": "2024-06-05",
        "summary": "Point-of-interest (POI) recommendation, a form of context-aware\nrecommendation, takes into account spatio-temporal constraints and contexts\nlike distance, peak business hours, and previous user check-ins. Given the\nability of these kinds of systems to influence not just the consumer's travel\nexperience, but also the POI's business, it is important to consider fairness\nfrom multiple perspectives. Unfortunately, these systems tend to provide less\naccurate recommendations to inactive users, and less exposure to unpopular\nPOIs. The goal of this paper is to develop a post-filter methodology that\nincorporates provider and consumer fairness factors into pre-existing\nrecommendation models, to satisfy fairness metrics like item exposure, and\nperformance metrics like precision and distance, making the system more\nsustainable to both consumers and providers. Experiments have shown that using\na linear scoring model for provider fairness in re-scoring recommended items\nyields the best tradeoff between performance and long-tail exposure, in some\ncases without a significant decrease in precision. When attempting to address\nconsumer fairness by recommending more popular POIs to inactive users, the\nresult was an increase in precision for only some recommendation models and\ndatasets. Finally, when considering the tradeoff between both parameters, the\ncombinations that reached the Pareto front of consumer and provider fairness,\nunfortunately, achieved the lowest precision values. We find that the nature of\nthis tradeoff depends heavily on the model and the dataset.",
        "translated": ""
    },
    {
        "title": "Exploring User Retrieval Integration towards Large Language Models for\n  Cross-Domain Sequential Recommendation",
        "url": "http://arxiv.org/abs/2406.03085v1",
        "pub_date": "2024-06-05",
        "summary": "Cross-Domain Sequential Recommendation (CDSR) aims to mine and transfer\nusers' sequential preferences across different domains to alleviate the\nlong-standing cold-start issue. Traditional CDSR models capture collaborative\ninformation through user and item modeling while overlooking valuable semantic\ninformation. Recently, Large Language Model (LLM) has demonstrated powerful\nsemantic reasoning capabilities, motivating us to introduce them to better\ncapture semantic information. However, introducing LLMs to CDSR is non-trivial\ndue to two crucial issues: seamless information integration and domain-specific\ngeneration. To this end, we propose a novel framework named URLLM, which aims\nto improve the CDSR performance by exploring the User Retrieval approach and\ndomain grounding on LLM simultaneously. Specifically, we first present a novel\ndual-graph sequential model to capture the diverse information, along with an\nalignment and contrastive learning method to facilitate domain knowledge\ntransfer. Subsequently, a user retrieve-generation model is adopted to\nseamlessly integrate the structural information into LLM, fully harnessing its\nemergent inferencing ability. Furthermore, we propose a domain-specific\nstrategy and a refinement module to prevent out-of-domain generation. Extensive\nexperiments on Amazon demonstrated the information integration and\ndomain-specific generation ability of URLLM in comparison to state-of-the-art\nbaselines. Our code is available at https://github.com/TingJShen/URLLM",
        "translated": ""
    },
    {
        "title": "Path-Specific Causal Reasoning for Fairness-aware Cognitive Diagnosis",
        "url": "http://arxiv.org/abs/2406.03064v1",
        "pub_date": "2024-06-05",
        "summary": "Cognitive Diagnosis~(CD), which leverages students and exercise data to\npredict students' proficiency levels on different knowledge concepts, is one of\nfundamental components in Intelligent Education. Due to the scarcity of\nstudent-exercise interaction data, most existing methods focus on making the\nbest use of available data, such as exercise content and student\ninformation~(e.g., educational context). Despite the great progress, the abuse\nof student sensitive information has not been paid enough attention. Due to the\nimportant position of CD in Intelligent Education, employing sensitive\ninformation when making diagnosis predictions will cause serious social issues.\nMoreover, data-driven neural networks are easily misled by the shortcut between\ninput data and output prediction, exacerbating this problem. Therefore, it is\ncrucial to eliminate the negative impact of sensitive information in CD models.\nIn response, we argue that sensitive attributes of students can also provide\nuseful information, and only the shortcuts directly related to the sensitive\ninformation should be eliminated from the diagnosis process. Thus, we employ\ncausal reasoning and design a novel Path-Specific Causal Reasoning Framework\n(PSCRF) to achieve this goal. Specifically, we first leverage an encoder to\nextract features and generate embeddings for general information and sensitive\ninformation of students. Then, we design a novel attribute-oriented predictor\nto decouple the sensitive attributes, in which fairness-related sensitive\nfeatures will be eliminated and other useful information will be retained.\nFinally, we designed a multi-factor constraint to ensure the performance of\nfairness and diagnosis performance simultaneously. Extensive experiments over\nreal-world datasets (e.g., PISA dataset) demonstrate the effectiveness of our\nproposed PSCRF.",
        "translated": ""
    },
    {
        "title": "Docs2KG: Unified Knowledge Graph Construction from Heterogeneous\n  Documents Assisted by Large Language Models",
        "url": "http://arxiv.org/abs/2406.02962v1",
        "pub_date": "2024-06-05",
        "summary": "Even for a conservative estimate, 80% of enterprise data reside in\nunstructured files, stored in data lakes that accommodate heterogeneous\nformats. Classical search engines can no longer meet information seeking needs,\nespecially when the task is to browse and explore for insight formulation. In\nother words, there are no obvious search keywords to use. Knowledge graphs, due\nto their natural visual appeals that reduce the human cognitive load, become\nthe winning candidate for heterogeneous data integration and knowledge\nrepresentation.\n  In this paper, we introduce Docs2KG, a novel framework designed to extract\nmultimodal information from diverse and heterogeneous unstructured documents,\nincluding emails, web pages, PDF files, and Excel files. Dynamically generates\na unified knowledge graph that represents the extracted key information,\nDocs2KG enables efficient querying and exploration of document data lakes.\nUnlike existing approaches that focus on domain-specific data sources or\npre-designed schemas, Docs2KG offers a flexible and extensible solution that\ncan adapt to various document structures and content types. The proposed\nframework unifies data processing supporting a multitude of downstream tasks\nwith improved domain interpretability. Docs2KG is publicly accessible at\nhttps://docs2kg.ai4wa.com, and a demonstration video is available at\nhttps://docs2kg.ai4wa.com/Video.",
        "translated": ""
    },
    {
        "title": "The Task-oriented Queries Benchmark (ToQB)",
        "url": "http://arxiv.org/abs/2406.02943v1",
        "pub_date": "2024-06-05",
        "summary": "Task-oriented queries (e.g., one-shot queries to play videos, order food, or\ncall a taxi) are crucial for assessing the quality of virtual assistants,\nchatbots, and other large language model (LLM)-based services. However, a\nstandard benchmark for task-oriented queries is not yet available, as existing\nbenchmarks in the relevant NLP (Natural Language Processing) fields have\nprimarily focused on task-oriented dialogues. Thus, we present a new\nmethodology for efficiently generating the Task-oriented Queries Benchmark\n(ToQB) using existing task-oriented dialogue datasets and an LLM service. Our\nmethodology involves formulating the underlying NLP task to summarize the\noriginal intent of a speaker in each dialogue, detailing the key steps to\nperform the devised NLP task using an LLM service, and outlining a framework\nfor automating a major part of the benchmark generation process. Through a case\nstudy encompassing three domains (i.e., two single-task domains and one\nmulti-task domain), we demonstrate how to customize the LLM prompts (e.g.,\nomitting system utterances or speaker labels) for those three domains and\ncharacterize the generated task-oriented queries. The generated ToQB dataset is\nmade available to the public. We further discuss new domains that can be added\nto ToQB by community contributors and its practical applications.",
        "translated": ""
    },
    {
        "title": "A Bi-metric Framework for Fast Similarity Search",
        "url": "http://arxiv.org/abs/2406.02891v1",
        "pub_date": "2024-06-05",
        "summary": "We propose a new \"bi-metric\" framework for designing nearest neighbor data\nstructures. Our framework assumes two dissimilarity functions: a ground-truth\nmetric that is accurate but expensive to compute, and a proxy metric that is\ncheaper but less accurate. In both theory and practice, we show how to\nconstruct data structures using only the proxy metric such that the query\nprocedure achieves the accuracy of the expensive metric, while only using a\nlimited number of calls to both metrics. Our theoretical results instantiate\nthis framework for two popular nearest neighbor search algorithms: DiskANN and\nCover Tree. In both cases we show that, as long as the proxy metric used to\nconstruct the data structure approximates the ground-truth metric up to a\nbounded factor, our data structure achieves arbitrarily good approximation\nguarantees with respect to the ground-truth metric. On the empirical side, we\napply the framework to the text retrieval problem with two dissimilarity\nfunctions evaluated by ML models with vastly different computational costs. We\nobserve that for almost all data sets in the MTEB benchmark, our approach\nachieves a considerably better accuracy-efficiency tradeoff than the\nalternatives, such as re-ranking.",
        "translated": ""
    },
    {
        "title": "Item-Language Model for Conversational Recommendation",
        "url": "http://arxiv.org/abs/2406.02844v1",
        "pub_date": "2024-06-05",
        "summary": "Large-language Models (LLMs) have been extremely successful at tasks like\ncomplex dialogue understanding, reasoning and coding due to their emergent\nabilities. These emergent abilities have been extended with multi-modality to\ninclude image, audio, and video capabilities. Recommender systems, on the other\nhand, have been critical for information seeking and item discovery needs.\nRecently, there have been attempts to apply LLMs for recommendations. One\ndifficulty of current attempts is that the underlying LLM is usually not\ntrained on the recommender system data, which largely contains user interaction\nsignals and is often not publicly available. Another difficulty is user\ninteraction signals often have a different pattern from natural language text,\nand it is currently unclear if the LLM training setup can learn more\nnon-trivial knowledge from interaction signals compared with traditional\nrecommender system methods. Finally, it is difficult to train multiple LLMs for\ndifferent use-cases, and to retain the original language and reasoning\nabilities when learning from recommender system data. To address these three\nlimitations, we propose an Item-Language Model (ILM), which is composed of an\nitem encoder to produce text-aligned item representations that encode user\ninteraction signals, and a frozen LLM that can understand those item\nrepresentations with preserved pretrained knowledge. We conduct extensive\nexperiments which demonstrate both the importance of the language-alignment and\nof user interaction knowledge in the item encoder.",
        "translated": ""
    },
    {
        "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
        "url": "http://arxiv.org/abs/2406.04331v1",
        "pub_date": "2024-06-06",
        "summary": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable output, via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Then,\ngiven any alignment task, we instruct a concept partitioner to efficiently\nannotate the concepts as benign or undesirable. Finally, at inference time, we\ndecompose the LLM activations along the concept dictionary via sparse coding,\nto accurately represent the activation as a linear combination of the benign\nand undesirable components. By removing the latter ones from the activation, we\nreorient the behavior of LLMs towards alignment goals. We conduct experiments\non tasks such as response detoxification, faithfulness enhancement, and\nsentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.",
        "translated": ""
    },
    {
        "title": "Measuring and Addressing Indexical Bias in Information Retrieval",
        "url": "http://arxiv.org/abs/2406.04298v1",
        "pub_date": "2024-06-06",
        "summary": "Information Retrieval (IR) systems are designed to deliver relevant content,\nbut traditional systems may not optimize rankings for fairness, neutrality, or\nthe balance of ideas. Consequently, IR can often introduce indexical biases, or\nbiases in the positional order of documents. Although indexical bias can\ndemonstrably affect people's opinion, voting patterns, and other behaviors,\nthese issues remain understudied as the field lacks reliable metrics and\nprocedures for automatically measuring indexical bias. Towards this end, we\nintroduce the PAIR framework, which supports automatic bias audits for ranked\ndocuments or entire IR systems. After introducing DUO, the first\ngeneral-purpose automatic bias metric, we run an extensive evaluation of 8 IR\nsystems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k\nqueries spanning 1.4k controversial issue topics. A human behavioral study\nvalidates our approach, showing that our bias metric can help predict when and\nhow indexical bias will shift a reader's opinion.",
        "translated": ""
    },
    {
        "title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval",
        "url": "http://arxiv.org/abs/2406.04292v1",
        "pub_date": "2024-06-06",
        "summary": "Multi-modal retrieval becomes increasingly popular in practice. However, the\nexisting retrievers are mostly text-oriented, which lack the capability to\nprocess visual information. Despite the presence of vision-language models like\nCLIP, the current methods are severely limited in representing the text-only\nand image-only data. In this work, we present a new embedding model VISTA for\nuniversal multi-modal retrieval. Our work brings forth threefold technical\ncontributions. Firstly, we introduce a flexible architecture which extends a\npowerful text encoder with the image understanding capability by introducing\nvisual token embeddings. Secondly, we develop two data generation strategies,\nwhich bring high-quality composed image-text to facilitate the training of the\nembedding model. Thirdly, we introduce a multi-stage training algorithm, which\nfirst aligns the visual token embedding with the text encoder using massive\nweakly labeled data, and then develops multi-modal representation capability\nusing the generated composed image-text data. In our experiments, VISTA\nachieves superior performances across a variety of multi-modal retrieval tasks\nin both zero-shot and supervised settings. Our model, data, and source code are\navailable at https://github.com/FlagOpen/FlagEmbedding.",
        "translated": ""
    },
    {
        "title": "Data Measurements for Decentralized Data Markets",
        "url": "http://arxiv.org/abs/2406.04257v1",
        "pub_date": "2024-06-06",
        "summary": "Decentralized data markets can provide more equitable forms of data\nacquisition for machine learning. However, to realize practical marketplaces,\nefficient techniques for seller selection need to be developed. We propose and\nbenchmark federated data measurements to allow a data buyer to find sellers\nwith relevant and diverse datasets. Diversity and relevance measures enable a\nbuyer to make relative comparisons between sellers without requiring\nintermediate brokers and training task-dependent models.",
        "translated": ""
    },
    {
        "title": "On The Persona-based Summarization of Domain-Specific Documents",
        "url": "http://arxiv.org/abs/2406.03986v1",
        "pub_date": "2024-06-06",
        "summary": "In an ever-expanding world of domain-specific knowledge, the increasing\ncomplexity of consuming, and storing information necessitates the generation of\nsummaries from large information repositories. However, every persona of a\ndomain has different requirements of information and hence their summarization.\nFor example, in the healthcare domain, a persona-based (such as Doctor, Nurse,\nPatient etc.) approach is imperative to deliver targeted medical information\nefficiently. Persona-based summarization of domain-specific information by\nhumans is a high cognitive load task and is generally not preferred. The\nsummaries generated by two different humans have high variability and do not\nscale in cost and subject matter expertise as domains and personas grow.\nFurther, AI-generated summaries using generic Large Language Models (LLMs) may\nnot necessarily offer satisfactory accuracy for different domains unless they\nhave been specifically trained on domain-specific data and can also be very\nexpensive to use in day-to-day operations. Our contribution in this paper is\ntwo-fold: 1) We present an approach to efficiently fine-tune a domain-specific\nsmall foundation LLM using a healthcare corpus and also show that we can\neffectively evaluate the summarization quality using AI-based critiquing. 2) We\nfurther show that AI-based critiquing has good concordance with Human-based\ncritiquing of the summaries. Hence, such AI-based pipelines to generate\ndomain-specific persona-based summaries can be easily scaled to other domains\nsuch as legal, enterprise documents, education etc. in a very efficient and\ncost-effective manner.",
        "translated": ""
    },
    {
        "title": "Beyond Similarity: Personalized Federated Recommendation with Composite\n  Aggregation",
        "url": "http://arxiv.org/abs/2406.03933v1",
        "pub_date": "2024-06-06",
        "summary": "Federated recommendation aims to collect global knowledge by aggregating\nlocal models from massive devices, to provide recommendations while ensuring\nprivacy. Current methods mainly leverage aggregation functions invented by\nfederated vision community to aggregate parameters from similar clients, e.g.,\nclustering aggregation. Despite considerable performance, we argue that it is\nsuboptimal to apply them to federated recommendation directly. This is mainly\nreflected in the disparate model architectures. Different from structured\nparameters like convolutional neural networks in federated vision, federated\nrecommender models usually distinguish itself by employing one-to-one item\nembedding table. Such a discrepancy induces the challenging embedding skew\nissue, which continually updates the trained embeddings but ignores the\nnon-trained ones during aggregation, thus failing to predict future items\naccurately. To this end, we propose a personalized Federated recommendation\nmodel with Composite Aggregation (FedCA), which not only aggregates similar\nclients to enhance trained embeddings, but also aggregates complementary\nclients to update non-trained embeddings. Besides, we formulate the overall\nlearning process into a unified optimization algorithm to jointly learn the\nsimilarity and complementarity. Extensive experiments on several real-world\ndatasets substantiate the effectiveness of our proposed model. The source codes\nare available at https://github.com/hongleizhang/FedCA.",
        "translated": ""
    },
    {
        "title": "Polyhedral Conic Classifier for CTR Prediction",
        "url": "http://arxiv.org/abs/2406.03892v1",
        "pub_date": "2024-06-06",
        "summary": "This paper introduces a novel approach for click-through rate (CTR)\nprediction within industrial recommender systems, addressing the inherent\nchallenges of numerical imbalance and geometric asymmetry. These challenges\nstem from imbalanced datasets, where positive (click) instances occur less\nfrequently than negatives (non-clicks), and geometrically asymmetric\ndistributions, where positive samples exhibit visually coherent patterns while\nnegatives demonstrate greater diversity. To address these challenges, we have\nused a deep neural network classifier that uses the polyhedral conic functions.\nThis classifier is similar to the one-class classifiers in spirit and it\nreturns compact polyhedral acceptance regions to separate the positive class\nsamples from the negative samples that have diverse distributions. Extensive\nexperiments have been conducted to test the proposed approach using\nstate-of-the-art (SOTA) CTR prediction models on four public datasets, namely\nCriteo, Avazu, MovieLens and Frappe. The experimental evaluations highlight the\nsuperiority of our proposed approach over Binary Cross Entropy (BCE) Loss,\nwhich is widely used in CTR prediction tasks.",
        "translated": ""
    },
    {
        "title": "Reducing the climate impact of data portals: a case study",
        "url": "http://arxiv.org/abs/2406.03858v1",
        "pub_date": "2024-06-06",
        "summary": "The carbon footprint share of the information and communication technology\n(ICT) sector has steadily increased in the past decade and is predicted to make\nup as much as 23 \\% of global emissions in 2030. This shows a pressing need for\ndevelopers, including the information retrieval community, to make their code\nmore energy-efficient. In this project proposal, we discuss techniques to\nreduce the energy footprint of the MaRDI (Mathematical Research Data\nInitiative) Portal, a MediaWiki-based knowledge base. In future work, we plan\nto implement these changes and provide concrete measurements on the gain in\nenergy efficiency. Researchers developing similar knowledge bases can adapt our\nmeasures to reduce their environmental footprint. In this way, we are working\non mitigating the climate impact of Information Retrieval research.",
        "translated": ""
    },
    {
        "title": "XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the\n  Multilingual Generation of News Headlines and Tags",
        "url": "http://arxiv.org/abs/2406.03776v1",
        "pub_date": "2024-06-06",
        "summary": "Millions of news articles published online daily can overwhelm readers.\nHeadlines and entity (topic) tags are essential for guiding readers to decide\nif the content is worth their time. While headline generation has been\nextensively studied, tag generation remains largely unexplored, yet it offers\nreaders better access to topics of interest. The need for conciseness in\ncapturing readers' attention necessitates improved content selection strategies\nfor identifying salient and relevant segments within lengthy articles, thereby\nguiding language models effectively. To address this, we propose to leverage\nauxiliary information such as images and captions embedded in the articles to\nretrieve relevant sentences and utilize instruction tuning with variations to\ngenerate both headlines and tags for news articles in a multilingual context.\nTo make use of the auxiliary information, we have compiled a dataset named\nXL-HeadTags, which includes 20 languages across 6 diverse language families.\nThrough extensive evaluation, we demonstrate the effectiveness of our\nplug-and-play multimodal-multilingual retrievers for both tasks. Additionally,\nwe have developed a suite of tools for processing and evaluating multilingual\ntexts, significantly contributing to the research community by enabling more\naccurate and efficient analysis across languages.",
        "translated": ""
    },
    {
        "title": "Attribute-Aware Implicit Modality Alignment for Text Attribute Person\n  Search",
        "url": "http://arxiv.org/abs/2406.03721v1",
        "pub_date": "2024-06-06",
        "summary": "Text attribute person search aims to find specific pedestrians through given\ntextual attributes, which is very meaningful in the scene of searching for\ndesignated pedestrians through witness descriptions. The key challenge is the\nsignificant modality gap between textual attributes and images. Previous\nmethods focused on achieving explicit representation and alignment through\nunimodal pre-trained models. Nevertheless, the absence of inter-modality\ncorrespondence in these models may lead to distortions in the local information\nof intra-modality. Moreover, these methods only considered the alignment of\ninter-modality and ignored the differences between different attribute\ncategories. To mitigate the above problems, we propose an Attribute-Aware\nImplicit Modality Alignment (AIMA) framework to learn the correspondence of\nlocal representations between textual attributes and images and combine global\nrepresentation matching to narrow the modality gap. Firstly, we introduce the\nCLIP model as the backbone and design prompt templates to transform attribute\ncombinations into structured sentences. This facilitates the model's ability to\nbetter understand and match image details. Next, we design a Masked Attribute\nPrediction (MAP) module that predicts the masked attributes after the\ninteraction of image and masked textual attribute features through multi-modal\ninteraction, thereby achieving implicit local relationship alignment. Finally,\nwe propose an Attribute-IoU Guided Intra-Modal Contrastive (A-IoU IMC) loss,\naligning the distribution of different textual attributes in the embedding\nspace with their IoU distribution, achieving better semantic arrangement.\nExtensive experiments on the Market-1501 Attribute, PETA, and PA100K datasets\nshow that the performance of our proposed method significantly surpasses the\ncurrent state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Corpus Poisoning via Approximate Greedy Gradient Descent",
        "url": "http://arxiv.org/abs/2406.05087v1",
        "pub_date": "2024-06-07",
        "summary": "Dense retrievers are widely used in information retrieval and have also been\nsuccessfully extended to other knowledge intensive areas such as language\nmodels, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they\nhave recently been shown to be vulnerable to corpus poisoning attacks in which\na malicious user injects a small fraction of adversarial passages into the\nretrieval corpus to trick the system into returning these passages among the\ntop-ranked results for a broad set of user queries. Further study is needed to\nunderstand the extent to which these attacks could limit the deployment of\ndense retrievers in real-world applications. In this work, we propose\nApproximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval\nsystems based on the widely used HotFlip method for efficiently generating\nadversarial passages. We demonstrate that AGGD can select a higher quality set\nof token-level perturbations than HotFlip by replacing its random token\nsampling with a more structured search. Experimentally, we show that our method\nachieves a high attack success rate on several datasets and using several\nretrievers, and can generalize to unseen queries and new domains. Notably, our\nmethod is extremely effective in attacking the ANCE retrieval model, achieving\nattack success rates that are 17.6\\% and 13.37\\% higher on the NQ and MS MARCO\ndatasets, respectively, compared to HotFlip. Additionally, we demonstrate\nAGGD's potential to replace HotFlip in other adversarial attacks, such as\nknowledge poisoning of RAG systems.\\footnote{Code can be find in\n\\url{https://github.com/JinyanSu1/AGGD}}",
        "translated": ""
    },
    {
        "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
        "url": "http://arxiv.org/abs/2406.05085v1",
        "pub_date": "2024-06-07",
        "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, synthetic datasets, and real-world use\ncases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in\nrelevance over standard RAG baselines. MRAG can be seamlessly integrated with\nexisting RAG frameworks and benchmarking tools like RAGAS as well as different\nclasses of data stores.",
        "translated": ""
    },
    {
        "title": "CHIQ: Contextual History Enhancement for Improving Query Rewriting in\n  Conversational Search",
        "url": "http://arxiv.org/abs/2406.05013v1",
        "pub_date": "2024-06-07",
        "summary": "In this paper, we study how open-source large language models (LLMs) can be\neffectively deployed for improving query rewriting in conversational search,\nespecially for ambiguous queries. We introduce CHIQ, a two-step method that\nleverages the capabilities of LLMs to resolve ambiguities in the conversation\nhistory before query rewriting. This approach contrasts with prior studies that\npredominantly use closed-source LLMs to directly generate search queries from\nconversation history. We demonstrate on five well-established benchmarks that\nCHIQ leads to state-of-the-art results across most settings, showing highly\ncompetitive performances with systems leveraging closed-source LLMs. Our study\nprovides a first step towards leveraging open-source LLMs in conversational\nsearch, as a competitive alternative to the prevailing reliance on commercial\nLLMs. Data, models, and source code will be publicly available upon acceptance\nat https://github.com/fengranMark/CHIQ.",
        "translated": ""
    },
    {
        "title": "QAGCF: Graph Collaborative Filtering for Q&amp;A Recommendation",
        "url": "http://arxiv.org/abs/2406.04828v1",
        "pub_date": "2024-06-07",
        "summary": "Question and answer (Q&amp;A) platforms usually recommend question-answer pairs\nto meet users' knowledge acquisition needs, unlike traditional recommendations\nthat recommend only one item. This makes user behaviors more complex, and\npresents two challenges for Q&amp;A recommendation, including: the collaborative\ninformation entanglement, which means user feedback is influenced by either the\nquestion or the answer; and the semantic information entanglement, where\nquestions are correlated with their corresponding answers, and correlations\nalso exist among different question-answer pairs. Traditional recommendation\nmethods treat the question-answer pair as a whole or only consider the answer\nas a single item, which overlooks the two challenges and cannot effectively\nmodel user interests. To address these challenges, we introduce Question &amp;\nAnswer Graph Collaborative Filtering (QAGCF), a graph neural network model that\ncreates separate graphs for collaborative and semantic views to disentangle the\ninformation in question-answer pairs. The collaborative view disentangles\nquestions and answers to individually model collaborative information, while\nthe semantic view captures the semantic information both within and between\nquestion-answer pairs. These views are further merged into a global graph to\nintegrate the collaborative and semantic information. Polynomial-based graph\nfilters are used to address the high heterophily issues of the global graph.\nAdditionally, contrastive learning is utilized to obtain robust embeddings\nduring training. Extensive experiments on industrial and public datasets\ndemonstrate that QAGCF consistently outperforms baselines and achieves\nstate-of-the-art results.",
        "translated": ""
    },
    {
        "title": "Scaling Automatic Extraction of Pseudocode",
        "url": "http://arxiv.org/abs/2406.04635v1",
        "pub_date": "2024-06-07",
        "summary": "Pseudocode in a scholarly paper provides a concise way to express the\nalgorithms implemented therein. Pseudocode can also be thought of as an\nintermediary representation that helps bridge the gap between programming\nlanguages and natural languages. Having access to a large collection of\npseudocode can provide various benefits ranging from enhancing algorithmic\nunderstanding, facilitating further algorithmic design, to empowering NLP or\ncomputer vision based models for tasks such as automated code generation and\noptical character recognition (OCR). We have created a large pseudocode\ncollection by extracting nearly 320,000 pseudocode examples from arXiv papers.\nThis process involved scanning over $2.2$ million scholarly papers, with 1,000\nof them being manually inspected and labeled. Our approach encompasses an\nextraction mechanism tailored to optimize the coverage and a validation\nmechanism based on random sampling to check its accuracy and reliability, given\nthe inherent heterogeneity of the collection. In addition, we offer insights\ninto common pseudocode structures, supported by clustering and statistical\nanalyses. Notably, these analyses indicate an exponential-like growth in the\nusage of pseudocodes, highlighting their increasing significance.",
        "translated": ""
    },
    {
        "title": "Error Bounds of Supervised Classification from Information-Theoretic\n  Perspective",
        "url": "http://arxiv.org/abs/2406.04567v1",
        "pub_date": "2024-06-07",
        "summary": "There remains a list of unanswered research questions on deep learning (DL),\nincluding the remarkable generalization power of overparametrized neural\nnetworks, the efficient optimization performance despite the non-convexity, and\nthe mechanisms behind flat minima in generalization. In this paper, we adopt an\ninformation-theoretic perspective to explore the theoretical foundations of\nsupervised classification using deep neural networks (DNNs). Our analysis\nintroduces the concepts of fitting error and model risk, which, together with\ngeneralization error, constitute an upper bound on the expected risk. We\ndemonstrate that the generalization errors are bounded by the complexity,\ninfluenced by both the smoothness of distribution and the sample size.\nConsequently, task complexity serves as a reliable indicator of the dataset's\nquality, guiding the setting of regularization hyperparameters. Furthermore,\nthe derived upper bound fitting error links the back-propagated gradient,\nNeural Tangent Kernel (NTK), and the model's parameter count with the fitting\nerror. Utilizing the triangle inequality, we establish an upper bound on the\nexpected risk. This bound offers valuable insights into the effects of\noverparameterization, non-convex optimization, and the flat minima in\nDNNs.Finally, empirical verification confirms a significant positive\ncorrelation between the derived theoretical bounds and the practical expected\nrisk, confirming the practical relevance of the theoretical findings.",
        "translated": ""
    },
    {
        "title": "Better Late Than Never: Formulating and Benchmarking Recommendation\n  Editing",
        "url": "http://arxiv.org/abs/2406.04553v1",
        "pub_date": "2024-06-06",
        "summary": "Recommendation systems play a pivotal role in suggesting items to users based\non their preferences. However, in online platforms, these systems inevitably\noffer unsuitable recommendations due to limited model capacity, poor data\nquality, or evolving user interests. Enhancing user experience necessitates\nefficiently rectify such unsuitable recommendation behaviors. This paper\nintroduces a novel and significant task termed recommendation editing, which\nfocuses on modifying known and unsuitable recommendation behaviors.\nSpecifically, this task aims to adjust the recommendation model to eliminate\nknown unsuitable items without accessing training data or retraining the model.\nWe formally define the problem of recommendation editing with three primary\nobjectives: strict rectification, collaborative rectification, and concentrated\nrectification. Three evaluation metrics are developed to quantitatively assess\nthe achievement of each objective. We present a straightforward yet effective\nbenchmark for recommendation editing using novel Editing Bayesian Personalized\nRanking Loss. To demonstrate the effectiveness of the proposed method, we\nestablish a comprehensive benchmark that incorporates various methods from\nrelated fields. Codebase is available at\nhttps://github.com/cycl2018/Recommendation-Editing.",
        "translated": ""
    },
    {
        "title": "GNNAnatomy: Systematic Generation and Evaluation of Multi-Level\n  Explanations for Graph Neural Networks",
        "url": "http://arxiv.org/abs/2406.04548v1",
        "pub_date": "2024-06-06",
        "summary": "Graph Neural Networks (GNNs) have proven highly effective in various machine\nlearning (ML) tasks involving graphs, such as node/graph classification and\nlink prediction. However, explaining the decisions made by GNNs poses\nchallenges because of the aggregated relational information based on graph\nstructure, leading to complex data transformations. Existing methods for\nexplaining GNNs often face limitations in systematically exploring diverse\nsubstructures and evaluating results in the absence of ground truths. To\naddress this gap, we introduce GNNAnatomy, a model- and dataset-agnostic visual\nanalytics system designed to facilitate the generation and evaluation of\nmulti-level explanations for GNNs. In GNNAnatomy, we employ graphlets to\nelucidate GNN behavior in graph-level classification tasks. By analyzing the\nassociations between GNN classifications and graphlet frequencies, we formulate\nhypothesized factual and counterfactual explanations. To validate a\nhypothesized graphlet explanation, we introduce two metrics: (1) the\ncorrelation between its frequency and the classification confidence, and (2)\nthe change in classification confidence after removing this substructure from\nthe original graph. To demonstrate the effectiveness of GNNAnatomy, we conduct\ncase studies on both real-world and synthetic graph datasets from various\ndomains. Additionally, we qualitatively compare GNNAnatomy with a\nstate-of-the-art GNN explainer, demonstrating the utility and versatility of\nour design.",
        "translated": ""
    },
    {
        "title": "Negative Feedback for Music Personalization",
        "url": "http://arxiv.org/abs/2406.04488v1",
        "pub_date": "2024-06-06",
        "summary": "Next-item recommender systems are often trained using only positive feedback\nwith randomly-sampled negative feedback. We show the benefits of using real\nnegative feedback both as inputs into the user sequence and also as negative\ntargets for training a next-song recommender system for internet radio. In\nparticular, using explicit negative samples during training helps reduce\ntraining time by ~60% while also improving test accuracy by ~6%; adding user\nskips as additional inputs also can considerably increase user coverage\nalongside slightly improving accuracy. We test the impact of using a large\nnumber of random negative samples to capture a 'harder' one and find that the\ntest accuracy increases with more randomly-sampled negatives, but only to a\npoint. Too many random negatives leads to false negatives that limits the lift,\nwhich is still lower than if using true negative feedback. We also find that\nthe test accuracy is fairly robust with respect to the proportion of different\nfeedback types, and compare the learned embeddings for different feedback\ntypes.",
        "translated": ""
    },
    {
        "title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance\n  Assessor",
        "url": "http://arxiv.org/abs/2406.06519v1",
        "pub_date": "2024-06-10",
        "summary": "Copious amounts of relevance judgments are necessary for the effective\ntraining and accurate evaluation of retrieval systems. Conventionally, these\njudgments are made by human assessors, rendering this process expensive and\nlaborious. A recent study by Thomas et al. from Microsoft Bing suggested that\nlarge language models (LLMs) can accurately perform the relevance assessment\ntask and provide human-quality judgments, but unfortunately their study did not\nyield any reusable software artifacts. Our work presents UMBRELA (a recursive\nacronym that stands for UMbrela is the Bing RELevance Assessor), an open-source\ntoolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o\nmodel and adds more nuance to the original paper. Across Deep Learning Tracks\nfrom TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate\nhighly with rankings generated by effective multi-stage retrieval systems. Our\ntoolkit is designed to be easily extensible and can be integrated into existing\nmulti-stage retrieval and evaluation pipelines, offering researchers a valuable\nresource for studying retrieval evaluation methodologies. UMBRELA will be used\nin the TREC 2024 RAG Track to aid in relevance assessments, and we envision our\ntoolkit becoming a foundation for further innovation in the field. UMBRELA is\navailable at https://github.com/castorini/umbrela.",
        "translated": ""
    },
    {
        "title": "Survey for Landing Generative AI in Social and E-commerce Recsys -- the\n  Industry Perspectives",
        "url": "http://arxiv.org/abs/2406.06475v1",
        "pub_date": "2024-06-10",
        "summary": "Recently, generative AI (GAI), with their emerging capabilities, have\npresented unique opportunities for augmenting and revolutionizing industrial\nrecommender systems (Recsys). Despite growing research efforts at the\nintersection of these fields, the integration of GAI into industrial Recsys\nremains in its infancy, largely due to the intricate nature of modern\nindustrial Recsys infrastructure, operations, and product sophistication.\nDrawing upon our experiences in successfully integrating GAI into several major\nsocial and e-commerce platforms, this survey aims to comprehensively examine\nthe underlying system and AI foundations, solution frameworks, connections to\nkey research advancements, as well as summarize the practical insights and\nchallenges encountered in the endeavor to integrate GAI into industrial Recsys.\nAs pioneering work in this domain, we hope outline the representative\ndevelopments of relevant fields, shed lights on practical GAI adoptions in the\nindustry, and motivate future research.",
        "translated": ""
    },
    {
        "title": "Evaluating the Retrieval Component in LLM-Based Question Answering\n  Systems",
        "url": "http://arxiv.org/abs/2406.06458v1",
        "pub_date": "2024-06-10",
        "summary": "Question answering systems (QA) utilizing Large Language Models (LLMs)\nheavily depend on the retrieval component to provide them with domain-specific\ninformation and reduce the risk of generating inaccurate responses or\nhallucinations. Although the evaluation of retrievers dates back to the early\nresearch in Information Retrieval, assessing their performance within LLM-based\nchatbots remains a challenge.\n  This study proposes a straightforward baseline for evaluating retrievers in\nRetrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate\nthat this evaluation framework provides a better image of how the retriever\nperforms and is more aligned with the overall performance of the QA system.\nAlthough conventional metrics such as precision, recall, and F1 score may not\nfully capture LLMs' capabilities - as they can yield accurate responses despite\nimperfect retrievers - our method considers LLMs' strengths to ignore\nirrelevant contexts, as well as potential errors and hallucinations in their\nresponses.",
        "translated": ""
    },
    {
        "title": "Combining Embeddings and Domain Knowledge for Job Posting Duplicate\n  Detection",
        "url": "http://arxiv.org/abs/2406.06257v1",
        "pub_date": "2024-06-10",
        "summary": "Job descriptions are posted on many online channels, including company\nwebsites, job boards or social media platforms. These descriptions are usually\npublished with varying text for the same job, due to the requirements of each\nplatform or to target different audiences. However, for the purpose of\nautomated recruitment and assistance of people working with these texts, it is\nhelpful to aggregate job postings across platforms and thus detect duplicate\ndescriptions that refer to the same job. In this work, we propose an approach\nfor detecting duplicates in job descriptions. We show that combining\noverlap-based character similarity with text embedding and keyword matching\nmethods lead to convincing results. In particular, we show that although no\napproach individually achieves satisfying performance, a combination of string\ncomparison, deep textual embeddings, and the use of curated weighted lookup\nlists for specific skills leads to a significant boost in overall performance.\nA tool based on our approach is being used in production and feedback from\nreal-life use confirms our evaluation.",
        "translated": ""
    },
    {
        "title": "Black carbon plumes from gas flaring in North Africa identified from\n  multi-spectral imagery with deep learning",
        "url": "http://arxiv.org/abs/2406.06183v1",
        "pub_date": "2024-06-10",
        "summary": "Black carbon (BC) is an important pollutant aerosol emitted by numerous human\nactivities, including gas flaring. Improper combustion in flaring activities\ncan release large amounts of BC, which is harmful to human health and has a\nstrong climate warming effect. To our knowledge, no study has ever directly\nmonitored BC emissions from satellite imagery. Previous works quantified BC\nemissions indirectly, by applying emission coefficients to flaring volumes\nestimated from satellite imagery. Here, we develop a deep learning framework\nand apply it to Sentinel-2 imagery over North Africa during 2022 to detect and\nquantify BC emissions from gas flaring. We find that BC emissions in this\nregion amount to about 1 million tCO$_{2,\\mathrm{eq}}$, or 1 million passenger\ncars, more than a quarter of which are due to 10 sites alone. This work\ndemonstrates the operational monitoring of BC emissions from flaring, a key\nstep in implementing effective mitigation policies to reduce the climate impact\nof oil and gas operations.",
        "translated": ""
    },
    {
        "title": "Greedy SLIM: A SLIM-Based Approach For Preference Elicitation",
        "url": "http://arxiv.org/abs/2406.06061v1",
        "pub_date": "2024-06-10",
        "summary": "Preference elicitation is an active learning approach to tackle the\ncold-start problem of recommender systems. Roughly speaking, new users are\nasked to rate some carefully selected items in order to compute appropriate\nrecommendations for them. To the best of our knowledge, we are the first to\npropose a method for preference elicitation that is based on SLIM , a\nstate-of-the-art technique for top-N recommendation. Our approach mainly\nconsists of a new training technique for SLIM, which we call Greedy SLIM. This\ntechnique iteratively selects items for the training in order to minimize the\nSLIM loss greedily. We conduct offline experiments as well as a user study to\nassess the performance of this new method. The results are remarkable,\nespecially with respect to the user study. We conclude that Greedy SLIM seems\nto be more suitable for preference elicitation than widely used methods based\non latent factor models.",
        "translated": ""
    },
    {
        "title": "Modeling User Retention through Generative Flow Networks",
        "url": "http://arxiv.org/abs/2406.06043v1",
        "pub_date": "2024-06-10",
        "summary": "Recommender systems aim to fulfill the user's daily demands. While most\nexisting research focuses on maximizing the user's engagement with the system,\nit has recently been pointed out that how frequently the users come back for\nthe service also reflects the quality and stability of recommendations.\nHowever, optimizing this user retention behavior is non-trivial and poses\nseveral challenges including the intractable leave-and-return user activities,\nthe sparse and delayed signal, and the uncertain relations between users'\nretention and their immediate feedback towards each item in the recommendation\nlist. In this work, we regard the retention signal as an overall estimation of\nthe user's end-of-session satisfaction and propose to estimate this signal\nthrough a probabilistic flow. This flow-based modeling technique can\nback-propagate the retention reward towards each recommended item in the user\nsession, and we show that the flow combined with traditional learning-to-rank\nobjectives eventually optimizes a non-discounted cumulative reward for both\nimmediate user feedback and user retention. We verify the effectiveness of our\nmethod through both offline empirical studies on two public datasets and online\nA/B tests in an industrial platform.",
        "translated": ""
    },
    {
        "title": "A WT-ResNet based fault diagnosis model for the urban rail train\n  transmission system",
        "url": "http://arxiv.org/abs/2406.06031v1",
        "pub_date": "2024-06-10",
        "summary": "This study presents a novel fault diagnosis model for urban rail transit\nsystems based on Wavelet Transform Residual Neural Network (WT-ResNet). The\nmodel integrates the advantages of wavelet transform for feature extraction and\nResNet for pattern recognition, offering enhanced diagnostic accuracy and\nrobustness. Experimental results demonstrate the effectiveness of the proposed\nmodel in identifying faults in urban rail trains, paving the way for improved\nmaintenance strategies and reduced downtime.",
        "translated": ""
    },
    {
        "title": "Thanking the World: Exploring Gender-Based Differences in Acknowledgment\n  Patterns and Support Systems in Theses",
        "url": "http://arxiv.org/abs/2406.06006v1",
        "pub_date": "2024-06-10",
        "summary": "Research on acknowledgment sections of scientific papers has gained\nsignificant attention, but there remains a dearth of studies examining\nacknowledgments in the context of Electronic Theses and Dissertations. This\npaper addresses this gap by investigating the sources of support for male and\nfemale researchers in completing their master's or doctoral theses, focusing on\nthe discipline of Library and Information Science. We utilize a novel method of\nextracting the various types of support systems that are acknowledged in 1252\nETDs using RoBERTa-based models. The most prominent forms of support\nacknowledged by researchers are academic, moral, financial, and religious\nsupport. While there are no significant gender-based differences in religious\nand financial support, the ratio of academic to moral support acknowledged by\nresearchers shows strong gender-based variation. Additionally, advisors display\na preference for supervising same-gender researchers. By comprehending the\nnuances of support systems and the unique challenges faced by researchers of\ndifferent genders, we can foster a more inclusive and supportive academic\nenvironment. The insights gained from this research have implications for\nimproving mentoring practices and promoting gender equality in academia.",
        "translated": ""
    },
    {
        "title": "Explainable AI for Mental Disorder Detection via Social Media: A survey\n  and outlook",
        "url": "http://arxiv.org/abs/2406.05984v1",
        "pub_date": "2024-06-10",
        "summary": "Mental health constitutes a complex and pervasive global challenge, affecting\nmillions of lives and often leading to severe consequences. In this paper, we\nconduct a thorough survey to explore the intersection of data science,\nartificial intelligence, and mental healthcare, focusing on the recent\ndevelopments of mental disorder detection through online social media (OSM). A\nsignificant portion of the population actively engages in OSM platforms,\ncreating a vast repository of personal data that holds immense potential for\nmental health analytics. The paper navigates through traditional diagnostic\nmethods, state-of-the-art data- and AI-driven research studies, and the\nemergence of explainable AI (XAI) models for mental healthcare. We review\nstate-of-the-art machine learning methods, particularly those based on modern\ndeep learning, while emphasising the need for explainability in healthcare AI\nmodels. The experimental design section provides insights into prevalent\npractices, including available datasets and evaluation approaches. We also\nidentify key issues and challenges in the field and propose promising future\nresearch directions. As mental health decisions demand transparency,\ninterpretability, and ethical considerations, this paper contributes to the\nongoing discourse on advancing XAI in mental healthcare through social media.\nThe comprehensive overview presented here aims to guide researchers,\npractitioners, and policymakers in developing the area of mental disorder\ndetection.",
        "translated": ""
    },
    {
        "title": "Matryoshka Representation Learning for Recommendation",
        "url": "http://arxiv.org/abs/2406.07432v1",
        "pub_date": "2024-06-11",
        "summary": "Representation learning is essential for deep-neural-network-based\nrecommender systems to capture user preferences and item features within\nfixed-dimensional user and item vectors. Unlike existing representation\nlearning methods that either treat each user preference and item feature\nuniformly or categorize them into discrete clusters, we argue that in the real\nworld, user preferences and item features are naturally expressed and organized\nin a hierarchical manner, leading to a new direction for representation\nlearning. In this paper, we introduce a novel matryoshka representation\nlearning method for recommendation (MRL4Rec), by which we restructure user and\nitem vectors into matryoshka representations with incrementally dimensional and\noverlapping vector spaces to explicitly represent user preferences and item\nfeatures at different hierarchical levels. We theoretically establish that\nconstructing training triplets specific to each level is pivotal in\nguaranteeing accurate matryoshka representation learning. Subsequently, we\npropose the matryoshka negative sampling mechanism to construct training\ntriplets, which further ensures the effectiveness of the matryoshka\nrepresentation learning in capturing hierarchical user preferences and item\nfeatures. The experiments demonstrate that MRL4Rec can consistently and\nsubstantially outperform a number of state-of-the-art competitors on several\nreal-life datasets. Our code is publicly available at\nhttps://github.com/Riwei-HEU/MRL.",
        "translated": ""
    },
    {
        "title": "Graph Reasoning for Explainable Cold Start Recommendation",
        "url": "http://arxiv.org/abs/2406.07420v1",
        "pub_date": "2024-06-11",
        "summary": "The cold start problem, where new users or items have no interaction history,\nremains a critical challenge in recommender systems (RS). A common solution\ninvolves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural\nNetworks (GNNs). Since KGs incorporate auxiliary data and not just user/item\ninteractions, these methods can make relevant recommendations for cold users or\nitems. Graph Reasoning (GR) methods, however, find paths from users to items to\nrecommend using relations in the KG and, in the context of RS, have been used\nfor interpretability. In this study, we propose GRECS: a framework for adapting\nGR to cold start recommendations. By utilizing explicit paths starting for\nusers rather than relying only on entity embeddings, GRECS can find items\ncorresponding to users' preferences by navigating the graph, even when limited\ninformation about users is available. Our experiments show that GRECS mitigates\nthe cold start problem and outperforms competitive baselines across 5 standard\ndatasets while being explainable. This study highlights the potential of GR for\ndeveloping explainable recommender systems better suited for managing cold\nusers and items.",
        "translated": ""
    },
    {
        "title": "Text Information Retrieval in Tetun: A Preliminary Study",
        "url": "http://arxiv.org/abs/2406.07331v1",
        "pub_date": "2024-06-11",
        "summary": "Tetun is one of Timor-Leste's official languages alongside Portuguese. It is\na low-resource language with over 932,400 speakers that started developing when\nTimor-Leste restored its independence in 2002. The media mainly uses Tetun, and\nmore than ten national online newspapers actively broadcast news in Tetun every\nday. However, since information retrieval-based solutions for Tetun do not\nexist, finding Tetun information on the internet is challenging. This work aims\nto investigate and develop solutions that can enable the application of\ninformation retrieval techniques to develop search solutions for Tetun. We\npresent a preliminary result of an experiment conducted on the task of ad-hoc\nretrieval in Tetun.",
        "translated": ""
    },
    {
        "title": "Fetch-A-Set: A Large-Scale OCR-Free Benchmark for Historical Document\n  Retrieval",
        "url": "http://arxiv.org/abs/2406.07315v1",
        "pub_date": "2024-06-11",
        "summary": "This paper introduces Fetch-A-Set (FAS), a comprehensive benchmark tailored\nfor legislative historical document analysis systems, addressing the challenges\nof large-scale document retrieval in historical contexts. The benchmark\ncomprises a vast repository of documents dating back to the XVII century,\nserving both as a training resource and an evaluation benchmark for retrieval\nsystems. It fills a critical gap in the literature by focusing on complex\nextractive tasks within the domain of cultural heritage. The proposed benchmark\ntackles the multifaceted problem of historical document analysis, including\ntext-to-image retrieval for queries and image-to-text topic extraction from\ndocument fragments, all while accommodating varying levels of document\nlegibility. This benchmark aims to spur advancements in the field by providing\nbaselines and data for the development and evaluation of robust historical\ndocument retrieval systems, particularly in scenarios characterized by wide\nhistorical spectrum.",
        "translated": ""
    },
    {
        "title": "Exploring Large Language Models for Relevance Judgments in Tetun",
        "url": "http://arxiv.org/abs/2406.07299v1",
        "pub_date": "2024-06-11",
        "summary": "The Cranfield paradigm has served as a foundational approach for developing\ntest collections, with relevance judgments typically conducted by human\nassessors. However, the emergence of large language models (LLMs) has\nintroduced new possibilities for automating these tasks. This paper explores\nthe feasibility of using LLMs to automate relevance assessments, particularly\nwithin the context of low-resource languages. In our study, LLMs are employed\nto automate relevance judgment tasks, by providing a series of query-document\npairs in Tetun as the input text. The models are tasked with assigning\nrelevance scores to each pair, where these scores are then compared to those\nfrom human annotators to evaluate the inter-annotator agreement levels. Our\ninvestigation reveals results that align closely with those reported in studies\nof high-resource languages.",
        "translated": ""
    },
    {
        "title": "Which Country Is This? Automatic Country Ranking of Street View Photos",
        "url": "http://arxiv.org/abs/2406.07227v1",
        "pub_date": "2024-06-11",
        "summary": "In this demonstration, we present Country Guesser, a live system that guesses\nthe country that a photo is taken in. In particular, given a Google Street View\nimage, our federated ranking model uses a combination of computer vision,\nmachine learning and text retrieval methods to compute a ranking of likely\ncountries of the location shown in a given image from Street View.\nInterestingly, using text-based features to probe large pre-trained language\nmodels can assist to provide cross-modal supervision. We are not aware of\nprevious country guessing systems informed by visual and textual features.",
        "translated": ""
    },
    {
        "title": "EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image\n  Visual Stimuli of Multi-Granularity Labels",
        "url": "http://arxiv.org/abs/2406.07151v1",
        "pub_date": "2024-06-11",
        "summary": "Identifying and reconstructing what we see from brain activity gives us a\nspecial insight into investigating how the biological visual system represents\nthe world. While recent efforts have achieved high-performance image\nclassification and high-quality image reconstruction from brain signals\ncollected by Functional Magnetic Resonance Imaging (fMRI) or\nmagnetoencephalogram (MEG), the expensiveness and bulkiness of these devices\nmake relevant applications difficult to generalize to practical applications.\nOn the other hand, Electroencephalography (EEG), despite its advantages of ease\nof use, cost-efficiency, high temporal resolution, and non-invasive nature, has\nnot been fully explored in relevant studies due to the lack of comprehensive\ndatasets. To address this gap, we introduce EEG-ImageNet, a novel EEG dataset\ncomprising recordings from 16 subjects exposed to 4000 images selected from the\nImageNet dataset. EEG-ImageNet consists of 5 times EEG-image pairs larger than\nexisting similar EEG benchmarks. EEG-ImageNet is collected with image stimuli\nof multi-granularity labels, i.e., 40 images with coarse-grained labels and 40\nwith fine-grained labels. Based on it, we establish benchmarks for object\nclassification and image reconstruction. Experiments with several commonly used\nmodels show that the best models can achieve object classification with\naccuracy around 60% and image reconstruction with two-way identification around\n64%. These results demonstrate the dataset's potential to advance EEG-based\nvisual brain-computer interfaces, understand the visual perception of\nbiological systems, and provide potential applications in improving machine\nvisual models.",
        "translated": ""
    },
    {
        "title": "Progressive Query Expansion for Retrieval Over Cost-constrained Data\n  Sources",
        "url": "http://arxiv.org/abs/2406.07136v1",
        "pub_date": "2024-06-11",
        "summary": "Query expansion has been employed for a long time to improve the accuracy of\nquery retrievers. Earlier works relied on pseudo-relevance feedback (PRF)\ntechniques, which augment a query with terms extracted from documents retrieved\nin a first stage. However, the documents may be noisy hindering the\neffectiveness of the ranking. To avoid this, recent studies have instead used\nLarge Language Models (LLMs) to generate additional content to expand a query.\nThese techniques are prone to hallucination and also focus on the LLM usage\ncost. However, the cost may be dominated by the retrieval in several important\npractical scenarios, where the corpus is only available via APIs which charge a\nfee per retrieved document. We propose combining classic PRF techniques with\nLLMs and create a progressive query expansion algorithm ProQE that iteratively\nexpands the query as it retrieves more documents. ProQE is compatible with both\nsparse and dense retrieval systems. Our experimental results on four retrieval\ndatasets show that ProQE outperforms state-of-the-art baselines by 37% and is\nthe most cost-effective.",
        "translated": ""
    },
    {
        "title": "The Treatment of Ties in Rank-Biased Overlap",
        "url": "http://arxiv.org/abs/2406.07121v1",
        "pub_date": "2024-06-11",
        "summary": "Rank-Biased Overlap (RBO) is a similarity measure for indefinite rankings: it\nis top-weighted, and can be computed when only a prefix of the rankings is\nknown or when they have only some items in common. It is widely used for\ninstance to analyze differences between search engines by comparing the\nrankings of documents they retrieve for the same queries. In these situations,\nthough, it is very frequent to find tied documents that have the same score.\nUnfortunately, the treatment of ties in RBO remains superficial and incomplete,\nin the sense that it is not clear how to calculate it from the ranking prefixes\nonly. In addition, the existing way of dealing with ties is very different from\nthe one traditionally followed in the field of Statistics, most notably found\nin rank correlation coefficients such as Kendall's and Spearman's. In this\npaper we propose a generalized formulation for RBO to handle ties, thanks to\nwhich we complete the original definitions by showing how to perform prefix\nevaluation. We also use it to fully develop two variants that align with the\nones found in the Statistics literature: one when there is a reference ranking\nto compare to, and one when there is not. Overall, these three variants provide\nresearchers with flexibility when comparing rankings with RBO, by clearly\ndetermining what ties mean, and how they should be treated. Finally, using both\nsynthetic and TREC data, we demonstrate the use of these new tie-aware RBO\nmeasures. We show that the scores may differ substantially from the original\ntie-unaware RBO measure, where ties had to be broken at random or by arbitrary\ncriteria such as by document ID. Overall, these results evidence the need for a\nproper account of ties in rank similarity measures such as RBO.",
        "translated": ""
    },
    {
        "title": "Unlocking the Potential of the Metaverse for Innovative and Immersive\n  Digital Care",
        "url": "http://arxiv.org/abs/2406.07114v1",
        "pub_date": "2024-06-11",
        "summary": "The Metaverse, a persistent, immersive virtual environment, has the immense\npotential to revolutionize healthcare by transforming patient care, medical\neducation, and research. This paper explores the applications, benefits, and\nchallenges associated with this transformative technology, highlighting its\nability to improve patient engagement, communication, access to information,\nand health outcomes. The paper also examines how the analysis of Metaverse data\nusing machine learning techniques can unlock insights to further enhance\nhealthcare applications. The discussion summarizes key findings, analyzes the\nsignificance and practical implications of Metaverse integration, and\nidentifies areas for future research. It underscores the role of major tech\ncompanies in developing Metaverse-based solutions and the importance of\naddressing emerging opportunities and challenges to unlock the transformative\npotential of this technology in healthcare. The paper concludes by emphasizing\nthe need for collaboration between stakeholders to ensure the ethical and\neffective implementation of these technologies, ultimately leading to a more\naccessible, personalized, and efficient healthcare system.",
        "translated": ""
    },
    {
        "title": "Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens",
        "url": "http://arxiv.org/abs/2406.08477v1",
        "pub_date": "2024-06-12",
        "summary": "Characterizing users and items through vector representations is crucial for\nvarious tasks in recommender systems. Recent approaches attempt to apply Large\nLanguage Models (LLMs) in recommendation through a question and answer format,\nwhere real users and items (e.g., Item No.2024) are represented with\nin-vocabulary tokens (e.g., \"item\", \"20\", \"24\"). However, since LLMs are\ntypically pretrained on natural language tasks, these in-vocabulary tokens lack\nthe expressive power for distinctive users and items, thereby weakening the\nrecommendation ability even after fine-tuning on recommendation tasks. In this\npaper, we explore how to effectively tokenize users and items in LLM-based\nrecommender systems. We emphasize the role of out-of-vocabulary (OOV) tokens in\naddition to the in-vocabulary ones and claim the memorization of OOV tokens\nthat capture correlations of users/items as well as diversity of OOV tokens. By\nclustering the learned representations from historical user-item interactions,\nwe make the representations of user/item combinations share the same OOV tokens\nif they have similar properties. Furthermore, integrating these OOV tokens into\nthe LLM's vocabulary allows for better distinction between users and items and\nenhanced capture of user-item relationships during fine-tuning on downstream\ntasks. Our proposed framework outperforms existing state-of-the-art methods\nacross various downstream recommendation tasks.",
        "translated": ""
    },
    {
        "title": "Bridging the Gap: Unravelling Local Government Data Sharing Barriers in\n  Estonia and Beyond",
        "url": "http://arxiv.org/abs/2406.08461v1",
        "pub_date": "2024-06-12",
        "summary": "Estonia's digital government success has received global acclaim, yet its\nOpen Government Data (OGD) initiatives, especially at the local level,\nencounter persistent challenges. Despite significant progress of national OGD\ninitiative in OGD rankings, local governments lag in OGD provision. This study\naims to examine barriers hindering municipalities from openly sharing OGD.\nEmploying a qualitative approach through interviews with Estonian\nmunicipalities and drawing on the OGD-adapted Innovation Resistance Theory\nmodel, the study sheds light on barriers impeding OGD sharing. Practical\nrecommendations are proposed to bridge the gap between national policies and\nlocal implementation, including enhancing awareness, improving data governance\nframeworks, and fostering collaboration be-tween local and national\nauthorities. By addressing overlooked weaknesses in the Estonian open data\necosystem and providing actionable recommendations, this research contributes\nto a more resilient and sustainable open data ecosystem. Additionally, by\nvalidating the OGD-adapted Innovation Resistance Theory model and proposing a\nrevised version tailored for local government contexts, the study advances\ntheoretical frameworks for understanding data sharing resistance. Ultimately,\nthis study serves as a call to action for policymakers and practitioners to\nprioritize local OGD initiatives.",
        "translated": ""
    },
    {
        "title": "Wiki Entity Summarization Benchmark",
        "url": "http://arxiv.org/abs/2406.08435v1",
        "pub_date": "2024-06-12",
        "summary": "Entity summarization aims to compute concise summaries for entities in\nknowledge graphs. Existing datasets and benchmarks are often limited to a few\nhundred entities and discard graph structure in source knowledge graphs. This\nlimitation is particularly pronounced when it comes to ground-truth summaries,\nwhere there exist only a few labeled summaries for evaluation and training. We\npropose WikES, a comprehensive benchmark comprising of entities, their\nsummaries, and their connections. Additionally, WikES features a dataset\ngenerator to test entity summarization algorithms in different areas of the\nknowledge graph. Importantly, our approach combines graph algorithms and NLP\nmodels as well as different data sources such that WikES does not require human\nannotation, rendering the approach cost-effective and generalizable to multiple\ndomains. Finally, WikES is scalable and capable of capturing the complexities\nof knowledge graphs in terms of topology and semantics. WikES features existing\ndatasets for comparison. Empirical studies of entity summarization methods\nconfirm the usefulness of our benchmark. Data, code, and models are available\nat: https://github.com/msorkhpar/wiki-entity-summarization.",
        "translated": ""
    },
    {
        "title": "Boosting Multimedia Recommendation via Separate Generic and Unique\n  Awareness",
        "url": "http://arxiv.org/abs/2406.08270v1",
        "pub_date": "2024-06-12",
        "summary": "Multimedia recommendation, which incorporates various modalities (e.g.,\nimages, texts, etc.) into user or item representation to improve recommendation\nquality, has received widespread attention. Recent methods mainly focus on\ncross-modal alignment with self-supervised learning to obtain higher quality\nrepresentation. Despite remarkable performance, we argue that there is still a\nlimitation: completely aligning representation undermines modality-unique\ninformation. We consider that cross-modal alignment is right, but it should not\nbe the entirety, as different modalities contain generic information between\nthem, and each modality also contains unique information. Simply aligning each\nmodality may ignore modality-unique features, thus degrading the performance of\nmultimedia recommendation. To tackle the above limitation, we propose a\nSeparate Alignment aNd Distancing framework (SAND) for multimedia\nrecommendation, which concurrently learns both modal-unique and -generic\nrepresentation to achieve more comprehensive items representation. First, we\nsplit each modal feature into generic and unique part. Then, in the alignment\nmodule, for better integration of semantic information between different\nmodalities , we design a SoloSimLoss to align generic modalities. Furthermore,\nin the distancing module, we aim to distance the unique modalities from the\nmodal-generic so that each modality retains its unique and complementary\ninformation. In the light of the flexibility of our framework, we give two\ntechnical solutions, the more capable mutual information minimization and the\nsimple negative l2 distance. Finally, extensive experimental results on three\npopular datasets demonstrate the effectiveness and generalization of our\nproposed framework.",
        "translated": ""
    },
    {
        "title": "GPT4Rec: Graph Prompt Tuning for Streaming Recommendation",
        "url": "http://arxiv.org/abs/2406.08229v1",
        "pub_date": "2024-06-12",
        "summary": "In the realm of personalized recommender systems, the challenge of adapting\nto evolving user preferences and the continuous influx of new users and items\nis paramount. Conventional models, typically reliant on a static training-test\napproach, struggle to keep pace with these dynamic demands. Streaming\nrecommendation, particularly through continual graph learning, has emerged as a\nnovel solution. However, existing methods in this area either rely on\nhistorical data replay, which is increasingly impractical due to stringent data\nprivacy regulations; or are inability to effectively address the over-stability\nissue; or depend on model-isolation and expansion strategies. To tackle these\ndifficulties, we present GPT4Rec, a Graph Prompt Tuning method for streaming\nRecommendation. Given the evolving user-item interaction graph, GPT4Rec first\ndisentangles the graph patterns into multiple views. After isolating specific\ninteraction patterns and relationships in different views, GPT4Rec utilizes\nlightweight graph prompts to efficiently guide the model across varying\ninteraction patterns within the user-item graph. Firstly, node-level prompts\nare employed to instruct the model to adapt to changes in the attributes or\nproperties of individual nodes within the graph. Secondly, structure-level\nprompts guide the model in adapting to broader patterns of connectivity and\nrelationships within the graph. Finally, view-level prompts are innovatively\ndesigned to facilitate the aggregation of information from multiple\ndisentangled views. These prompt designs allow GPT4Rec to synthesize a\ncomprehensive understanding of the graph, ensuring that all vital aspects of\nthe user-item interactions are considered and effectively integrated.\nExperiments on four diverse real-world datasets demonstrate the effectiveness\nand efficiency of our proposal.",
        "translated": ""
    },
    {
        "title": "Graph Bottlenecked Social Recommendation",
        "url": "http://arxiv.org/abs/2406.08214v1",
        "pub_date": "2024-06-12",
        "summary": "With the emergence of social networks, social recommendation has become an\nessential technique for personalized services. Recently, graph-based social\nrecommendations have shown promising results by capturing the high-order social\ninfluence. Most empirical studies of graph-based social recommendations\ndirectly take the observed social networks into formulation, and produce user\npreferences based on social homogeneity. Despite the effectiveness, we argue\nthat social networks in the real-world are inevitably noisy~(existing redundant\nsocial relations), which may obstruct precise user preference characterization.\nNevertheless, identifying and removing redundant social relations is\nchallenging due to a lack of labels. In this paper, we focus on learning the\ndenoised social structure to facilitate recommendation tasks from an\ninformation bottleneck perspective. Specifically, we propose a novel Graph\nBottlenecked Social Recommendation (GBSR) framework to tackle the social noise\nissue.GBSR is a model-agnostic social denoising framework, that aims to\nmaximize the mutual information between the denoised social graph and\nrecommendation labels, meanwhile minimizing it between the denoised social\ngraph and the original one. This enables GBSR to learn the minimal yet\nsufficient social structure, effectively reducing redundant social relations\nand enhancing social recommendations. Technically, GBSR consists of two\nelaborate components, preference-guided social graph refinement, and HSIC-based\nbottleneck learning. Extensive experimental results demonstrate the superiority\nof the proposed GBSR, including high performances and good generality combined\nwith various backbones. Our code is available at:\nhttps://github.com/yimutianyang/KDD24-GBSR.",
        "translated": ""
    },
    {
        "title": "Prediction of the Realisation of an Information Need: An EEG Study",
        "url": "http://arxiv.org/abs/2406.08105v1",
        "pub_date": "2024-06-12",
        "summary": "One of the foundational goals of Information Retrieval (IR) is to satisfy\nsearchers' Information Needs (IN). Understanding how INs physically manifest\nhas long been a complex and elusive process. However, recent studies utilising\nElectroencephalography (EEG) data have provided real-time insights into the\nneural processes associated with INs. Unfortunately, they have yet to\ndemonstrate how this insight can practically benefit the search experience. As\nsuch, within this study, we explore the ability to predict the realisation of\nIN within EEG data across 14 subjects whilst partaking in a Question-Answering\n(Q/A) task. Furthermore, we investigate the combinations of EEG features that\nyield optimal predictive performance, as well as identify regions within the\nQ/A queries where a subject's realisation of IN is more pronounced. The\nfindings from this work demonstrate that EEG data is sufficient for the\nreal-time prediction of the realisation of an IN across all subjects with an\naccuracy of 73.5\\% (SD 2.6\\%) and on a per-subject basis with an accuracy of\n90.1\\% (SD 22.1\\%). This work helps to close the gap by bridging theoretical\nneuroscientific advancements with tangible improvements in information\nretrieval practices, paving the way for real-time prediction of the realisation\nof IN.",
        "translated": ""
    },
    {
        "title": "A Self-boosted Framework for Calibrated Ranking",
        "url": "http://arxiv.org/abs/2406.08010v1",
        "pub_date": "2024-06-12",
        "summary": "Scale-calibrated ranking systems are ubiquitous in real-world applications\nnowadays, which pursue accurate ranking quality and calibrated probabilistic\npredictions simultaneously. For instance, in the advertising ranking system,\nthe predicted click-through rate (CTR) is utilized for ranking and required to\nbe calibrated for the downstream cost-per-click ads bidding. Recently,\nmulti-objective based methods have been wildly adopted as a standard approach\nfor Calibrated Ranking, which incorporates the combination of two loss\nfunctions: a pointwise loss that focuses on calibrated absolute values and a\nranking loss that emphasizes relative orderings. However, when applied to\nindustrial online applications, existing multi-objective CR approaches still\nsuffer from two crucial limitations. First, previous methods need to aggregate\nthe full candidate list within a single mini-batch to compute the ranking loss.\nSuch aggregation strategy violates extensive data shuffling which has long been\nproven beneficial for preventing overfitting, and thus degrades the training\neffectiveness. Second, existing multi-objective methods apply the two\ninherently conflicting loss functions on a single probabilistic prediction,\nwhich results in a sub-optimal trade-off between calibration and ranking. To\ntackle the two limitations, we propose a Self-Boosted framework for Calibrated\nRanking (SBCR).",
        "translated": ""
    },
    {
        "title": "Counteracting Duration Bias in Video Recommendation via Counterfactual\n  Watch Time",
        "url": "http://arxiv.org/abs/2406.07932v1",
        "pub_date": "2024-06-12",
        "summary": "In video recommendation, an ongoing effort is to satisfy users' personalized\ninformation needs by leveraging their logged watch time. However, watch time\nprediction suffers from duration bias, hindering its ability to reflect users'\ninterests accurately. Existing label-correction approaches attempt to uncover\nuser interests through grouping and normalizing observed watch time according\nto video duration. Although effective to some extent, we found that these\napproaches regard completely played records (i.e., a user watches the entire\nvideo) as equally high interest, which deviates from what we observed on real\ndatasets: users have varied explicit feedback proportion when completely\nplaying videos. In this paper, we introduce the counterfactual watch time(CWT),\nthe potential watch time a user would spend on the video if its duration is\nsufficiently long. Analysis shows that the duration bias is caused by the\ntruncation of CWT due to the video duration limitation, which usually occurs on\nthose completely played records. Besides, a Counterfactual Watch Model (CWM) is\nproposed, revealing that CWT equals the time users get the maximum benefit from\nvideo recommender systems. Moreover, a cost-based transform function is defined\nto transform the CWT into the estimation of user interest, and the model can be\nlearned by optimizing a counterfactual likelihood function defined over\nobserved user watch times. Extensive experiments on three real video\nrecommendation datasets and online A/B testing demonstrated that CWM\neffectively enhanced video recommendation accuracy and counteracted the\nduration bias.",
        "translated": ""
    },
    {
        "title": "DeTriever: Decoder-representation-based Retriever for Improving NL2SQL\n  In-Context Learning",
        "url": "http://arxiv.org/abs/2406.07913v1",
        "pub_date": "2024-06-12",
        "summary": "While in-context Learning (ICL) has proven to be an effective technique to\nimprove the performance of Large Language Models (LLMs) in a variety of complex\ntasks, notably in translating natural language questions into Structured Query\nLanguage (NL2SQL), the question of how to select the most beneficial\ndemonstration examples remains an open research problem. While prior works\noften adapted off-the-shelf encoders to retrieve examples dynamically, an\ninherent discrepancy exists in the representational capacities between the\nexternal retrievers and the LLMs. Further, optimizing the selection of examples\nis a non-trivial task, since there are no straightforward methods to assess the\nrelative benefits of examples without performing pairwise inference. To address\nthese shortcomings, we propose DeTriever, a novel demonstration retrieval\nframework that learns a weighted combination of LLM hidden states, where rich\nsemantic information is encoded. To train the model, we propose a proxy score\nthat estimates the relative benefits of examples based on the similarities\nbetween output queries. Experiments on two popular NL2SQL benchmarks\ndemonstrate that our method significantly outperforms the state-of-the-art\nbaselines on one-shot NL2SQL tasks.",
        "translated": ""
    },
    {
        "title": "Can't Hide Behind the API: Stealing Black-Box Commercial Embedding\n  Models",
        "url": "http://arxiv.org/abs/2406.09355v1",
        "pub_date": "2024-06-13",
        "summary": "Embedding models that generate representation vectors from natural language\ntext are widely used, reflect substantial investments, and carry significant\ncommercial value. Companies such as OpenAI and Cohere have developed competing\nembedding models accessed through APIs that require users to pay for usage. In\nthis architecture, the models are \"hidden\" behind APIs, but this does not mean\nthat they are \"well guarded\". We present, to our knowledge, the first effort to\n\"steal\" these models for retrieval by training local models on text-embedding\npairs obtained from the commercial APIs. Our experiments show using standard\nbenchmarks that it is possible to efficiently replicate the retrieval\neffectiveness of the commercial embedding models using an attack that costs\nonly around $200 to train (presumably) smaller models with fewer dimensions.\nOur findings raise important considerations for deploying commercial embedding\nmodels and suggest measures to mitigate the risk of model theft.",
        "translated": ""
    },
    {
        "title": "Master of Disaster: A Disaster-Related Event Monitoring System From News\n  Streams",
        "url": "http://arxiv.org/abs/2406.09323v1",
        "pub_date": "2024-06-13",
        "summary": "The need for a disaster-related event monitoring system has arisen due to the\nsocietal and economic impact caused by the increasing number of severe disaster\nevents. An event monitoring system should be able to extract event-related\ninformation from texts, and discriminates event instances. We demonstrate our\nopen-source event monitoring system, namely, Master of Disaster (MoD), which\nreceives news streams, extracts event information, links extracted information\nto a knowledge graph (KG), in this case Wikidata, and discriminates event\ninstances visually. The goal of event visualization is to group event mentions\nreferring to the same real-world event instance so that event instance\ndiscrimination can be achieved by visual screening.",
        "translated": ""
    },
    {
        "title": "Khmer Semantic Search Engine: Digital Information Access and Document\n  Retrieval",
        "url": "http://arxiv.org/abs/2406.09320v1",
        "pub_date": "2024-06-13",
        "summary": "The search engine process is crucial for document content retrieval. For\nKhmer documents, a tool is needed to extract essential keywords. Despite the\ndaily generation of significant Khmer content, Cambodians struggle to find\nnecessary documents due to the lack of an effective semantic searching tool.\nEven Google does not deliver high accuracy for Khmer content. Semantic search\nengines improve search results by employing advanced algorithms to understand\nvarious content types. With the rise in Khmer digital content such as reports,\narticles, and social media feedback enhanced search capabilities are essential.\nThis research proposes the first Khmer Semantic Search Engine (KSE), designed\nto improve traditional Khmer search methods. Utilizing semantic matching\ntechniques and formally annotated semantic content, our tool extracts\nmeaningful keywords from user queries performs precise matching, and provides\nthe best matching offline documents and online URL documents. We propose two\nsemantic search frameworks based on keyword extraction and semantic search\nmatching. Additionally, we developed tools for data preparation, including\ndocument addition and manual keyword extraction. To evaluate performance, we\ncreated a ground truth dataset and discussed issues related to searching and\nsemantic search. Our findings show how understanding search term semantics can\nlead to more accurate results.",
        "translated": ""
    },
    {
        "title": "On Softmax Direct Preference Optimization for Recommendation",
        "url": "http://arxiv.org/abs/2406.09215v1",
        "pub_date": "2024-06-13",
        "summary": "Recommender systems aim to predict personalized rankings based on user\npreference data. With the rise of Language Models (LMs), LM-based recommenders\nhave been widely explored due to their extensive world knowledge and powerful\nreasoning abilities. Most of the LM-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target\nresponse and fine-tuning LM with a language modeling loss. However, the current\nobjective fails to fully leverage preference data and is not optimized for\npersonalized ranking tasks, which hinders the performance of LM-based\nrecommenders. Inspired by the current advancement of Direct Preference\nOptimization (DPO) in human preference alignment and the success of softmax\nloss in recommendations, we propose Softmax-DPO (\\textbf{S-DPO}) to instill\nranking information into the LM to help LM-based recommenders distinguish\npreferred items from negatives, rather than solely focusing on positives.\nSpecifically, we incorporate multiple negatives in user preference data and\ndevise an alternative version of DPO loss tailored for LM-based recommenders,\nconnected to softmax sampling strategies. Theoretically, we bridge S-DPO with\nthe softmax loss over negative sampling and find that it has a side effect of\nmining hard negatives, which assures its exceptional capabilities in\nrecommendation tasks. Empirically, extensive experiments conducted on three\nreal-world datasets demonstrate the superiority of S-DPO to effectively model\nuser preference and further boost recommendation performance while mitigating\nthe data likelihood decline issue of DPO. Our codes are available at\nhttps://github.com/chenyuxin1999/S-DPO.",
        "translated": ""
    },
    {
        "title": "Reducing Task Discrepancy of Text Encoders for Zero-Shot Composed Image\n  Retrieval",
        "url": "http://arxiv.org/abs/2406.09188v1",
        "pub_date": "2024-06-13",
        "summary": "Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable searches. Due to\nthe expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR\nsetting has been actively studied to eliminate the need for human-collected\ntriplet datasets. The mainstream of ZS-CIR employs an efficient projection\nmodule that projects a CLIP image embedding to the CLIP text token embedding\nspace, while fixing the CLIP encoders. Using the projected image embedding,\nthese methods generate image-text composed features by using the pre-trained\ntext encoder. However, their CLIP image and text encoders suffer from the task\ndiscrepancy between the pre-training task (text $\\leftrightarrow$ image) and\nthe target CIR task (image + text $\\leftrightarrow$ image). Conceptually, we\nneed expensive triplet samples to reduce the discrepancy, but we use cheap text\ntriplets instead and update the text encoder. To that end, we introduce the\nReducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD),\na plug-and-play training scheme for the text encoder that enhances its\ncapability using a novel target-anchored text contrastive learning. We also\npropose two additional techniques to improve the proposed learning scheme: a\nhard negatives-based refined batch sampling strategy and a sophisticated\nconcatenation scheme. Integrating RTD into the state-of-the-art\nprojection-based ZS-CIR methods significantly improves performance across\nvarious datasets and backbones, demonstrating its efficiency and\ngeneralizability.",
        "translated": ""
    },
    {
        "title": "Contextual Distillation Model for Diversified Recommendation",
        "url": "http://arxiv.org/abs/2406.09021v1",
        "pub_date": "2024-06-13",
        "summary": "The diversity of recommendation is equally crucial as accuracy in improving\nuser experience. Existing studies, e.g., Determinantal Point Process (DPP) and\nMaximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively\nselect items that optimize both accuracy and diversity. However, prior methods\ntypically exhibit quadratic complexity, limiting their applications to the\nre-ranking stage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking and ranking stages. In\nthis paper, we propose Contextual Distillation Model (CDM), an efficient\nrecommendation model that addresses diversification, suitable for the\ndeployment in all stages of industrial recommendation pipelines. Specifically,\nCDM utilizes the candidate items in the same user request as context to enhance\nthe diversification of the results. We propose a contrastive context encoder\nthat employs attention mechanisms to model both positive and negative contexts.\nFor the training of CDM, we compare each target item with its context embedding\nand utilize the knowledge distillation framework to learn the win probability\nof each target item under the MMR algorithm, where the teacher is derived from\nMMR outputs. During inference, ranking is performed through a linear\ncombination of the recommendation and student model scores, ensuring both\ndiversity and efficiency. We perform offline evaluations on two industrial\ndatasets and conduct online A/B test of CDM on the short-video platform\nKuaiShou. The considerable enhancements observed in both recommendation quality\nand diversity, as shown by metrics, provide strong superiority for the\neffectiveness of CDM.",
        "translated": ""
    },
    {
        "title": "Robust Information Retrieval",
        "url": "http://arxiv.org/abs/2406.08891v1",
        "pub_date": "2024-06-13",
        "summary": "Beyond effectiveness, the robustness of an information retrieval (IR) system\nis increasingly attracting attention. When deployed, a critical technology such\nas IR should not only deliver strong performance on average but also have the\nability to handle a variety of exceptional situations. In recent years,\nresearch into the robustness of IR has seen significant growth, with numerous\nresearchers offering extensive analyses and proposing myriad strategies to\naddress robustness challenges. In this tutorial, we first provide background\ninformation covering the basics and a taxonomy of robustness in IR. Then, we\nexamine adversarial robustness and out-of-distribution (OOD) robustness within\nIR-specific contexts, extensively reviewing recent progress in methods to\nenhance robustness. The tutorial concludes with a discussion on the robustness\nof IR in the context of large language models (LLMs), highlighting ongoing\nchallenges and promising directions for future research. This tutorial aims to\ngenerate broader attention to robustness issues in IR, facilitate an\nunderstanding of the relevant literature, and lower the barrier to entry for\ninterested researchers and practitioners.",
        "translated": ""
    },
    {
        "title": "Self-supervised Graph Neural Network for Mechanical CAD Retrieval",
        "url": "http://arxiv.org/abs/2406.08863v1",
        "pub_date": "2024-06-13",
        "summary": "CAD (Computer-Aided Design) plays a crucial role in mechanical industry,\nwhere large numbers of similar-shaped CAD parts are often created. Efficiently\nreusing these parts is key to reducing design and production costs for\nenterprises. Retrieval systems are vital for achieving CAD reuse, but the\ncomplex shapes of CAD models are difficult to accurately describe using text or\nkeywords, making traditional retrieval methods ineffective. While existing\nrepresentation learning approaches have been developed for CAD, manually\nlabeling similar samples in these methods is expensive. Additionally, CAD\nmodels' unique parameterized data structure presents challenges for applying\nexisting 3D shape representation learning techniques directly. In this work, we\npropose GC-CAD, a self-supervised contrastive graph neural network-based method\nfor mechanical CAD retrieval that directly models parameterized CAD raw files.\nGC-CAD consists of two key modules: structure-aware representation learning and\ncontrastive graph learning framework. The method leverages graph neural\nnetworks to extract both geometric and topological information from CAD models,\ngenerating feature representations. We then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model to train\nwithout manual labels and generate retrieval-ready representations.\nExperimental results on four datasets including human evaluation demonstrate\nthat the proposed method achieves significant accuracy improvements and up to\n100 times efficiency improvement over the baseline methods.",
        "translated": ""
    },
    {
        "title": "How Powerful is Graph Filtering for Recommendation",
        "url": "http://arxiv.org/abs/2406.08827v1",
        "pub_date": "2024-06-13",
        "summary": "It has been shown that the effectiveness of graph convolutional network (GCN)\nfor recommendation is attributed to the spectral graph filtering. Most\nGCN-based methods consist of a graph filter or followed by a low-rank mapping\noptimized based on supervised training. However, we show two limitations\nsuppressing the power of graph filtering: (1) Lack of generality. Due to the\nvaried noise distribution, graph filters fail to denoise sparse data where\nnoise is scattered across all frequencies, while supervised training results in\nworse performance on dense data where noise is concentrated in middle\nfrequencies that can be removed by graph filters without training. (2) Lack of\nexpressive power. We theoretically show that linear GCN (LGCN) that is\neffective on collaborative filtering (CF) cannot generate arbitrary embeddings,\nimplying the possibility that optimal data representation might be unreachable.\n  To tackle the first limitation, we show close relation between noise\ndistribution and the sharpness of spectrum where a sharper spectral\ndistribution is more desirable causing data noise to be separable from\nimportant features without training. Based on this observation, we propose a\ngeneralized graph normalization G^2N to adjust the sharpness of spectral\ndistribution in order to redistribute data noise to assure that it can be\nremoved by graph filtering without training. As for the second limitation, we\npropose an individualized graph filter (IGF) adapting to the different\nconfidence levels of the user preference that interactions can reflect, which\nis proved to be able to generate arbitrary embeddings. By simplifying LGCN, we\nfurther propose a simplified graph filtering (SGFCF) which only requires the\ntop-K singular values for recommendation. Finally, experimental results on four\ndatasets with different density settings demonstrate the effectiveness and\nefficiency of our proposed methods.",
        "translated": ""
    },
    {
        "title": "DIET: Customized Slimming for Incompatible Networks in Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2406.08804v1",
        "pub_date": "2024-06-13",
        "summary": "Due to the continuously improving capabilities of mobile edges, recommender\nsystems start to deploy models on edges to alleviate network congestion caused\nby frequent mobile requests. Several studies have leveraged the proximity of\nedge-side to real-time data, fine-tuning them to create edge-specific models.\nDespite their significant progress, these methods require substantial on-edge\ncomputational resources and frequent network transfers to keep the model up to\ndate. The former may disrupt other processes on the edge to acquire\ncomputational resources, while the latter consumes network bandwidth, leading\nto a decrease in user satisfaction. In response to these challenges, we propose\na customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploys\nthe same generic backbone (potentially incompatible for a specific edge) to all\ndevices. To minimize frequent bandwidth usage and storage consumption in\npersonalization, DIET tailors specific subnets for each edge based on its past\ninteractions, learning to generate slimming subnets(diets) within incompatible\nnetworks for efficient transfer. It also takes the inter-layer relationships\ninto account, empirically reducing inference time while obtaining more suitable\ndiets. We further explore the repeated modules within networks and propose a\nmore storage-efficient framework, DIETING, which utilizes a single layer of\nparameters to represent the entire network, achieving comparably excellent\nperformance. The experiments across four state-of-the-art datasets and two\nwidely used models demonstrate the superior accuracy in recommendation and\nefficiency in transmission and storage of our framework.",
        "translated": ""
    },
    {
        "title": "HIRO: Hierarchical Information Retrieval Optimization",
        "url": "http://arxiv.org/abs/2406.09979v1",
        "pub_date": "2024-06-14",
        "summary": "Large Language Models (LLMs) excel in natural language tasks but face\nlimitations due to static training datasets, resulting in outdated or\ncontextually shallow responses. Retrieval-Augmented Generation (RAG) addresses\nthis by integrating real-time external knowledge, enhancing model accuracy and\ncredibility, especially for knowledge-intensive tasks. However, RAG-enhanced\nLLMs struggle with long contexts, causing them to \"choke\" on information\noverload, compromising response quality. Recent RAG applications use\nhierarchical data structures for storing documents, organized at various levels\nof summarization and information density. In this context, we introduce HIRO\n(Hierarchical Information Retrieval Optimization), a novel querying approach\nfor RAG applications using hierarchical structures for storing documents. HIRO\nemploys DFS-based recursive similarity score calculation and branch pruning to\nminimize the context returned to the LLM without informational loss. HIRO\noutperforms existing querying mechanisms on the NarrativeQA dataset by an\nabsolute performance gain of 10.85%.",
        "translated": ""
    },
    {
        "title": "Harm Mitigation in Recommender Systems under User Preference Dynamics",
        "url": "http://arxiv.org/abs/2406.09882v1",
        "pub_date": "2024-06-14",
        "summary": "We consider a recommender system that takes into account the interplay\nbetween recommendations, the evolution of user interests, and harmful content.\nWe model the impact of recommendations on user behavior, particularly the\ntendency to consume harmful content. We seek recommendation policies that\nestablish a tradeoff between maximizing click-through rate (CTR) and mitigating\nharm. We establish conditions under which the user profile dynamics have a\nstationary point, and propose algorithms for finding an optimal recommendation\npolicy at stationarity. We experiment on a semi-synthetic movie recommendation\nsetting initialized with real data and observe that our policies outperform\nbaselines at simultaneously maximizing CTR and mitigating harm.",
        "translated": ""
    },
    {
        "title": "Unraveling Anomalies in Time: Unsupervised Discovery and Isolation of\n  Anomalous Behavior in Bio-regenerative Life Support System Telemetry",
        "url": "http://arxiv.org/abs/2406.09825v1",
        "pub_date": "2024-06-14",
        "summary": "The detection of abnormal or critical system states is essential in condition\nmonitoring. While much attention is given to promptly identifying anomalies, a\nretrospective analysis of these anomalies can significantly enhance our\ncomprehension of the underlying causes of observed undesired behavior. This\naspect becomes particularly critical when the monitored system is deployed in a\nvital environment. In this study, we delve into anomalies within the domain of\nBio-Regenerative Life Support Systems (BLSS) for space exploration and analyze\nanomalies found in telemetry data stemming from the EDEN ISS space greenhouse\nin Antarctica. We employ time series clustering on anomaly detection results to\ncategorize various types of anomalies in both uni- and multivariate settings.\nWe then assess the effectiveness of these methods in identifying systematic\nanomalous behavior. Additionally, we illustrate that the anomaly detection\nmethods MDI and DAMP produce complementary results, as previously indicated by\nresearch.",
        "translated": ""
    },
    {
        "title": "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from\n  Corporate Climate Disclosures",
        "url": "http://arxiv.org/abs/2406.09818v1",
        "pub_date": "2024-06-14",
        "summary": "To handle the vast amounts of qualitative data produced in corporate climate\ncommunication, stakeholders increasingly rely on Retrieval Augmented Generation\n(RAG) systems. However, a significant gap remains in evaluating domain-specific\ninformation retrieval - the basis for answer generation. To address this\nchallenge, this work simulates the typical tasks of a sustainability analyst by\nexamining 30 sustainability reports with 16 detailed climate-related questions.\nAs a result, we obtain a dataset with over 8.5K unique question-source-answer\npairs labeled by different levels of relevance. Furthermore, we develop a use\ncase with the dataset to investigate the integration of expert knowledge into\ninformation retrieval with embeddings. Although we show that incorporating\nexpert knowledge works, we also outline the critical limitations of embeddings\nin knowledge-intensive downstream domains like climate change communication.",
        "translated": ""
    },
    {
        "title": "Soil nitrogen forecasting from environmental variables provided by\n  multisensor remote sensing images",
        "url": "http://arxiv.org/abs/2406.09812v1",
        "pub_date": "2024-06-14",
        "summary": "This study introduces a framework for forecasting soil nitrogen content,\nleveraging multi-modal data, including multi-sensor remote sensing images and\nadvanced machine learning methods. We integrate the Land Use/Land Cover Area\nFrame Survey (LUCAS) database, which covers European and UK territory, with\nenvironmental variables from satellite sensors to create a dataset of novel\nfeatures. We further test a broad range of machine learning algorithms,\nfocusing on tree-based models such as CatBoost, LightGBM, and XGBoost. We test\nthe proposed methods with a variety of land cover classes, including croplands\nand grasslands to ensure the robustness of this approach. Our results\ndemonstrate that the CatBoost model surpasses other methods in accuracy. This\nresearch advances the field of agricultural management and environmental\nmonitoring and demonstrates the significant potential of integrating\nmultisensor remote sensing data with machine learning for environmental\nanalysis.",
        "translated": ""
    },
    {
        "title": "IFA: Interaction Fidelity Attention for Entire Lifelong Behaviour\n  Sequence Modeling",
        "url": "http://arxiv.org/abs/2406.09742v1",
        "pub_date": "2024-06-14",
        "summary": "The lifelong user behavior sequence provides abundant information of user\npreference and gains impressive improvement in the recommendation task, however\nincreases computational consumption significantly. To meet the severe latency\nrequirement in online service, a short sub-sequence is sampled based on\nsimilarity to the target item. Unfortunately, items not in the sub-sequence are\nabandoned, leading to serious information loss.\n  In this paper, we propose a new efficient paradigm to model the full lifelong\nsequence, which is named as \\textbf{I}nteraction \\textbf{F}idelity\n\\textbf{A}ttention (\\textbf{IFA}). In IFA, we input all target items in the\ncandidate set into the model at once, and leverage linear transformer to reduce\nthe time complexity of the cross attention between the candidate set and the\nsequence without any interaction information loss. We also additionally model\nthe relationship of all target items for optimal set generation, and design\nloss function for better consistency of training and inference. We demonstrate\nthe effectiveness and efficiency of our model by off-line and online\nexperiments in the recommender system of Kuaishou.",
        "translated": ""
    },
    {
        "title": "Enhancing Text Corpus Exploration with Post Hoc Explanations and\n  Comparative Design",
        "url": "http://arxiv.org/abs/2406.09686v1",
        "pub_date": "2024-06-14",
        "summary": "Text corpus exploration (TCE) spans the range of exploratory search tasks: it\ngoes beyond simple retrieval to include item discovery and learning about the\ncorpus and topic. Systems support TCE with tools such as similarity-based\nrecommendations and embedding-based spatial maps. However, these tools address\nspecific tasks; current systems lack the flexibility to support the range of\ntasks encountered in practice and the iterative, multiscale, workflows users\nemploy. In this paper, we provide methods that enhance TCE tools with post hoc\nexplanations and multiscale, comparative designs to provide flexible support\nfor user needs. We introduce salience functions as a mechanism to provide post\nhoc explanations of similarity, recommendations, and spatial placement. This\npost hoc strategy allows our approach to complement a variety of underlying\nalgorithms; the salience functions provide both exemplar- and feature-based\nexplanations at scales ranging from individual documents through to the entire\ncorpus. These explanations are incorporated into a set of views that operate at\nmultiple scales. The views use design elements that explicitly support\ncomparison to enable flexible integration. Together, these form an approach\nthat provides a flexible toolset that can address a range of tasks. We\ndemonstrate our approach in a prototype system that enables the exploration of\ncorpora of paper abstracts and newspaper archives. Examples illustrate how our\napproach enables the system to flexibly support a wide range of tasks and\nworkflows that emerge in user scenarios. A user study confirms that researchers\nare able to use our system to achieve a variety of tasks.",
        "translated": ""
    },
    {
        "title": "Enhancing Knowledge Retrieval with In-Context Learning and Semantic\n  Search through Generative AI",
        "url": "http://arxiv.org/abs/2406.09621v1",
        "pub_date": "2024-06-13",
        "summary": "Retrieving and extracting knowledge from extensive research documents and\nlarge databases presents significant challenges for researchers, students, and\nprofessionals in today's information-rich era. Existing retrieval systems,\nwhich rely on general-purpose Large Language Models (LLMs), often fail to\nprovide accurate responses to domain-specific inquiries. Additionally, the high\ncost of pretraining or fine-tuning LLMs for specific domains limits their\nwidespread adoption. To address these limitations, we propose a novel\nmethodology that combines the generative capabilities of LLMs with the fast and\naccurate retrieval capabilities of vector databases. This advanced retrieval\nsystem can efficiently handle both tabular and non-tabular data, understand\nnatural language user queries, and retrieve relevant information without\nfine-tuning. The developed model, Generative Text Retrieval (GTR), is adaptable\nto both unstructured and structured data with minor refinement. GTR was\nevaluated on both manually annotated and public datasets, achieving over 90%\naccuracy and delivering truthful outputs in 87% of cases. Our model achieved\nstate-of-the-art performance with a Rouge-L F1 score of 0.98 on the MSMARCO\ndataset. The refined model, Generative Tabular Text Retrieval (GTR-T),\ndemonstrated its efficiency in large database querying, achieving an Execution\nAccuracy (EX) of 0.82 and an Exact-Set-Match (EM) accuracy of 0.60 on the\nSpider dataset, using an open-source LLM. These efforts leverage Generative AI\nand In-Context Learning to enhance human-text interaction and make advanced AI\ncapabilities more accessible. By integrating robust retrieval systems with\npowerful LLMs, our approach aims to democratize access to sophisticated AI\ntools, improving the efficiency, accuracy, and scalability of AI-driven\ninformation retrieval and database querying.",
        "translated": ""
    },
    {
        "title": "Multi-Modal Retrieval For Large Language Model Based Speech Recognition",
        "url": "http://arxiv.org/abs/2406.09618v1",
        "pub_date": "2024-06-13",
        "summary": "Retrieval is a widely adopted approach for improving language models\nleveraging external information. As the field moves towards multi-modal large\nlanguage models, it is important to extend the pure text based methods to\nincorporate other modalities in retrieval as well for applications across the\nwide spectrum of machine learning tasks and data types. In this work, we\npropose multi-modal retrieval with two approaches: kNN-LM and cross-attention\ntechniques. We demonstrate the effectiveness of our retrieval approaches\nempirically by applying them to automatic speech recognition tasks with access\nto external information. Under this setting, we show that speech-based\nmulti-modal retrieval outperforms text based retrieval, and yields up to 50 %\nimprovement in word error rate over the multi-modal language model baseline.\nFurthermore, we achieve state-of-the-art recognition results on the\nSpoken-Squad question answering dataset.",
        "translated": ""
    },
    {
        "title": "A Systematic Review of Generative AI for Teaching and Learning Practice",
        "url": "http://arxiv.org/abs/2406.09520v1",
        "pub_date": "2024-06-13",
        "summary": "The use of generative artificial intelligence (GenAI) in academia is a\nsubjective and hotly debated topic. Currently, there are no agreed guidelines\ntowards the usage of GenAI systems in higher education (HE) and, thus, it is\nstill unclear how to make effective use of the technology for teaching and\nlearning practice. This paper provides an overview of the current state of\nresearch on GenAI for teaching and learning in HE. To this end, this study\nconducted a systematic review of relevant studies indexed by Scopus, using the\npreferred reporting items for systematic reviews and meta-analyses (PRISMA)\nguidelines. The search criteria revealed a total of 625 research papers, of\nwhich 355 met the final inclusion criteria. The findings from the review showed\nthe current state and the future trends in documents, citations, document\nsources/authors, keywords, and co-authorship. The research gaps identified\nsuggest that while some authors have looked at understanding the detection of\nAI-generated text, it may be beneficial to understand how GenAI can be\nincorporated into supporting the educational curriculum for assessments,\nteaching, and learning delivery. Furthermore, there is a need for additional\ninterdisciplinary, multidimensional studies in HE through collaboration. This\nwill strengthen the awareness and understanding of students, tutors, and other\nstakeholders, which will be instrumental in formulating guidelines, frameworks,\nand policies for GenAI usage.",
        "translated": ""
    },
    {
        "title": "DiffMM: Multi-Modal Diffusion Model for Recommendation",
        "url": "http://arxiv.org/abs/2406.11781v1",
        "pub_date": "2024-06-17",
        "summary": "The rise of online multi-modal sharing platforms like TikTok and YouTube has\nenabled personalized recommender systems to incorporate multiple modalities\n(such as visual, textual, and acoustic) into user representations. However,\naddressing the challenge of data sparsity in these systems remains a key issue.\nTo address this limitation, recent research has introduced self-supervised\nlearning techniques to enhance recommender systems. However, these methods\noften rely on simplistic random augmentation or intuitive cross-view\ninformation, which can introduce irrelevant noise and fail to accurately align\nthe multi-modal context with user-item interaction modeling. To fill this\nresearch gap, we propose a novel multi-modal graph diffusion model for\nrecommendation called DiffMM. Our framework integrates a modality-aware graph\ndiffusion model with a cross-modal contrastive learning paradigm to improve\nmodality-aware user representation learning. This integration facilitates\nbetter alignment between multi-modal feature information and collaborative\nrelation modeling. Our approach leverages diffusion models' generative\ncapabilities to automatically generate a user-item graph that is aware of\ndifferent modalities, facilitating the incorporation of useful multi-modal\nknowledge in modeling user-item interactions. We conduct extensive experiments\non three public datasets, consistently demonstrating the superiority of our\nDiffMM over various competitive baselines. For open-sourced model\nimplementation details, you can access the source codes of our proposed\nframework at: https://github.com/HKUDS/DiffMM .",
        "translated": ""
    },
    {
        "title": "Multi-Layer Ranking with Large Language Models for News Source\n  Recommendation",
        "url": "http://arxiv.org/abs/2406.11745v1",
        "pub_date": "2024-06-17",
        "summary": "To seek reliable information sources for news events, we introduce a novel\ntask of expert recommendation, which aims to identify trustworthy sources based\non their previously quoted statements. To achieve this, we built a novel\ndataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourced\nfrom a collection of news articles. We formulate the recommendation task as the\nretrieval of experts based on their likelihood of being associated with a given\nquery. We also propose a multi-layer ranking framework employing Large Language\nModels to improve the recommendation performance. Our results show that\nemploying an in-context learning based LLM ranker and a multi-layer\nranking-based filter significantly improve both the predictive quality and\nbehavioural quality of the recommender system.",
        "translated": ""
    },
    {
        "title": "Graph Neural Re-Ranking via Corpus Graph",
        "url": "http://arxiv.org/abs/2406.11720v1",
        "pub_date": "2024-06-17",
        "summary": "Re-ranking systems aim to reorder an initial list of documents to satisfy\nbetter the information needs associated with a user-provided query. Modern\nre-rankers predominantly rely on neural network models, which have proven\nhighly effective in representing samples from various modalities. However,\nthese models typically evaluate query-document pairs in isolation, neglecting\nthe underlying document distribution that could enhance the quality of the\nre-ranked list. To address this limitation, we propose Graph Neural Re-Ranking\n(GNRR), a pipeline based on Graph Neural Networks (GNNs), that enables each\nquery to consider documents distribution during inference. Our approach models\ndocument relationships through corpus subgraphs and encodes their\nrepresentations using GNNs. Through extensive experiments, we demonstrate that\nGNNs effectively capture cross-document interactions, improving performance on\npopular ranking metrics. In TREC-DL19, we observe a relative improvement of\n5.8% in Average Precision compared to our baseline. These findings suggest that\nintegrating the GNN segment offers significant advantages, especially in\nscenarios where understanding the broader context of documents is crucial.",
        "translated": ""
    },
    {
        "title": "Prompts as Auto-Optimized Training Hyperparameters: Training\n  Best-in-Class IR Models from Scratch with 10 Gold Labels",
        "url": "http://arxiv.org/abs/2406.11706v1",
        "pub_date": "2024-06-17",
        "summary": "We develop a method for training small-scale (under 100M parameter) neural\ninformation retrieval models with as few as 10 gold relevance labels. The\nmethod depends on generating synthetic queries for documents using a language\nmodel (LM), and the key step is that we automatically optimize the LM prompt\nthat is used to generate these queries based on training quality. In\nexperiments with the BIRCO benchmark, we find that models trained with our\nmethod outperform RankZephyr and are competitive with RankLLama, both of which\nare 7B parameter models trained on over 100K labels. These findings point to\nthe power of automatic prompt optimization for synthetic dataset generation.",
        "translated": ""
    },
    {
        "title": "TourRank: Utilizing Large Language Models for Documents Ranking with a\n  Tournament-Inspired Strategy",
        "url": "http://arxiv.org/abs/2406.11678v1",
        "pub_date": "2024-06-17",
        "summary": "Large Language Models (LLMs) are increasingly employed in zero-shot documents\nranking, yielding commendable results. However, several significant challenges\nstill persist in LLMs for ranking: (1) LLMs are constrained by limited input\nlength, precluding them from processing a large number of documents\nsimultaneously; (2) The output document sequence is influenced by the input\norder of documents, resulting in inconsistent ranking outcomes; (3) Achieving a\nbalance between cost and ranking performance is quite challenging. To tackle\nthese issues, we introduce a novel documents ranking method called TourRank,\nwhich is inspired by the tournament mechanism. This approach alleviates the\nimpact of LLM's limited input length through intelligent grouping, while the\ntournament-like points system ensures robust ranking, mitigating the influence\nof the document input sequence. We test TourRank with different LLMs on the\nTREC DL datasets and the BEIR benchmark. Experimental results show that\nTourRank achieves state-of-the-art performance at a reasonable cost.",
        "translated": ""
    },
    {
        "title": "Long Code Arena: a Set of Benchmarks for Long-Context Code Models",
        "url": "http://arxiv.org/abs/2406.11612v1",
        "pub_date": "2024-06-17",
        "summary": "Nowadays, the fields of code and natural language processing are evolving\nrapidly. In particular, models become better at processing long context windows\n- supported context sizes have increased by orders of magnitude over the last\nfew years. However, there is a shortage of benchmarks for code processing that\ngo beyond a single file of context, while the most popular ones are limited to\na single method. With this work, we aim to close this gap by introducing Long\nCode Arena, a suite of six benchmarks for code processing tasks that require\nproject-wide context. These tasks cover different aspects of code processing:\nlibrary-based code generation, CI builds repair, project-level code completion,\ncommit message generation, bug localization, and module summarization. For each\ntask, we provide a manually verified dataset for testing, an evaluation suite,\nand open-source baseline solutions based on popular LLMs to showcase the usage\nof the dataset and to simplify adoption by other researchers. We publish the\nbenchmark page on HuggingFace Spaces with the leaderboard, links to HuggingFace\nHub for all the datasets, and link to the GitHub repository with baselines:\nhttps://huggingface.co/spaces/JetBrains-Research/long-code-arena.",
        "translated": ""
    },
    {
        "title": "CoSQA+: Enhancing Code Search Dataset with Matching Code",
        "url": "http://arxiv.org/abs/2406.11589v1",
        "pub_date": "2024-06-17",
        "summary": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets are problematic: either using unrealistic\nqueries, or with mismatched codes, and typically using one-to-one query-code\npairing, which fails to reflect the reality that a query might have multiple\nvalid code matches. This paper introduces CoSQA+, pairing high-quality queries\n(reused from CoSQA) with multiple suitable codes. We collect code candidates\nfrom diverse sources and form candidate pairs by pairing queries with these\ncodes. Utilizing the power of large language models (LLMs), we automate pair\nannotation, filtering, and code generation for queries without suitable\nmatches. Through extensive experiments, CoSQA+ has demonstrated superior\nquality over CoSQA. Models trained on CoSQA+ exhibit improved performance.\nFurthermore, we propose a new metric Mean Multi-choice Reciprocal Rank (MMRR),\nto assess one-to-N code search performance. We provide the code and data at\nhttps://github.com/DeepSoftwareAnalytics/CoSQA_Plus.",
        "translated": ""
    },
    {
        "title": "Making Alice Appear Like Bob: A Probabilistic Preference Obfuscation\n  Method For Implicit Feedback Recommendation Models",
        "url": "http://arxiv.org/abs/2406.11505v1",
        "pub_date": "2024-06-17",
        "summary": "Users' interaction or preference data used in recommender systems carry the\nrisk of unintentionally revealing users' private attributes (e.g., gender or\nrace). This risk becomes particularly concerning when the training data\ncontains user preferences that can be used to infer these attributes,\nespecially if they align with common stereotypes. This major privacy issue\nallows malicious attackers or other third parties to infer users' protected\nattributes. Previous efforts to address this issue have added or removed parts\nof users' preferences prior to or during model training to improve privacy,\nwhich often leads to decreases in recommendation accuracy. In this work, we\nintroduce SBO, a novel probabilistic obfuscation method for user preference\ndata designed to improve the accuracy--privacy trade-off for such\nrecommendation scenarios. We apply SBO to three state-of-the-art recommendation\nmodels (i.e., BPR, MultVAE, and LightGCN) and two popular datasets (i.e.,\nMovieLens-1M and LFM-2B). Our experiments reveal that SBO outperforms\ncomparable approaches with respect to the accuracy--privacy trade-off.\nSpecifically, we can reduce the leakage of users' protected attributes while\nmaintaining on-par recommendation accuracy.",
        "translated": ""
    },
    {
        "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG\n  Systems: A Comparative Study of Performance and Scalability",
        "url": "http://arxiv.org/abs/2406.11424v1",
        "pub_date": "2024-06-17",
        "summary": "This paper presents an analysis of open-source large language models (LLMs)\nand their application in Retrieval-Augmented Generation (RAG) tasks, specific\nfor enterprise-specific data sets scraped from their websites. With the\nincreasing reliance on LLMs in natural language processing, it is crucial to\nevaluate their performance, accessibility, and integration within specific\norganizational contexts. This study examines various open-source LLMs, explores\ntheir integration into RAG frameworks using enterprise-specific data, and\nassesses the performance of different open-source embeddings in enhancing the\nretrieval and generation process. Our findings indicate that open-source LLMs,\ncombined with effective embedding techniques, can significantly improve the\naccuracy and efficiency of RAG systems, offering a viable alternative to\nproprietary solutions for enterprises.",
        "translated": ""
    },
    {
        "title": "They're All Doctors: Synthesizing Diverse Counterfactuals to Mitigate\n  Associative Bias",
        "url": "http://arxiv.org/abs/2406.11331v1",
        "pub_date": "2024-06-17",
        "summary": "Vision Language Models (VLMs) such as CLIP are powerful models; however they\ncan exhibit unwanted biases, making them less safe when deployed directly in\napplications such as text-to-image, text-to-video retrievals, reverse search,\nor classification tasks. In this work, we propose a novel framework to generate\nsynthetic counterfactual images to create a diverse and balanced dataset that\ncan be used to fine-tune CLIP. Given a set of diverse synthetic base images\nfrom text-to-image models, we leverage off-the-shelf segmentation and\ninpainting models to place humans with diverse visual appearances in context.\nWe show that CLIP trained on such datasets learns to disentangle the human\nappearance from the context of an image, i.e., what makes a doctor is not\ncorrelated to the person's visual appearance, like skin color or body type, but\nto the context, such as background, the attire they are wearing, or the objects\nthey are holding. We demonstrate that our fine-tuned CLIP model, $CF_\\alpha$,\nimproves key fairness metrics such as MaxSkew, MinSkew, and NDKL by 40-66\\% for\nimage retrieval tasks, while still achieving similar levels of performance in\ndownstream tasks. We show that, by design, our model retains maximal\ncompatibility with the original CLIP models, and can be easily controlled to\nsupport different accuracy versus fairness trade-offs in a plug-n-play fashion.",
        "translated": ""
    },
    {
        "title": "Influence Maximization via Graph Neural Bandits",
        "url": "http://arxiv.org/abs/2406.12835v1",
        "pub_date": "2024-06-18",
        "summary": "We consider a ubiquitous scenario in the study of Influence Maximization\n(IM), in which there is limited knowledge about the topology of the diffusion\nnetwork. We set the IM problem in a multi-round diffusion campaign, aiming to\nmaximize the number of distinct users that are influenced. Leveraging the\ncapability of bandit algorithms to effectively balance the objectives of\nexploration and exploitation, as well as the expressivity of neural networks,\nour study explores the application of neural bandit algorithms to the IM\nproblem. We propose the framework IM-GNB (Influence Maximization with Graph\nNeural Bandits), where we provide an estimate of the users' probabilities of\nbeing influenced by influencers (also known as diffusion seeds). This initial\nestimate forms the basis for constructing both an exploitation graph and an\nexploration one. Subsequently, IM-GNB handles the exploration-exploitation\ntradeoff, by selecting seed nodes in real-time using Graph Convolutional\nNetworks (GCN), in which the pre-estimated graphs are employed to refine the\ninfluencers' estimated rewards in each contextual setting. Through extensive\nexperiments on two large real-world datasets, we demonstrate the effectiveness\nof IM-GNB compared with other baseline methods, significantly improving the\nspread outcome of such diffusion campaigns, when the underlying network is\nunknown.",
        "translated": ""
    },
    {
        "title": "News Without Borders: Domain Adaptation of Multilingual Sentence\n  Embeddings for Cross-lingual News Recommendation",
        "url": "http://arxiv.org/abs/2406.12634v1",
        "pub_date": "2024-06-18",
        "summary": "Rapidly growing numbers of multilingual news consumers pose an increasing\nchallenge to news recommender systems in terms of providing customized\nrecommendations. First, existing neural news recommenders, even when powered by\nmultilingual language models (LMs), suffer substantial performance losses in\nzero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of\nfine-tuning the backbone LM of a neural recommender on task-specific data is\ncomputationally expensive and infeasible in few-shot recommendation and\ncold-start setups, where data is scarce or completely unavailable. In this\nwork, we propose a news-adapted sentence encoder (NaSE), domain-specialized\nfrom a pretrained massively multilingual sentence encoder (SE). To this end, we\nconstruct and leverage PolyNews and PolyNewsParallel, two multilingual\nnews-specific corpora. With the news-adapted multilingual SE in place, we test\nthe effectiveness of (i.e., question the need for) supervised fine-tuning for\nnews recommendation, and propose a simple and strong baseline based on (i)\nfrozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE\nachieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot\nnews recommendation.",
        "translated": ""
    },
    {
        "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
        "url": "http://arxiv.org/abs/2406.12593v1",
        "pub_date": "2024-06-18",
        "summary": "Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSIs need full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nrehearsal-free, prompt-based approach for instance-wise incremental learning in\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that PromptDSI matches IncDSI in managing\nforgetting while significantly enhancing recall by over 4% on new corpora.",
        "translated": ""
    },
    {
        "title": "Behavior-Dependent Linear Recurrent Units for Efficient Sequential\n  Recommendation",
        "url": "http://arxiv.org/abs/2406.12580v1",
        "pub_date": "2024-06-18",
        "summary": "Sequential recommender systems aims to predict the users' next interaction\nthrough user behavior modeling with various operators like RNNs and attentions.\nHowever, existing models generally fail to achieve the three golden principles\nfor sequential recommendation simultaneously, i.e., training efficiency,\nlow-cost inference, and strong performance. To this end, we propose RecBLR, an\nEfficient Sequential Recommendation Model based on Behavior-Dependent Linear\nRecurrent Units to accomplish the impossible triangle of the three principles.\nBy incorporating gating mechanisms and behavior-dependent designs into linear\nrecurrent units, our model significantly enhances user behavior modeling and\nrecommendation performance. Furthermore, we unlock the parallelizable training\nas well as inference efficiency for our model by designing a hardware-aware\nscanning acceleration algorithm with a customized CUDA kernel. Extensive\nexperiments on real-world datasets with varying lengths of user behavior\nsequences demonstrate RecBLR's remarkable effectiveness in simultaneously\nachieving all three golden principles - strong recommendation performance,\ntraining efficiency, and low-cost inference, while exhibiting excellent\nscalability to datasets with long user interaction histories.",
        "translated": ""
    },
    {
        "title": "Predicting Award Winning Research Papers at Publication Time",
        "url": "http://arxiv.org/abs/2406.12535v1",
        "pub_date": "2024-06-18",
        "summary": "In recent years, many studies have been focusing on predicting the scientific\nimpact of research papers. Most of these predictions are based on citations\ncount or rely on features obtainable only from already published papers. In\nthis study, we predict the likelihood for a research paper of winning an award\nonly relying on information available at publication time. For each paper, we\nbuild the citation subgraph induced from its bibliography. We initially\nconsider some features of this subgraph, such as the density and the global\nclustering coefficient, to make our prediction. Then, we mix this information\nwith textual features, extracted from the abstract and the title, to obtain a\nmore accurate final prediction. We made our experiments considering the\nArnetMiner citation graph, while the ground truth on award-winning papers has\nbeen obtained from a collection of best paper awards from 32 computer science\nconferences. In our experiment, we obtained an encouraging F1 score of 0.694.\nRemarkably, The high recall and the low false negatives rate, show how the\nmodel performs very well at identifying papers that will not win an award. This\nbehavior can help researchers in getting a first evaluation of their work at\npublication time. Lastly, we made some first experiments on interpretability.\nOur results highlight some interesting patterns both in topological and textual\nfeatures.",
        "translated": ""
    },
    {
        "title": "LLM4MSR: An LLM-Enhanced Paradigm for Multi-Scenario Recommendation",
        "url": "http://arxiv.org/abs/2406.12529v1",
        "pub_date": "2024-06-18",
        "summary": "As the demand for more personalized recommendation grows and a dramatic boom\nin commercial scenarios arises, the study on multi-scenario recommendation\n(MSR) has attracted much attention, which uses the data from all scenarios to\nsimultaneously improve their recommendation performance. However, existing\nmethods tend to integrate insufficient scenario knowledge and neglect learning\npersonalized cross-scenario preferences, thus leading to suboptimal performance\nand inadequate interpretability. Meanwhile, though large language model (LLM)\nhas shown great capability of reasoning and capturing semantic information, the\nhigh inference latency and high computation cost of tuning hinder its\nimplementation in industrial recommender systems. To fill these gaps, we\npropose an effective efficient interpretable LLM-enhanced paradigm LLM4MSR in\nthis work. Specifically, we first leverage LLM to uncover multi-level knowledge\nincluding scenario correlations and users' cross-scenario interests from the\ndesigned scenario- and user-level prompt without fine-tuning the LLM, then\nadopt hierarchical meta networks to generate multi-level meta layers to\nexplicitly improves the scenario-aware and personalized recommendation\ncapability. Our experiments on KuaiSAR-small, KuaiSAR, and Amazon datasets\nvalidate two significant advantages of LLM4MSR: (i) the effectiveness and\ncompatibility with different multi-scenario backbone models (achieving 1.5%,\n1%, and 40% AUC improvement on three datasets), (ii) high efficiency and\ndeployability on industrial recommender systems, and (iii) improved\ninterpretability. The implemented code and data is available to ease\nreproduction.",
        "translated": ""
    },
    {
        "title": "Improving Multi-modal Recommender Systems by Denoising and Aligning\n  Multi-modal Content and User Feedback",
        "url": "http://arxiv.org/abs/2406.12501v1",
        "pub_date": "2024-06-18",
        "summary": "Multi-modal recommender systems (MRSs) are pivotal in diverse online web\nplatforms and have garnered considerable attention in recent years. However,\nprevious studies overlook the challenges of (1) noisy multi-modal content, (2)\nnoisy user feedback, and (3) aligning multi-modal content with user feedback.\nIn order to tackle these challenges, we propose Denoising and Aligning\nMulti-modal Recommender System (DA-MRS). To mitigate multi-modal noise, DA-MRS\nfirst constructs item-item graphs determined by consistent content similarity\nacross modalities. To denoise user feedback, DA-MRS associates the probability\nof observed feedback with multi-modal content and devises a denoised BPR loss.\nFurthermore, DA-MRS implements Alignment guided by User preference to enhance\ntask-specific item representation and Alignment guided by graded Item relations\nto provide finer-grained alignment. Extensive experiments verify that DA-MRS is\na plug-and-play framework and achieves significant and consistent improvements\nacross various datasets, backbone models, and noisy scenarios.",
        "translated": ""
    },
    {
        "title": "RIGL: A Unified Reciprocal Approach for Tracing the Independent and\n  Group Learning Processes",
        "url": "http://arxiv.org/abs/2406.12465v1",
        "pub_date": "2024-06-18",
        "summary": "In the realm of education, both independent learning and group learning are\nesteemed as the most classic paradigms. The former allows learners to\nself-direct their studies, while the latter is typically characterized by\nteacher-directed scenarios. Recent studies in the field of intelligent\neducation have leveraged deep temporal models to trace the learning process,\ncapturing the dynamics of students' knowledge states, and have achieved\nremarkable performance. However, existing approaches have primarily focused on\nmodeling the independent learning process, with the group learning paradigm\nreceiving less attention. Moreover, the reciprocal effect between the two\nlearning processes, especially their combined potential to foster holistic\nstudent development, remains inadequately explored. To this end, in this paper,\nwe propose RIGL, a unified Reciprocal model to trace knowledge states at both\nthe individual and group levels, drawing from the Independent and Group\nLearning processes. Specifically, we first introduce a time frame-aware\nreciprocal embedding module to concurrently model both student and group\nresponse interactions across various time frames. Subsequently, we employ\nreciprocal enhanced learning modeling to fully exploit the comprehensive and\ncomplementary information between the two behaviors. Furthermore, we design a\nrelation-guided temporal attentive network, comprised of dynamic graph modeling\ncoupled with a temporal self-attention mechanism. It is used to delve into the\ndynamic influence of individual and group interactions throughout the learning\nprocesses. Conclusively, we introduce a bias-aware contrastive learning module\nto bolster the stability of the model's training. Extensive experiments on four\nreal-world educational datasets clearly demonstrate the effectiveness of the\nproposed RIGL model.",
        "translated": ""
    },
    {
        "title": "LLM-enhanced Reranking in Recommender Systems",
        "url": "http://arxiv.org/abs/2406.12433v1",
        "pub_date": "2024-06-18",
        "summary": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria. The code for this implementation is publicly\navailable.",
        "translated": ""
    },
    {
        "title": "A Gradient Accumulation Method for Dense Retriever under Memory\n  Constraint",
        "url": "http://arxiv.org/abs/2406.12356v1",
        "pub_date": "2024-06-18",
        "summary": "InfoNCE loss is commonly used to train dense retriever in information\nretrieval tasks. It is well known that a large batch is essential to stable and\neffective training with InfoNCE loss, which requires significant hardware\nresources. Due to the dependency of large batch, dense retriever has bottleneck\nof application and research. Recently, memory reduction methods have been\nbroadly adopted to resolve the hardware bottleneck by decomposing forward and\nbackward or using a memory bank. However, current methods still suffer from\nslow and unstable training. To address these issues, we propose \\longmodelname\\\n(\\modelname), a stable and efficient memory reduction method for dense\nretriever trains that uses a dual memory bank structure to leverage previously\ngenerated query and passage representations. Experiments on widely used five\ninformation retrieval datasets indicate that \\modelname\\ can surpass not only\nexisting memory reduction methods but also high-resource scenario. Moreover,\ntheoretical analysis and experimental results confirm that \\modelname\\ provides\nmore stable dual-encoder training than current memory bank utilization methods.",
        "translated": ""
    },
    {
        "title": "LARP: Language Audio Relational Pre-training for Cold-Start Playlist\n  Continuation",
        "url": "http://arxiv.org/abs/2406.14333v1",
        "pub_date": "2024-06-20",
        "summary": "As online music consumption increasingly shifts towards playlist-based\nlistening, the task of playlist continuation, in which an algorithm suggests\nsongs to extend a playlist in a personalized and musically cohesive manner, has\nbecome vital to the success of music streaming. Currently, many existing\nplaylist continuation approaches rely on collaborative filtering methods to\nperform recommendation. However, such methods will struggle to recommend songs\nthat lack interaction data, an issue known as the cold-start problem. Current\napproaches to this challenge design complex mechanisms for extracting\nrelational signals from sparse collaborative data and integrating them into\ncontent representations. However, these approaches leave content representation\nlearning out of scope and utilize frozen, pre-trained content models that may\nnot be aligned with the distribution or format of a specific musical setting.\nFurthermore, even the musical state-of-the-art content modules are either (1)\nincompatible with the cold-start setting or (2) unable to effectively integrate\ncross-modal and relational signals. In this paper, we introduce LARP, a\nmulti-modal cold-start playlist continuation model, to effectively overcome\nthese limitations. LARP is a three-stage contrastive learning framework that\nintegrates both multi-modal and relational signals into its learned\nrepresentations. Our framework uses increasing stages of task-specific\nabstraction: within-track (language-audio) contrastive loss, track-track\ncontrastive loss, and track-playlist contrastive loss. Experimental results on\ntwo publicly available datasets demonstrate the efficacy of LARP over uni-modal\nand multi-modal models for playlist continuation in a cold-start setting. Code\nand dataset are released at: https://github.com/Rsalganik1123/LARP.",
        "translated": ""
    },
    {
        "title": "Reproducibility in Machine Learning-based Research: Overview, Barriers\n  and Drivers",
        "url": "http://arxiv.org/abs/2406.14325v1",
        "pub_date": "2024-06-20",
        "summary": "Research in various fields is currently experiencing challenges regarding the\nreproducibility of results. This problem is also prevalent in machine learning\n(ML) research. The issue arises primarily due to unpublished data and/or source\ncode and the sensitivity of ML training conditions. Although different\nsolutions have been proposed to address this issue, such as using ML platforms,\nthe level of reproducibility in ML-driven research remains unsatisfactory.\nTherefore, in this article, we discuss the reproducibility of ML-driven\nresearch with three main aims: (i) identify the barriers to reproducibility\nwhen applying ML in research as well as categorize the barriers to different\ntypes of reproducibility (description, code, data, and experiment\nreproducibility), (ii) identify potential drivers such as tools, practices, and\ninterventions that support ML reproducibility as well as distinguish between\ntechnology-driven drivers, procedural drivers, and drivers related to awareness\nand education, and (iii) map the drivers to the barriers. With this work, we\nhope to provide insights and contribute to the decision-making process\nregarding the adoption of different solutions to support ML reproducibility.",
        "translated": ""
    },
    {
        "title": "Optimizing Novelty of Top-k Recommendations using Large Language Models\n  and Reinforcement Learning",
        "url": "http://arxiv.org/abs/2406.14169v1",
        "pub_date": "2024-06-20",
        "summary": "Given an input query, a recommendation model is trained using user feedback\ndata (e.g., click data) to output a ranked list of items. In real-world\nsystems, besides accuracy, an important consideration for a new model is\nnovelty of its top-k recommendations w.r.t. an existing deployed model.\nHowever, novelty of top-k items is a difficult goal to optimize a model for,\nsince it involves a non-differentiable sorting operation on the model's\npredictions. Moreover, novel items, by definition, do not have any user\nfeedback data. Given the semantic capabilities of large language models, we\naddress these problems using a reinforcement learning (RL) formulation where\nlarge language models provide feedback for the novel items. However, given\nmillions of candidate items, the sample complexity of a standard RL algorithm\ncan be prohibitively high. To reduce sample complexity, we reduce the top-k\nlist reward to a set of item-wise rewards and reformulate the state space to\nconsist of &lt;query, item&gt; tuples such that the action space is reduced to a\nbinary decision; and show that this reformulation results in a significantly\nlower complexity when the number of items is large. We evaluate the proposed\nalgorithm on improving novelty for a query-ad recommendation task on a\nlarge-scale search engine. Compared to supervised finetuning on recent &lt;query,\nad&gt; pairs, the proposed RL-based algorithm leads to significant novelty gains\nwith minimal loss in recall. We obtain similar results on the ORCAS\nquery-webpage matching dataset and a product recommendation dataset based on\nAmazon reviews.",
        "translated": ""
    },
    {
        "title": "DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in\n  Retrieval Augmented Generation",
        "url": "http://arxiv.org/abs/2406.14162v1",
        "pub_date": "2024-06-20",
        "summary": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information or excessively include irrelevant information? To allay\nthese concerns, it is necessary to annotate domain-specific benchmarks to\nevaluate information retrieval (IR) performance, as relevance definitions vary\nacross queries and domains. Furthermore, such benchmarks should be\ncost-efficiently annotated to avoid annotation selection bias. In this paper,\nwe propose DIRAS (Domain-specific Information Retrieval Annotation with\nScalability), a manual-annotation-free schema that fine-tunes open-sourced LLMs\nto annotate relevance labels with calibrated relevance probabilities. Extensive\nevaluation shows that DIRAS fine-tuned models achieve GPT-4-level performance\non annotating and ranking unseen (query, document) pairs, and is helpful for\nreal-world RAG development.",
        "translated": ""
    },
    {
        "title": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers",
        "url": "http://arxiv.org/abs/2406.14117v1",
        "pub_date": "2024-06-20",
        "summary": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered.",
        "translated": ""
    },
    {
        "title": "Taxonomy-Guided Zero-Shot Recommendations with LLMs",
        "url": "http://arxiv.org/abs/2406.14043v1",
        "pub_date": "2024-06-20",
        "summary": "With the emergence of large language models (LLMs) and their ability to\nperform a variety of tasks, their application in recommender systems (RecSys)\nhas shown promise. However, we are facing significant challenges when deploying\nLLMs into RecSys, such as limited prompt length, unstructured item information,\nand un-constrained generation of recommendations, leading to sub-optimal\nperformance. To address these issues, we propose a novel method using a\ntaxonomy dictionary. This method provides a systematic framework for\ncategorizing and organizing items, improving the clarity and structure of item\ninformation. By incorporating the taxonomy dictionary into LLM prompts, we\nachieve efficient token utilization and controlled feature generation, leading\nto more accurate and contextually relevant recommendations. Our Taxonomy-guided\nRecommendation (TaxRec) approach features a two-step process: one-time taxonomy\ncategorization and LLM-based recommendation, enabling zero-shot recommendations\nwithout the need for domain-specific fine-tuning. Experimental results\ndemonstrate TaxRec significantly enhances recommendation quality compared to\ntraditional zero-shot approaches, showcasing its efficacy as personal\nrecommender with LLMs. Code is available at\nhttps://github.com/yueqingliang1/TaxRec.",
        "translated": ""
    },
    {
        "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic\n  Collaboration",
        "url": "http://arxiv.org/abs/2406.14017v1",
        "pub_date": "2024-06-20",
        "summary": "Generative retrieval has recently emerged as a promising approach to\nsequential recommendation, framing candidate item retrieval as an\nautoregressive sequence generation problem. However, existing generative\nmethods typically focus solely on either behavioral or semantic aspects of item\ninformation, neglecting their complementary nature and thus resulting in\nlimited effectiveness. To address this limitation, we introduce EAGER, a novel\ngenerative recommendation framework that seamlessly integrates both behavioral\nand semantic information. Specifically, we identify three key challenges in\ncombining these two types of information: a unified generative architecture\ncapable of handling two feature types, ensuring sufficient and independent\nlearning for each type, and fostering subtle interactions that enhance\ncollaborative information utilization. To achieve these goals, we propose (1) a\ntwo-stream generation architecture leveraging a shared encoder and two separate\ndecoders to decode behavior tokens and semantic tokens with a confidence-based\nranking strategy; (2) a global contrastive task with summary tokens to achieve\ndiscriminative decoding for each type of information; and (3) a semantic-guided\ntransfer task designed to implicitly promote cross-interactions through\nreconstruction and estimation objectives. We validate the effectiveness of\nEAGER on four public benchmarks, demonstrating its superior performance\ncompared to existing methods.",
        "translated": ""
    },
    {
        "title": "Do Not Wait: Learning Re-Ranking Model Without User Feedback At Serving\n  Time in E-Commerce",
        "url": "http://arxiv.org/abs/2406.14004v1",
        "pub_date": "2024-06-20",
        "summary": "Recommender systems have been widely used in e-commerce, and re-ranking\nmodels are playing an increasingly significant role in the domain, which\nleverages the inter-item influence and determines the final recommendation\nlists. Online learning methods keep updating a deployed model with the latest\navailable samples to capture the shifting of the underlying data distribution\nin e-commerce. However, they depend on the availability of real user feedback,\nwhich may be delayed by hours or even days, such as item purchases, leading to\na lag in model enhancement. In this paper, we propose a novel extension of\nonline learning methods for re-ranking modeling, which we term LAST, an acronym\nfor Learning At Serving Time. It circumvents the requirement of user feedback\nby using a surrogate model to provide the instructional signal needed to steer\nmodel improvement. Upon receiving an online request, LAST finds and applies a\nmodel modification on the fly before generating a recommendation result for the\nrequest. The modification is request-specific and transient. It means the\nmodification is tailored to and only to the current request to capture the\nspecific context of the request. After a request, the modification is\ndiscarded, which helps to prevent error propagation and stabilizes the online\nlearning procedure since the predictions of the surrogate model may be\ninaccurate. Most importantly, as a complement to feedback-based online learning\nmethods, LAST can be seamlessly integrated into existing online learning\nsystems to create a more adaptive and responsive recommendation experience.\nComprehensive experiments, both offline and online, affirm that LAST\noutperforms state-of-the-art re-ranking models.",
        "translated": ""
    },
    {
        "title": "Unifying Graph Convolution and Contrastive Learning in Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2406.13996v1",
        "pub_date": "2024-06-20",
        "summary": "Graph-based models and contrastive learning have emerged as prominent methods\nin Collaborative Filtering (CF). While many existing models in CF incorporate\nthese methods in their design, there seems to be a limited depth of analysis\nregarding the foundational principles behind them. This paper bridges graph\nconvolution, a pivotal element of graph-based models, with contrastive learning\nthrough a theoretical framework. By examining the learning dynamics and\nequilibrium of the contrastive loss, we offer a fresh lens to understand\ncontrastive learning via graph theory, emphasizing its capability to capture\nhigh-order connectivity. Building on this analysis, we further show that the\ngraph convolutional layers often used in graph-based models are not essential\nfor high-order connectivity modeling and might contribute to the risk of\noversmoothing. Stemming from our findings, we introduce Simple Contrastive\nCollaborative Filtering (SCCF), a simple and effective algorithm based on a\nnaive embedding model and a modified contrastive loss. The efficacy of the\nalgorithm is demonstrated through extensive experiments across four public\ndatasets. The experiment code is available at\n\\url{https://github.com/wu1hong/SCCF}. \\end{abstract}",
        "translated": ""
    },
    {
        "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
        "url": "http://arxiv.org/abs/2406.13941v1",
        "pub_date": "2024-06-20",
        "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
        "translated": ""
    },
    {
        "title": "STARD: A Chinese Statute Retrieval Dataset with Real Queries Issued by\n  Non-professionals",
        "url": "http://arxiv.org/abs/2406.15313v1",
        "pub_date": "2024-06-21",
        "summary": "Statute retrieval aims to find relevant statutory articles for specific\nqueries. This process is the basis of a wide range of legal applications such\nas legal advice, automated judicial decisions, legal document drafting, etc.\nExisting statute retrieval benchmarks focus on formal and professional queries\nfrom sources like bar exams and legal case documents, thereby neglecting\nnon-professional queries from the general public, which often lack precise\nlegal terminology and references. To address this gap, we introduce the STAtute\nRetrieval Dataset (STARD), a Chinese dataset comprising 1,543 query cases\ncollected from real-world legal consultations and 55,348 candidate statutory\narticles. Unlike existing statute retrieval datasets, which primarily focus on\nprofessional legal queries, STARD captures the complexity and diversity of real\nqueries from the general public. Through a comprehensive evaluation of various\nretrieval baselines, we reveal that existing retrieval approaches all fall\nshort of these real queries issued by non-professional users. The best method\nonly achieves a Recall@100 of 0.907, suggesting the necessity for further\nexploration and additional research in this area.\n  All the codes and datasets are available at:\nhttps://github.com/oneal2000/STARD/tree/main",
        "translated": ""
    },
    {
        "title": "Towards Fine-Grained Citation Evaluation in Generated Text: A\n  Comparative Analysis of Faithfulness Metrics",
        "url": "http://arxiv.org/abs/2406.15264v1",
        "pub_date": "2024-06-21",
        "summary": "Large language models (LLMs) often produce unsupported or unverifiable\ninformation, known as \"hallucinations.\" To mitigate this, retrieval-augmented\nLLMs incorporate citations, grounding the content in verifiable sources.\nDespite such developments, manually assessing how well a citation supports the\nassociated statement remains a major challenge. Previous studies use\nfaithfulness metrics to estimate citation support automatically but are limited\nto binary classification, overlooking fine-grained citation support in\npractical scenarios. To investigate the effectiveness of faithfulness metrics\nin fine-grained scenarios, we propose a comparative evaluation framework that\nassesses the metric effectiveness in distinguishinging citations between\nthree-category support levels: full, partial, and no support. Our framework\nemploys correlation analysis, classification evaluation, and retrieval\nevaluation to measure the alignment between metric scores and human judgments\ncomprehensively. Our results show no single metric consistently excels across\nall evaluations, revealing the complexity of assessing fine-grained support.\nBased on the findings, we provide practical recommendations for developing more\neffective metrics.",
        "translated": ""
    },
    {
        "title": "Retrieval Augmented Zero-Shot Text Classification",
        "url": "http://arxiv.org/abs/2406.15241v1",
        "pub_date": "2024-06-21",
        "summary": "Zero-shot text learning enables text classifiers to handle unseen classes\nefficiently, alleviating the need for task-specific training data. A simple\napproach often relies on comparing embeddings of query (text) to those of\npotential classes. However, the embeddings of a simple query sometimes lack\nrich contextual information, which hinders the classification performance.\nTraditionally, this has been addressed by improving the embedding model with\nexpensive training. We introduce QZero, a novel training-free knowledge\naugmentation approach that reformulates queries by retrieving supporting\ncategories from Wikipedia to improve zero-shot text classification performance.\nOur experiments across six diverse datasets demonstrate that QZero enhances\nperformance for state-of-the-art static and contextual embedding models without\nthe need for retraining. Notably, in News and medical topic classification\ntasks, QZero improves the performance of even the largest OpenAI embedding\nmodel by at least 5% and 3%, respectively. Acting as a knowledge amplifier,\nQZero enables small word embedding models to achieve performance levels\ncomparable to those of larger contextual models, offering the potential for\nsignificant computational savings. Additionally, QZero offers meaningful\ninsights that illuminate query context and verify topic relevance, aiding in\nunderstanding model predictions. Overall, QZero improves embedding-based\nzero-shot classifiers while maintaining their simplicity. This makes it\nparticularly valuable for resource-constrained environments and domains with\nconstantly evolving information.",
        "translated": ""
    },
    {
        "title": "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\n  Document Analysis",
        "url": "http://arxiv.org/abs/2406.15187v1",
        "pub_date": "2024-06-21",
        "summary": "The use of Retrieval-Augmented Generation (RAG) has improved Large Language\nModels (LLMs) in collaborating with external data, yet significant challenges\nexist in real-world scenarios. In areas such as academic literature and finance\nquestion answering, data are often found in raw text and tables in HTML or PDF\nformats, which can be lengthy and highly unstructured. In this paper, we\nintroduce a benchmark suite, namely Unstructured Document Analysis (UDA), that\ninvolves 2,965 real-world documents and 29,590 expert-annotated Q&amp;A pairs. We\nrevisit popular LLM- and RAG-based solutions for document analysis and evaluate\nthe design choices and answer qualities across multiple document domains and\ndiverse query types. Our evaluation yields interesting findings and highlights\nthe importance of data parsing and retrieval. We hope our benchmark can shed\nlight and better serve real-world document analysis applications. The benchmark\nsuite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",
        "translated": ""
    },
    {
        "title": "Évaluation des capacités de réponse de larges modèles de langage\n  (LLM) pour des questions d'historiens",
        "url": "http://arxiv.org/abs/2406.15173v1",
        "pub_date": "2024-06-21",
        "summary": "Large Language Models (LLMs) like ChatGPT or Bard have revolutionized\ninformation retrieval and captivated the audience with their ability to\ngenerate custom responses in record time, regardless of the topic. In this\narticle, we assess the capabilities of various LLMs in producing reliable,\ncomprehensive, and sufficiently relevant responses about historical facts in\nFrench. To achieve this, we constructed a testbed comprising numerous\nhistory-related questions of varying types, themes, and levels of difficulty.\nOur evaluation of responses from ten selected LLMs reveals numerous\nshortcomings in both substance and form. Beyond an overall insufficient\naccuracy rate, we highlight uneven treatment of the French language, as well as\nissues related to verbosity and inconsistency in the responses provided by\nLLMs.",
        "translated": ""
    },
    {
        "title": "Hierarchical thematic classification of major conference proceedings",
        "url": "http://arxiv.org/abs/2406.14983v1",
        "pub_date": "2024-06-21",
        "summary": "In this paper, we develop a decision support system for the hierarchical text\nclassification. We consider text collections with a fixed hierarchical\nstructure of topics given by experts in the form of a tree. The system sorts\nthe topics by relevance to a given document. The experts choose one of the most\nrelevant topics to finish the classification. We propose a weighted\nhierarchical similarity function to calculate topic relevance. The function\ncalculates the similarity of a document and a tree branch. The weights in this\nfunction determine word importance. We use the entropy of words to estimate the\nweights.\n  The proposed hierarchical similarity function formulates a joint hierarchical\nthematic classification probability model of the document topics, parameters,\nand hyperparameters. The variational Bayesian inference gives a closed-form EM\nalgorithm. The EM algorithm estimates the parameters and calculates the\nprobability of a topic for a given document. Compared to hierarchical\nmulticlass SVM, hierarchical PLSA with adaptive regularization, and\nhierarchical naive Bayes, the weighted hierarchical similarity function has\nbetter improvement in ranking accuracy in an abstract collection of a major\nconference EURO and a website collection of industrial companies.",
        "translated": ""
    },
    {
        "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
        "url": "http://arxiv.org/abs/2406.14972v1",
        "pub_date": "2024-06-21",
        "summary": "Retrieval Augmented Generation (RAG) represents a significant advancement in\nartificial intelligence combining a retrieval phase with a generative phase,\nwith the latter typically being powered by large language models (LLMs). The\ncurrent common practices in RAG involve using \"instructed\" LLMs, which are\nfine-tuned with supervised training to enhance their ability to follow\ninstructions and are aligned with human preferences using state-of-the-art\ntechniques. Contrary to popular belief, our study demonstrates that base models\noutperform their instructed counterparts in RAG tasks by 20% on average under\nour experimental settings. This finding challenges the prevailing assumptions\nabout the superiority of instructed LLMs in RAG applications. Further\ninvestigations reveal a more nuanced situation, questioning fundamental aspects\nof RAG and suggesting the need for broader discussions on the topic; or, as\nFromm would have it, \"Seldom is a glance at the statistics enough to understand\nthe meaning of the figures\".",
        "translated": ""
    },
    {
        "title": "IDentity with Locality: An ideal hash for gene sequence search",
        "url": "http://arxiv.org/abs/2406.14901v1",
        "pub_date": "2024-06-21",
        "summary": "Gene sequence search is a fundamental operation in computational genomics.\nDue to the petabyte scale of genome archives, most gene search systems now use\nhashing-based data structures such as Bloom Filters (BF). The state-of-the-art\nsystems such as Compact bit-slicing signature index (COBS) and Repeated And\nMerged Bloom filters (RAMBO) use BF with Random Hash (RH) functions for gene\nrepresentation and identification. The standard recipe is to cast the gene\nsearch problem as a sequence of membership problems testing if each subsequent\ngene substring (called kmer) of Q is present in the set of kmers of the entire\ngene database D. We observe that RH functions, which are crucial to the memory\nand the computational advantage of BF, are also detrimental to the system\nperformance of gene-search systems. While subsequent kmers being queried are\nlikely very similar, RH, oblivious to any similarity, uniformly distributes the\nkmers to different parts of potentially large BF, thus triggering excessive\ncache misses and causing system slowdown. We propose a novel hash function\ncalled the Identity with Locality (IDL) hash family, which co-locates the keys\nclose in input space without causing collisions. This approach ensures both\ncache locality and key preservation. IDL functions can be a drop-in replacement\nfor RH functions and help improve the performance of information retrieval\nsystems. We give a simple but practical construction of IDL function families\nand show that replacing the RH with IDL functions reduces cache misses by a\nfactor of 5x, thus improving query and indexing times of SOTA methods such as\nCOBS and RAMBO by factors up to 2x without compromising their quality. We also\nprovide a theoretical analysis of the false positive rate of BF with IDL\nfunctions. Our hash function is the first study that bridges Locality Sensitive\nHash (LSH) and RH to obtain cache efficiency.",
        "translated": ""
    },
    {
        "title": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue\n  for LLM-based Recommendation",
        "url": "http://arxiv.org/abs/2406.14900v1",
        "pub_date": "2024-06-21",
        "summary": "Adapting Large Language Models (LLMs) for recommendation requires careful\nconsideration of the decoding process, given the inherent differences between\ngenerating items and natural language. Existing approaches often directly apply\nLLMs' original decoding methods. However, we find these methods encounter\nsignificant challenges: 1) amplification bias -- where standard length\nnormalization inflates scores for items containing tokens with generation\nprobabilities close to 1 (termed ghost tokens), and 2) homogeneity issue --\ngenerating multiple similar or repetitive items for a user. To tackle these\nchallenges, we introduce a new decoding approach named Debiasing-Diversifying\nDecoding (D3). D3 disables length normalization for ghost tokens to alleviate\namplification bias, and it incorporates a text-free assistant model to\nencourage tokens less frequently generated by LLMs for counteracting\nrecommendation homogeneity. Extensive experiments on real-world datasets\ndemonstrate the method's effectiveness in enhancing accuracy and diversity.",
        "translated": ""
    },
    {
        "title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of\n  Large Language Models in Lexical Entailment Recognition",
        "url": "http://arxiv.org/abs/2406.14894v1",
        "pub_date": "2024-06-21",
        "summary": "Verbs form the backbone of language, providing the structure and meaning to\nsentences. Yet, their intricate semantic nuances pose a longstanding challenge.\nUnderstanding verb relations through the concept of lexical entailment is\ncrucial for comprehending sentence meanings and grasping verb dynamics. This\nwork investigates the capabilities of eight Large Language Models in\nrecognizing lexical entailment relations among verbs through differently\ndevised prompting strategies and zero-/few-shot settings over verb pairs from\ntwo lexical databases, namely WordNet and HyperLex. Our findings unveil that\nthe models can tackle the lexical entailment recognition task with moderately\ngood performance, although at varying degree of effectiveness and under\ndifferent conditions. Also, utilizing few-shot prompting can enhance the\nmodels' performance. However, perfectly solving the task arises as an unmet\nchallenge for all examined LLMs, which raises an emergence for further research\ndevelopments on this topic.",
        "translated": ""
    },
    {
        "title": "Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024\n  Retrieval-Augmented Generation Track",
        "url": "http://arxiv.org/abs/2406.16828v1",
        "pub_date": "2024-06-24",
        "summary": "Did you try out the new Bing Search? Or maybe you fiddled around with Google\nAI~Overviews? These might sound familiar because the modern-day search stack\nhas recently evolved to include retrieval-augmented generation (RAG) systems.\nThey allow searching and incorporating real-time data into large language\nmodels (LLMs) to provide a well-informed, attributed, concise summary in\ncontrast to the traditional search paradigm that relies on displaying a ranked\nlist of documents. Therefore, given these recent advancements, it is crucial to\nhave an arena to build, test, visualize, and systematically evaluate RAG-based\nsearch systems. With this in mind, we propose the TREC 2024 RAG Track to foster\ninnovation in evaluating RAG systems. In our work, we lay out the steps we've\nmade towards making this track a reality -- we describe the details of our\nreusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1\ncollection choice, release the development topics for the track, and\nstandardize the I/O definitions which assist the end user. Next, using\nRagnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's\nGPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface\nfor an interactive arena allowing benchmarking pairwise RAG systems by\ncrowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve\na unified standard for future RAG systems.",
        "translated": ""
    },
    {
        "title": "Meta-experiments: Improving experimentation through experimentation",
        "url": "http://arxiv.org/abs/2406.16629v1",
        "pub_date": "2024-06-24",
        "summary": "A/B testing is widexly used in the industry to optimize customer facing\nwebsites. Many companies employ experimentation specialists to facilitate and\nimprove the process of A/B testing. Here, we present the application of A/B\ntesting to this improvement effort itself, by running experiments on the\nexperimentation process, which we call 'meta-experiments'. We discuss the\nchallenges of this approach using the example of one of our meta-experiments,\nwhich helped experimenters to run more sufficiently powered A/B tests. We also\npoint out the benefits of 'dog fooding' for the experimentation specialists\nwhen running their own experiments.",
        "translated": ""
    },
    {
        "title": "Star+: A New Multi-Domain Model for CTR Prediction",
        "url": "http://arxiv.org/abs/2406.16568v1",
        "pub_date": "2024-06-24",
        "summary": "In this paper, we introduce Star+, a novel multi-domain model for\nclick-through rate (CTR) prediction inspired by the Star model. Traditional\nsingle-domain approaches and existing multi-task learning techniques face\nchallenges in multi-domain environments due to their inability to capture\ndomain-specific data distributions and complex inter-domain relationships.\nStar+ addresses these limitations by enhancing the interaction between shared\nand domain-specific information through various fusion strategies, such as add,\nadaptive add, concatenation, and gating fusions, to find the optimal balance\nbetween domain-specific and shared information. We also investigate the impact\nof different normalization techniques, including layer normalization, batch\nnormalization, and partition normalization, on the performance of our model.\nOur extensive experiments on both industrial and public datasets demonstrate\nthat Star+ significantly improves prediction accuracy and efficiency. This work\ncontributes to the advancement of recommendation systems by providing a robust,\nscalable, and adaptive solution for multi-domain environments.",
        "translated": ""
    },
    {
        "title": "Cross-domain Transfer of Valence Preferences via a Meta-optimization\n  Approach",
        "url": "http://arxiv.org/abs/2406.16494v1",
        "pub_date": "2024-06-24",
        "summary": "Cross-domain recommendation offers a potential avenue for alleviating data\nsparsity and cold-start problems. Embedding and mapping, as a classic\ncross-domain research genre, aims to identify a common mapping function to\nperform representation transformation between two domains. Nevertheless,\nprevious coarse-grained preference representations, non-personalized mapping\nfunctions, and excessive reliance on overlapping users limit their performance,\nespecially in scenarios where overlapping users are sparse. To address\naforementioned challenges, we propose a novel cross-domain approach, namely\nCVPM. CVPM formalizes cross-domain interest transfer as a hybrid architecture\nof parametric meta-learning and self-supervised learning, which not only\ntransfers user preferences at a finer level, but also enables signal\nenhancement with the knowledge of non-overlapping users. Specifically, with\ndeep insights into user preferences and valence preference theory, we believe\nthat there exists significant difference between users' positive preferences\nand negative behaviors, and thus employ differentiated encoders to learn their\ndistributions. In particular, we further utilize the pre-trained model and item\npopularity to sample pseudo-interaction items to ensure the integrity of both\ndistributions. To guarantee the personalization of preference transfer, we\ntreat each user's mapping as two parts, the common transformation and the\npersonalized bias, where the network used to generate the personalized bias is\noutput by a meta-learner. Furthermore, in addition to the supervised loss for\noverlapping users, we design contrastive tasks for non-overlapping users from\nboth group and individual-levels to avoid model skew and enhance the semantics\nof representations. Exhaustive data analysis and extensive experimental results\ndemonstrate the effectiveness and advancement of our proposed framework.",
        "translated": ""
    },
    {
        "title": "Context-augmented Retrieval: A Novel Framework for Fast Information\n  Retrieval based Response Generation using Large Language Model",
        "url": "http://arxiv.org/abs/2406.16383v1",
        "pub_date": "2024-06-24",
        "summary": "Generating high-quality answers consistently by providing contextual\ninformation embedded in the prompt passed to the Large Language Model (LLM) is\ndependent on the quality of information retrieval. As the corpus of contextual\ninformation grows, the answer/inference quality of Retrieval Augmented\nGeneration (RAG) based Question Answering (QA) systems declines. This work\nsolves this problem by combining classical text classification with the Large\nLanguage Model (LLM) to enable quick information retrieval from the vector\nstore and ensure the relevancy of retrieved information. For the same, this\nwork proposes a new approach Context Augmented retrieval (CAR), where\npartitioning of vector database by real-time classification of information\nflowing into the corpus is done. CAR demonstrates good quality answer\ngeneration along with significant reduction in information retrieval and answer\ngeneration time.",
        "translated": ""
    },
    {
        "title": "On the Role of Long-tail Knowledge in Retrieval Augmented Large Language\n  Models",
        "url": "http://arxiv.org/abs/2406.16367v1",
        "pub_date": "2024-06-24",
        "summary": "Retrieval augmented generation (RAG) exhibits outstanding performance in\npromoting the knowledge capabilities of large language models (LLMs) with\nretrieved documents related to user queries. However, RAG only focuses on\nimproving the response quality of LLMs via enhancing queries indiscriminately\nwith retrieved information, paying little attention to what type of knowledge\nLLMs really need to answer original queries more accurately. In this paper, we\nsuggest that long-tail knowledge is crucial for RAG as LLMs have already\nremembered common world knowledge during large-scale pre-training. Based on our\nobservation, we propose a simple but effective long-tail knowledge detection\nmethod for LLMs. Specifically, the novel Generative Expected Calibration Error\n(GECE) metric is derived to measure the ``long-tailness'' of knowledge based on\nboth statistics and semantics. Hence, we retrieve relevant documents and infuse\nthem into the model for patching knowledge loopholes only when the input query\nrelates to long-tail knowledge. Experiments show that, compared to existing RAG\npipelines, our method achieves over 4x speedup in average inference time and\nconsistent performance improvement in downstream tasks.",
        "translated": ""
    },
    {
        "title": "A Survey on Intent-aware Recommender Systems",
        "url": "http://arxiv.org/abs/2406.16350v1",
        "pub_date": "2024-06-24",
        "summary": "Many modern online services feature personalized recommendations. A central\nchallenge when providing such recommendations is that the reason why an\nindividual user accesses the service may change from visit to visit or even\nduring an ongoing usage session. To be effective, a recommender system should\ntherefore aim to take the users' probable intent of using the service at a\ncertain point in time into account. In recent years, researchers have thus\nstarted to address this challenge by incorporating intent-awareness into\nrecommender systems. Correspondingly, a number of technical approaches were put\nforward, including diversification techniques, intent prediction models or\nlatent intent modeling approaches. In this paper, we survey and categorize\nexisting approaches to building the next generation of Intent-Aware Recommender\nSystems (IARS). Based on an analysis of current evaluation practices, we\noutline open gaps and possible future directions in this area, which in\nparticular include the consideration of additional interaction signals and\ncontextual information to further improve the effectiveness of such systems.",
        "translated": ""
    },
    {
        "title": "DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task",
        "url": "http://arxiv.org/abs/2406.16332v1",
        "pub_date": "2024-06-24",
        "summary": "Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly apply a\ndemonstration retriever to retrieve demonstrations and use top-$k$\ndemonstrations for in-context learning (ICL). Although effective, this approach\noverlooks the dependencies between demonstrations, leading to inferior\nperformance of few-shot ICL in the passage ranking task. In this paper, we\nformulate the demonstration selection as a \\textit{retrieve-then-rerank}\nprocess and introduce the DemoRank framework. In this framework, we first use\nLLM feedback to train a demonstration retriever and construct a novel\ndependency-aware training samples to train a demonstration reranker to improve\nfew-shot ICL. The construction of such training samples not only considers\ndemonstration dependencies but also performs in an efficient way. Extensive\nexperiments demonstrate DemoRank's effectiveness in in-domain scenarios and\nstrong generalization to out-of-domain scenarios. Our codes are available\nat~\\url{https://github.com/8421BCD/DemoRank}.",
        "translated": ""
    },
    {
        "title": "A Mechanism for Optimizing Media Recommender Systems",
        "url": "http://arxiv.org/abs/2406.16212v1",
        "pub_date": "2024-06-23",
        "summary": "A mechanism is described that addresses the fundamental trade off between\nmedia producers who want to increase reach and consumers who provide attention\nbased on the rate of utility received, and where overreach negatively impacts\nthat rate. An optimal solution can be achieved when the media source considers\nthe impact of overreach in a cost function used in determining the optimal\ndistribution of content to maximize individual consumer utility and\nparticipation. The result is a Nash equilibrium between producer and consumer\nthat is also Pareto efficient. Comparison with the literature on Recommender\nsystems highlights the advantages of the mechanism.The review suggests\nadvancements over that literature including identifying an optimal content\nvolume for the consumer and improvements for handling multiple objectives A\npractical algorithm to generate the optimal distribution for each consumer is\nprovided.",
        "translated": ""
    },
    {
        "title": "SimCE: Simplifying Cross-Entropy Loss for Collaborative Filtering",
        "url": "http://arxiv.org/abs/2406.16170v1",
        "pub_date": "2024-06-23",
        "summary": "The learning objective is integral to collaborative filtering systems, where\nthe Bayesian Personalized Ranking (BPR) loss is widely used for learning\ninformative backbones. However, BPR often experiences slow convergence and\nsuboptimal local optima, partially because it only considers one negative item\nfor each positive item, neglecting the potential impacts of other unobserved\nitems. To address this issue, the recently proposed Sampled Softmax\nCross-Entropy (SSM) compares one positive sample with multiple negative\nsamples, leading to better performance. Our comprehensive experiments confirm\nthat recommender systems consistently benefit from multiple negative samples\nduring training. Furthermore, we introduce a \\underline{Sim}plified Sampled\nSoftmax \\underline{C}ross-\\underline{E}ntropy Loss (SimCE), which simplifies\nthe SSM using its upper bound. Our validation on 12 benchmark datasets, using\nboth MF and LightGCN backbones, shows that SimCE significantly outperforms both\nBPR and SSM.",
        "translated": ""
    },
    {
        "title": "Light-weight End-to-End Graph Interest Network for CTR Prediction in\n  E-commerce Search",
        "url": "http://arxiv.org/abs/2406.17745v1",
        "pub_date": "2024-06-25",
        "summary": "Click-through-rate (CTR) prediction has an essential impact on improving user\nexperience and revenue in e-commerce search. With the development of deep\nlearning, graph-based methods are well exploited to utilize graph structure\nextracted from user behaviors and other information to help embedding learning.\nHowever, most of the previous graph-based methods mainly focus on\nrecommendation scenarios, and therefore their graph structures highly depend on\nitem's sequential information from user behaviors, ignoring query's sequential\nsignal and query-item correlation. In this paper, we propose a new approach\nnamed Light-weight End-to-End Graph Interest Network (EGIN) to effectively mine\nusers' search interests and tackle previous challenges. (i) EGIN utilizes query\nand item's correlation and sequential information from the search system to\nbuild a heterogeneous graph for better CTR prediction in e-commerce search.\n(ii) EGIN's graph embedding learning shares the same training input and is\njointly trained with CTR prediction, making the end-to-end framework effortless\nto deploy in large-scale search systems. The proposed EGIN is composed of three\nparts: query-item heterogeneous graph, light-weight graph sampling, and\nmulti-interest network. The query-item heterogeneous graph captures correlation\nand sequential information of query and item efficiently by the proposed\nlight-weight graph sampling. The multi-interest network is well designed to\nutilize graph embedding to capture various similarity relationships between\nquery and item to enhance the final CTR prediction. We conduct extensive\nexperiments on both public and industrial datasets to demonstrate the\neffectiveness of the proposed EGIN. At the same time, the training cost of\ngraph learning is relatively low compared with the main CTR prediction task,\nensuring efficiency in practical applications.",
        "translated": ""
    },
    {
        "title": "LumberChunker: Long-Form Narrative Document Segmentation",
        "url": "http://arxiv.org/abs/2406.17526v1",
        "pub_date": "2024-06-25",
        "summary": "Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively\nprompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the\nmost competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker",
        "translated": ""
    },
    {
        "title": "ACE: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine\n  Semantic Modeling",
        "url": "http://arxiv.org/abs/2406.17507v1",
        "pub_date": "2024-06-25",
        "summary": "Generative retrieval, which has demonstrated effectiveness in text-to-text\nretrieval, utilizes a sequence-to-sequence model to directly generate candidate\nidentifiers based on natural language queries. Without explicitly computing the\nsimilarity between queries and candidates, generative retrieval surpasses\ndual-tower models in both speed and accuracy on large-scale corpora, providing\nnew insights for cross-modal retrieval. However, constructing identifiers for\nmultimodal data remains an untapped problem, and the modality gap between\nnatural language queries and multimodal candidates hinders retrieval\nperformance due to the absence of additional encoders. To this end, we propose\na pioneering generAtive Cross-modal rEtrieval framework (ACE), which is a\ncomprehensive framework for end-to-end cross-modal retrieval based on\ncoarse-to-fine semantic modeling. We propose combining K-Means and RQ-VAE to\nconstruct coarse and fine tokens, serving as identifiers for multimodal data.\nCorrespondingly, we design the coarse-to-fine feature fusion strategy to\nefficiently align natural language queries and candidate identifiers. ACE is\nthe first work to comprehensively demonstrate the feasibility of generative\napproach on text-to-image/audio/video retrieval, challenging the dominance of\nthe embedding-based dual-tower architecture. Extensive experiments show that\nACE achieves state-of-the-art performance in cross-modal retrieval and\noutperforms the strong baselines on Recall@1 by 15.27% on average.",
        "translated": ""
    },
    {
        "title": "Performative Debias with Fair-exposure Optimization Driven by Strategic\n  Agents in Recommender Systems",
        "url": "http://arxiv.org/abs/2406.17475v1",
        "pub_date": "2024-06-25",
        "summary": "Data bias, e.g., popularity impairs the dynamics of two-sided markets within\nrecommender systems. This overshadows the less visible but potentially\nintriguing long-tail items that could capture user interest. Despite the\nabundance of research surrounding this issue, it still poses challenges and\nremains a hot topic in academic circles. Along this line, in this paper, we\ndeveloped a re-ranking approach in dynamic settings with fair-exposure\noptimization driven by strategic agents. Designed for the producer side, the\nexecution of agents assumes content creators can modify item features based on\nstrategic incentives to maximize their exposure. This iterative process entails\nan end-to-end optimization, employing differentiable ranking operators that\nsimultaneously target accuracy and fairness. Joint objectives ensure the\nperformance of recommendations while enhancing the visibility of tail items. We\nalso leveraged the performativity nature of predictions to illustrate how\nstrategic learning influences content creators to shift towards fairness\nefficiently, thereby incentivizing features of tail items. Through\ncomprehensive experiments on both public and industrial datasets, we have\nsubstantiated the effectiveness and dominance of the proposed method especially\non unveiling the potential of tail items.",
        "translated": ""
    },
    {
        "title": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens",
        "url": "http://arxiv.org/abs/2406.17378v1",
        "pub_date": "2024-06-25",
        "summary": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nembedding LLMs, the obtained text embedding will be able to be aligned with the\nkey tokens in the input text. We first fully analyze this phenomenon on eight\nembedding LLMs and show that this phenomenon is universal and is not affected\nby model architecture, training strategy, and embedding method. With a deeper\nanalysis, we then find that the main change in embedding space between the\nembedding LLMs and their original generative LLMs is in the first principal\ncomponent. By adjusting the first principal component, we can align text\nembedding with the key tokens. Finally, we give several examples to demonstrate\nthe vast application potential of this finding: (1) we propose a simple and\npractical sparse retrieval method based on the aligned tokens, which can\nachieve 80\\% of the dense retrieval effect of the same model while reducing the\ncomputation significantly; (2) we show that our findings provide a fresh\nperspective to help understand fuzzy concepts (e.g., semantic relatedness vs.\nsemantic similarity) and emerging technologies (e.g., instruction-following\nembedding) in this field.",
        "translated": ""
    },
    {
        "title": "A Thorough Performance Benchmarking on Lightweight Embedding-based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2406.17335v1",
        "pub_date": "2024-06-25",
        "summary": "Since the creation of the Web, recommender systems (RSs) have been an\nindispensable mechanism in information filtering. State-of-the-art RSs\nprimarily depend on categorical features, which ecoded by embedding vectors,\nresulting in excessively large embedding tables. To prevent over-parameterized\nembedding tables from harming scalability, both academia and industry have seen\nincreasing efforts in compressing RS embeddings. However, despite the\nprosperity of lightweight embedding-based RSs (LERSs), a wide diversity is seen\nin evaluation protocols, resulting in obstacles when relating LERS performance\nto real-world usability. Moreover, despite the common goal of lightweight\nembeddings, LERSs are evaluated with a single choice between the two main\nrecommendation tasks -- collaborative filtering and content-based\nrecommendation. This lack of discussions on cross-task transferability hinders\nthe development of unified, more scalable solutions. Motivated by these issues,\nthis study investigates various LERSs' performance, efficiency, and cross-task\ntransferability via a thorough benchmarking process. Additionally, we propose\nan efficient embedding compression method using magnitude pruning, which is an\neasy-to-deploy yet highly competitive baseline that outperforms various complex\nLERSs. Our study reveals the distinct performance of LERSs across the two\ntasks, shedding light on their effectiveness and generalizability. To support\nedge-based recommendations, we tested all LERSs on a Raspberry Pi 4, where the\nefficiency bottleneck is exposed. Finally, we conclude this paper with critical\nsummaries of LERS performance, model selection suggestions, and underexplored\nchallenges around LERSs for future research. To encourage future research, we\npublish source codes and artifacts at \\href{this\nlink}{https://github.com/chenxing1999/recsys-benchmark}.",
        "translated": ""
    },
    {
        "title": "Hyperbolic Knowledge Transfer in Cross-Domain Recommendation System",
        "url": "http://arxiv.org/abs/2406.17289v1",
        "pub_date": "2024-06-25",
        "summary": "Cross-Domain Recommendation (CDR) seeks to utilize knowledge from different\ndomains to alleviate the problem of data sparsity in the target recommendation\ndomain, and it has been gaining more attention in recent years. Although there\nhave been notable advancements in this area, most current methods represent\nusers and items in Euclidean space, which is not ideal for handling long-tail\ndistributed data in recommendation systems. Additionally, adding data from\nother domains can worsen the long-tail characteristics of the entire dataset,\nmaking it harder to train CDR models effectively. Recent studies have shown\nthat hyperbolic methods are particularly suitable for modeling long-tail\ndistributions, which has led us to explore hyperbolic representations for users\nand items in CDR scenarios. However, due to the distinct characteristics of the\ndifferent domains, applying hyperbolic representation learning to CDR tasks is\nquite challenging. In this paper, we introduce a new framework called\nHyperbolic Contrastive Learning (HCTS), designed to capture the unique features\nof each domain while enabling efficient knowledge transfer between domains. We\nachieve this by embedding users and items from each domain separately and\nmapping them onto distinct hyperbolic manifolds with adjustable curvatures for\nprediction. To improve the representations of users and items in the target\ndomain, we develop a hyperbolic contrastive learning module for knowledge\ntransfer. Extensive experiments on real-world datasets demonstrate that\nhyperbolic manifolds are a promising alternative to Euclidean space for CDR\ntasks.",
        "translated": ""
    },
    {
        "title": "Debiased Recommendation with Noisy Feedback",
        "url": "http://arxiv.org/abs/2406.17182v1",
        "pub_date": "2024-06-24",
        "summary": "Ratings of a user to most items in recommender systems are usually missing\nnot at random (MNAR), largely because users are free to choose which items to\nrate. To achieve unbiased learning of the prediction model under MNAR data,\nthree typical solutions have been proposed, including error-imputation-based\n(EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods.\nHowever, these methods ignore an alternative form of bias caused by the\ninconsistency between the observed ratings and the users' true preferences,\nalso known as noisy feedback or outcome measurement errors (OME), e.g., due to\npublic opinion or low-quality data collection process. In this work, we study\nintersectional threats to the unbiased learning of the prediction model from\ndata MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and\nOME-DR estimators, which largely extend the existing estimators to combat OME\nin real-world recommendation scenarios. Next, we theoretically prove the\nunbiasedness and generalization bound of the proposed estimators. We further\npropose an alternate denoising training approach to achieve unbiased learning\nof the prediction model under MNAR data with OME. Extensive experiments are\nconducted on three real-world datasets and one semi-synthetic dataset to show\nthe effectiveness of our proposed approaches. The code is available at\nhttps://github.com/haoxuanli-pku/KDD24-OME-DR.",
        "translated": ""
    },
    {
        "title": "DEXTER: A Benchmark for open-domain Complex Question Answering using\n  LLMs",
        "url": "http://arxiv.org/abs/2406.17158v1",
        "pub_date": "2024-06-24",
        "summary": "Open-domain complex Question Answering (QA) is a difficult task with\nchallenges in evidence retrieval and reasoning. The complexity of such\nquestions could stem from questions being compositional, hybrid evidence, or\nambiguity in questions. While retrieval performance for classical QA tasks is\nwell explored, their capabilities for heterogeneous complex retrieval tasks,\nespecially in an open-domain setting, and the impact on downstream QA\nperformance, are relatively unexplored. To address this, in this work, we\npropose a benchmark composing diverse complex QA tasks and provide a toolkit to\nevaluate state-of-the-art pre-trained dense and sparse retrieval models in an\nopen-domain setting. We observe that late interaction models and surprisingly\nlexical models like BM25 perform well compared to other pre-trained dense\nretrieval models. In addition, since context-based reasoning is critical for\nsolving complex QA tasks, we also evaluate the reasoning capabilities of LLMs\nand the impact of retrieval performance on their reasoning capabilities.\nThrough experiments, we observe that much progress is to be made in retrieval\nfor complex QA to improve downstream QA performance. Our software and related\ndata can be accessed at https://github.com/VenkteshV/DEXTER",
        "translated": ""
    },
    {
        "title": "UniRec: A Dual Enhancement of Uniformity and Frequency in Sequential\n  Recommendations",
        "url": "http://arxiv.org/abs/2406.18470v1",
        "pub_date": "2024-06-26",
        "summary": "Representation learning in sequential recommendation is critical for\naccurately modeling user interaction patterns and improving recommendation\nprecision. However, existing approaches predominantly emphasize item-to-item\ntransitions, often neglecting the time intervals between interactions, which\nare closely related to behavior pattern changes. Additionally, broader\ninteraction attributes, such as item frequency, are frequently overlooked. We\nfound that both sequences with more uniform time intervals and items with\nhigher frequency yield better prediction performance. Conversely, non-uniform\nsequences exacerbate user interest drift and less-frequent items are difficult\nto model due to sparse sampling, presenting unique challenges inadequately\naddressed by current methods. In this paper, we propose UniRec, a novel\nbidirectional enhancement sequential recommendation method. UniRec leverages\nsequence uniformity and item frequency to enhance performance, particularly\nimproving the representation of non-uniform sequences and less-frequent items.\nThese two branches mutually reinforce each other, driving comprehensive\nperformance optimization in complex sequential recommendation scenarios.\nAdditionally, we present a multidimensional time module to further enhance\nadaptability. To the best of our knowledge, UniRec is the first method to\nutilize the characteristics of uniformity and frequency for feature\naugmentation. Comparing with eleven advanced models across four datasets, we\ndemonstrate that UniRec outperforms SOTA models significantly. The code is\navailable at https://github.com/Linxi000/UniRec.",
        "translated": ""
    },
    {
        "title": "The Effects of Data Split Strategies on the Offline Experiments for CTR\n  Prediction",
        "url": "http://arxiv.org/abs/2406.18320v1",
        "pub_date": "2024-06-26",
        "summary": "Click-through rate (CTR) prediction is a crucial task in online advertising\nto recommend products that users are likely to be interested in. To identify\nthe best-performing models, rigorous model evaluation is necessary. Offline\nexperimentation plays a significant role in selecting models for live user-item\ninteractions, despite the value of online experimentation like A/B testing,\nwhich has its own limitations and risks. Often, the correlation between offline\nperformance metrics and actual online model performance is inadequate. One main\nreason for this discrepancy is the common practice of using random splits to\ncreate training, validation, and test datasets in CTR prediction. In contrast,\nreal-world CTR prediction follows a temporal order. Therefore, the methodology\nused in offline evaluation, particularly the data splitting strategy, is\ncrucial. This study aims to address the inconsistency between current offline\nevaluation methods and real-world use cases, by focusing on data splitting\nstrategies. To examine the impact of different data split strategies on offline\nperformance, we conduct extensive experiments using both random and temporal\nsplits on a large open benchmark dataset, Criteo.",
        "translated": ""
    },
    {
        "title": "Effects of Using Synthetic Data on Deep Recommender Models' Performance",
        "url": "http://arxiv.org/abs/2406.18286v1",
        "pub_date": "2024-06-26",
        "summary": "Recommender systems are essential for enhancing user experiences by\nsuggesting items based on individual preferences. However, these systems\nfrequently face the challenge of data imbalance, characterized by a\npredominance of negative interactions over positive ones. This imbalance can\nresult in biased recommendations favoring popular items. This study\ninvestigates the effectiveness of synthetic data generation in addressing data\nimbalances within recommender systems. Six different methods were used to\ngenerate synthetic data. Our experimental approach involved generating\nsynthetic data using these methods and integrating the generated samples into\nthe original dataset. Our results show that the inclusion of generated negative\nsamples consistently improves the Area Under the Curve (AUC) scores. The\nsignificant impact of synthetic negative samples highlights the potential of\ndata augmentation strategies to address issues of data sparsity and imbalance,\nultimately leading to improved performance of recommender systems.",
        "translated": ""
    },
    {
        "title": "Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with\n  1-to-K Contrastive Learning",
        "url": "http://arxiv.org/abs/2406.18254v1",
        "pub_date": "2024-06-26",
        "summary": "Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search,\nwhich aims to break the barriers between modality and language simultaneously\nand achieves image-text retrieval in the multi-lingual scenario with a single\nmodel. In recent years, excellent progress has been made based on cross-lingual\ncross-modal pre-training; particularly, the methods based on contrastive\nlearning on large-scale data have significantly improved retrieval tasks.\nHowever, these methods directly follow the existing pre-training methods in the\ncross-lingual or cross-modal domain, leading to two problems of inconsistency\nin CCR: The methods with cross-lingual style suffer from the intra-modal error\npropagation, resulting in inconsistent recall performance across languages in\nthe whole dataset. The methods with cross-modal style suffer from the\ninter-modal optimization direction bias, resulting in inconsistent rank across\nlanguages within each instance, which cannot be reflected by Recall@K. To solve\nthese problems, we propose a simple but effective 1-to-K contrastive learning\nmethod, which treats each language equally and eliminates error propagation and\noptimization bias. In addition, we propose a new evaluation metric, Mean Rank\nVariance (MRV), to reflect the rank inconsistency across languages within each\ninstance. Extensive experiments on four CCR datasets show that our method\nimproves both recall rates and MRV with smaller-scale pre-trained data,\nachieving the new state-of-art.",
        "translated": ""
    },
    {
        "title": "Concordance in basal cell carcinoma diagnosis. Building a proper ground\n  truth to train Artificial Intelligence tools",
        "url": "http://arxiv.org/abs/2406.18240v1",
        "pub_date": "2024-06-26",
        "summary": "Background: The existence of different basal cell carcinoma (BCC) clinical\ncriteria cannot be objectively validated. An adequate ground-truth is needed to\ntrain an artificial intelligence (AI) tool that explains the BCC diagnosis by\nproviding its dermoscopic features. Objectives: To determine the consensus\namong dermatologists on dermoscopic criteria of 204 BCC. To analyze the\nperformance of an AI tool when the ground-truth is inferred. Methods: A single\ncenter, diagnostic and prospective study was conducted to analyze the agreement\nin dermoscopic criteria by four dermatologists and then derive a reference\nstandard. 1434 dermoscopic images have been used, that were taken by a primary\nhealth physician, sent via teledermatology, and diagnosed by a dermatologist.\nThey were randomly selected from the teledermatology platform (2019-2021). 204\nof them were tested with an AI tool; the remainder trained it. The performance\nof the AI tool trained using the ground-truth of one dermatologist versus the\nground-truth statistically inferred from the consensus of four dermatologists\nwas analyzed using McNemar's test and Hamming distance. Results: Dermatologists\nachieve perfect agreement in the diagnosis of BCC (Fleiss-Kappa=0.9079), and a\nhigh correlation with the biopsy (PPV=0.9670). However, there is low agreement\nin detecting some dermoscopic criteria. Statistical differences were found in\nthe performance of the AI tool trained using the ground-truth of one\ndermatologist versus the ground-truth statistically inferred from the consensus\nof four dermatologists. Conclusions: Care should be taken when training an AI\ntool to determine the BCC patterns present in a lesion. Ground-truth should be\nestablished from multiple dermatologists.",
        "translated": ""
    },
    {
        "title": "Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode\n  and Effects Analysis",
        "url": "http://arxiv.org/abs/2406.18114v1",
        "pub_date": "2024-06-26",
        "summary": "Failure mode and effects analysis (FMEA) is a critical tool for mitigating\npotential failures, particular during ramp-up phases of new products. However,\nits effectiveness is often limited by the missing reasoning capabilities of the\nFMEA tools, which are usually tabular structured. Meanwhile, large language\nmodels (LLMs) offer novel prospects for fine-tuning on custom datasets for\nreasoning within FMEA contexts. However, LLMs face challenges in tasks that\nrequire factual knowledge, a gap that retrieval-augmented generation (RAG)\napproaches aim to fill. RAG retrieves information from a non-parametric data\nstore and uses a language model to generate responses. Building on this idea,\nwe propose to advance the non-parametric data store with a knowledge graph\n(KG). By enhancing the RAG framework with a KG, our objective is to leverage\nanalytical and semantic question-answering capabilities on FMEA data. This\npaper contributes by presenting a new ontology for FMEA observations, an\nalgorithm for creating vector embeddings from the FMEA KG, and a KG enhanced\nRAG framework. Our approach is validated through a human study and we measure\nthe performance of the context retrieval recall and precision.",
        "translated": ""
    },
    {
        "title": "Efficient Document Ranking with Learnable Late Interactions",
        "url": "http://arxiv.org/abs/2406.17968v1",
        "pub_date": "2024-06-25",
        "summary": "Cross-Encoder (CE) and Dual-Encoder (DE) models are two fundamental\napproaches for query-document relevance in information retrieval. To predict\nrelevance, CE models use joint query-document embeddings, while DE models\nmaintain factorized query and document embeddings; usually, the former has\nhigher quality while the latter benefits from lower latency. Recently,\nlate-interaction models have been proposed to realize more favorable\nlatency-quality tradeoffs, by using a DE structure followed by a lightweight\nscorer based on query and document token embeddings. However, these lightweight\nscorers are often hand-crafted, and there is no understanding of their\napproximation power; further, such scorers require access to individual\ndocument token embeddings, which imposes an increased latency and storage\nburden. In this paper, we propose novel learnable late-interaction models\n(LITE) that resolve these issues. Theoretically, we prove that LITE is a\nuniversal approximator of continuous scoring functions, even for relatively\nsmall embedding dimension. Empirically, LITE outperforms previous\nlate-interaction models such as ColBERT on both in-domain and zero-shot\nre-ranking tasks. For instance, experiments on MS MARCO passage re-ranking show\nthat LITE not only yields a model with better generalization, but also lowers\nlatency and requires 0.25x storage compared to ColBERT.",
        "translated": ""
    },
    {
        "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data\n  Normalization",
        "url": "http://arxiv.org/abs/2406.17961v1",
        "pub_date": "2024-06-25",
        "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in parsing textual data and generating code. However, their\nperformance in tasks involving tabular data, especially those requiring\nsymbolic reasoning, faces challenges due to the structural variance and\ninconsistency in table cell values often found in web tables. In this paper, we\nintroduce NormTab, a novel framework aimed at enhancing the symbolic reasoning\nperformance of LLMs by normalizing web tables. We study table normalization as\na stand-alone, one-time preprocessing step using LLMs to support symbolic\nreasoning on tabular data. Our experimental evaluation, conducted on\nchallenging web table datasets such as WikiTableQuestion and TabFact,\ndemonstrates that leveraging NormTab significantly improves symbolic reasoning\nperformance, showcasing the importance and effectiveness of web table\nnormalization for enhancing LLM-based symbolic reasoning tasks.",
        "translated": ""
    },
    {
        "title": "Which Neurons Matter in IR? Applying Integrated Gradients-based Methods\n  to Understand Cross-Encoders",
        "url": "http://arxiv.org/abs/2406.19309v1",
        "pub_date": "2024-06-27",
        "summary": "With the recent addition of Retrieval-Augmented Generation (RAG), the scope\nand importance of Information Retrieval (IR) has expanded. As a result, the\nimportance of a deeper understanding of IR models also increases. However,\ninterpretability in IR remains under-explored, especially when it comes to the\nmodels' inner mechanisms. In this paper, we explore the possibility of adapting\nIntegrated Gradient-based methods in an IR context to identify the role of\nindividual neurons within the model. In particular, we provide new insights\ninto the role of what we call \"relevance\" neurons, as well as how they deal\nwith unseen data. Finally, we carry out an in-depth pruning study to validate\nour findings.",
        "translated": ""
    },
    {
        "title": "Grounded and Transparent Response Generation for Conversational\n  Information-Seeking Systems",
        "url": "http://arxiv.org/abs/2406.19281v1",
        "pub_date": "2024-06-27",
        "summary": "While previous conversational information-seeking (CIS) research has focused\non passage retrieval, reranking, and query rewriting, the challenge of\nsynthesizing retrieved information into coherent responses remains. The\nproposed research delves into the intricacies of response generation in CIS\nsystems. Open-ended information-seeking dialogues introduce multiple challenges\nthat may lead to potential pitfalls in system responses. The study focuses on\ngenerating responses grounded in the retrieved passages and being transparent\nabout the system's limitations. Specific research questions revolve around\nobtaining confidence-enriched information nuggets, automatic detection of\nincomplete or incorrect responses, generating responses communicating the\nsystem's limitations, and evaluating enhanced responses. By addressing these\nresearch tasks the study aspires to contribute to the advancement of\nconversational response generation, fostering more trustworthy interactions in\nCIS dialogues, and paving the way for grounded and transparent systems to meet\nusers' needs in an information-driven world.",
        "translated": ""
    },
    {
        "title": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts",
        "url": "http://arxiv.org/abs/2406.19237v1",
        "pub_date": "2024-06-27",
        "summary": "Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.",
        "translated": ""
    },
    {
        "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
        "url": "http://arxiv.org/abs/2406.19150v1",
        "pub_date": "2024-06-27",
        "summary": "The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.",
        "translated": ""
    },
    {
        "title": "Statements: Universal Information Extraction from Tables with Large\n  Language Models for ESG KPIs",
        "url": "http://arxiv.org/abs/2406.19102v1",
        "pub_date": "2024-06-27",
        "summary": "Environment, Social, and Governance (ESG) KPIs assess an organization's\nperformance on issues such as climate change, greenhouse gas emissions, water\nconsumption, waste management, human rights, diversity, and policies. ESG\nreports convey this valuable quantitative information through tables.\nUnfortunately, extracting this information is difficult due to high variability\nin the table structure as well as content. We propose Statements, a novel\ndomain agnostic data structure for extracting quantitative facts and related\ninformation. We propose translating tables to statements as a new supervised\ndeep-learning universal information extraction task. We introduce SemTabNet - a\ndataset of over 100K annotated tables. Investigating a family of T5-based\nStatement Extraction Models, our best model generates statements which are 82%\nsimilar to the ground-truth (compared to baseline of 21%). We demonstrate the\nadvantages of statements by applying our model to over 2700 tables from ESG\nreports. The homogeneous nature of statements permits exploratory data analysis\non expansive information found in large collections of ESG reports.",
        "translated": ""
    },
    {
        "title": "Efficient course recommendations with T5-based ranking and summarization",
        "url": "http://arxiv.org/abs/2406.19018v1",
        "pub_date": "2024-06-27",
        "summary": "In this paper, we implement and evaluate a two-stage retrieval pipeline for a\ncourse recommender system that ranks courses for skill-occupation pairs. The\nin-production recommender system BrightFit provides course recommendations from\nmultiple sources. Some of the course descriptions are long and noisy, while\nretrieval and ranking in an online system have to be highly efficient. We\ndeveloped a two-step retrieval pipeline with RankT5 finetuned on MSMARCO as\nre-ranker. We compare two summarizers for course descriptions: a LongT5 model\nthat we finetuned for the task, and a generative LLM (Vicuna) with in-context\nlearning. We experiment with quantization to reduce the size of the ranking\nmodel and increase inference speed. We evaluate our rankers on two newly\nlabelled datasets, with an A/B test, and with a user questionnaire. On the two\nlabelled datasets, our proposed two-stage ranking with automatic summarization\nachieves a substantial improvement over the in-production (BM25) ranker:\nnDCG@10 scores improve from 0.482 to 0.684 and from 0.447 to 0.844 on the two\ndatasets. We also achieve a 40% speed-up by using a quantized version of\nRankT5. The improved quality of the ranking was confirmed by the questionnaire\ncompleted by 29 respondents, but not by the A/B test. In the A/B test, a higher\nclickthrough rate was observed for the BM25-ranking than for the proposed\ntwo-stage retrieval. We conclude that T5-based re-ranking and summarization for\nonline course recommendation can obtain much better effectiveness than\nsingle-step lexical retrieval, and that quantization has a large effect on\nRankT5. In the online evaluation, however, other factors than relevance play a\nrole (such as speed and interpretability of the retrieval results), as well as\nindividual preferences.",
        "translated": ""
    },
    {
        "title": "Towards a Formal Characterization of User Simulation Objectives in\n  Conversational Information Access",
        "url": "http://arxiv.org/abs/2406.19007v1",
        "pub_date": "2024-06-27",
        "summary": "User simulation is a promising approach for automatically training and\nevaluating conversational information access agents, enabling the generation of\nsynthetic dialogues and facilitating reproducible experiments at scale.\nHowever, the objectives of user simulation for the different uses remain\nloosely defined, hindering the development of effective simulators. In this\nwork, we formally characterize the distinct objectives for user simulators:\ntraining aims to maximize behavioral similarity to real users, while evaluation\nfocuses on the accurate prediction of real-world conversational agent\nperformance. Through an empirical study, we demonstrate that optimizing for one\nobjective does not necessarily lead to improved performance on the other. This\nfinding underscores the need for tailored design considerations depending on\nthe intended use of the simulator. By establishing clear objectives and\nproposing concrete measures to evaluate user simulators against those\nobjectives, we pave the way for the development of simulators that are\nspecifically tailored to their intended use, ultimately leading to more\neffective conversational agents.",
        "translated": ""
    },
    {
        "title": "Amplify Graph Learning for Recommendation via Sparsity Completion",
        "url": "http://arxiv.org/abs/2406.18984v1",
        "pub_date": "2024-06-27",
        "summary": "Graph learning models have been widely deployed in collaborative filtering\n(CF) based recommendation systems. Due to the issue of data sparsity, the graph\nstructure of the original input lacks potential positive preference edges,\nwhich significantly reduces the performance of recommendations. In this paper,\nwe study how to enhance the graph structure for CF more effectively, thereby\noptimizing the representation of graph nodes. Previous works introduced matrix\ncompletion techniques into CF, proposing the use of either stochastic\ncompletion methods or superficial structure completion to address this issue.\nHowever, most of these approaches employ random numerical filling that lack\ncontrol over noise perturbations and limit the in-depth exploration of\nhigher-order interaction features of nodes, resulting in biased graph\nrepresentations.\n  In this paper, we propose an Amplify Graph Learning framework based on\nSparsity Completion (called AGL-SC). First, we utilize graph neural network to\nmine direct interaction features between user and item nodes, which are used as\nthe inputs of the encoder. Second, we design a factorization-based method to\nmine higher-order interaction features. These features serve as perturbation\nfactors in the latent space of the hidden layer to facilitate generative\nenhancement. Finally, by employing the variational inference, the above\nmulti-order features are integrated to implement the completion and enhancement\nof missing graph structures. We conducted benchmark and strategy experiments on\nfour real-world datasets related to recommendation tasks. The experimental\nresults demonstrate that AGL-SC significantly outperforms the state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "Multi-modal Food Recommendation using Clustering and Self-supervised\n  Learning",
        "url": "http://arxiv.org/abs/2406.18962v1",
        "pub_date": "2024-06-27",
        "summary": "Food recommendation systems serve as pivotal components in the realm of\ndigital lifestyle services, designed to assist users in discovering recipes and\nfood items that resonate with their unique dietary predilections. Typically,\nmulti-modal descriptions offer an exhaustive profile for each recipe, thereby\nensuring recommendations that are both personalized and accurate. Our\npreliminary investigation of two datasets indicates that pre-trained\nmulti-modal dense representations might precipitate a deterioration in\nperformance compared to ID features when encapsulating interactive\nrelationships. This observation implies that ID features possess a relative\nsuperiority in modeling interactive collaborative signals. Consequently,\ncontemporary cutting-edge methodologies augment ID features with multi-modal\ninformation as supplementary features, overlooking the latent semantic\nrelations between recipes. To rectify this, we present CLUSSL, a novel food\nrecommendation framework that employs clustering and self-supervised learning.\nSpecifically, CLUSSL formulates a modality-specific graph tailored to each\nmodality with discrete/continuous features, thereby transforming semantic\nfeatures into structural representation. Furthermore, CLUSSL procures recipe\nrepresentations pertinent to different modalities via graph convolutional\noperations. A self-supervised learning objective is proposed to foster\nindependence between recipe representations derived from different unimodal\ngraphs. Comprehensive experiments on real-world datasets substantiate that\nCLUSSL consistently surpasses state-of-the-art recommendation benchmarks in\nperformance.",
        "translated": ""
    },
    {
        "title": "A Surprisingly Simple yet Effective Multi-Query Rewriting Method for\n  Conversational Passage Retrieval",
        "url": "http://arxiv.org/abs/2406.18960v1",
        "pub_date": "2024-06-27",
        "summary": "Conversational passage retrieval is challenging as it often requires the\nresolution of references to previous utterances and needs to deal with the\ncomplexities of natural language, such as coreference and ellipsis. To address\nthese challenges, pre-trained sequence-to-sequence neural query rewriters are\ncommonly used to generate a single de-contextualized query based on\nconversation history. Previous research shows that combining multiple query\nrewrites for the same user utterance has a positive effect on retrieval\nperformance. We propose the use of a neural query rewriter to generate multiple\nqueries and show how to integrate those queries in the passage retrieval\npipeline efficiently. The main strength of our approach lies in its simplicity:\nit leverages how the beam search algorithm works and can produce multiple query\nrewrites at no additional cost. Our contributions further include devising ways\nto utilize multi-query rewrites in both sparse and dense first-pass retrieval.\nWe demonstrate that applying our approach on top of a standard passage\nretrieval pipeline delivers state-of-the-art performance without sacrificing\nefficiency.",
        "translated": ""
    },
    {
        "title": "Interactive Topic Models with Optimal Transport",
        "url": "http://arxiv.org/abs/2406.19928v1",
        "pub_date": "2024-06-28",
        "summary": "Topic models are widely used to analyze document collections. While they are\nvaluable for discovering latent topics in a corpus when analysts are unfamiliar\nwith the corpus, analysts also commonly start with an understanding of the\ncontent present in a corpus. This may be through categories obtained from an\ninitial pass over the corpus or a desire to analyze the corpus through a\npredefined set of categories derived from a high level theoretical framework\n(e.g. political ideology). In these scenarios analysts desire a topic modeling\napproach which incorporates their understanding of the corpus while supporting\nvarious forms of interaction with the model. In this work, we present EdTM, as\nan approach for label name supervised topic modeling. EdTM models topic\nmodeling as an assignment problem while leveraging LM/LLM based document-topic\naffinities and using optimal transport for making globally coherent\ntopic-assignments. In experiments, we show the efficacy of our framework\ncompared to few-shot LLM classifiers, and topic models based on clustering and\nLDA. Further, we show EdTM's ability to incorporate various forms of analyst\nfeedback and while remaining robust to noisy analyst inputs.",
        "translated": ""
    },
    {
        "title": "Rateless Stochastic Coding for Delay-constrained Semantic Communication",
        "url": "http://arxiv.org/abs/2406.19804v1",
        "pub_date": "2024-06-28",
        "summary": "We consider the problem of joint source-channel coding with distortion and\nperception constraints from a rateless perspective, the purpose of which is to\nsettle the balance between reliability (distortion/perception) and\neffectiveness (rate) of transmission over uncertain channels. We find a new\nfinite-blocklength bound for the achievable joint source-channel code rate with\nthe above two constraints. To achieve a superior rateless characteristic of\nJSCC coding, we perform multi-level optimization on various finite-blocklength\ncodes. Based on these two, we then propose a new JSCC coding scheme called\nrateless stochastic coding (RSC). We experimentally demonstrate that the\nproposed RSC can achieve variable rates of transmission maintaining an\nexcellent trade-off between distortion and perception.",
        "translated": ""
    },
    {
        "title": "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case\n  Reformulation",
        "url": "http://arxiv.org/abs/2406.19760v1",
        "pub_date": "2024-06-28",
        "summary": "Legal case retrieval for sourcing similar cases is critical in upholding\njudicial fairness. Different from general web search, legal case retrieval\ninvolves processing lengthy, complex, and highly specialized legal documents.\nExisting methods in this domain often overlook the incorporation of legal\nexpert knowledge, which is crucial for accurately understanding and modeling\nlegal cases, leading to unsatisfactory retrieval performance. This paper\nintroduces KELLER, a legal knowledge-guided case reformulation approach based\non large language models (LLMs) for effective and interpretable legal case\nretrieval. By incorporating professional legal knowledge about crimes and law\narticles, we enable large language models to accurately reformulate the\noriginal legal case into concise sub-facts of crimes, which contain the\nessential information of the case. Extensive experiments on two legal case\nretrieval benchmarks demonstrate superior retrieval performance and robustness\non complex legal case queries of KELLER over existing methods.",
        "translated": ""
    },
    {
        "title": "Doc2Token: Bridging Vocabulary Gap by Predicting Missing Tokens for\n  E-commerce Search",
        "url": "http://arxiv.org/abs/2406.19647v1",
        "pub_date": "2024-06-28",
        "summary": "Addressing the \"vocabulary mismatch\" issue in information retrieval is a\ncentral challenge for e-commerce search engines, because product pages often\nmiss important keywords that customers search for. Doc2Query[1] is a popular\ndocument-expansion technique that predicts search queries for a document and\nincludes the predicted queries with the document for retrieval. However, this\napproach can be inefficient for e-commerce search, because the predicted query\ntokens are often already present in the document. In this paper, we propose\nDoc2Token, a technique that predicts relevant tokens (instead of queries) that\nare missing from the document and includes these tokens in the document for\nretrieval. For the task of predicting missing tokens, we introduce a new\nmetric, \"novel ROUGE score\". Doc2Token is demonstrated to be superior to\nDoc2Query in terms of novel ROUGE score and diversity of predictions. Doc2Token\nalso exhibits efficiency gains by reducing both training and inference times.\nWe deployed the feature to production and observed significant revenue gain in\nan online A/B test, and launched the feature to full traffic on Walmart.com.\n  [1] R. Nogueira, W. Yang, J. Lin, K. Cho, Document expansion by query\nprediction, arXiv preprint arXiv:1904.08375 (2019)",
        "translated": ""
    },
    {
        "title": "TocBERT: Medical Document Structure Extraction Using Bidirectional\n  Transformers",
        "url": "http://arxiv.org/abs/2406.19526v1",
        "pub_date": "2024-06-27",
        "summary": "Text segmentation holds paramount importance in the field of Natural Language\nProcessing (NLP). It plays an important role in several NLP downstream tasks\nlike information retrieval and document summarization. In this work, we propose\na new solution, namely TocBERT, for segmenting texts using bidirectional\ntransformers. TocBERT represents a supervised solution trained on the detection\nof titles and sub-titles from their semantic representations. This task was\nformulated as a named entity recognition (NER) problem. The solution has been\napplied on a medical text segmentation use-case where the Bio-ClinicalBERT\nmodel is fine-tuned to segment discharge summaries of the MIMIC-III dataset.\nThe performance of TocBERT has been evaluated on a human-labeled ground truth\ncorpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a\nlinear text segmentation problem and 72.8% on a hierarchical text segmentation\nproblem. It outperformed a carefully designed rule-based solution, particularly\nin distinguishing titles from subtitles.",
        "translated": ""
    },
    {
        "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in\n  LLMs",
        "url": "http://arxiv.org/abs/2407.02485v1",
        "pub_date": "2024-07-02",
        "summary": "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.",
        "translated": ""
    },
    {
        "title": "Reliable Confidence Intervals for Information Retrieval Evaluation Using\n  Generative A.I",
        "url": "http://arxiv.org/abs/2407.02464v1",
        "pub_date": "2024-07-02",
        "summary": "The traditional evaluation of information retrieval (IR) systems is generally\nvery costly as it requires manual relevance annotation from human experts.\nRecent advancements in generative artificial intelligence -- specifically large\nlanguage models (LLMs) -- can generate relevance annotations at an enormous\nscale with relatively small computational costs. Potentially, this could\nalleviate the costs traditionally associated with IR evaluation and make it\napplicable to numerous low-resource applications. However, generated relevance\nannotations are not immune to (systematic) errors, and as a result, directly\nusing them for evaluation produces unreliable results.\n  In this work, we propose two methods based on prediction-powered inference\nand conformal risk control that utilize computer-generated relevance\nannotations to place reliable confidence intervals (CIs) around IR evaluation\nmetrics. Our proposed methods require a small number of reliable annotations\nfrom which the methods can statistically analyze the errors in the generated\nannotations. Using this information, we can place CIs around evaluation metrics\nwith strong theoretical guarantees. Unlike existing approaches, our conformal\nrisk control method is specifically designed for ranking metrics and can vary\nits CIs per query and document. Our experimental results show that our CIs\naccurately capture both the variance and bias in evaluation based on LLM\nannotations, better than the typical empirical bootstrapping estimates. We hope\nour contributions bring reliable evaluation to the many IR applications where\nthis was traditionally infeasible.",
        "translated": ""
    },
    {
        "title": "Towards Training Music Taggers on Synthetic Data",
        "url": "http://arxiv.org/abs/2407.02156v1",
        "pub_date": "2024-07-02",
        "summary": "Most contemporary music tagging systems rely on large volumes of annotated\ndata. As an alternative, we investigate the extent to which synthetically\ngenerated music excerpts can improve tagging systems when only small annotated\ncollections are available. To this end, we release GTZAN-synth, a synthetic\ndataset that follows the taxonomy of the well-known GTZAN dataset while being\nten times larger in data volume. We first observe that simply adding this\nsynthetic dataset to the training split of GTZAN does not result into\nperformance improvements. We then proceed to investigating domain adaptation,\ntransfer learning and fine-tuning strategies for the task at hand and draw the\nconclusion that the last two options yield an increase in accuracy. Overall,\nthe proposed approach can be considered as a first guide in a promising field\nfor future research.",
        "translated": ""
    },
    {
        "title": "Joint-Dataset Learning and Cross-Consistent Regularization for\n  Text-to-Motion Retrieval",
        "url": "http://arxiv.org/abs/2407.02104v1",
        "pub_date": "2024-07-02",
        "summary": "Pose-estimation methods enable extracting human motion from common videos in\nthe structured form of 3D skeleton sequences. Despite great application\nopportunities, effective content-based access to such spatio-temporal motion\ndata is a challenging problem. In this paper, we focus on the recently\nintroduced text-motion retrieval tasks, which aim to search for database\nmotions that are the most relevant to a specified natural-language textual\ndescription (text-to-motion) and vice-versa (motion-to-text). Despite recent\nefforts to explore these promising avenues, a primary challenge remains the\ninsufficient data available to train robust text-motion models effectively. To\naddress this issue, we propose to investigate joint-dataset learning - where we\ntrain on multiple text-motion datasets simultaneously - together with the\nintroduction of a Cross-Consistent Contrastive Loss function (CCCL), which\nregularizes the learned text-motion common space by imposing uni-modal\nconstraints that augment the representation ability of the trained network. To\nlearn a proper motion representation, we also introduce a transformer-based\nmotion encoder, called MoT++, which employs spatio-temporal attention to\nprocess sequences of skeleton data. We demonstrate the benefits of the proposed\napproaches on the widely-used KIT Motion-Language and HumanML3D datasets. We\nperform detailed experimentation on joint-dataset learning and cross-dataset\nscenarios, showing the effectiveness of each introduced module in a carefully\nconducted ablation study and, in turn, pointing out the limitations of\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Why does in-context learning fail sometimes? Evaluating in-context\n  learning on open and closed questions",
        "url": "http://arxiv.org/abs/2407.02028v1",
        "pub_date": "2024-07-02",
        "summary": "We measure the performance of in-context learning as a function of task\nnovelty and difficulty for open and closed questions. For that purpose, we\ncreated a novel benchmark consisting of hard scientific questions, each paired\nwith a context of various relevancy. We show that counter-intuitively, a\ncontext that is more aligned with the topic does not always help more than a\nless relevant context. This effect is especially visible for open questions and\nquestions of high difficulty or novelty. This result reveals a fundamental\ndifference between the treatment of close-form and open-form questions by\nlarge-language models and shows a need for a more robust evaluation of\nin-context learning on the variety of different types of questions. It also\nposes a new question of how to optimally select a context for large language\nmodels, especially in the context of Retrieval Augmented Generation (RAG)\nsystems. Our results suggest that the answer to this question can be highly\napplication-dependent and might be contingent on factors including the format\nof the question, the perceived difficulty level of the questions, and the\nnovelty or popularity of the information we seek.",
        "translated": ""
    },
    {
        "title": "Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph\n  Completion",
        "url": "http://arxiv.org/abs/2407.01994v1",
        "pub_date": "2024-07-02",
        "summary": "High-quality and high-coverage rule sets are imperative to the success of\nNeuro-Symbolic Knowledge Graph Completion (NS-KGC) models, because they form\nthe basis of all symbolic inferences. Recent literature builds neural models\nfor generating rule sets, however, preliminary experiments show that they\nstruggle with maintaining high coverage. In this work, we suggest three simple\naugmentations to existing rule sets: (1) transforming rules to their abductive\nforms, (2) generating equivalent rules that use inverse forms of constituent\nrelations and (3) random walks that propose new rules. Finally, we prune\npotentially low quality rules. Experiments over four datasets and five\nruleset-baseline settings suggest that these simple augmentations consistently\nimprove results, and obtain up to 7.1 pt MRR and 8.5 pt Hits@1 gains over using\nrules without augmentations.",
        "translated": ""
    },
    {
        "title": "MeMemo: On-device Retrieval Augmentation for Private and Personalized\n  Text Generation",
        "url": "http://arxiv.org/abs/2407.01972v1",
        "pub_date": "2024-07-02",
        "summary": "Retrieval-augmented text generation (RAG) addresses the common limitations of\nlarge language models (LLMs), such as hallucination, by retrieving information\nfrom an updatable external knowledge base. However, existing approaches often\nrequire dedicated backend servers for data storage and retrieval, thereby\nlimiting their applicability in use cases that require strict data privacy,\nsuch as personal finance, education, and medicine. To address the pressing need\nfor client-side dense retrieval, we introduce MeMemo, the first open-source\nJavaScript toolkit that adapts the state-of-the-art approximate nearest\nneighbor search technique HNSW to browser environments. Developed with modern\nand native Web technologies, such as IndexedDB and Web Workers, our toolkit\nleverages client-side hardware capabilities to enable researchers and\ndevelopers to efficiently search through millions of high-dimensional vectors\nin the browser. MeMemo enables exciting new design and research opportunities,\nsuch as private and personalized content creation and interactive prototyping,\nas demonstrated in our example application RAG Playground. Reflecting on our\nwork, we discuss the opportunities and challenges for on-device dense\nretrieval. MeMemo is available at https://github.com/poloclub/mememo.",
        "translated": ""
    },
    {
        "title": "AdaCQR: Enhancing Query Reformulation for Conversational Search via\n  Sparse and Dense Retrieval Alignment",
        "url": "http://arxiv.org/abs/2407.01965v1",
        "pub_date": "2024-07-02",
        "summary": "Conversational Query Reformulation (CQR) has significantly advanced in\naddressing the challenges of conversational search, particularly those stemming\nfrom the latent user intent and the need for historical context. Recent works\naimed to boost the performance of CRQ through alignment. However, they are\ndesigned for one specific retrieval system, which potentially results in poor\ngeneralization. To overcome this limitation, we present a novel framework\nAdaCQR. By aligning reformulation models with both term-based and\nsemantic-based retrieval systems, AdaCQR enhances the generalizability of\ninformation-seeking queries across diverse retrieval environments through a\ndual-phase training strategy. We also developed two effective approaches for\nacquiring superior labels and diverse input candidates, boosting the efficiency\nand robustness of the framework. Experimental evaluations on the TopiOCQA and\nQReCC datasets demonstrate that AdaCQR significantly outperforms existing\nmethods, offering both quantitative and qualitative improvements in\nconversational query reformulation.",
        "translated": ""
    },
    {
        "title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In\n  Log Analysis",
        "url": "http://arxiv.org/abs/2407.01896v1",
        "pub_date": "2024-07-02",
        "summary": "Log analysis is crucial for ensuring the orderly and stable operation of\ninformation systems, particularly in the field of Artificial Intelligence for\nIT Operations (AIOps). Large Language Models (LLMs) have demonstrated\nsignificant potential in natural language processing tasks. In the AIOps\ndomain, they excel in tasks such as anomaly detection, root cause analysis of\nfaults, operations and maintenance script generation, and alert information\nsummarization. However, the performance of current LLMs in log analysis tasks\nremains inadequately validated. To address this gap, we introduce LogEval, a\ncomprehensive benchmark suite designed to evaluate the capabilities of LLMs in\nvarious log analysis tasks for the first time. This benchmark covers tasks such\nas log parsing, log anomaly detection, log fault diagnosis, and log\nsummarization. LogEval evaluates each task using 4,000 publicly available log\ndata entries and employs 15 different prompts for each task to ensure a\nthorough and fair assessment. By rigorously evaluating leading LLMs, we\ndemonstrate the impact of various LLM technologies on log analysis performance,\nfocusing on aspects such as self-consistency and few-shot contextual learning.\nWe also discuss findings related to model quantification, Chinese-English\nquestion-answering evaluation, and prompt engineering. These findings provide\ninsights into the strengths and weaknesses of LLMs in multilingual environments\nand the effectiveness of different prompt strategies. Various evaluation\nmethods are employed for different tasks to accurately measure the performance\nof LLMs in log analysis, ensuring a comprehensive assessment. The insights\ngained from LogEvals evaluation reveal the strengths and limitations of LLMs in\nlog analysis tasks, providing valuable guidance for researchers and\npractitioners.",
        "translated": ""
    },
    {
        "title": "Investigating Nudges toward Related Sellers on E-commerce Marketplaces:\n  A Case Study on Amazon",
        "url": "http://arxiv.org/abs/2407.01732v1",
        "pub_date": "2024-07-01",
        "summary": "E-commerce marketplaces provide business opportunities to millions of sellers\nworldwide. Some of these sellers have special relationships with the\nmarketplace by virtue of using their subsidiary services (e.g., fulfillment\nand/or shipping services provided by the marketplace) -- we refer to such\nsellers collectively as Related Sellers. When multiple sellers offer to sell\nthe same product, the marketplace helps a customer in selecting an offer (by a\nseller) through (a) a default offer selection algorithm, (b) showing features\nabout each of the offers and the corresponding sellers (price, seller\nperformance metrics, seller's number of ratings etc.), and (c) finally\nevaluating the sellers along these features. In this paper, we perform an\nend-to-end investigation into how the above apparatus can nudge customers\ntoward the Related Sellers on Amazon's four different marketplaces in India,\nUSA, Germany and France. We find that given explicit choices, customers'\npreferred offers and algorithmically selected offers can be significantly\ndifferent. We highlight that Amazon is adopting different performance metric\nevaluation policies for different sellers, potentially benefiting Related\nSellers. For instance, such policies result in notable discrepancy between the\nactual performance metric and the presented performance metric of Related\nSellers. We further observe that among the seller-centric features visible to\ncustomers, sellers' number of ratings influences their decisions the most, yet\nit may not reflect the true quality of service by the seller, rather reflecting\nthe scale at which the seller operates, thereby implicitly steering customers\ntoward larger Related Sellers. Moreover, when customers are shown the rectified\nmetrics for the different sellers, their preference toward Related Sellers is\nalmost halved.",
        "translated": ""
    },
    {
        "title": "CoIR: A Comprehensive Benchmark for Code Information Retrieval Models",
        "url": "http://arxiv.org/abs/2407.02883v1",
        "pub_date": "2024-07-03",
        "summary": "Despite the substantial success of Information Retrieval (IR) in various NLP\ntasks, most IR systems predominantly handle queries and corpora in natural\nlanguage, neglecting the domain of code retrieval. Code retrieval is critically\nimportant yet remains under-explored, with existing methods and benchmarks\ninadequately representing the diversity of code in various domains and tasks.\nAddressing this gap, we present \\textbf{\\name} (\\textbf{Co}de\n\\textbf{I}nformation \\textbf{R}etrieval Benchmark), a robust and comprehensive\nbenchmark specifically designed to assess code retrieval capabilities. \\name\ncomprises \\textbf{ten} meticulously curated code datasets, spanning\n\\textbf{eight} distinctive retrieval tasks across \\textbf{seven} diverse\ndomains. We first discuss the construction of \\name and its diverse dataset\ncomposition. Further, we evaluate nine widely used retrieval models using\n\\name, uncovering significant difficulties in performing code retrieval tasks\neven with state-of-the-art systems. To facilitate easy adoption and integration\nwithin existing research workflows, \\name has been developed as a user-friendly\nPython framework, readily installable via pip. It shares same data schema as\nother popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark\nevaluations. Through \\name, we aim to invigorate research in the code retrieval\ndomain, providing a versatile benchmarking tool that encourages further\ndevelopment and exploration of code retrieval systems\\footnote{\\url{\nhttps://github.com/CoIR-team/coir}}.",
        "translated": ""
    },
    {
        "title": "CRUISE on Quantum Computing for Feature Selection in Recommender Systems",
        "url": "http://arxiv.org/abs/2407.02839v1",
        "pub_date": "2024-07-03",
        "summary": "Using Quantum Computers to solve problems in Recommender Systems that\nclassical computers cannot address is a worthwhile research topic. In this\npaper, we use Quantum Annealers to address the feature selection problem in\nrecommendation algorithms. This feature selection problem is a Quadratic\nUnconstrained Binary Optimization(QUBO) problem. By incorporating\nCounterfactual Analysis, we significantly improve the performance of the\nitem-based KNN recommendation algorithm compared to using pure Mutual\nInformation. Extensive experiments have demonstrated that the use of\nCounterfactual Analysis holds great promise for addressing such problems.",
        "translated": ""
    },
    {
        "title": "LANE: Logic Alignment of Non-tuning Large Language Models and Online\n  Recommendation Systems for Explainable Reason Generation",
        "url": "http://arxiv.org/abs/2407.02833v1",
        "pub_date": "2024-07-03",
        "summary": "The explainability of recommendation systems is crucial for enhancing user\ntrust and satisfaction. Leveraging large language models (LLMs) offers new\nopportunities for comprehensive recommendation logic generation. However, in\nexisting related studies, fine-tuning LLM models for recommendation tasks\nincurs high computational costs and alignment issues with existing systems,\nlimiting the application potential of proven proprietary/closed-source LLM\nmodels, such as GPT-4. In this work, our proposed effective strategy LANE\naligns LLMs with online recommendation systems without additional LLMs tuning,\nreducing costs and improving explainability. This innovative approach addresses\nkey challenges in integrating language models with recommendation systems while\nfully utilizing the capabilities of powerful proprietary models. Specifically,\nour strategy operates through several key components: semantic embedding, user\nmulti-preference extraction using zero-shot prompting, semantic alignment, and\nexplainable recommendation generation using Chain of Thought (CoT) prompting.\nBy embedding item titles instead of IDs and utilizing multi-head attention\nmechanisms, our approach aligns the semantic features of user preferences with\nthose of candidate items, ensuring coherent and user-aligned recommendations.\nSufficient experimental results including performance comparison, questionnaire\nvoting, and visualization cases prove that our method can not only ensure\nrecommendation performance, but also provide easy-to-understand and reasonable\nrecommendation logic.",
        "translated": ""
    },
    {
        "title": "Learning Positional Attention for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2407.02793v1",
        "pub_date": "2024-07-03",
        "summary": "Self-attention-based networks have achieved remarkable performance in\nsequential recommendation tasks. A crucial component of these models is\npositional encoding. In this study, we delve into the learned positional\nembedding, demonstrating that it often captures the distance between tokens.\nBuilding on this insight, we introduce novel attention models that directly\nlearn positional relations. Extensive experiments reveal that our proposed\nmodels, \\textbf{PARec} and \\textbf{FPARec} outperform previous\nself-attention-based approaches.Our code is available at the link for anonymous\nreview: https://anonymous.4open.science/ r/FPARec-2C55/",
        "translated": ""
    },
    {
        "title": "Supporting Cross-language Cross-project Bug Localization Using\n  Pre-trained Language Models",
        "url": "http://arxiv.org/abs/2407.02732v1",
        "pub_date": "2024-07-03",
        "summary": "Automatically locating a bug within a large codebase remains a significant\nchallenge for developers. Existing techniques often struggle with\ngeneralizability and deployment due to their reliance on application-specific\ndata and large model sizes. This paper proposes a novel pre-trained language\nmodel (PLM) based technique for bug localization that transcends project and\nlanguage boundaries. Our approach leverages contrastive learning to enhance the\nrepresentation of bug reports and source code. It then utilizes a novel ranking\napproach that combines commit messages and code segments. Additionally, we\nintroduce a knowledge distillation technique that reduces model size for\npractical deployment without compromising performance.\n  This paper presents several key benefits. By incorporating code segment and\ncommit message analysis alongside traditional file-level examination, our\ntechnique achieves better bug localization accuracy. Furthermore, our model\nexcels at generalizability - trained on code from various projects and\nlanguages, it can effectively identify bugs in unseen codebases. To address\ncomputational limitations, we propose a CPU-compatible solution. In essence,\nproposed work presents a highly effective, generalizable, and efficient bug\nlocalization technique with the potential to real-world deployment.",
        "translated": ""
    },
    {
        "title": "Optimizing Nepali PDF Extraction: A Comparative Study of Parser and OCR\n  Technologies",
        "url": "http://arxiv.org/abs/2407.04577v1",
        "pub_date": "2024-07-05",
        "summary": "This research compares PDF parsing and Optical Character Recognition (OCR)\nmethods for extracting Nepali content from PDFs. PDF parsing offers fast and\naccurate extraction but faces challenges with non-Unicode Nepali fonts. OCR,\nspecifically PyTesseract, overcomes these challenges, providing versatility for\nboth digital and scanned PDFs. The study reveals that while PDF parsers are\nfaster, their accuracy fluctuates based on PDF types. In contrast, OCRs, with a\nfocus on PyTesseract, demonstrate consistent accuracy at the expense of\nslightly longer extraction times. Considering the project's emphasis on Nepali\nPDFs, PyTesseract emerges as the most suitable library, balancing extraction\nspeed and accuracy.",
        "translated": ""
    },
    {
        "title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models",
        "url": "http://arxiv.org/abs/2407.04573v1",
        "pub_date": "2024-07-05",
        "summary": "Vector retrieval algorithms are vital for semantic queries in the evolving\nlandscape of Large Language Models (LLMs). Retrieving vectors that\nsimultaneously meet criteria for both similarity and diversity significantly\nenhances the capabilities of LLM-based agents. Despite the widespread use of\nthe Maximal Marginal Relevance (MMR) in retrieval scenarios with relevance and\ndiversity requirements, fluctuations caused by variations in the parameter $\n\\lambda $ within the MMR complicate the determination of the optimization\ntrajectory in vector spaces, thus obscuring the direction of enhancement.\nMoreover, there is a lack of a robust theoretical analysis for the constraints\nof similarity and diversity in retrieval processes. This paper introduces a\nnovel approach to characterizing both constraints through the relationship\nbetween the sum vector and the query vector. The proximity of these vectors\naddresses the similarity constraint, while necessitating that individual\nvectors within the sum vector divergently align with the query vector to\nsatisfy the diversity constraint. We also formulate a new combinatorial\noptimization challenge, taking a selection of $k$ vectors from a set of\ncandidates such that their sum vector maximally aligns with the query vector, a\nproblem we demonstrate to be NP-complete. This establishes the profound\ndifficulty of pursuing similarity and diversity simultaneously in vector\nretrieval and lays a theoretical groundwork for further research. Additionally,\nwe present the heuristic algorithm Vectors Retrieval with Similarity and\nDiversity (VRSD) which not only has a definitive optimization goal and eschews\nthe need for preset parameters but also offers a modest reduction in time\ncomplexity compared to MMR. Empirical validation further confirm that VRSD\nsignificantly surpasses MMR across various datasets.",
        "translated": ""
    },
    {
        "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning",
        "url": "http://arxiv.org/abs/2407.04528v1",
        "pub_date": "2024-07-05",
        "summary": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis of between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.",
        "translated": ""
    },
    {
        "title": "EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context",
        "url": "http://arxiv.org/abs/2407.04472v1",
        "pub_date": "2024-07-05",
        "summary": "Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.",
        "translated": ""
    },
    {
        "title": "An Interactive Multi-modal Query Answering System with\n  Retrieval-Augmented Large Language Models",
        "url": "http://arxiv.org/abs/2407.04217v1",
        "pub_date": "2024-07-05",
        "summary": "Retrieval-augmented Large Language Models (LLMs) have reshaped traditional\nquery-answering systems, offering unparalleled user experiences. However,\nexisting retrieval techniques often struggle to handle multi-modal query\ncontexts. In this paper, we present an interactive Multi-modal Query Answering\n(MQA) system, empowered by our newly developed multi-modal retrieval framework\nand navigation graph index, integrated with cutting-edge LLMs. It comprises\nfive core components: Data Preprocessing, Vector Representation, Index\nConstruction, Query Execution, and Answer Generation, all orchestrated by a\ndedicated coordinator to ensure smooth data flow from input to answer\ngeneration. One notable aspect of MQA is its utilization of contrastive\nlearning to assess the significance of different modalities, facilitating\nprecise measurement of multi-modal information similarity. Furthermore, the\nsystem achieves efficient retrieval through our advanced navigation graph\nindex, refined using computational pruning techniques. Another highlight of our\nsystem is its pluggable processing framework, allowing seamless integration of\nembedding models, graph indexes, and LLMs. This flexibility provides users\ndiverse options for gaining insights from their multi-modal knowledge base. A\npreliminary video introduction of MQA is available at\nhttps://youtu.be/xvUuo2ZIqWk.",
        "translated": ""
    },
    {
        "title": "Leveraging Topic Specificity and Social Relationships for Expert Finding\n  in Community Question Answering Platforms",
        "url": "http://arxiv.org/abs/2407.04018v1",
        "pub_date": "2024-07-04",
        "summary": "Online Community Question Answering (CQA) platforms have become indispensable\ntools for users seeking expert solutions to their technical queries. The\neffectiveness of these platforms relies on their ability to identify and direct\nquestions to the most knowledgeable users within the community, a process known\nas Expert Finding (EF). EF accuracy is crucial for increasing user engagement\nand the reliability of provided answers. Despite recent advancements in EF\nmethodologies, blending the diverse information sources available on CQA\nplatforms for effective expert identification remains challenging. In this\npaper, we present TUEF, a Topic-oriented User-Interaction model for Expert\nFinding, which aims to fully and transparently leverage the heterogeneous\ninformation available within online question-answering communities. TUEF\nintegrates content and social data by constructing a multi-layer graph that\nmaps out user relationships based on their answering patterns on specific\ntopics. By combining these sources of information, TUEF identifies the most\nrelevant and knowledgeable users for any given question and ranks them using\nlearning-to-rank techniques. Our findings indicate that TUEF's topic-oriented\nmodel significantly enhances performance, particularly in large communities\ndiscussing well-defined topics. Additionally, we show that the interpretable\nlearning-to-rank algorithm integrated into TUEF offers transparency and\nexplainability with minimal performance trade-offs. The exhaustive experiments\nconducted on six different CQA communities of Stack Exchange show that TUEF\noutperforms all competitors with a minimum performance boost of 42.42% in P@1,\n32.73% in NDCG@3, 21.76% in R@5, and 29.81% in MRR, excelling in both the\nevaluation approaches present in the previous literature.",
        "translated": ""
    },
    {
        "title": "Query-oriented Data Augmentation for Session Search",
        "url": "http://arxiv.org/abs/2407.03720v1",
        "pub_date": "2024-07-04",
        "summary": "Modeling contextual information in a search session has drawn more and more\nattention when understanding complex user intents. Recent methods are all\ndata-driven, i.e., they train different models on large-scale search log data\nto identify the relevance between search contexts and candidate documents. The\ncommon training paradigm is to pair the search context with different candidate\ndocuments and train the model to rank the clicked documents higher than the\nunclicked ones. However, this paradigm neglects the symmetric nature of the\nrelevance between the session context and document, i.e., the clicked documents\ncan also be paired with different search contexts when training. In this work,\nwe propose query-oriented data augmentation to enrich search logs and empower\nthe modeling. We generate supplemental training pairs by altering the most\nimportant part of a search context, i.e., the current query, and train our\nmodel to rank the generated sequence along with the original sequence. This\napproach enables models to learn that the relevance of a document may vary as\nthe session context changes, leading to a better understanding of users' search\npatterns. We develop several strategies to alter the current query, resulting\nin new training data with varying degrees of difficulty. Through\nexperimentation on two extensive public search logs, we have successfully\ndemonstrated the effectiveness of our model.",
        "translated": ""
    },
    {
        "title": "Heterogeneous Hypergraph Embedding for Recommendation Systems",
        "url": "http://arxiv.org/abs/2407.03665v1",
        "pub_date": "2024-07-04",
        "summary": "Recent advancements in recommender systems have focused on integrating\nknowledge graphs (KGs) to leverage their auxiliary information. The core idea\nof KG-enhanced recommenders is to incorporate rich semantic information for\nmore accurate recommendations. However, two main challenges persist: i)\nNeglecting complex higher-order interactions in the KG-based user-item network,\npotentially leading to sub-optimal recommendations, and ii) Dealing with the\nheterogeneous modalities of input sources, such as user-item bipartite graphs\nand KGs, which may introduce noise and inaccuracies. To address these issues,\nwe present a novel Knowledge-enhanced Heterogeneous Hypergraph Recommender\nSystem (KHGRec). KHGRec captures group-wise characteristics of both the\ninteraction network and the KG, modeling complex connections in the KG. Using a\ncollaborative knowledge heterogeneous hypergraph (CKHG), it employs two\nhypergraph encoders to model group-wise interdependencies and ensure\nexplainability. Additionally, it fuses signals from the input graphs with\ncross-view self-supervised learning and attention mechanisms. Extensive\nexperiments on four real-world datasets show our model's superiority over\nvarious state-of-the-art baselines, with an average 5.18\\% relative\nimprovement. Additional tests on noise resilience, missing data, and cold-start\nproblems demonstrate the robustness of our KHGRec framework. Our model and\nevaluation datasets are publicly available at\n\\url{https://github.com/viethungvu1998/KHGRec}.",
        "translated": ""
    },
    {
        "title": "Reviewers of Educational Immersive and Extended Reality (XR)\n  experiences: Who is creating these reviews and why?",
        "url": "http://arxiv.org/abs/2407.03650v1",
        "pub_date": "2024-07-04",
        "summary": "This paper presents a scoping review of literature to examine who is\nreviewing educational immersive or extended reality - eduXR experiences and\nwhy. EduXR experiences in augmented, virtual or mixed reality take many forms,\nfrom supporting manual training, engaging learners in conservation, to provide\nopportunities for social connection. For users of eduXR, reviews of an\nexperience can provide information that helps them determine whether it will\nmeet their learning needs or not. The source of the review, that is, who they\nare and why they have conducted the review, is critical in helping the user\njudge the reviews quality and relevance. At present, there is no settled review\nsystem in place for eduXR, though relevant frameworks exist for serious games\nreview with relevance and overlap for some, but not all, eduXR experiences.\nWhile some authors have engaged in preparing a detailed review structure for\neduXR, there remains a need for a clear and simple way for users of eduXR to\nknow details about reviewers, e.g., who and why, to help make it easier for\nusers to identify relevant reviews and gain useful insight about eduXR\nexperiences. To help address this issue, we conducted a scoping review asking\nthe question; Who is creating eduXR reviews, and why? We identified 16 papers\nthat present an academic evaluation on the review process of eduXR reviews. The\n16 papers were analysed, coding for who themes and why themes over two separate\ncycles, using thematic analysis. An analysis looked to examine what we know\nregarding who is providing the reviews, and why, to help us to understand what\nenables, inhibits and what is yet unknown about how the eduXR community goes\nabout making informed choices regarding the eduXR experiences they engage with.",
        "translated": ""
    },
    {
        "title": "BM25S: Orders of magnitude faster lexical search via eager sparse\n  scoring",
        "url": "http://arxiv.org/abs/2407.03618v1",
        "pub_date": "2024-07-04",
        "summary": "We introduce BM25S, an efficient Python-based implementation of BM25 that\nonly depends on Numpy and Scipy. BM25S achieves up to a 500x speedup compared\nto the most popular Python-based framework by eagerly computing BM25 scores\nduring indexing and storing them into sparse matrices. It also achieves\nconsiderable speedups compared to highly optimized Java-based implementations,\nwhich are used by popular commercial products. Finally, BM25S reproduces the\nexact implementation of five BM25 variants based on Kamphuis et al. (2020) by\nextending eager scoring to non-sparse variants using a novel score shifting\nmethod. The code can be found at https://github.com/xhluca/bm25s",
        "translated": ""
    },
    {
        "title": "Transfer Learning with Self-Supervised Vision Transformers for Snake\n  Identification",
        "url": "http://arxiv.org/abs/2407.06178v1",
        "pub_date": "2024-07-08",
        "summary": "We present our approach for the SnakeCLEF 2024 competition to predict snake\nspecies from images. We explore and use Meta's DINOv2 vision transformer model\nfor feature extraction to tackle species' high variability and visual\nsimilarity in a dataset of 182,261 images. We perform exploratory analysis on\nembeddings to understand their structure, and train a linear classifier on the\nembeddings to predict species. Despite achieving a score of 39.69, our results\nshow promise for DINOv2 embeddings in snake identification. All code for this\nproject is available at https://github.com/dsgt-kaggle-clef/snakeclef-2024.",
        "translated": ""
    },
    {
        "title": "MERGE -- A Bimodal Dataset for Static Music Emotion Recognition",
        "url": "http://arxiv.org/abs/2407.06060v1",
        "pub_date": "2024-07-08",
        "summary": "The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a severe lack of\npublic and sizeable bimodal databases has hampered the development and\nimprovement of bimodal audio-lyrics systems. This article proposes three new\naudio, lyrics, and bimodal MER research datasets, collectively called MERGE,\ncreated using a semi-automatic approach. To comprehensively assess the proposed\ndatasets and establish a baseline for benchmarking, we conducted several\nexperiments for each modality, using feature engineering, machine learning, and\ndeep learning methodologies. In addition, we propose and validate fixed\ntrain-validate-test splits. The obtained results confirm the viability of the\nproposed datasets, achieving the best overall result of 79.21% F1-score for\nbimodal classification using a deep neural network.",
        "translated": ""
    },
    {
        "title": "Academic Article Recommendation Using Multiple Perspectives",
        "url": "http://arxiv.org/abs/2407.05836v1",
        "pub_date": "2024-07-08",
        "summary": "We argue that Content-based filtering (CBF) and Graph-based methods (GB)\ncomplement one another in Academic Search recommendations. The scientific\nliterature can be viewed as a conversation between authors and the audience.\nCBF uses abstracts to infer authors' positions, and GB uses citations to infer\nresponses from the audience. In this paper, we describe nine differences\nbetween CBF and GB, as well as synergistic opportunities for hybrid\ncombinations. Two embeddings will be used to illustrate these opportunities:\n(1) Specter, a CBF method based on BERT-like deepnet encodings of abstracts,\nand (2) ProNE, a GB method based on spectral clustering of more than 200M\npapers and 2B citations from Semantic Scholar.",
        "translated": ""
    },
    {
        "title": "New Directions in Text Classification Research: Maximizing The\n  Performance of Sentiment Classification from Limited Data",
        "url": "http://arxiv.org/abs/2407.05627v1",
        "pub_date": "2024-07-08",
        "summary": "The stakeholders' needs in sentiment analysis for various issues, whether\npositive or negative, are speed and accuracy. One new challenge in sentiment\nanalysis tasks is the limited training data, which often leads to suboptimal\nmachine learning models and poor performance on test data. This paper discusses\nthe problem of text classification based on limited training data (300 to 600\nsamples) into three classes: positive, negative, and neutral. A benchmark\ndataset is provided for training and testing data on the issue of Kaesang\nPangarep's appointment as Chairman of PSI. External data for aggregation and\naugmentation purposes are provided, consisting of two datasets: the topic of\nCovid Vaccination sentiment and an open topic. The official score used is the\nF1-score, which balances precision and recall among the three classes,\npositive, negative, and neutral. A baseline score is provided as a reference\nfor researchers for unoptimized classification methods. The optimized score is\nprovided as a reference for the target score to be achieved by any proposed\nmethod. Both scoring (baseline and optimized) use the SVM method, which is\nwidely reported as the state-of-the-art in conventional machine learning\nmethods. The F1-scores achieved by the baseline and optimized methods are\n40.83% and 51.28%, respectively.",
        "translated": ""
    },
    {
        "title": "Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models",
        "url": "http://arxiv.org/abs/2407.05502v1",
        "pub_date": "2024-07-07",
        "summary": "With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are\nplaying a pivotal role in information search and are being adopted globally.\nAlthough the multilingual capability of LLMs offers new opportunities to bridge\nthe language barrier, do these capabilities translate into real-life scenarios\nwhere linguistic divide and knowledge conflicts between multilingual sources\nare known occurrences? In this paper, we studied LLM's linguistic preference in\na RAG-based information search setting. We found that LLMs displayed systemic\nbias towards information in the same language as the query language in both\ninformation retrieval and answer generation. Furthermore, in scenarios where\nthere is little information in the language of the query, LLMs prefer documents\nin high-resource languages, reinforcing the dominant views. Such bias exists\nfor both factual and opinion-based queries. Our results highlight the\nlinguistic divide within multilingual LLMs in information search systems. The\nseemingly beneficial multilingual capability of LLMs may backfire on\ninformation parity by reinforcing language-specific information cocoons or\nfilter bubbles further marginalizing low-resource views.",
        "translated": ""
    },
    {
        "title": "Language Models Encode Collaborative Signals in Recommendation",
        "url": "http://arxiv.org/abs/2407.05441v1",
        "pub_date": "2024-07-07",
        "summary": "Recent studies empirically indicate that language models (LMs) encode rich\nworld knowledge beyond mere semantics, attracting significant attention across\nvarious fields. However, in the recommendation domain, it remains uncertain\nwhether LMs implicitly encode user preference information. Contrary to the\nprevailing understanding that LMs and traditional recommender models learn two\ndistinct representation spaces due to a huge gap in language and behavior\nmodeling objectives, this work rethinks such understanding and explores\nextracting a recommendation space directly from the language representation\nspace. Surprisingly, our findings demonstrate that item representations, when\nlinearly mapped from advanced LM representations, yield superior recommendation\nperformance. This outcome suggests the homomorphism between the language\nrepresentation space and an effective recommendation space, implying that\ncollaborative signals may indeed be encoded within advanced LMs. Motivated by\nthese findings, we propose a simple yet effective collaborative filtering (CF)\nmodel named AlphaRec, which utilizes language representations of item textual\nmetadata (e.g., titles) instead of traditional ID-based embeddings.\nSpecifically, AlphaRec is comprised of three main components: a multilayer\nperceptron (MLP), graph convolution, and contrastive learning (CL) loss\nfunction, making it extremely easy to implement and train. Our empirical\nresults show that AlphaRec outperforms leading ID-based CF models on multiple\ndatasets, marking the first instance of such a recommender with text embeddings\nachieving this level of performance. Moreover, AlphaRec introduces a new\nlanguage-representation-based CF paradigm with several desirable advantages:\nbeing easy to implement, lightweight, rapid convergence, superior zero-shot\nrecommendation abilities in new domains, and being aware of user intention.",
        "translated": ""
    },
    {
        "title": "MelodyVis: Visual Analytics for Melodic Patterns in Sheet Music",
        "url": "http://arxiv.org/abs/2407.05427v1",
        "pub_date": "2024-07-07",
        "summary": "Manual melody detection is a tedious task requiring high expertise level,\nwhile automatic detection is often not expressive or powerful enough. Thus, we\npresent MelodyVis, a visual application designed in collaboration with\nmusicology experts to explore melodic patterns in digital sheet music.\nMelodyVis features five connected views, including a Melody Operator Graph and\na Voicing Timeline. The system utilizes eight atomic operators, such as\ntransposition and mirroring, to capture melody repetitions and variations.\nUsers can start their analysis by manually selecting patterns in the sheet\nview, and then identifying other patterns based on the selected samples through\nan interactive exploration process. We conducted a user study to investigate\nthe effectiveness and usefulness of our approach and its integrated melodic\noperators, including usability and mental load questions. We compared the\nanalysis executed by 25 participants with and without the operators. The study\nresults indicate that the participants could identify at least twice as many\npatterns with activated operators. MelodyVis allows analysts to steer the\nanalysis process and interpret results. Our study also confirms the usefulness\nof MelodyVis in supporting common analytical tasks in melodic analysis, with\nparticipants reporting improved pattern identification and interpretation.\nThus, MelodyVis addresses the limitations of fully-automated approaches,\nenabling music analysts to step into the analysis process and uncover and\nunderstand intricate melodic patterns and transformations in sheet music.",
        "translated": ""
    },
    {
        "title": "Towards Bridging the Cross-modal Semantic Gap for Multi-modal\n  Recommendation",
        "url": "http://arxiv.org/abs/2407.05420v1",
        "pub_date": "2024-07-07",
        "summary": "Multi-modal recommendation greatly enhances the performance of recommender\nsystems by modeling the auxiliary information from multi-modality contents.\nMost existing multi-modal recommendation models primarily exploit multimedia\ninformation propagation processes to enrich item representations and directly\nutilize modal-specific embedding vectors independently obtained from upstream\npre-trained models. However, this might be inappropriate since the abundant\ntask-specific semantics remain unexplored, and the cross-modality semantic gap\nhinders the recommendation performance.\n  Inspired by the recent progress of the cross-modal alignment model CLIP, in\nthis paper, we propose a novel \\textbf{CLIP} \\textbf{E}nhanced\n\\textbf{R}ecommender (\\textbf{CLIPER}) framework to bridge the semantic gap\nbetween modalities and extract fine-grained multi-view semantic information.\nSpecifically, we introduce a multi-view modality-alignment approach for\nrepresentation extraction and measure the semantic similarity between\nmodalities. Furthermore, we integrate the multi-view multimedia representations\ninto downstream recommendation models. Extensive experiments conducted on three\npublic datasets demonstrate the consistent superiority of our model over\nstate-of-the-art multi-modal recommendation models.",
        "translated": ""
    },
    {
        "title": "Multimodal Language Models for Domain-Specific Procedural Video\n  Summarization",
        "url": "http://arxiv.org/abs/2407.05419v1",
        "pub_date": "2024-07-07",
        "summary": "Videos serve as a powerful medium to convey ideas, tell stories, and provide\ndetailed instructions, especially through long-format tutorials. Such tutorials\nare valuable for learning new skills at one's own pace, yet they can be\noverwhelming due to their length and dense content. Viewers often seek specific\ninformation, like precise measurements or step-by-step execution details,\nmaking it essential to extract and summarize key segments efficiently. An\nintelligent, time-sensitive video assistant capable of summarizing and\ndetecting highlights in long videos is highly sought after. Recent advancements\nin Multimodal Large Language Models offer promising solutions to develop such\nan assistant. Our research explores the use of multimodal models to enhance\nvideo summarization and step-by-step instruction generation within specific\ndomains. These models need to understand temporal events and relationships\namong actions across video frames. Our approach focuses on fine-tuning TimeChat\nto improve its performance in specific domains: cooking and medical procedures.\nBy training the model on domain-specific datasets like Tasty for cooking and\nMedVidQA for medical procedures, we aim to enhance its ability to generate\nconcise, accurate summaries of instructional videos. We curate and restructure\nthese datasets to create high-quality video-centric instruction data. Our\nfindings indicate that when finetuned on domain-specific procedural data,\nTimeChat can significantly improve the extraction and summarization of key\ninstructional steps in long-format videos. This research demonstrates the\npotential of specialized multimodal models to assist with practical tasks by\nproviding personalized, step-by-step guidance tailored to the unique aspects of\neach domain.",
        "translated": ""
    },
    {
        "title": "Beyond Check-in Counts: Redefining Popularity for POI Recommendation\n  with Users and Recency",
        "url": "http://arxiv.org/abs/2407.05360v1",
        "pub_date": "2024-07-07",
        "summary": "The next POI (point of interest) recommendation aims to predict users'\nimmediate future movements based on their prior records and present\ncircumstances, which will be very beneficial to service providers as well as\nusers. The popularity of the POI over time is one of the primary deciding\nfactors for choosing the next POI to visit. The majority of research in recent\ntimes has paid more attention to the number of check-ins to define the\npopularity of a point of interest, disregarding the temporal impact or number\nof people checking in for a particular POI. In this paper, we propose a\nrecency-oriented definition of popularity that takes into account the temporal\neffect on POI's popularity, the number of check-ins, as well as the number of\npeople who registered those check-ins. Thus, recent check-ins get prioritized\nwith more weight compared to the older ones. Experimental results demonstrate\nthat performance is better with recency-aware popularity definitions for POIs\nthan with solely check-in count-based popularity definitions.",
        "translated": ""
    },
    {
        "title": "Empirical analysis of Biding Precedent efficiency in the Brazilian\n  Supreme Court via Similar Case Retrieval",
        "url": "http://arxiv.org/abs/2407.07004v1",
        "pub_date": "2024-07-09",
        "summary": "Binding precedents (S\\'umulas Vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26\nand 37, at the highest court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval. The contributions of this article are therefore twofold: on the\nmathematical side, we compare the uses of different methods of Natural Language\nProcessing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,\nwhereas on the legal side, we contrast the inefficiency of these binding\nprecedents with a set of hypotheses that may justify their repeated usage. We\nobserve that the deep learning models performed significantly worse in the\nspecific Similar Case Retrieval task and that the reasons for binding\nprecedents to fail in responding to repetitive demand are heterogeneous and\ncase-dependent, making it impossible to single out a specific cause.",
        "translated": ""
    },
    {
        "title": "Robust Neural Information Retrieval: An Adversarial and\n  Out-of-distribution Perspective",
        "url": "http://arxiv.org/abs/2407.06992v1",
        "pub_date": "2024-07-09",
        "summary": "Recent advances in neural information retrieval (IR) models have\nsignificantly enhanced their effectiveness over various IR tasks. The\nrobustness of these models, essential for ensuring their reliability in\npractice, has also garnered significant attention. With a wide array of\nresearch on robust IR being proposed, we believe it is the opportune moment to\nconsolidate the current status, glean insights from existing methodologies, and\nlay the groundwork for future development. We view the robustness of IR to be a\nmultifaceted concept, emphasizing its necessity against adversarial attacks,\nout-of-distribution (OOD) scenarios and performance variance. With a focus on\nadversarial and OOD robustness, we dissect robustness solutions for dense\nretrieval models (DRMs) and neural ranking models (NRMs), respectively,\nrecognizing them as pivotal components of the neural IR pipeline. We provide an\nin-depth discussion of existing methods, datasets, and evaluation metrics,\nshedding light on challenges and future directions in the era of large language\nmodels. To the best of our knowledge, this is the first comprehensive survey on\nthe robustness of neural IR models, and we will also be giving our first\ntutorial presentation at SIGIR 2024\n\\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the\norganization of existing work, we introduce a Benchmark for robust IR (BestIR),\na heterogeneous evaluation benchmark for robust neural information retrieval,\nwhich is publicly available at \\url{https://github.com/Davion-Liu/BestIR}. We\nhope that this study provides useful clues for future research on the\nrobustness of IR models and helps to develop trustworthy search engines\n\\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.",
        "translated": ""
    },
    {
        "title": "Fine-grained large-scale content recommendations for MSX sellers",
        "url": "http://arxiv.org/abs/2407.06910v1",
        "pub_date": "2024-07-09",
        "summary": "One of the most critical tasks of Microsoft sellers is to meticulously track\nand nurture potential business opportunities through proactive engagement and\ntailored solutions. Recommender systems play a central role to help sellers\nachieve their goals. In this paper, we present a content recommendation model\nwhich surfaces various types of content (technical documentation, comparison\nwith competitor products, customer success stories etc.) that sellers can share\nwith their customers or use for their own self-learning. The model operates at\nthe opportunity level which is the lowest possible granularity and the most\nrelevant one for sellers. It is based on semantic matching between metadata\nfrom the contents and carefully selected attributes of the opportunities.\nConsidering the volume of seller-managed opportunities in organizations such as\nMicrosoft, we show how to perform efficient semantic matching over a very large\nnumber of opportunity-content combinations. The main challenge is to ensure\nthat the top-5 relevant contents for each opportunity are recommended out of a\ntotal of $\\approx 40,000$ published contents. We achieve this target through an\nextensive comparison of different model architectures and feature selection.\nFinally, we further examine the quality of the recommendations in a\nquantitative manner using a combination of human domain experts as well as by\nusing the recently proposed \"LLM as a judge\" framework.",
        "translated": ""
    },
    {
        "title": "Positive-Unlabelled Learning for Improving Image-based Recommender\n  System Explainability",
        "url": "http://arxiv.org/abs/2407.06740v1",
        "pub_date": "2024-07-09",
        "summary": "Among the existing approaches for visual-based Recommender System (RS)\nexplainability, utilizing user-uploaded item images as efficient, trustable\nexplanations is a promising option. However, current models following this\nparadigm assume that, for any user, all images uploaded by other users can be\nconsidered negative training examples (i.e. bad explanatory images), an\ninadvertedly naive labelling assumption that contradicts the rationale of the\napproach. This work proposes a new explainer training pipeline by leveraging\nPositive-Unlabelled (PU) Learning techniques to train image-based explainer\nwith refined subsets of reliable negative examples for each user selected\nthrough a novel user-personalized, two-step, similarity-based PU Learning\nalgorithm. Computational experiments show this PU-based approach outperforms\nthe state-of-the-art non-PU method in six popular real-world datasets, proving\nthat an improvement of visual-based RS explainability can be achieved by\nmaximizing training data quality rather than increasing model complexity.",
        "translated": ""
    },
    {
        "title": "Analyzing the Effectiveness of Listwise Reranking with Positional\n  Invariance on Temporal Generalizability",
        "url": "http://arxiv.org/abs/2407.06716v1",
        "pub_date": "2024-07-09",
        "summary": "Benchmarking the performance of information retrieval (IR) methods are mostly\nconducted within a fixed set of documents (static corpora). However, in\nreal-world web search engine environments, the document set is continuously\nupdated and expanded. Addressing these discrepancies and measuring the temporal\npersistence of IR systems is crucial. By investigating the LongEval benchmark,\nspecifically designed for such dynamic environments, our findings demonstrate\nthe effectiveness of a listwise reranking approach, which proficiently handles\ninaccuracies induced by temporal distribution shifts. Among listwise rerankers,\nour findings show that ListT5, which effectively mitigates the positional bias\nproblem by adopting the Fusion-in-Decoder architecture, is especially\neffective, and more so, as temporal drift increases, on the test-long subset.",
        "translated": ""
    },
    {
        "title": "Embark on DenseQuest: A System for Selecting the Best Dense Retriever\n  for a Custom Collection",
        "url": "http://arxiv.org/abs/2407.06685v1",
        "pub_date": "2024-07-09",
        "summary": "In this demo we present a web-based application for selecting an effective\npre-trained dense retriever to use on a private collection. Our system,\nDenseQuest, provides unsupervised selection and ranking capabilities to predict\nthe best dense retriever among a pool of available dense retrievers, tailored\nto an uploaded target collection. DenseQuest implements a number of existing\napproaches, including a recent, highly effective method powered by Large\nLanguage Models (LLMs), which requires neither queries nor relevance judgments.\nThe system is designed to be intuitive and easy to use for those information\nretrieval engineers and researchers who need to identify a general-purpose\ndense retrieval model to encode or search a new private target collection. Our\ndemonstration illustrates conceptual architecture and the different use case\nscenarios of the system implemented on the cloud, enabling universal access and\nuse. DenseQuest is available at https://densequest.ielab.io.",
        "translated": ""
    },
    {
        "title": "AutoTask: Task Aware Multi-Faceted Single Model for Multi-Task Ads\n  Relevance",
        "url": "http://arxiv.org/abs/2407.06549v1",
        "pub_date": "2024-07-09",
        "summary": "Ads relevance models are crucial in determining the relevance between user\nsearch queries and ad offers, often framed as a classification problem. The\ncomplexity of modeling increases significantly with multiple ad types and\nvarying scenarios that exhibit both similarities and differences. In this work,\nwe introduce a novel multi-faceted attention model that performs task aware\nfeature combination and cross task interaction modeling. Our technique\nformulates the feature combination problem as \"language\" modeling with\nauto-regressive attentions across both feature and task dimensions.\nSpecifically, we introduce a new dimension of task ID encoding for task\nrepresentations, thereby enabling precise relevance modeling across diverse ad\nscenarios with substantial improvement in generality capability for unseen\ntasks. We demonstrate that our model not only effectively handles the increased\ncomputational and maintenance demands as scenarios proliferate, but also\noutperforms generalized DNN models and even task-specific models across a\nspectrum of ad applications using a single unified model.",
        "translated": ""
    },
    {
        "title": "Multi-Label Plant Species Classification with Self-Supervised Vision\n  Transformers",
        "url": "http://arxiv.org/abs/2407.06298v1",
        "pub_date": "2024-07-08",
        "summary": "We present a transfer learning approach using a self-supervised Vision\nTransformer (DINOv2) for the PlantCLEF 2024 competition, focusing on the\nmulti-label plant species classification. Our method leverages both base and\nfine-tuned DINOv2 models to extract generalized feature embeddings. We train\nclassifiers to predict multiple plant species within a single image using these\nrich embeddings. To address the computational challenges of the large-scale\ndataset, we employ Spark for distributed data processing, ensuring efficient\nmemory management and processing across a cluster of workers. Our data\nprocessing pipeline transforms images into grids of tiles, classifying each\ntile, and aggregating these predictions into a consolidated set of\nprobabilities. Our results demonstrate the efficacy of combining transfer\nlearning with advanced data processing techniques for multi-label image\nclassification tasks. Our code is available at\nhttps://github.com/dsgt-kaggle-clef/plantclef-2024.",
        "translated": ""
    },
    {
        "title": "Enhancing HNSW Index for Real-Time Updates: Addressing Unreachable\n  Points and Performance Degradation",
        "url": "http://arxiv.org/abs/2407.07871v1",
        "pub_date": "2024-07-10",
        "summary": "The approximate nearest neighbor search (ANNS) is a fundamental and essential\ncomponent in information retrieval, with graph-based methodologies\ndemonstrating superior performance compared to alternative approaches.\nExtensive research efforts have been dedicated to improving search efficiency\nby developing various graph-based indices, such as HNSW (Hierarchical Navigable\nSmall World). However, the performance of HNSW and most graph-based indices\nbecomes unacceptable when faced with a large number of real-time deletions,\ninsertions, and updates. Furthermore, during update operations, HNSW can result\nin some data points becoming unreachable, a situation we refer to as the\n`unreachable points phenomenon'. This phenomenon could significantly affect the\nsearch accuracy of the graph in certain situations.\n  To address these issues, we present efficient measures to overcome the\nshortcomings of HNSW, specifically addressing poor performance over long\nperiods of delete and update operations and resolving the issues caused by the\nunreachable points phenomenon. Our proposed MN-RU algorithm effectively\nimproves update efficiency and suppresses the growth rate of unreachable\npoints, ensuring better overall performance and maintaining the integrity of\nthe graph. Our results demonstrate that our methods outperform existing\napproaches. Furthermore, since our methods are based on HNSW, they can be\neasily integrated with existing indices widely used in the industrial field,\nmaking them practical for future real-world applications. Code is available at\nhttps://github.com/xwt1/ICPADS-MN-RU.git",
        "translated": ""
    },
    {
        "title": "Systematic Evaluation of Neural Retrieval Models on the Touché 2020\n  Argument Retrieval Subset of BEIR",
        "url": "http://arxiv.org/abs/2407.07790v1",
        "pub_date": "2024-07-10",
        "summary": "The zero-shot effectiveness of neural retrieval models is often evaluated on\nthe BEIR benchmark -- a combination of different IR evaluation datasets.\nInterestingly, previous studies found that particularly on the BEIR subset\nTouch\\'e 2020, an argument retrieval task, neural retrieval models are\nconsiderably less effective than BM25. Still, so far, no further investigation\nhas been conducted on what makes argument retrieval so \"special\". To more\ndeeply analyze the respective potential limits of neural retrieval models, we\nrun a reproducibility study on the Touch\\'e 2020 data. In our study, we focus\non two experiments: (i) a black-box evaluation (i.e., no model retraining),\nincorporating a theoretical exploration using retrieval axioms, and (ii) a data\ndenoising evaluation involving post-hoc relevance judgments. Our black-box\nevaluation reveals an inherent bias of neural models towards retrieving short\npassages from the Touch\\'e 2020 data, and we also find that quite a few of the\nneural models' results are unjudged in the Touch\\'e 2020 data. As many of the\nshort Touch\\'e passages are not argumentative and thus non-relevant per se, and\nas the missing judgments complicate fair comparison, we denoise the Touch\\'e\n2020 data by excluding very short passages (less than 20 words) and by\naugmenting the unjudged data with post-hoc judgments following the Touch\\'e\nguidelines. On the denoised data, the effectiveness of the neural models\nimproves by up to 0.52 in nDCG@10, but BM25 is still more effective. Our code\nand the augmented Touch\\'e 2020 dataset are available at\n\\url{https://github.com/castorini/touche-error-analysis}.",
        "translated": ""
    },
    {
        "title": "Evaluating the method reproducibility of deep learning models in the\n  biodiversity domain",
        "url": "http://arxiv.org/abs/2407.07550v1",
        "pub_date": "2024-07-10",
        "summary": "Artificial Intelligence (AI) is revolutionizing biodiversity research by\nenabling advanced data analysis, species identification, and habitats\nmonitoring, thereby enhancing conservation efforts. Ensuring reproducibility in\nAI-driven biodiversity research is crucial for fostering transparency,\nverifying results, and promoting the credibility of ecological findings.This\nstudy investigates the reproducibility of deep learning (DL) methods within the\nbiodiversity domain. We design a methodology for evaluating the reproducibility\nof biodiversity-related publications that employ DL techniques across three\nstages. We define ten variables essential for method reproducibility, divided\ninto four categories: resource requirements, methodological information,\nuncontrolled randomness, and statistical considerations. These categories\nsubsequently serve as the basis for defining different levels of\nreproducibility. We manually extract the availability of these variables from a\ncurated dataset comprising 61 publications identified using the keywords\nprovided by biodiversity experts. Our study shows that the dataset is shared in\n47% of the publications; however, a significant number of the publications lack\ncomprehensive information on deep learning methods, including details regarding\nrandomness.",
        "translated": ""
    },
    {
        "title": "Metasurface-based Snapshot Shortwave-Infrared Hyperspectral Image\n  Reconstruction with Inter and Intra Prior Learning Network",
        "url": "http://arxiv.org/abs/2407.07503v1",
        "pub_date": "2024-07-10",
        "summary": "Shortwave-infrared(SWIR) spectral information,ranging from 1 {\\mu}m to\n2.5{\\mu}m, breaks the limitations of traditional color cameras in acquiring\nscene information and has been used in many fields. However, conventional SWIR\nhyperspectral imaging systems face challenges due to their bulky setups and low\nacquisition speed. In this work, we introduce a snapshot SWIR hyperspectral\nimaging system based on a metasurface filter and a corresponding filter\nselection method to achieve the lowest correlation coefficient among these\nfilters.This systemhas the advantages of small size and snapshot imaging. We\npropose a novel inter and intra prior learning unfolding framework proposed to\nachieve high-quality SWIR hyperspectral image reconstruction, which bridges the\ngap between prior learning and cross-stage information interaction. We also\ndesign an adaptive feature transfer mechanism to adaptively the transfer\ncontextual correlation of multi-scale encoder features to prevent detailed\ninformation loss in the decoder. Experiment results demonstrate that our method\ncan reconstruct HSI with high speed and superior performance over existing\nmethods.",
        "translated": ""
    },
    {
        "title": "Multi-objective Learning to Rank by Model Distillation",
        "url": "http://arxiv.org/abs/2407.07181v1",
        "pub_date": "2024-07-09",
        "summary": "In online marketplaces, search ranking's objective is not only to purchase or\nconversion (primary objective), but to also the purchase outcomes(secondary\nobjectives), e.g. order cancellation(or return), review rating, customer\nservice inquiries, platform long term growth. Multi-objective learning to rank\nhas been widely studied to balance primary and secondary objectives. But\ntraditional approaches in industry face some challenges including expensive\nparameter tuning leads to sub-optimal solution, suffering from imbalanced data\nsparsity issue, and being not compatible with ad-hoc objective. In this paper,\nwe propose a distillation-based ranking solution for multi-objective ranking,\nwhich optimizes the end-to-end ranking system at Airbnb across multiple ranking\nmodels on different objectives along with various considerations to optimize\ntraining and serving efficiency to meet industry standards. We found it\nperforms much better than traditional approaches, it doesn't only significantly\nincreases primary objective by a large margin but also meet secondary\nobjectives constraints and improve model stability. We also demonstrated the\nproposed system could be further simplified by model self-distillation. Besides\nthis, we did additional simulations to show that this approach could also help\nus efficiently inject ad-hoc non-differentiable business objective into the\nranking system while enabling us to balance our optimization objectives.",
        "translated": ""
    },
    {
        "title": "FAR-Trans: An Investment Dataset for Financial Asset Recommendation",
        "url": "http://arxiv.org/abs/2407.08692v1",
        "pub_date": "2024-07-11",
        "summary": "Financial asset recommendation (FAR) is a sub-domain of recommender systems\nwhich identifies useful financial securities for investors, with the\nexpectation that they will invest capital on the recommended assets. FAR\nsolutions analyse and learn from multiple data sources, including time series\npricing data, customer profile information and expectations, as well as past\ninvestments. However, most models have been developed over proprietary\ndatasets, making a comparison over a common benchmark impossible. In this\npaper, we aim to solve this problem by introducing FAR-Trans, the first public\ndataset for FAR, containing pricing information and retail investor\ntransactions acquired from a large European financial institution. We also\nprovide a bench-marking comparison between eleven FAR algorithms over the data\nfor use as future baselines. The dataset can be downloaded from\nhttps://doi.org/10.5525/gla.researchdata.1658 .",
        "translated": ""
    },
    {
        "title": "From Real to Cloned Singer Identification",
        "url": "http://arxiv.org/abs/2407.08647v1",
        "pub_date": "2024-07-11",
        "summary": "Cloned voices of popular singers sound increasingly realistic and have gained\npopularity over the past few years. They however pose a threat to the industry\ndue to personality rights concerns. As such, methods to identify the original\nsinger in synthetic voices are needed. In this paper, we investigate how singer\nidentification methods could be used for such a task. We present three\nembedding models that are trained using a singer-level contrastive learning\nscheme, where positive pairs consist of segments with vocals from the same\nsingers. These segments can be mixtures for the first model, vocals for the\nsecond, and both for the third. We demonstrate that all three models are highly\ncapable of identifying real singers. However, their performance deteriorates\nwhen classifying cloned versions of singers in our evaluation set. This is\nespecially true for models that use mixtures as an input. These findings\nhighlight the need to understand the biases that exist within singer\nidentification systems, and how they can influence the identification of voice\ndeepfakes in music.",
        "translated": ""
    },
    {
        "title": "Multi-Group Proportional Representation",
        "url": "http://arxiv.org/abs/2407.08571v1",
        "pub_date": "2024-07-11",
        "summary": "Image search and retrieval tasks can perpetuate harmful stereotypes, erase\ncultural identities, and amplify social disparities. Current approaches to\nmitigate these representational harms balance the number of retrieved items\nacross population groups defined by a small number of (often binary)\nattributes. However, most existing methods overlook intersectional groups\ndetermined by combinations of group attributes, such as gender, race, and\nethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel\nmetric that measures representation across intersectional groups. We develop\npractical methods for estimating MPR, provide theoretical guarantees, and\npropose optimization algorithms to ensure MPR in retrieval. We demonstrate that\nexisting methods optimizing for equal and proportional representation metrics\nmay fail to promote MPR. Crucially, our work shows that optimizing MPR yields\nmore proportional representation across multiple intersectional groups\nspecified by a rich function class, often with minimal compromise in retrieval\naccuracy.",
        "translated": ""
    },
    {
        "title": "ADMM Based Semi-Structured Pattern Pruning Framework For Transformer",
        "url": "http://arxiv.org/abs/2407.08334v1",
        "pub_date": "2024-07-11",
        "summary": "NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework on quantization to demonstrate\nits generalization and use SR-STE to avoid gradient vanishing problem. We\nconduct extensive experiments on classification tasks over GLUE datasets.\nSignificantly, we achieve 50% percent compression ratio while maintaining 55.4%\nMatthews correlation on COLA, 68.8% accuracy on RTE and overall score 80.1. Our\nframework also perform well on other tasks on GLUE datasets.",
        "translated": ""
    },
    {
        "title": "Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval\n  Augmented Generation Systems",
        "url": "http://arxiv.org/abs/2407.08275v1",
        "pub_date": "2024-07-11",
        "summary": "The choice of embedding model is a crucial step in the design of Retrieval\nAugmented Generation (RAG) systems. Given the sheer volume of available\noptions, identifying clusters of similar models streamlines this model\nselection process. Relying solely on benchmark performance scores only allows\nfor a weak assessment of model similarity. Thus, in this study, we evaluate the\nsimilarity of embedding models within the context of RAG systems. Our\nassessment is two-fold: We use Centered Kernel Alignment to compare embeddings\non a pair-wise level. Additionally, as it is especially pertinent to RAG\nsystems, we evaluate the similarity of retrieval results between these models\nusing Jaccard and rank similarity. We compare different families of embedding\nmodels, including proprietary ones, across five datasets from the popular\nBenchmark Information Retrieval (BEIR). Through our experiments we identify\nclusters of models corresponding to model families, but interestingly, also\nsome inter-family clusters. Furthermore, our analysis of top-k retrieval\nsimilarity reveals high-variance at low k values. We also identify possible\nopen-source alternatives to proprietary models, with Mistral exhibiting the\nhighest similarity to OpenAI models.",
        "translated": ""
    },
    {
        "title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs",
        "url": "http://arxiv.org/abs/2407.08227v1",
        "pub_date": "2024-07-11",
        "summary": "X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel technique to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. To address this, we introduce a pioneering approach to clinical\ndata augmentation that employs large language models (LLMs) to generate patient\ncontextual synthetic data. This methodology is crucial for training more robust\ndeep learning models in healthcare. It preserves the integrity of real patient\ndata while enriching the dataset with contextually relevant synthetic features,\nsignificantly enhancing model performance. DALL-M uses a three-phase feature\ngeneration process: (i) clinical context storage, (ii) expert query generation,\nand (iii) context-aware feature augmentation. DALL-M generates new, clinically\nrelevant features by synthesizing chest X-ray images and reports. Applied to\n799 cases using nine features from the MIMIC-IV dataset, it created an\naugmented set of 91 features. This is the first work to generate contextual\nvalues for existing and new features based on patients' X-ray reports, gender,\nand age and to produce new contextual knowledge during data augmentation.\nEmpirical validation with machine learning models, including Decision Trees,\nRandom Forests, XGBoost, and TabNET, showed significant performance\nimprovements. Incorporating augmented features increased the F1 score by 16.5%\nand Precision and Recall by approximately 25%. DALL-M addresses a critical gap\nin clinical data augmentation, offering a robust framework for generating\ncontextually enriched datasets.",
        "translated": ""
    },
    {
        "title": "CADC: Encoding User-Item Interactions for Compressing Recommendation\n  Model Training Data",
        "url": "http://arxiv.org/abs/2407.08108v1",
        "pub_date": "2024-07-11",
        "summary": "Deep learning recommendation models (DLRMs) are at the heart of the current\ne-commerce industry. However, the amount of training data used to train these\nlarge models is growing exponentially, leading to substantial training hurdles.\nThe training dataset contains two primary types of information: content-based\ninformation (features of users and items) and collaborative information\n(interactions between users and items). One approach to reduce the training\ndataset is to remove user-item interactions. But that significantly diminishes\ncollaborative information, which is crucial for maintaining accuracy due to its\ninclusion of interaction histories. This loss profoundly impacts DLRM\nperformance.\n  This paper makes an important observation that if one can capture the\nuser-item interaction history to enrich the user and item embeddings, then the\ninteraction history can be compressed without losing model accuracy. Thus, this\nwork, Collaborative Aware Data Compression (CADC), takes a two-step approach to\ntraining dataset compression. In the first step, we use matrix factorization of\nthe user-item interaction matrix to create a novel embedding representation for\nboth the users and items. Once the user and item embeddings are enriched by the\ninteraction history information the approach then applies uniform random\nsampling of the training dataset to drastically reduce the training dataset\nsize while minimizing model accuracy drop. The source code of CADC is available\nat\n\\href{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}.",
        "translated": ""
    },
    {
        "title": "FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in\n  Domain-specific Scenarios",
        "url": "http://arxiv.org/abs/2407.08035v1",
        "pub_date": "2024-07-10",
        "summary": "Large Language Models (LLMs) have provided a new pathway for Named Entity\nRecognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting\nmethods avoid the need for training, conserve substantial computational\nresources, and rely on minimal annotated data. Previous studies have achieved\ncomparable performance to fully supervised BERT-based fine-tuning approaches on\ngeneral NER benchmarks. However, none of the previous approaches has\ninvestigated the efficiency of LLM-based few-shot learning in domain-specific\nscenarios. To address this gap, we introduce FsPONER, a novel approach for\noptimizing few-shot prompts, and evaluate its performance on domain-specific\nNER datasets, with a focus on industrial manufacturing and maintenance, while\nusing multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna.\nFsPONER consists of three few-shot selection methods based on random sampling,\nTF-IDF vectors, and a combination of both. We compare these methods with a\ngeneral-purpose GPT-NER method as the number of few-shot examples increases and\nevaluate their optimal NER performance against fine-tuned BERT and LLaMA\n2-chat. In the considered real-world scenarios with data scarcity, FsPONER with\nTF-IDF surpasses fine-tuned models by approximately 10% in F1 score.",
        "translated": ""
    },
    {
        "title": "DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment",
        "url": "http://arxiv.org/abs/2407.08008v1",
        "pub_date": "2024-07-10",
        "summary": "We present working notes for DS@GT team in the eRisk 2024 for Tasks 1 and 3.\nWe propose a ranking system for Task 1 that predicts symptoms of depression\nbased on the Beck Depression Inventory (BDI-II) questionnaire using binary\nclassifiers trained on question relevancy as a proxy for ranking. We find that\nbinary classifiers are not well calibrated for ranking, and perform poorly\nduring evaluation. For Task 3, we use embeddings from BERT to predict the\nseverity of eating disorder symptoms based on user post history. We find that\nclassical machine learning models perform well on the task, and end up\ncompetitive with the baseline models. Representation of text data is crucial in\nboth tasks, and we find that sentence transformers are a powerful tool for\ndownstream modeling. Source code and models are available at\n\\url{https://github.com/dsgt-kaggle-clef/erisk-2024}.",
        "translated": ""
    },
    {
        "title": "Automated Neural Patent Landscaping in the Small Data Regime",
        "url": "http://arxiv.org/abs/2407.08001v1",
        "pub_date": "2024-07-10",
        "summary": "Patent landscaping is the process of identifying all patents related to a\nparticular technological area, and is important for assessing various aspects\nof the intellectual property context. Traditionally, constructing patent\nlandscapes is intensely laborious and expensive, and the rapid expansion of\npatenting activity in recent decades has driven an increasing need for\nefficient and effective automated patent landscaping approaches. In particular,\nit is critical that we be able to construct patent landscapes using a minimal\nnumber of labeled examples, as labeling patents for a narrow technology area\nrequires highly specialized (and hence expensive) technical knowledge. We\npresent an automated neural patent landscaping system that demonstrates\nsignificantly improved performance on difficult examples (0.69 $F_1$ on 'hard'\nexamples, versus 0.6 for previously reported systems), and also significant\nimprovements with much less training data (overall 0.75 $F_1$ on as few as 24\nexamples). Furthermore, in evaluating such automated landscaping systems,\nacquiring good data is challenge; we demonstrate a higher-quality training data\ngeneration procedure by merging Abood and Feltenberger's (2018)\n\"seed/anti-seed\" approach with active learning to collect difficult labeled\nexamples near the decision boundary. Using this procedure we created a new\ndataset of labeled AI patents for training and testing. As in prior work we\ncompare our approach with a number of baseline systems, and we release our code\nand data for others to build upon.",
        "translated": ""
    },
    {
        "title": "Mitigating Entity-Level Hallucination in Large Language Models",
        "url": "http://arxiv.org/abs/2407.09417v1",
        "pub_date": "2024-07-12",
        "summary": "The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.",
        "translated": ""
    },
    {
        "title": "Deep Bag-of-Words Model: An Efficient and Interpretable Relevance\n  Architecture for Chinese E-Commerce",
        "url": "http://arxiv.org/abs/2407.09395v1",
        "pub_date": "2024-07-12",
        "summary": "Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...",
        "translated": ""
    },
    {
        "title": "PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with\n  User-Centric Agents",
        "url": "http://arxiv.org/abs/2407.09394v1",
        "pub_date": "2024-07-12",
        "summary": "Large Language Models (LLMs) struggle with generating reliable outputs due to\noutdated knowledge and hallucinations. Retrieval-Augmented Generation (RAG)\nmodels address this by enhancing LLMs with external knowledge, but often fail\nto personalize the retrieval process. This paper introduces PersonaRAG, a novel\nframework incorporating user-centric agents to adapt retrieval and generation\nbased on real-time user data and interactions. Evaluated across various\nquestion answering datasets, PersonaRAG demonstrates superiority over baseline\nmodels, providing tailored answers to user needs. The results suggest promising\ndirections for user-adapted information retrieval systems.",
        "translated": ""
    },
    {
        "title": "Context Embeddings for Efficient Answer Generation in RAG",
        "url": "http://arxiv.org/abs/2407.09252v1",
        "pub_date": "2024-07-12",
        "summary": "Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.",
        "translated": ""
    },
    {
        "title": "Movie Recommendation with Poster Attention via Multi-modal Transformer\n  Feature Fusion",
        "url": "http://arxiv.org/abs/2407.09157v1",
        "pub_date": "2024-07-12",
        "summary": "Pre-trained models learn general representations from large datsets which can\nbe fine-turned for specific tasks to significantly reduce training time.\nPre-trained models like generative pretrained transformers (GPT), bidirectional\nencoder representations from transformers (BERT), vision transfomers (ViT) have\nbecome a cornerstone of current research in machine learning. This study\nproposes a multi-modal movie recommendation system by extract features of the\nwell designed posters for each movie and the narrative text description of the\nmovie. This system uses the BERT model to extract the information of text\nmodality, the ViT model applied to extract the information of poster/image\nmodality, and the Transformer architecture for feature fusion of all modalities\nto predict users' preference. The integration of pre-trained foundational\nmodels with some smaller data sets in downstream applications capture\nmulti-modal content features in a more comprehensive manner, thereby providing\nmore accurate recommendations. The efficiency of the proof-of-concept model is\nverified by the standard benchmark problem the MovieLens 100K and 1M datasets.\nThe prediction accuracy of user ratings is enhanced in comparison to the\nbaseline algorithm, thereby demonstrating the potential of this cross-modal\nalgorithm to be applied for movie or video recommendation.",
        "translated": ""
    },
    {
        "title": "AI-Powered Immersive Assistance for Interactive Task Execution in\n  Industrial Environments",
        "url": "http://arxiv.org/abs/2407.09147v1",
        "pub_date": "2024-07-12",
        "summary": "Many industrial sectors rely on well-trained employees that are able to\noperate complex machinery. In this work, we demonstrate an AI-powered immersive\nassistance system that supports users in performing complex tasks in industrial\nenvironments. Specifically, our system leverages a VR environment that\nresembles a juice mixer setup. This digital twin of a physical setup simulates\ncomplex industrial machinery used to mix preparations or liquids (e.g., similar\nto the pharmaceutical industry) and includes various containers, sensors,\npumps, and flow controllers. This setup demonstrates our system's capabilities\nin a controlled environment while acting as a proof-of-concept for broader\nindustrial applications. The core components of our multimodal AI assistant are\na large language model and a speech-to-text model that process a video and\naudio recording of an expert performing the task in a VR environment. The video\nand speech input extracted from the expert's video enables it to provide\nstep-by-step guidance to support users in executing complex tasks. This\ndemonstration showcases the potential of our AI-powered assistant to reduce\ncognitive load, increase productivity, and enhance safety in industrial\nenvironments.",
        "translated": ""
    },
    {
        "title": "Distinct citation distributions complicate research evaluations. A\n  single indicator that universally reveals research efficiency cannot be\n  formulated",
        "url": "http://arxiv.org/abs/2407.09138v1",
        "pub_date": "2024-07-12",
        "summary": "Purpose: Analyze the diversity of citation distributions to publications in\ndifferent research topics to investigate the accuracy of size-independent,\nrank-based indicators. Top percentile-based indicators are the most common\nindicators of this type, and the evaluations of Japan are the most evident\nmisjudgments. Design/methodology/approach: The distributions of citations to\npublications from countries and in journals in several research topics were\nanalyzed along with the corresponding global publications using histograms with\nlogarithmic binning, double rank plots, and normal probability plots of\nlog-transformed numbers of citations. Findings: Size-independent, top\npercentile-based indicators are accurate when the global ranks of local\npublications fit a power law, but deviations in the least cited papers are\nfrequent in countries and occur in all journals with high impact factors. In\nthese cases, a single indicator is misleading. Comparisons of proportions of\nuncited papers are the best way to predict these deviations. Research\nlimitations: The study is fundamentally analytical; its results describe\nmathematical facts that are self-evident. Practical implications: Respectable\ninstitutions, such as the OECD, European Commission, US National Science Board,\nand others, produce research country rankings and individual evaluations using\nsize-independent percentile indicators that are misleading in many countries.\nThese misleading evaluations should be discontinued because they cause\nconfusion among research policymakers and lead to incorrect research policies.\nOriginality/value: Studies linking the lower tail of citation distribution,\nincluding uncited papers, to percentile research indicators have not been\nperformed previously. The present results demonstrate that studies of this type\nare necessary to find reliable procedures for research assessments.",
        "translated": ""
    },
    {
        "title": "A Look Into News Avoidance Through AWRS: An Avoidance-Aware Recommender\n  System",
        "url": "http://arxiv.org/abs/2407.09137v1",
        "pub_date": "2024-07-12",
        "summary": "In recent years, journalists have expressed concerns about the increasing\ntrend of news article avoidance, especially within specific domains. This issue\nhas been exacerbated by the rise of recommender systems. Our research indicates\nthat recommender systems should consider avoidance as a fundamental factor. We\nargue that news articles can be characterized by three principal elements:\nexposure, relevance, and avoidance, all of which are closely interconnected. To\naddress these challenges, we introduce AWRS, an Avoidance-Aware Recommender\nSystem. This framework incorporates avoidance awareness when recommending news,\nbased on the premise that news article avoidance conveys significant\ninformation about user preferences. Evaluation results on three news datasets\nin different languages (English, Norwegian, and Japanese) demonstrate that our\nmethod outperforms existing approaches.",
        "translated": ""
    },
    {
        "title": "Multi-Modal Dataset Creation for Federated~Learning with DICOM\n  Structured Reports",
        "url": "http://arxiv.org/abs/2407.09064v1",
        "pub_date": "2024-07-12",
        "summary": "Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.",
        "translated": ""
    },
    {
        "title": "Time-Frequency Analysis of Variable-Length WiFi CSI Signals for Person\n  Re-Identification",
        "url": "http://arxiv.org/abs/2407.09045v1",
        "pub_date": "2024-07-12",
        "summary": "Person re-identification (ReID), as a crucial technology in the field of\nsecurity, plays an important role in security detection and people counting.\nCurrent security and monitoring systems largely rely on visual information,\nwhich may infringe on personal privacy and be susceptible to interference from\npedestrian appearances and clothing in certain scenarios. Meanwhile, the\nwidespread use of routers offers new possibilities for ReID. This letter\nintroduces a method using WiFi Channel State Information (CSI), leveraging the\nmultipath propagation characteristics of WiFi signals as a basis for\ndistinguishing different pedestrian features. We propose a two-stream network\nstructure capable of processing variable-length data, which analyzes the\namplitude in the time domain and the phase in the frequency domain of WiFi\nsignals, fuses time-frequency information through continuous lateral\nconnections, and employs advanced objective functions for representation and\nmetric learning. Tested on a dataset collected in the real world, our method\nachieves 93.68% mAP and 98.13% Rank-1.",
        "translated": ""
    },
    {
        "title": "BiasScanner: Automatic Detection and Classification of News Bias to\n  Strengthen Democracy",
        "url": "http://arxiv.org/abs/2407.10829v1",
        "pub_date": "2024-07-15",
        "summary": "The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).",
        "translated": ""
    },
    {
        "title": "SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate\n  Retrieval for Lifelong Sequential Recommendation",
        "url": "http://arxiv.org/abs/2407.10714v1",
        "pub_date": "2024-07-15",
        "summary": "The modeling of users' behaviors is crucial in modern recommendation systems.\nA lot of research focuses on modeling users' lifelong sequences, which can be\nextremely long and sometimes exceed thousands of items. These models use the\ntarget item to search for the most relevant items from the historical sequence.\nHowever, training lifelong sequences in click through rate (CTR) prediction or\npersonalized search ranking (PSR) is extremely difficult due to the\ninsufficient learning problem of ID embedding, especially when the IDs in the\nlifelong sequence features do not exist in the samples of training dataset.\nAdditionally, existing target attention mechanisms struggle to learn the\nmulti-modal representations of items in the sequence well. The distribution of\nmulti-modal embedding (text, image and attributes) output of user's interacted\nitems are not properly aligned and there exist divergence across modalities. We\nalso observe that users' search query sequences and item browsing sequences can\nfully depict users' intents and benefit from each other. To address these\nchallenges, we propose a unified lifelong multi-modal sequence model called\nSEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.\nSpecifically, a network called Pretraining Search Unit (PSU) learns the\nlifelong sequences of multi-modal query-item pairs in a pretraining-finetuning\nmanner with multiple objectives: multi-modal alignment, next query-item pair\nprediction, query-item relevance prediction, etc. After pretraining, the\ndownstream model restores the pretrained embedding as initialization and\nfinetunes the network. To accelerate the online retrieval speed of multi-modal\nembedding, we propose a multi-modal codebook-based product quantization\nstrategy to approximate the exact attention calculati",
        "translated": ""
    },
    {
        "title": "$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity",
        "url": "http://arxiv.org/abs/2407.10691v1",
        "pub_date": "2024-07-15",
        "summary": "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,\nrespectively, averaged on queries containing multiple subqueries from five\nscientific retrieval datasets. Moreover, the efficacy of two downstream\nscientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$to boost the application of LLMs in the scientific domain.",
        "translated": ""
    },
    {
        "title": "General algorithm of assigning raster features to vector maps at any\n  resolution or scale",
        "url": "http://arxiv.org/abs/2407.10599v1",
        "pub_date": "2024-07-15",
        "summary": "The fusion of multi-source data is essential for a comprehensive analysis of\ngeographic applications. Due to distinct data structures, the fusion process\ntends to encounter technical difficulties in terms of preservation of the\nintactness of each source data. Furthermore, a lack of generalized methods is a\nproblem when the method is expected to be applicable in multiple resolutions,\nsizes, or scales of raster and vector data, to what is being processed. In this\nstudy, we propose a general algorithm of assigning features from raster data\n(concentrations of air pollutants) to vector components (roads represented by\nedges) in city maps through the iterative construction of virtual layers to\nexpand geolocation from a city centre to boundaries in a 2D projected map. The\nconstruction follows the rule of perfect squares with a slight difference\ndepending on the oddness or evenness of the ratio of city size to raster\nresolution. We demonstrate the algorithm by applying it to assign accurate\nPM$_{2.5}$ and NO$_{2}$ concentrations to roads in 1692 cities globally for a\npotential graph-based pollution analysis. This method could pave the way for\nagile studies on urgent climate issues by providing a generic and efficient\nmethod to accurately fuse multiple datasets of varying scales and compositions.",
        "translated": ""
    },
    {
        "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
        "url": "http://arxiv.org/abs/2407.10380v1",
        "pub_date": "2024-07-15",
        "summary": "Cognitive textual and visual reasoning tasks, such as puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. While LLMs and VLMs, through extensive\ntraining on large amounts of human-curated data, have attained a high level of\npseudo-human intelligence in some common sense reasoning tasks, they still\nstruggle with more complex reasoning tasks that require cognitive\nunderstanding. In this work, we introduce a new dataset, NTSEBench, designed to\nevaluate the cognitive multi-modal reasoning and problem-solving skills of\nlarge models. The dataset comprises 2,728 multiple-choice questions comprising\nof a total of 4,642 images across 26 categories sampled from the NTSE\nexamination conducted nationwide in India, featuring both visual and textual\ngeneral aptitude questions that do not rely on rote learning. We establish\nbaselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a\ncomparison between open source and propriety models, we propose four distinct\nmodeling strategies to handle different modalities (text and images) in the\ndataset instances.",
        "translated": ""
    },
    {
        "title": "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems",
        "url": "http://arxiv.org/abs/2407.10283v1",
        "pub_date": "2024-07-14",
        "summary": "Quantitative information plays a crucial role in understanding and\ninterpreting the content of documents. Many user queries contain quantities and\ncannot be resolved without understanding their semantics, e.g., ``car that\ncosts less than $10k''. Yet, modern search engines apply the same ranking\nmechanisms for both words and quantities, overlooking magnitude and unit\ninformation. In this paper, we introduce two quantity-aware ranking techniques\ndesigned to rank both the quantity and textual content either jointly or\nindependently. These techniques incorporate quantity information in available\nretrieval systems and can address queries with numerical conditions equal,\ngreater than, and less than. To evaluate the effectiveness of our proposed\nmodels, we introduce two novel quantity-aware benchmark datasets in the domains\nof finance and medicine and compare our method against various lexical and\nneural models. The code and data are available under\nhttps://github.com/satya77/QuantityAwareRankers.",
        "translated": ""
    },
    {
        "title": "GenSco: Can Question Decomposition based Passage Alignment improve\n  Question Answering?",
        "url": "http://arxiv.org/abs/2407.10245v1",
        "pub_date": "2024-07-14",
        "summary": "Retrieval augmented generation (RAG) with large language models (LLMs) for\nQuestion Answering (QA) entails furnishing relevant context within the prompt\nto facilitate the LLM in answer generation. During the generation, inaccuracies\nor hallucinations frequently occur due to two primary factors: inadequate or\ndistracting context in the prompts, and the inability of LLMs to effectively\nreason through the facts. In this paper, we investigate whether providing\naligned context via a carefully selected passage sequence leads to better\nanswer generation by the LLM for multi-hop QA. We introduce, \"GenSco\", a novel\napproach of selecting passages based on the predicted decomposition of the\nmulti-hop questions}. The framework consists of two distinct LLMs: (i)\nGenerator LLM, which is used for question decomposition and final answer\ngeneration; (ii) an auxiliary open-sourced LLM, used as the scorer, to\nsemantically guide the Generator for passage selection. The generator is\ninvoked only once for the answer generation, resulting in a cost-effective and\nefficient approach. We evaluate on three broadly established multi-hop question\nanswering datasets: 2WikiMultiHop, Adversarial HotPotQA and MuSiQue and achieve\nan absolute gain of $15.1$ and $5.9$ points in Exact Match score with respect\nto the best performing baselines over MuSiQue and 2WikiMultiHop respectively.",
        "translated": ""
    },
    {
        "title": "Towards Robust Recommendation via Decision Boundary-aware Graph\n  Contrastive Learning",
        "url": "http://arxiv.org/abs/2407.10184v1",
        "pub_date": "2024-07-14",
        "summary": "In recent years, graph contrastive learning (GCL) has received increasing\nattention in recommender systems due to its effectiveness in reducing bias\ncaused by data sparsity. However, most existing GCL models rely on heuristic\napproaches and usually assume entity independence when constructing contrastive\nviews. We argue that these methods struggle to strike a balance between\nsemantic invariance and view hardness across the dynamic training process, both\nof which are critical factors in graph contrastive learning.\n  To address the above issues, we propose a novel GCL-based recommendation\nframework RGCL, which effectively maintains the semantic invariance of\ncontrastive pairs and dynamically adapts as the model capability evolves\nthrough the training process. Specifically, RGCL first introduces decision\nboundary-aware adversarial perturbations to constrain the exploration space of\ncontrastive augmented views, avoiding the decrease of task-specific\ninformation. Furthermore, to incorporate global user-user and item-item\ncollaboration relationships for guiding on the generation of hard contrastive\nviews, we propose an adversarial-contrastive learning objective to construct a\nrelation-aware view-generator. Besides, considering that unsupervised GCL could\npotentially narrower margins between data points and the decision boundary,\nresulting in decreased model robustness, we introduce the adversarial examples\nbased on maximum perturbations to achieve margin maximization. We also provide\ntheoretical analyses on the effectiveness of our designs. Through extensive\nexperiments on five public datasets, we demonstrate the superiority of RGCL\ncompared against twelve baseline models.",
        "translated": ""
    },
    {
        "title": "Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature\n  Interactions",
        "url": "http://arxiv.org/abs/2407.10112v1",
        "pub_date": "2024-07-14",
        "summary": "In recommendation systems, new items are continuously introduced, initially\nlacking interaction records but gradually accumulating them over time.\nAccurately predicting the click-through rate (CTR) for these items is crucial\nfor enhancing both revenue and user experience. While existing methods focus on\nenhancing item ID embeddings for new items within general CTR models, they tend\nto adopt a global feature interaction approach, often overshadowing new items\nwith sparse data by those with abundant interactions. Addressing this, our work\nintroduces EmerG, a novel approach that warms up cold-start CTR prediction by\nlearning item-specific feature interaction patterns. EmerG utilizes\nhypernetworks to generate an item-specific feature graph based on item\ncharacteristics, which is then processed by a Graph Neural Network (GNN). This\nGNN is specially tailored to provably capture feature interactions at any order\nthrough a customized message passing mechanism. We further design a meta\nlearning strategy that optimizes parameters of hypernetworks and GNN across\nvarious item CTR prediction tasks, while only adjusting a minimal set of\nitem-specific parameters within each task. This strategy effectively reduces\nthe risk of overfitting when dealing with limited data. Extensive experiments\non benchmark datasets validate that EmerG consistently performs the best given\nno, a few and sufficient instances of new items.",
        "translated": ""
    },
    {
        "title": "All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems\n  Across the LLM Era",
        "url": "http://arxiv.org/abs/2407.10081v1",
        "pub_date": "2024-07-14",
        "summary": "Recommender systems (RS) are vital for managing information overload and\ndelivering personalized content, responding to users' diverse information\nneeds. The emergence of large language models (LLMs) offers a new horizon for\nredefining recommender systems with vast general knowledge and reasoning\ncapabilities. Standing across this LLM era, we aim to integrate recommender\nsystems into a broader picture, and pave the way for more comprehensive\nsolutions for future research. Therefore, we first offer a comprehensive\noverview of the technical progression of recommender systems, particularly\nfocusing on language foundation models and their applications in\nrecommendation. We identify two evolution paths of modern recommender systems\n-- via list-wise recommendation and conversational recommendation. These two\npaths finally converge at LLM agents with superior capabilities of long-term\nmemory, reflection, and tool intelligence. Along these two paths, we point out\nthat the information effectiveness of the recommendation is increased, while\nthe user's acquisition cost is decreased. Technical features, research\nmethodologies, and inherent challenges for each milestone along the path are\ncarefully investigated -- from traditional list-wise recommendation to\nLLM-enhanced recommendation to recommendation with LLM agents. Finally, we\nhighlight several unresolved challenges crucial for the development of future\npersonalization technologies and interfaces and discuss the future prospects.",
        "translated": ""
    },
    {
        "title": "Harnessing Large Language Models for Multimodal Product Bundling",
        "url": "http://arxiv.org/abs/2407.11712v1",
        "pub_date": "2024-07-16",
        "summary": "Product bundling provides clients with a strategic combination of individual\nitems.And it has gained significant attention in recent years as a fundamental\nprerequisite for online services. Recent methods utilize multimodal information\nthrough sophisticated extractors for bundling, but remain limited by inferior\nsemantic understanding, the restricted scope of knowledge, and an inability to\nhandle cold-start issues.Despite the extensive knowledge and complex reasoning\ncapabilities of large language models (LLMs), their direct utilization fails to\nprocess multimodalities and exploit their knowledge for multimodal product\nbundling. Adapting LLMs for this purpose involves demonstrating the synergies\namong different modalities and designing an effective optimization strategy for\nbundling, which remains challenging.To this end, we introduce Bundle-LLM to\nbridge the gap between LLMs and product bundling tasks. Sepcifically, we\nutilize a hybrid item tokenization to integrate multimodal information, where a\nsimple yet powerful multimodal fusion module followed by a trainable projector\nembeds all non-textual features into a single token. This module not only\nexplicitly exhibits the interplays among modalities but also shortens the\nprompt length, thereby boosting efficiency.By designing a prompt template, we\nformulate product bundling as a multiple-choice question given candidate items.\nFurthermore, we adopt progressive optimization strategy to fine-tune the LLMs\nfor disentangled objectives, achieving effective product bundling capability\nwith comprehensive multimodal semantic understanding.Extensive experiments on\nfour datasets from two application domains show that our approach outperforms a\nrange of state-of-the-art (SOTA) methods.",
        "translated": ""
    },
    {
        "title": "A Comprehensive Evaluation of Large Language Models on Temporal Event\n  Forecasting",
        "url": "http://arxiv.org/abs/2407.11638v1",
        "pub_date": "2024-07-16",
        "summary": "Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation(RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast,\nincorporating raw texts in specific complex events and fine-tuning LLMs\nsignificantly improves performance. Moreover, enhanced with retrieval modules,\nLLM can effectively capture temporal relational patterns hidden in historical\nevents. Meanwhile, issues such as popularity bias and the long-tail problem\nstill persist in LLMs, particularly in the RAG-based method. These findings not\nonly deepen our understanding of LLM-based event forecasting methods but also\nhighlight several promising research directions.We consider that this\ncomprehensive evaluation, along with the identified research opportunities,\nwill significantly contribute to future research on temporal event forecasting\nthrough LLMs.",
        "translated": ""
    },
    {
        "title": "Interactions with Generative Information Retrieval Systems",
        "url": "http://arxiv.org/abs/2407.11605v1",
        "pub_date": "2024-07-16",
        "summary": "At its core, information access and seeking is an interactive process. In\nexisting search engines, interactions are limited to a few pre-defined actions,\nsuch as \"requery\", \"click on a document\", \"scrolling up/down\", \"going to the\nnext result page\", \"leaving the search engine\", etc. A major benefit of moving\ntowards generative IR systems is enabling users with a richer expression of\ninformation need and feedback and free-form interactions in natural language\nand beyond. In other words, the actions users take are no longer limited by the\nclickable links and buttons available on the search engine result page and\nusers can express themselves freely through natural language. This can go even\nbeyond natural language, through images, videos, gestures, and sensors using\nmulti-modal generative IR systems. This chapter briefly discusses the role of\ninteraction in generative IR systems. We will first discuss different ways\nusers can express their information needs by interacting with generative IR\nsystems. We then explain how users can provide explicit or implicit feedback to\ngenerative IR systems and how they can consume such feedback. Next, we will\ncover how users interactively can refine retrieval results. We will expand upon\nmixed-initiative interactions and discuss clarification and preference\nelicitation in more detail. We then discuss proactive generative IR systems,\nincluding context-aware recommendation, following up past conversations,\ncontributing to multi-party conversations, and feedback requests. Providing\nexplanation is another interaction type that we briefly discuss in this\nchapter. We will also briefly describe multi-modal interactions in generative\ninformation retrieval. Finally, we describe emerging frameworks and solutions\nfor user interfaces with generative AI systems.",
        "translated": ""
    },
    {
        "title": "A PLMs based protein retrieval framework",
        "url": "http://arxiv.org/abs/2407.11548v1",
        "pub_date": "2024-07-16",
        "summary": "Protein retrieval, which targets the deconstruction of the relationship\nbetween sequences, structures and functions, empowers the advancing of biology.\nBasic Local Alignment Search Tool (BLAST), a sequence-similarity-based\nalgorithm, has proved the efficiency of this field. Despite the existing tools\nfor protein retrieval, they prioritize sequence similarity and probably\noverlook proteins that are dissimilar but share homology or functionality. In\norder to tackle this problem, we propose a novel protein retrieval framework\nthat mitigates the bias towards sequence similarity. Our framework initiatively\nharnesses protein language models (PLMs) to embed protein sequences within a\nhigh-dimensional feature space, thereby enhancing the representation capacity\nfor subsequent analysis. Subsequently, an accelerated indexed vector database\nis constructed to facilitate expedited access and retrieval of dense vectors.\nExtensive experiments demonstrate that our framework can equally retrieve both\nsimilar and dissimilar proteins. Moreover, this approach enables the\nidentification of proteins that conventional methods fail to uncover. This\nframework will effectively assist in protein mining and empower the development\nof biology.",
        "translated": ""
    },
    {
        "title": "Bootstrapped Pre-training with Dynamic Identifier Prediction for\n  Generative Retrieval",
        "url": "http://arxiv.org/abs/2407.11504v1",
        "pub_date": "2024-07-16",
        "summary": "Generative retrieval uses differentiable search indexes to directly generate\nrelevant document identifiers in response to a query. Recent studies have\nhighlighted the potential of a strong generative retrieval model, trained with\ncarefully crafted pre-training tasks, to enhance downstream retrieval tasks via\nfine-tuning. However, the full power of pre-training for generative retrieval\nremains underexploited due to its reliance on pre-defined static document\nidentifiers, which may not align with evolving model parameters. In this work,\nwe introduce BootRet, a bootstrapped pre-training method for generative\nretrieval that dynamically adjusts document identifiers during pre-training to\naccommodate the continuing memorization of the corpus. BootRet involves three\nkey training phases: (i) initial identifier generation, (ii) pre-training via\ncorpus indexing and relevance prediction tasks, and (iii) bootstrapping for\nidentifier updates. To facilitate the pre-training phase, we further introduce\nnoisy documents and pseudo-queries, generated by large language models, to\nresemble semantic connections in both indexing and retrieval tasks.\nExperimental results demonstrate that BootRet significantly outperforms\nexisting pre-training generative retrieval baselines and performs well even in\nzero-shot settings.",
        "translated": ""
    },
    {
        "title": "EndoFinder: Online Image Retrieval for Explainable Colorectal Polyp\n  Diagnosis",
        "url": "http://arxiv.org/abs/2407.11401v1",
        "pub_date": "2024-07-16",
        "summary": "Determining the necessity of resecting malignant polyps during colonoscopy\nscreen is crucial for patient outcomes, yet challenging due to the\ntime-consuming and costly nature of histopathology examination. While deep\nlearning-based classification models have shown promise in achieving optical\nbiopsy with endoscopic images, they often suffer from a lack of explainability.\nTo overcome this limitation, we introduce EndoFinder, a content-based image\nretrieval framework to find the 'digital twin' polyp in the reference database\ngiven a newly detected polyp. The clinical semantics of the new polyp can be\ninferred referring to the matched ones. EndoFinder pioneers a polyp-aware image\nencoder that is pre-trained on a large polyp dataset in a self-supervised way,\nmerging masked image modeling with contrastive learning. This results in a\ngeneric embedding space ready for different downstream clinical tasks based on\nimage retrieval. We validate the framework on polyp re-identification and\noptical biopsy tasks, with extensive experiments demonstrating that EndoFinder\nnot only achieves explainable diagnostics but also matches the performance of\nsupervised classification models. EndoFinder's reliance on image retrieval has\nthe potential to support diverse downstream decision-making tasks during\nreal-time colonoscopy procedures.",
        "translated": ""
    },
    {
        "title": "Pacer and Runner: Cooperative Learning Framework between Single- and\n  Cross-Domain Sequential Recommendation",
        "url": "http://arxiv.org/abs/2407.11245v1",
        "pub_date": "2024-07-15",
        "summary": "Cross-Domain Sequential Recommendation (CDSR) improves recommendation\nperformance by utilizing information from multiple domains, which contrasts\nwith Single-Domain Sequential Recommendation (SDSR) that relies on a historical\ninteraction within a specific domain. However, CDSR may underperform compared\nto the SDSR approach in certain domains due to negative transfer, which occurs\nwhen there is a lack of relation between domains or different levels of data\nsparsity. To address the issue of negative transfer, our proposed CDSR model\nestimates the degree of negative transfer of each domain and adaptively assigns\nit as a weight factor to the prediction loss, to control gradient flows through\ndomains with significant negative transfer. To this end, our model compares the\nperformance of a model trained on multiple domains (CDSR) with a model trained\nsolely on the specific domain (SDSR) to evaluate the negative transfer of each\ndomain using our asymmetric cooperative network. In addition, to facilitate the\ntransfer of valuable cues between the SDSR and CDSR tasks, we developed an\nauxiliary loss that maximizes the mutual information between the representation\npairs from both tasks on a per-domain basis. This cooperative learning between\nSDSR and CDSR tasks is similar to the collaborative dynamics between pacers and\nrunners in a marathon. Our model outperformed numerous previous works in\nextensive experiments on two real-world industrial datasets across ten service\ndomains. We also have deployed our model in the recommendation system of our\npersonal assistant app service, resulting in 21.4% increase in click-through\nrate compared to existing models, which is valuable to real-world business.",
        "translated": ""
    },
    {
        "title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge\n  Bases",
        "url": "http://arxiv.org/abs/2407.12784v1",
        "pub_date": "2024-07-17",
        "summary": "LLM agents have demonstrated remarkable performance across various\napplications, primarily due to their advanced capabilities in reasoning,\nutilizing external knowledge and tools, calling APIs, and executing actions to\ninteract with environments. Current agents typically utilize a memory module or\na retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and\ninstances with similar embeddings from knowledge bases to inform task planning\nand execution. However, the reliance on unverified knowledge bases raises\nsignificant concerns about their safety and trustworthiness. To uncover such\nvulnerabilities, we propose a novel red teaming approach AgentPoison, the first\nbackdoor attack targeting generic and RAG-based LLM agents by poisoning their\nlong-term memory or RAG knowledge base. In particular, we form the trigger\ngeneration process as a constrained optimization to optimize backdoor triggers\nby mapping the triggered instances to a unique embedding space, so as to ensure\nthat whenever a user instruction contains the optimized backdoor trigger, the\nmalicious demonstrations are retrieved from the poisoned memory or knowledge\nbase with high probability. In the meantime, benign instructions without the\ntrigger will still maintain normal performance. Unlike conventional backdoor\nattacks, AgentPoison requires no additional model training or fine-tuning, and\nthe optimized backdoor trigger exhibits superior transferability, in-context\ncoherence, and stealthiness. Extensive experiments demonstrate AgentPoison's\neffectiveness in attacking three types of real-world LLM agents: RAG-based\nautonomous driving agent, knowledge-intensive QA agent, and healthcare\nEHRAgent. On each agent, AgentPoison achieves an average attack success rate\nhigher than 80% with minimal impact on benign performance (less than 1%) with a\npoison rate less than 0.1%.",
        "translated": ""
    },
    {
        "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2407.12580v1",
        "pub_date": "2024-07-17",
        "summary": "Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.",
        "translated": ""
    },
    {
        "title": "Search Engines, LLMs or Both? Evaluating Information Seeking Strategies\n  for Answering Health Questions",
        "url": "http://arxiv.org/abs/2407.12468v1",
        "pub_date": "2024-07-17",
        "summary": "Search engines have traditionally served as primary tools for information\nseeking. However, the new Large Language Models (LLMs) have recently\ndemonstrated remarkable capabilities in multiple tasks and, specifically, their\nadoption as question answering systems is becoming increasingly prevalent. It\nis expected that LLM-based conversational systems and traditional web engines\nwill continue to coexist in the future, supporting end users in various ways.\nBut there is a need for more scientific research on the effectiveness of both\ntypes of systems in facilitating accurate information seeking. In this study,\nwe focus on their merits in answering health questions. We conducted an\nextensive study comparing different web search engines, LLMs and\nretrieval-augmented (RAG) approaches. Our research reveals intriguing\nconclusions. For example, we observed that the quality of webpages potentially\nresponding to a health question does not decline as we navigate further down\nthe ranked lists. However, according to our evaluation, web engines are less\naccurate than LLMs in finding correct answers to health questions. On the other\nhand, LLMs are quite sensitive to the input prompts, and we also found out that\nRAG leads to highly effective information seeking methods.",
        "translated": ""
    },
    {
        "title": "RankTower: A Synergistic Framework for Enhancing Two-Tower Pre-Ranking\n  Model",
        "url": "http://arxiv.org/abs/2407.12385v1",
        "pub_date": "2024-07-17",
        "summary": "In large-scale ranking systems, cascading architectures have been widely\nadopted to achieve a balance between efficiency and effectiveness. The\npre-ranking module plays a vital role in selecting a subset of candidates for\nthe subsequent ranking module. It is crucial for the pre-ranking model to\nmaintain a balance between efficiency and accuracy to adhere to online latency\nconstraints. In this paper, we propose a novel neural network architecture\ncalled RankTower, which is designed to efficiently capture user-item\ninteractions while following the user-item decoupling paradigm to ensure online\ninference efficiency. The proposed approach employs a hybrid training objective\nthat learns from samples obtained from the full stage of the cascade ranking\nsystem, optimizing different objectives for varying sample spaces. This\nstrategy aims to enhance the pre-ranking model's ranking capability and\nimprovement alignment with the existing cascade ranking system. Experimental\nresults conducted on public datasets demonstrate that RankTower significantly\noutperforms state-of-the-art pre-ranking models.",
        "translated": ""
    },
    {
        "title": "Graph Signal Processing for Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2407.12374v1",
        "pub_date": "2024-07-17",
        "summary": "Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.",
        "translated": ""
    },
    {
        "title": "Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval",
        "url": "http://arxiv.org/abs/2407.12346v1",
        "pub_date": "2024-07-17",
        "summary": "The pre-trained vision and language (V\\&amp;L) models have substantially improved\nthe performance of cross-modal image-text retrieval. In general, however, V\\&amp;L\nmodels have limited retrieval performance for small objects because of the\nrough alignment between words and the small objects in the image. In contrast,\nit is known that human cognition is object-centric, and we pay more attention\nto important objects, even if they are small. To bridge this gap between the\nhuman cognition and the V\\&amp;L model's capability, we propose a cross-modal\nimage-text retrieval framework based on ``object-aware query perturbation.''\nThe proposed method generates a key feature subspace of the detected objects\nand perturbs the corresponding queries using this subspace to improve the\nobject awareness in the image. In our proposed method, object-aware cross-modal\nimage-text retrieval is possible while keeping the rich expressive power and\nretrieval performance of existing V\\&amp;L models without additional fine-tuning.\nComprehensive experiments on four public datasets show that our method\noutperforms conventional algorithms.",
        "translated": ""
    },
    {
        "title": "GUME: Graphs and User Modalities Enhancement for Long-Tail Multimodal\n  Recommendation",
        "url": "http://arxiv.org/abs/2407.12338v1",
        "pub_date": "2024-07-17",
        "summary": "Multimodal recommendation systems (MMRS) have received considerable attention\nfrom the research community due to their ability to jointly utilize information\nfrom user behavior and product images and text. Previous research has two main\nissues. First, many long-tail items in recommendation systems have limited\ninteraction data, making it difficult to learn comprehensive and informative\nrepresentations. However, past MMRS studies have overlooked this issue.\nSecondly, users' modality preferences are crucial to their behavior. However,\nprevious research has primarily focused on learning item modality\nrepresentations, while user modality representations have remained relatively\nsimplistic.To address these challenges, we propose a novel Graphs and User\nModalities Enhancement (GUME) for long-tail multimodal recommendation.\nSpecifically, we first enhance the user-item graph using multimodal similarity\nbetween items. This improves the connectivity of long-tail items and helps them\nlearn high-quality representations through graph propagation. Then, we\nconstruct two types of user modalities: explicit interaction features and\nextended interest features. By using the user modality enhancement strategy to\nmaximize mutual information between these two features, we improve the\ngeneralization ability of user modality representations. Additionally, we\ndesign an alignment strategy for modality data to remove noise from both\ninternal and external perspectives. Extensive experiments on four publicly\navailable datasets demonstrate the effectiveness of our approach.",
        "translated": ""
    },
    {
        "title": "Optimizing Query Generation for Enhanced Document Retrieval in RAG",
        "url": "http://arxiv.org/abs/2407.12325v1",
        "pub_date": "2024-07-17",
        "summary": "Large Language Models (LLMs) excel in various language tasks but they often\ngenerate incorrect information, a phenomenon known as \"hallucinations\".\nRetrieval-Augmented Generation (RAG) aims to mitigate this by using document\nretrieval for accurate responses. However, RAG still faces hallucinations due\nto vague queries. This study aims to improve RAG by optimizing query generation\nwith a query-document alignment score, refining queries using LLMs for better\nprecision and efficiency of document retrieval. Experiments have shown that our\napproach improves document retrieval, resulting in an average accuracy gain of\n1.6%.",
        "translated": ""
    },
    {
        "title": "ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via\n  Modal Fusion Map",
        "url": "http://arxiv.org/abs/2407.12315v1",
        "pub_date": "2024-07-17",
        "summary": "Multi-modal embeddings form the foundation for vision-language models, such\nas CLIP embeddings, the most widely used text-image embeddings. However, these\nembeddings are vulnerable to subtle misalignment of cross-modal features,\nresulting in decreased model performance and diminished generalization. To\naddress this problem, we design ModalChorus, an interactive system for visual\nprobing and alignment of multi-modal embeddings. ModalChorus primarily offers a\ntwo-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel\nparametric dimensionality reduction method that integrates both metric and\nnonmetric objectives to enhance modality fusion; and 2) embedding alignment\nthat allows users to interactively articulate intentions for both point-set and\nset-set alignments. Quantitative and qualitative comparisons for CLIP\nembeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and\ndata fusion (e.g., data context map) methods demonstrate the advantages of MFM\nin showcasing cross-modal features over common vision-language datasets. Case\nstudies reveal that ModalChorus can facilitate intuitive discovery of\nmisalignment and efficient re-alignment in scenarios ranging from zero-shot\nclassification to cross-modal retrieval and generation.",
        "translated": ""
    },
    {
        "title": "Mindful-RAG: A Study of Points of Failure in Retrieval Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2407.12216v1",
        "pub_date": "2024-07-16",
        "summary": "Large Language Models (LLMs) are proficient at generating coherent and\ncontextually relevant text but face challenges when addressing\nknowledge-intensive queries in domain-specific and factual question-answering\ntasks. Retrieval-augmented generation (RAG) systems mitigate this by\nincorporating external knowledge sources, such as structured knowledge graphs\n(KGs). However, LLMs often struggle to produce accurate answers despite access\nto KG-extracted information containing necessary facts. Our study investigates\nthis dilemma by analyzing error patterns in existing KG-based RAG methods and\nidentifying eight critical failure points. We observed that these errors\npredominantly occur due to insufficient focus on discerning the question's\nintent and adequately gathering relevant context from the knowledge graph\nfacts. Drawing on this analysis, we propose the Mindful-RAG approach, a\nframework designed for intent-based and contextually aligned knowledge\nretrieval. This method explicitly targets the identified failures and offers\nimprovements in the correctness and relevance of responses provided by LLMs,\nrepresenting a significant step forward from existing methods.",
        "translated": ""
    },
    {
        "title": "CellularLint: A Systematic Approach to Identify Inconsistent Behavior in\n  Cellular Network Specifications",
        "url": "http://arxiv.org/abs/2407.13742v1",
        "pub_date": "2024-07-18",
        "summary": "In recent years, there has been a growing focus on scrutinizing the security\nof cellular networks, often attributing security vulnerabilities to issues in\nthe underlying protocol design descriptions. These protocol design\nspecifications, typically extensive documents that are thousands of pages long,\ncan harbor inaccuracies, underspecifications, implicit assumptions, and\ninternal inconsistencies. In light of the evolving landscape, we introduce\nCellularLint--a semi-automatic framework for inconsistency detection within the\nstandards of 4G and 5G, capitalizing on a suite of natural language processing\ntechniques. Our proposed method uses a revamped few-shot learning mechanism on\ndomain-adapted large language models. Pre-trained on a vast corpus of cellular\nnetwork protocols, this method enables CellularLint to simultaneously detect\ninconsistencies at various levels of semantics and practical use cases. In\ndoing so, CellularLint significantly advances the automated analysis of\nprotocol specifications in a scalable fashion. In our investigation, we focused\non the Non-Access Stratum (NAS) and the security specifications of 4G and 5G\nnetworks, ultimately uncovering 157 inconsistencies with 82.67% accuracy. After\nverification of these inconsistencies on open-source implementations and 17\ncommercial devices, we confirm that they indeed have a substantial impact on\ndesign decisions, potentially leading to concerns related to privacy,\nintegrity, availability, and interoperability.",
        "translated": ""
    },
    {
        "title": "A Comprehensive Review of Recommender Systems: Transitioning from Theory\n  to Practice",
        "url": "http://arxiv.org/abs/2407.13699v1",
        "pub_date": "2024-07-18",
        "summary": "Recommender Systems (RS) play an integral role in enhancing user experiences\nby providing personalized item suggestions. This survey reviews the progress in\nRS inclusively from 2017 to 2024, effectively connecting theoretical advances\nwith practical applications. We explore the development from traditional RS\ntechniques like content-based and collaborative filtering to advanced methods\ninvolving deep learning, graph-based models, reinforcement learning, and large\nlanguage models. We also discuss specialized systems such as context-aware,\nreview-based, and fairness-aware RS. The primary goal of this survey is to\nbridge theory with practice. It addresses challenges across various sectors,\nincluding e-commerce, healthcare, and finance, emphasizing the need for\nscalable, real-time, and trustworthy solutions. Through this survey, we promote\nstronger partnerships between academic research and industry practices. The\ninsights offered by this survey aim to guide industry professionals in\noptimizing RS deployment and to inspire future research directions, especially\nin addressing emerging technological and societal trends",
        "translated": ""
    },
    {
        "title": "The Language of Infographics: Toward Understanding Conceptual Metaphor\n  Use in Scientific Storytelling",
        "url": "http://arxiv.org/abs/2407.13416v1",
        "pub_date": "2024-07-18",
        "summary": "We apply an approach from cognitive linguistics by mapping Conceptual\nMetaphor Theory (CMT) to the visualization domain to address patterns of visual\nconceptual metaphors that are often used in science infographics. Metaphors\nplay an essential part in visual communication and are frequently employed to\nexplain complex concepts. However, their use is often based on intuition,\nrather than following a formal process. At present, we lack tools and language\nfor understanding and describing metaphor use in visualization to the extent\nwhere taxonomy and grammar could guide the creation of visual components, e.g.,\ninfographics. Our classification of the visual conceptual mappings within\nscientific representations is based on the breakdown of visual components in\nexisting scientific infographics. We demonstrate the development of this\nmapping through a detailed analysis of data collected from four domains\n(biomedicine, climate, space, and anthropology) that represent a diverse range\nof visual conceptual metaphors used in the visual communication of science.\nThis work allows us to identify patterns of visual conceptual metaphor use\nwithin the domains, resolve ambiguities about why specific conceptual metaphors\nare used, and develop a better overall understanding of visual metaphor use in\nscientific infographics. Our analysis shows that ontological and orientational\nconceptual metaphors are the most widely applied to translate complex\nscientific concepts. To support our findings we developed a visual exploratory\ntool based on the collected database that places the individual infographics on\na spatio-temporal scale and illustrates the breakdown of visual conceptual\nmetaphors.",
        "translated": ""
    },
    {
        "title": "DCNv3: Towards Next Generation Deep Cross Network for CTR Prediction",
        "url": "http://arxiv.org/abs/2407.13349v1",
        "pub_date": "2024-07-18",
        "summary": "Deep &amp; Cross Network and its derivative models have become an important\nparadigm in click-through rate (CTR) prediction due to their effective balance\nbetween computational cost and performance. However, these models face four\nmajor limitations: (1) while most models claim to capture high-order feature\ninteractions, they often do so implicitly and non-interpretably through deep\nneural networks (DNN), which limits the trustworthiness of the model's\npredictions; (2) the performance of existing explicit feature interaction\nmethods is often weaker than that of implicit DNN, undermining their necessity;\n(3) many models fail to adaptively filter noise while enhancing the order of\nfeature interactions; (4) the fusion methods of most models cannot provide\nsuitable supervision signals for their different interaction methods.\n  To address the identified limitations, this paper proposes the next\ngeneration Deep Cross Network (DCNv3) and Shallow &amp; Deep Cross Network\n(SDCNv3). These models ensure interpretability in feature interaction modeling\nwhile exponentially increasing the order of feature interactions to achieve\ngenuine Deep Crossing rather than just Deep &amp; Cross. Additionally, we employ a\nSelf-Mask operation to filter noise and reduce the number of parameters in the\ncross network by half. In the fusion layer, we use a simple yet effective loss\nweight calculation method called Tri-BCE to provide appropriate supervision\nsignals. Comprehensive experiments on six datasets demonstrate the\neffectiveness, efficiency, and interpretability of DCNv3 and SDCNv3. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://anonymous.4open.science/r/DCNv3-E352.",
        "translated": ""
    },
    {
        "title": "Semantic-aware Representation Learning for Homography Estimation",
        "url": "http://arxiv.org/abs/2407.13284v1",
        "pub_date": "2024-07-18",
        "summary": "Homography estimation is the task of determining the transformation from an\nimage pair. Our approach focuses on employing detector-free feature matching\nmethods to address this issue. Previous work has underscored the importance of\nincorporating semantic information, however there still lacks an efficient way\nto utilize semantic information. Previous methods suffer from treating the\nsemantics as a pre-processing, causing the utilization of semantics overly\ncoarse-grained and lack adaptability when dealing with different tasks. In our\nwork, we seek another way to use the semantic information, that is\nsemantic-aware feature representation learning framework.Based on this, we\npropose SRMatcher, a new detector-free feature matching method, which\nencourages the network to learn integrated semantic feature\nrepresentation.Specifically, to capture precise and rich semantics, we leverage\nthe capabilities of recently popularized vision foundation models (VFMs)\ntrained on extensive datasets. Then, a cross-images Semantic-aware Fusion Block\n(SFB) is proposed to integrate its fine-grained semantic features into the\nfeature representation space. In this way, by reducing errors stemming from\nsemantic inconsistencies in matching pairs, our proposed SRMatcher is able to\ndeliver more accurate and realistic outcomes. Extensive experiments show that\nSRMatcher surpasses solid baselines and attains SOTA results on multiple\nreal-world datasets. Compared to the previous SOTA approach GeoFormer,\nSRMatcher increases the area under the cumulative curve (AUC) by about 11\\% on\nHPatches. Additionally, the SRMatcher could serve as a plug-and-play framework\nfor other matching methods like LoFTR, yielding substantial precision\nimprovement.",
        "translated": ""
    },
    {
        "title": "Aligning Explanations for Recommendation with Rating and Feature via\n  Maximizing Mutual Information",
        "url": "http://arxiv.org/abs/2407.13274v1",
        "pub_date": "2024-07-18",
        "summary": "Providing natural language-based explanations to justify recommendations\nhelps to improve users' satisfaction and gain users' trust. However, as current\nexplanation generation methods are commonly trained with an objective to mimic\nexisting user reviews, the generated explanations are often not aligned with\nthe predicted ratings or some important features of the recommended items, and\nthus, are suboptimal in helping users make informed decision on the\nrecommendation platform. To tackle this problem, we propose a flexible\nmodel-agnostic method named MMI (Maximizing Mutual Information) framework to\nenhance the alignment between the generated natural language explanations and\nthe predicted rating/important item features. Specifically, we propose to use\nmutual information (MI) as a measure for the alignment and train a neural MI\nestimator. Then, we treat a well-trained explanation generation model as the\nbackbone model and further fine-tune it through reinforcement learning with\nguidance from the MI estimator, which rewards a generated explanation that is\nmore aligned with the predicted rating or a pre-defined feature of the\nrecommended item. Experiments on three datasets demonstrate that our MMI\nframework can boost different backbone models, enabling them to outperform\nexisting baselines in terms of alignment with predicted ratings and item\nfeatures. Additionally, user studies verify that MI-enhanced explanations\nindeed facilitate users' decisions and are favorable compared with other\nbaselines due to their better alignment properties.",
        "translated": ""
    },
    {
        "title": "Compressed models are NOT miniature versions of large models",
        "url": "http://arxiv.org/abs/2407.13174v1",
        "pub_date": "2024-07-18",
        "summary": "Large neural models are often compressed before deployment. Model compression\nis necessary for many practical reasons, such as inference latency, memory\nfootprint, and energy consumption. Compressed models are assumed to be\nminiature versions of corresponding large neural models. However, we question\nthis belief in our work. We compare compressed models with corresponding large\nneural models using four model characteristics: prediction errors, data\nrepresentation, data distribution, and vulnerability to adversarial attack. We\nperform experiments using the BERT-large model and its five compressed\nversions. For all four model characteristics, compressed models significantly\ndiffer from the BERT-large model. Even among compressed models, they differ\nfrom each other on all four model characteristics. Apart from the expected loss\nin model performance, there are major side effects of using compressed models\nto replace large neural models.",
        "translated": ""
    },
    {
        "title": "Using LLMs to Investigate Correlations of Conversational Follow-up\n  Queries with User Satisfaction",
        "url": "http://arxiv.org/abs/2407.13166v1",
        "pub_date": "2024-07-18",
        "summary": "With large language models (LLMs), conversational search engines shift how\nusers retrieve information from the web by enabling natural conversations to\nexpress their search intents over multiple turns. Users' natural conversation\nembodies rich but implicit signals of users' search intents and evaluation of\nsearch results to understand user experience with the system. However, it is\nunderexplored how and why users ask follow-up queries to continue conversations\nwith conversational search engines and how the follow-up queries signal users'\nsatisfaction. From qualitative analysis of 250 conversational turns from an\nin-lab user evaluation of Naver Cue:, a commercial conversational search\nengine, we propose a taxonomy of 18 users' follow-up query patterns from\nconversational search, comprising two major axes: (1) users' motivations behind\ncontinuing conversations (N = 7) and (2) actions of follow-up queries (N = 11).\nCompared to the existing literature on query reformulations, we uncovered a new\nset of motivations and actions behind follow-up queries, including asking for\nsubjective opinions or providing natural language feedback on the engine's\nresponses. To analyze conversational search logs with our taxonomy in a\nscalable and efficient manner, we built an LLM-powered classifier (73%\naccuracy). With our classifier, we analyzed 2,061 conversational tuples\ncollected from real-world usage logs of Cue: and examined how the conversation\npatterns from our taxonomy correlates with satisfaction. Our initial findings\nsuggest some signals of dissatisfactions, such as Clarifying Queries, Excluding\nCondition, and Substituting Condition with follow-up queries. We envision our\napproach could contribute to automated evaluation of conversation search\nexperience by providing satisfaction signals and grounds for realistic user\nsimulations.",
        "translated": ""
    },
    {
        "title": "ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2407.13163v1",
        "pub_date": "2024-07-18",
        "summary": "Offline reinforcement learning (RL) is an effective tool for real-world\nrecommender systems with its capacity to model the dynamic interest of users\nand its interactive nature. Most existing offline RL recommender systems focus\non model-based RL through learning a world model from offline data and building\nthe recommendation policy by interacting with this model. Although these\nmethods have made progress in the recommendation performance, the effectiveness\nof model-based offline RL methods is often constrained by the accuracy of the\nestimation of the reward model and the model uncertainties, primarily due to\nthe extreme discrepancy between offline logged data and real-world data in user\ninteractions with online platforms. To fill this gap, a more accurate reward\nmodel and uncertainty estimation are needed for the model-based RL methods. In\nthis paper, a novel model-based Reward Shaping in Offline Reinforcement\nLearning for Recommender Systems, ROLeR, is proposed for reward and uncertainty\nestimation in recommendation systems. Specifically, a non-parametric reward\nshaping method is designed to refine the reward model. In addition, a flexible\nand more representative uncertainty penalty is designed to fit the needs of\nrecommendation systems. Extensive experiments conducted on four benchmark\ndatasets showcase that ROLeR achieves state-of-the-art performance compared\nwith existing baselines. The source code can be downloaded at\nhttps://github.com/ArronDZhang/ROLeR.",
        "translated": ""
    },
    {
        "title": "MLSA4Rec: Mamba Combined with Low-Rank Decomposed Self-Attention for\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2407.13135v1",
        "pub_date": "2024-07-18",
        "summary": "In applications such as e-commerce, online education, and streaming services,\nsequential recommendation systems play a critical role. Despite the excellent\nperformance of self-attention-based sequential recommendation models in\ncapturing dependencies between items in user interaction history, their\nquadratic complexity and lack of structural bias limit their applicability.\nRecently, some works have replaced the self-attention module in sequential\nrecommenders with Mamba, which has linear complexity and structural bias.\nHowever, these works have not noted the complementarity between the two\napproaches. To address this issue, this paper proposes a new hybrid\nrecommendation framework, Mamba combined with Low-Rank decomposed\nSelf-Attention for Sequential Recommendation (MLSA4Rec), whose complexity is\nlinear with respect to the length of the user's historical interaction\nsequence. Specifically, MLSA4Rec designs an efficient Mamba-LSA interaction\nmodule. This module introduces a low-rank decomposed self-attention (LSA)\nmodule with linear complexity and injects structural bias into it through\nMamba. The LSA module analyzes user preferences from a different perspective\nand dynamically guides Mamba to focus on important information in user\nhistorical interactions through a gated information transmission mechanism.\nFinally, MLSA4Rec combines user preference information refined by the Mamba and\nLSA modules to accurately predict the user's next possible interaction. To our\nknowledge, this is the first study to combine Mamba and self-attention in\nsequential recommendation systems. Experimental results show that MLSA4Rec\noutperforms existing self-attention and Mamba-based sequential recommendation\nmodels in recommendation accuracy on three real-world datasets, demonstrating\nthe great potential of Mamba and self-attention working together.",
        "translated": ""
    }
]